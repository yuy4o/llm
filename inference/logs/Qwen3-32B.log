INFO 08-07 12:31:22 [__init__.py:244] Automatically detected platform cuda.
INFO 08-07 12:31:25 [api_server.py:1395] vLLM API server version 0.9.2
INFO 08-07 12:31:25 [cli_args.py:325] non-default args: {'port': 12001, 'api_key': 'empty', 'model': '/data1/llm-models/Qwen3-32B', 'trust_remote_code': True, 'max_model_len': 2048, 'gpu_memory_utilization': 0.8}
INFO 08-07 12:31:31 [config.py:841] This model supports multiple tasks: {'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 08-07 12:31:31 [config.py:1472] Using max model len 2048
INFO 08-07 12:31:31 [api_server.py:268] Started engine process with PID 2379349
INFO 08-07 12:31:34 [__init__.py:244] Automatically detected platform cuda.
INFO 08-07 12:31:37 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2) with config: model='/data1/llm-models/Qwen3-32B', speculative_config=None, tokenizer='/data1/llm-models/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data1/llm-models/Qwen3-32B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":false,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":256,"local_cache_dir":null}, use_cached_outputs=True, 
INFO 08-07 12:31:37 [cuda.py:363] Using Flash Attention backend.
INFO 08-07 12:31:48 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 08-07 12:31:48 [model_runner.py:1171] Starting to load model /data1/llm-models/Qwen3-32B...
INFO 08-07 12:32:03 [default_loader.py:272] Loading weights took 14.38 seconds
INFO 08-07 12:32:03 [model_runner.py:1203] Model loading took 61.0347 GiB and 14.614904 seconds
INFO 08-07 12:32:05 [worker.py:294] Memory profiling takes 1.10 seconds
INFO 08-07 12:32:05 [worker.py:294] the current vLLM instance can use total_gpu_memory (79.14GiB) x gpu_memory_utilization (0.80) = 63.31GiB
INFO 08-07 12:32:05 [worker.py:294] model weights take 61.03GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 0.78GiB.
INFO 08-07 12:32:05 [executor_base.py:113] # cuda blocks: 199, # CPU blocks: 1024
INFO 08-07 12:32:05 [executor_base.py:118] Maximum concurrency for 2048 tokens per request: 1.55x
INFO 08-07 12:32:07 [model_runner.py:1513] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 08-07 12:32:26 [model_runner.py:1671] Graph capturing finished in 19 secs, took 0.46 GiB
INFO 08-07 12:32:26 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 22.63 seconds
WARNING 08-07 12:32:26 [config.py:1392] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
INFO 08-07 12:32:26 [serving_chat.py:125] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
INFO 08-07 12:32:26 [serving_completion.py:72] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
INFO 08-07 12:32:26 [api_server.py:1457] Starting vLLM API server 0 on http://0.0.0.0:12001
INFO 08-07 12:32:26 [launcher.py:29] Available routes are:
INFO 08-07 12:32:26 [launcher.py:37] Route: /openapi.json, Methods: GET, HEAD
INFO 08-07 12:32:26 [launcher.py:37] Route: /docs, Methods: GET, HEAD
INFO 08-07 12:32:26 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 08-07 12:32:26 [launcher.py:37] Route: /redoc, Methods: GET, HEAD
INFO 08-07 12:32:26 [launcher.py:37] Route: /health, Methods: GET
INFO 08-07 12:32:26 [launcher.py:37] Route: /load, Methods: GET
INFO 08-07 12:32:26 [launcher.py:37] Route: /ping, Methods: POST
INFO 08-07 12:32:26 [launcher.py:37] Route: /ping, Methods: GET
INFO 08-07 12:32:26 [launcher.py:37] Route: /tokenize, Methods: POST
INFO 08-07 12:32:26 [launcher.py:37] Route: /detokenize, Methods: POST
INFO 08-07 12:32:26 [launcher.py:37] Route: /v1/models, Methods: GET
INFO 08-07 12:32:26 [launcher.py:37] Route: /version, Methods: GET
INFO 08-07 12:32:26 [launcher.py:37] Route: /v1/chat/completions, Methods: POST
INFO 08-07 12:32:26 [launcher.py:37] Route: /v1/completions, Methods: POST
INFO 08-07 12:32:26 [launcher.py:37] Route: /v1/embeddings, Methods: POST
INFO 08-07 12:32:26 [launcher.py:37] Route: /pooling, Methods: POST
INFO 08-07 12:32:26 [launcher.py:37] Route: /classify, Methods: POST
INFO 08-07 12:32:26 [launcher.py:37] Route: /score, Methods: POST
INFO 08-07 12:32:26 [launcher.py:37] Route: /v1/score, Methods: POST
INFO 08-07 12:32:26 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST
INFO 08-07 12:32:26 [launcher.py:37] Route: /v1/audio/translations, Methods: POST
INFO 08-07 12:32:26 [launcher.py:37] Route: /rerank, Methods: POST
INFO 08-07 12:32:26 [launcher.py:37] Route: /v1/rerank, Methods: POST
INFO 08-07 12:32:26 [launcher.py:37] Route: /v2/rerank, Methods: POST
INFO 08-07 12:32:26 [launcher.py:37] Route: /invocations, Methods: POST
INFO 08-07 12:32:26 [launcher.py:37] Route: /metrics, Methods: GET
INFO:     172.27.221.3:43250 - "GET /v1/models HTTP/1.1" 200 OK
INFO 08-07 12:34:37 [chat_utils.py:444] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
ERROR 08-07 12:34:37 [serving_chat.py:222] Error in preprocessing prompt inputs
ERROR 08-07 12:34:37 [serving_chat.py:222] Traceback (most recent call last):
ERROR 08-07 12:34:37 [serving_chat.py:222]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/serving_chat.py", line 205, in create_chat_completion
ERROR 08-07 12:34:37 [serving_chat.py:222]     ) = await self._preprocess_chat(
ERROR 08-07 12:34:37 [serving_chat.py:222]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/serving_engine.py", line 837, in _preprocess_chat
ERROR 08-07 12:34:37 [serving_chat.py:222]     prompt_inputs = await self._tokenize_prompt_input_async(
ERROR 08-07 12:34:37 [serving_chat.py:222]   File "/usr/lib/python3.10/concurrent/futures/thread.py", line 58, in run
ERROR 08-07 12:34:37 [serving_chat.py:222]     result = self.fn(*self.args, **self.kwargs)
ERROR 08-07 12:34:37 [serving_chat.py:222]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/serving_engine.py", line 592, in _tokenize_prompt_input
ERROR 08-07 12:34:37 [serving_chat.py:222]     return next(
ERROR 08-07 12:34:37 [serving_chat.py:222]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/serving_engine.py", line 616, in _tokenize_prompt_inputs
ERROR 08-07 12:34:37 [serving_chat.py:222]     yield self._normalize_prompt_text_to_input(
ERROR 08-07 12:34:37 [serving_chat.py:222]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/serving_engine.py", line 499, in _normalize_prompt_text_to_input
ERROR 08-07 12:34:37 [serving_chat.py:222]     return self._validate_input(request, input_ids, input_text)
ERROR 08-07 12:34:37 [serving_chat.py:222]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/serving_engine.py", line 569, in _validate_input
ERROR 08-07 12:34:37 [serving_chat.py:222]     raise ValueError(
ERROR 08-07 12:34:37 [serving_chat.py:222] ValueError: This model's maximum context length is 2048 tokens. However, you requested 30009 tokens (9 in the messages, 30000 in the completion). Please reduce the length of the messages or completion.
INFO:     172.27.221.3:43250 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
INFO:     172.27.221.3:43604 - "GET /v1/models HTTP/1.1" 200 OK
INFO 08-07 12:34:47 [logger.py:43] Received request chatcmpl-65f290b2f6eb4bc286a0ee4180034081: prompt: '<|im_start|>user\nhi<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 08-07 12:34:47 [engine.py:317] Added request chatcmpl-65f290b2f6eb4bc286a0ee4180034081.
INFO 08-07 12:34:51 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 16.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%.
INFO:     172.27.221.3:43604 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 08-07 12:35:03 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.3 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
