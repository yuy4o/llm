(python310) jiangyy@yln-a800-8:/data1/jiangyy/gpt-oss-120b$ CUDA_VISIBLE_DEVICES=4,5 vllm serve /data1/jiangyy/gpt-oss-120b --dtype auto --
api-key empty --port 12001 --trust-remote-code --tensor-parallel-size 2 --pipeline-parallel-size 1 --served-model-name gpt --max-model-len 
32768 --gpu_memory_utilization 0.98
INFO 11-10 17:23:13 [__init__.py:216] Automatically detected platform cuda.
(APIServer pid=2619420) INFO 11-10 17:23:17 [api_server.py:1839] vLLM API server version 0.11.0
(APIServer pid=2619420) INFO 11-10 17:23:17 [utils.py:233] non-default args: {'model_tag': '/data1/jiangyy/gpt-oss-120b', 'port': 12001, 'api_key': ['empty'], 'model': '/data1/jiangyy/gpt-oss-120b', 'trust_remote_code': True, 'max_model_len': 32768, 'served_model_name': ['gpt'], 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.98}
(APIServer pid=2619420) The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
(APIServer pid=2619420) INFO 11-10 17:23:17 [model.py:547] Resolved architecture: GptOssForCausalLM
(APIServer pid=2619420) `torch_dtype` is deprecated! Use `dtype` instead!
(APIServer pid=2619420) ERROR 11-10 17:23:17 [config.py:278] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data1/jiangyy/gpt-oss-120b'. Use `repo_type` argument if needed., retrying 1 of 2
(APIServer pid=2619420) ERROR 11-10 17:23:19 [config.py:276] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data1/jiangyy/gpt-oss-120b'. Use `repo_type` argument if needed.
(APIServer pid=2619420) INFO 11-10 17:23:19 [model.py:1730] Downcasting torch.float32 to torch.bfloat16.
(APIServer pid=2619420) INFO 11-10 17:23:19 [model.py:1510] Using max model len 32768
(APIServer pid=2619420) INFO 11-10 17:23:19 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.
(APIServer pid=2619420) INFO 11-10 17:23:19 [config.py:271] Overriding max cuda graph capture size to 992 for performance.
INFO 11-10 17:23:23 [__init__.py:216] Automatically detected platform cuda.
(EngineCore_DP0 pid=2619783) INFO 11-10 17:23:26 [core.py:644] Waiting for init message from front-end.
(EngineCore_DP0 pid=2619783) INFO 11-10 17:23:26 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/data1/jiangyy/gpt-oss-120b', speculative_config=None, tokenizer='/data1/jiangyy/gpt-oss-120b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=mxfp4, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='openai_gptoss'), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=gpt, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[992,976,960,944,928,912,896,880,864,848,832,816,800,784,768,752,736,720,704,688,672,656,640,624,608,592,576,560,544,528,512,496,480,464,448,432,416,400,384,368,352,336,320,304,288,272,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":992,"local_cache_dir":null}
(EngineCore_DP0 pid=2619783) WARNING 11-10 17:23:26 [multiproc_executor.py:720] Reducing Torch parallelism from 68 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
(EngineCore_DP0 pid=2619783) INFO 11-10 17:23:26 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_dce61f67'), local_subscribe_addr='ipc:///tmp/a8b8d82f-f9da-472a-98a4-7f3856600335', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 11-10 17:23:28 [__init__.py:216] Automatically detected platform cuda.
INFO 11-10 17:23:28 [__init__.py:216] Automatically detected platform cuda.
INFO 11-10 17:23:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_fad4b655'), local_subscribe_addr='ipc:///tmp/b2c819a4-871e-4434-be25-b5f519da69e1', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 11-10 17:23:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e998469f'), local_subscribe_addr='ipc:///tmp/21b27d45-57fc-4e81-84fc-e782c7f2d7c2', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 11-10 17:23:34 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 11-10 17:23:34 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 11-10 17:23:34 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 11-10 17:23:34 [pynccl.py:103] vLLM is using nccl==2.27.3
yln-a800-8:2620087:2620087 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
yln-a800-8:2620087:2620087 [0] NCCL INFO Bootstrap: Using eth0:192.168.33.176<0>
yln-a800-8:2620087:2620087 [0] NCCL INFO cudaDriverVersion 13000
yln-a800-8:2620087:2620087 [0] NCCL INFO NCCL version 2.27.3+cuda12.9
yln-a800-8:2620088:2620088 [1] NCCL INFO cudaDriverVersion 13000
yln-a800-8:2620088:2620088 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
yln-a800-8:2620088:2620088 [1] NCCL INFO Bootstrap: Using eth0:192.168.33.176<0>
yln-a800-8:2620088:2620088 [1] NCCL INFO NCCL version 2.27.3+cuda12.9
yln-a800-8:2620087:2620087 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. 
yln-a800-8:2620087:2620087 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
yln-a800-8:2620088:2620088 [1] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. 
yln-a800-8:2620088:2620088 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
yln-a800-8:2620087:2620087 [0] NCCL INFO Failed to open libibverbs.so[.1]
yln-a800-8:2620087:2620087 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
yln-a800-8:2620088:2620088 [1] NCCL INFO Failed to open libibverbs.so[.1]
yln-a800-8:2620088:2620088 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
yln-a800-8:2620087:2620087 [0] NCCL INFO NET/Socket : Using [0]eth0:192.168.33.176<0>
yln-a800-8:2620088:2620088 [1] NCCL INFO NET/Socket : Using [0]eth0:192.168.33.176<0>
yln-a800-8:2620087:2620087 [0] NCCL INFO Initialized NET plugin Socket
yln-a800-8:2620088:2620088 [1] NCCL INFO Initialized NET plugin Socket
yln-a800-8:2620087:2620087 [0] NCCL INFO Assigned NET plugin Socket to comm
yln-a800-8:2620088:2620088 [1] NCCL INFO Assigned NET plugin Socket to comm
yln-a800-8:2620087:2620087 [0] NCCL INFO Using network Socket
yln-a800-8:2620088:2620088 [1] NCCL INFO Using network Socket
yln-a800-8:2620088:2620088 [1] NCCL INFO ncclCommInitRank comm 0x5597dd101240 rank 1 nranks 2 cudaDev 1 nvmlDev 5 busId f0 commId 0xb06c2dd38cd8974e - Init START
yln-a800-8:2620087:2620087 [0] NCCL INFO ncclCommInitRank comm 0x55f516103d90 rank 0 nranks 2 cudaDev 0 nvmlDev 4 busId e0 commId 0xb06c2dd38cd8974e - Init START
yln-a800-8:2620088:2620088 [1] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
yln-a800-8:2620087:2620087 [0] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
yln-a800-8:2620088:2620088 [1] NCCL INFO Bootstrap timings total 0.000579 (create 0.000024, send 0.000106, recv 0.000195, ring 0.000021, delay 0.000000)
yln-a800-8:2620087:2620087 [0] NCCL INFO Bootstrap timings total 0.000568 (create 0.000020, send 0.000093, recv 0.000165, ring 0.000020, delay 0.000000)
yln-a800-8:2620088:2620088 [1] NCCL INFO comm 0x5597dd101240 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0
yln-a800-8:2620087:2620087 [0] NCCL INFO comm 0x55f516103d90 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0
yln-a800-8:2620087:2620087 [0] NCCL INFO Channel 00/02 : 0 1
yln-a800-8:2620087:2620087 [0] NCCL INFO Channel 01/02 : 0 1
yln-a800-8:2620088:2620088 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
yln-a800-8:2620087:2620087 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
yln-a800-8:2620088:2620088 [1] NCCL INFO P2P Chunksize set to 131072
yln-a800-8:2620087:2620087 [0] NCCL INFO P2P Chunksize set to 131072
yln-a800-8:2620088:2620088 [1] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
yln-a800-8:2620087:2620087 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
yln-a800-8:2620087:2620087 [0] NCCL INFO Check P2P Type isAllDirectP2p 0 directMode 0
yln-a800-8:2620087:2620338 [0] NCCL INFO [Proxy Service] Device 0 CPU core 134
yln-a800-8:2620088:2620339 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 110
yln-a800-8:2620087:2620340 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 82
yln-a800-8:2620088:2620337 [1] NCCL INFO [Proxy Service] Device 1 CPU core 124
yln-a800-8:2620088:2620088 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
yln-a800-8:2620088:2620088 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
yln-a800-8:2620087:2620087 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
yln-a800-8:2620087:2620087 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
yln-a800-8:2620087:2620087 [0] NCCL INFO CC Off, workFifoBytes 1048576
yln-a800-8:2620088:2620088 [1] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
yln-a800-8:2620087:2620087 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
yln-a800-8:2620088:2620088 [1] NCCL INFO ncclCommInitRank comm 0x5597dd101240 rank 1 nranks 2 cudaDev 1 nvmlDev 5 busId f0 commId 0xb06c2dd38cd8974e - Init COMPLETE
yln-a800-8:2620087:2620087 [0] NCCL INFO ncclCommInitRank comm 0x55f516103d90 rank 0 nranks 2 cudaDev 0 nvmlDev 4 busId e0 commId 0xb06c2dd38cd8974e - Init COMPLETE
yln-a800-8:2620087:2620087 [0] NCCL INFO Init timings - ncclCommInitRank: rank 0 nranks 2 total 0.12 (kernels 0.11, alloc 0.01, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.00, connections 0.00, rest 0.00)
yln-a800-8:2620088:2620088 [1] NCCL INFO Init timings - ncclCommInitRank: rank 1 nranks 2 total 0.12 (kernels 0.10, alloc 0.01, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.00, connections 0.00, rest 0.00)
yln-a800-8:2620087:2620342 [0] NCCL INFO Channel 00 : 0[4] -> 1[5] via SHM/direct/direct
yln-a800-8:2620088:2620341 [1] NCCL INFO Channel 00 : 1[5] -> 0[4] via SHM/direct/direct
yln-a800-8:2620087:2620342 [0] NCCL INFO Channel 01 : 0[4] -> 1[5] via SHM/direct/direct
yln-a800-8:2620088:2620341 [1] NCCL INFO Channel 01 : 1[5] -> 0[4] via SHM/direct/direct
yln-a800-8:2620087:2620342 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
yln-a800-8:2620088:2620341 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
WARNING 11-10 17:23:34 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
WARNING 11-10 17:23:34 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
INFO 11-10 17:23:34 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
INFO 11-10 17:23:34 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
WARNING 11-10 17:23:34 [custom_all_reduce.py:154] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 11-10 17:23:34 [custom_all_reduce.py:154] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 11-10 17:23:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_1cdf7d0b'), local_subscribe_addr='ipc:///tmp/b558d683-8ce2-411d-8cdf-19219da8b8bc', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 11-10 17:23:34 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 11-10 17:23:34 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 11-10 17:23:34 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 11-10 17:23:34 [pynccl.py:103] vLLM is using nccl==2.27.3
yln-a800-8:2620087:2620087 [0] NCCL INFO Assigned NET plugin Socket to comm
yln-a800-8:2620087:2620087 [0] NCCL INFO Using network Socket
yln-a800-8:2620088:2620088 [1] NCCL INFO Assigned NET plugin Socket to comm
yln-a800-8:2620088:2620088 [1] NCCL INFO Using network Socket
yln-a800-8:2620088:2620088 [1] NCCL INFO ncclCommInitRank comm 0x5597e0c14df0 rank 1 nranks 2 cudaDev 1 nvmlDev 5 busId f0 commId 0xa77748b63f8a9a01 - Init START
yln-a800-8:2620087:2620087 [0] NCCL INFO ncclCommInitRank comm 0x55f519c3eea0 rank 0 nranks 2 cudaDev 0 nvmlDev 4 busId e0 commId 0xa77748b63f8a9a01 - Init START
yln-a800-8:2620088:2620088 [1] NCCL INFO Bootstrap timings total 0.000631 (create 0.000014, send 0.000072, recv 0.000380, ring 0.000023, delay 0.000000)
yln-a800-8:2620087:2620087 [0] NCCL INFO Bootstrap timings total 0.000500 (create 0.000104, send 0.000071, recv 0.000215, ring 0.000016, delay 0.000000)
yln-a800-8:2620087:2620087 [0] NCCL INFO comm 0x55f519c3eea0 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0
yln-a800-8:2620088:2620088 [1] NCCL INFO comm 0x5597e0c14df0 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0
yln-a800-8:2620087:2620087 [0] NCCL INFO Channel 00/02 : 0 1
yln-a800-8:2620087:2620087 [0] NCCL INFO Channel 01/02 : 0 1
yln-a800-8:2620088:2620088 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
yln-a800-8:2620087:2620087 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
yln-a800-8:2620087:2620087 [0] NCCL INFO P2P Chunksize set to 131072
yln-a800-8:2620088:2620088 [1] NCCL INFO P2P Chunksize set to 131072
yln-a800-8:2620087:2620087 [0] NCCL INFO Check P2P Type isAllDirectP2p 0 directMode 0
yln-a800-8:2620087:2620388 [0] NCCL INFO [Proxy Service] Device 0 CPU core 131
yln-a800-8:2620088:2620389 [1] NCCL INFO [Proxy Service] Device 1 CPU core 101
yln-a800-8:2620087:2620391 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 72
yln-a800-8:2620088:2620390 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 73
yln-a800-8:2620088:2620088 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
yln-a800-8:2620087:2620087 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
yln-a800-8:2620088:2620088 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
yln-a800-8:2620087:2620087 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
yln-a800-8:2620087:2620087 [0] NCCL INFO CC Off, workFifoBytes 1048576
yln-a800-8:2620088:2620088 [1] NCCL INFO ncclCommInitRank comm 0x5597e0c14df0 rank 1 nranks 2 cudaDev 1 nvmlDev 5 busId f0 commId 0xa77748b63f8a9a01 - Init COMPLETE
yln-a800-8:2620087:2620087 [0] NCCL INFO ncclCommInitRank comm 0x55f519c3eea0 rank 0 nranks 2 cudaDev 0 nvmlDev 4 busId e0 commId 0xa77748b63f8a9a01 - Init COMPLETE
yln-a800-8:2620087:2620087 [0] NCCL INFO Init timings - ncclCommInitRank: rank 0 nranks 2 total 0.01 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.00, connections 0.00, rest 0.00)
yln-a800-8:2620088:2620088 [1] NCCL INFO Init timings - ncclCommInitRank: rank 1 nranks 2 total 0.01 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.00, connections 0.00, rest 0.00)
yln-a800-8:2620087:2620392 [0] NCCL INFO Channel 00 : 0[4] -> 1[5] via SHM/direct/direct
yln-a800-8:2620088:2620393 [1] NCCL INFO Channel 00 : 1[5] -> 0[4] via SHM/direct/direct
yln-a800-8:2620087:2620392 [0] NCCL INFO Channel 01 : 0[4] -> 1[5] via SHM/direct/direct
yln-a800-8:2620088:2620393 [1] NCCL INFO Channel 01 : 1[5] -> 0[4] via SHM/direct/direct
yln-a800-8:2620088:2620393 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
yln-a800-8:2620087:2620392 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
INFO 11-10 17:23:34 [parallel_state.py:1208] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 11-10 17:23:34 [parallel_state.py:1208] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 11-10 17:23:34 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 11-10 17:23:34 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
(Worker_TP1 pid=2620088) INFO 11-10 17:23:35 [gpu_model_runner.py:2602] Starting to load model /data1/jiangyy/gpt-oss-120b...
(Worker_TP0 pid=2620087) INFO 11-10 17:23:35 [gpu_model_runner.py:2602] Starting to load model /data1/jiangyy/gpt-oss-120b...
(Worker_TP1 pid=2620088) INFO 11-10 17:23:35 [gpu_model_runner.py:2634] Loading model from scratch...
(Worker_TP0 pid=2620087) INFO 11-10 17:23:35 [gpu_model_runner.py:2634] Loading model from scratch...
(Worker_TP1 pid=2620088) INFO 11-10 17:23:35 [cuda.py:361] Using Triton backend on V1 engine.
(Worker_TP1 pid=2620088) INFO 11-10 17:23:35 [mxfp4.py:98] Using Marlin backend
(Worker_TP0 pid=2620087) INFO 11-10 17:23:35 [cuda.py:361] Using Triton backend on V1 engine.
(Worker_TP0 pid=2620087) INFO 11-10 17:23:35 [mxfp4.py:98] Using Marlin backend
Loading safetensors checkpoint shards:   0% Completed | 0/15 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   7% Completed | 1/15 [00:19<04:30, 19.29s/it]
Loading safetensors checkpoint shards:  13% Completed | 2/15 [00:41<04:33, 21.02s/it]
Loading safetensors checkpoint shards:  20% Completed | 3/15 [01:02<04:11, 20.93s/it]
Loading safetensors checkpoint shards:  27% Completed | 4/15 [01:31<04:24, 24.01s/it]
Loading safetensors checkpoint shards:  33% Completed | 5/15 [01:55<04:02, 24.22s/it]
Loading safetensors checkpoint shards:  40% Completed | 6/15 [02:16<03:27, 23.05s/it]
Loading safetensors checkpoint shards:  47% Completed | 7/15 [02:41<03:08, 23.57s/it]
Loading safetensors checkpoint shards:  53% Completed | 8/15 [03:05<02:47, 23.89s/it]
Loading safetensors checkpoint shards:  60% Completed | 9/15 [03:25<02:15, 22.64s/it]

Loading safetensors checkpoint shards:  67% Completed | 10/15 [03:45<01:48, 21.74s/it]
Loading safetensors checkpoint shards:  73% Completed | 11/15 [04:04<01:23, 20.88s/it]
Loading safetensors checkpoint shards:  80% Completed | 12/15 [04:27<01:04, 21.64s/it]
Loading safetensors checkpoint shards:  87% Completed | 13/15 [04:52<00:45, 22.58s/it]
Loading safetensors checkpoint shards:  93% Completed | 14/15 [05:15<00:22, 22.85s/it]
Loading safetensors checkpoint shards: 100% Completed | 15/15 [05:35<00:00, 21.99s/it]
Loading safetensors checkpoint shards: 100% Completed | 15/15 [05:35<00:00, 22.39s/it]
(Worker_TP0 pid=2620087) 
(Worker_TP1 pid=2620088) INFO 11-10 17:29:11 [default_loader.py:267] Loading weights took 335.87 seconds
(Worker_TP1 pid=2620088) WARNING 11-10 17:29:11 [marlin_utils_fp4.py:196] Your GPU does not have native support for FP4 computation but FP4 quantization is being used. Weight-only FP4 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.
(Worker_TP0 pid=2620087) INFO 11-10 17:29:11 [default_loader.py:267] Loading weights took 335.99 seconds
(Worker_TP0 pid=2620087) WARNING 11-10 17:29:11 [marlin_utils_fp4.py:196] Your GPU does not have native support for FP4 computation but FP4 quantization is being used. Weight-only FP4 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.
(Worker_TP1 pid=2620088) INFO 11-10 17:29:14 [gpu_model_runner.py:2653] Model loading took 34.3774 GiB and 338.377658 seconds
(Worker_TP0 pid=2620087) INFO 11-10 17:29:14 [gpu_model_runner.py:2653] Model loading took 34.3774 GiB and 338.452680 seconds
(Worker_TP1 pid=2620088) INFO 11-10 17:29:19 [backends.py:548] Using cache directory: /home/jiangyy/.cache/vllm/torch_compile_cache/1b7d82dbed/rank_1_0/backbone for vLLM's torch.compile
(Worker_TP1 pid=2620088) INFO 11-10 17:29:19 [backends.py:559] Dynamo bytecode transform time: 4.91 s
(Worker_TP0 pid=2620087) INFO 11-10 17:29:19 [backends.py:548] Using cache directory: /home/jiangyy/.cache/vllm/torch_compile_cache/1b7d82dbed/rank_0_0/backbone for vLLM's torch.compile
(Worker_TP0 pid=2620087) INFO 11-10 17:29:19 [backends.py:559] Dynamo bytecode transform time: 4.95 s
(Worker_TP1 pid=2620088) INFO 11-10 17:29:22 [backends.py:197] Cache the graph for dynamic shape for later use
(Worker_TP0 pid=2620087) INFO 11-10 17:29:22 [backends.py:197] Cache the graph for dynamic shape for later use
(Worker_TP0 pid=2620087) INFO 11-10 17:29:56 [backends.py:218] Compiling a graph for dynamic shape takes 36.60 s
(Worker_TP1 pid=2620088) INFO 11-10 17:29:56 [backends.py:218] Compiling a graph for dynamic shape takes 36.84 s
(Worker_TP1 pid=2620088) INFO 11-10 17:29:58 [marlin_utils.py:353] You are running Marlin kernel with bf16 on GPUs before SM90. You can consider change to fp16 to achieve better performance if possible.
(Worker_TP0 pid=2620087) INFO 11-10 17:29:58 [marlin_utils.py:353] You are running Marlin kernel with bf16 on GPUs before SM90. You can consider change to fp16 to achieve better performance if possible.
(Worker_TP0 pid=2620087) INFO 11-10 17:29:59 [monitor.py:34] torch.compile takes 41.55 s in total
(Worker_TP1 pid=2620088) INFO 11-10 17:29:59 [monitor.py:34] torch.compile takes 41.75 s in total
yln-a800-8:2620087:2620087 [0] NCCL INFO Comm config Blocking set to 1
yln-a800-8:2620088:2620088 [1] NCCL INFO Comm config Blocking set to 1
yln-a800-8:2620087:2629646 [0] NCCL INFO Assigned NET plugin Socket to comm
yln-a800-8:2620087:2629646 [0] NCCL INFO Using network Socket
yln-a800-8:2620088:2629647 [1] NCCL INFO Assigned NET plugin Socket to comm
yln-a800-8:2620088:2629647 [1] NCCL INFO Using network Socket
yln-a800-8:2620087:2629646 [0] NCCL INFO ncclCommInitRankConfig comm 0x55f538570380 rank 0 nranks 2 cudaDev 0 nvmlDev 4 busId e0 commId 0xba3cbd4c3921417a - Init START
yln-a800-8:2620088:2629647 [1] NCCL INFO ncclCommInitRankConfig comm 0x5597ff4fa040 rank 1 nranks 2 cudaDev 1 nvmlDev 5 busId f0 commId 0xba3cbd4c3921417a - Init START
yln-a800-8:2620087:2629646 [0] NCCL INFO Bootstrap timings total 0.000446 (create 0.000018, send 0.000094, recv 0.000160, ring 0.000014, delay 0.000000)
yln-a800-8:2620088:2629647 [1] NCCL INFO Bootstrap timings total 0.000363 (create 0.000018, send 0.000093, recv 0.000129, ring 0.000015, delay 0.000000)
yln-a800-8:2620087:2629646 [0] NCCL INFO comm 0x55f538570380 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0
yln-a800-8:2620088:2629647 [1] NCCL INFO comm 0x5597ff4fa040 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0
yln-a800-8:2620087:2629646 [0] NCCL INFO Channel 00/02 : 0 1
yln-a800-8:2620087:2629646 [0] NCCL INFO Channel 01/02 : 0 1
yln-a800-8:2620087:2629646 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
yln-a800-8:2620087:2629646 [0] NCCL INFO P2P Chunksize set to 131072
yln-a800-8:2620087:2629646 [0] NCCL INFO Check P2P Type isAllDirectP2p 0 directMode 0
yln-a800-8:2620088:2629647 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
yln-a800-8:2620088:2629647 [1] NCCL INFO P2P Chunksize set to 131072
yln-a800-8:2620087:2629662 [0] NCCL INFO [Proxy Service] Device 0 CPU core 80
yln-a800-8:2620088:2629663 [1] NCCL INFO [Proxy Service] Device 1 CPU core 86
yln-a800-8:2620087:2629664 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 88
yln-a800-8:2620088:2629665 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 90
yln-a800-8:2620088:2629647 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
yln-a800-8:2620088:2629647 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
yln-a800-8:2620087:2629646 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
yln-a800-8:2620087:2629646 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
yln-a800-8:2620087:2629646 [0] NCCL INFO CC Off, workFifoBytes 1048576
yln-a800-8:2620088:2629647 [1] NCCL INFO ncclCommInitRankConfig comm 0x5597ff4fa040 rank 1 nranks 2 cudaDev 1 nvmlDev 5 busId f0 commId 0xba3cbd4c3921417a - Init COMPLETE
yln-a800-8:2620087:2629646 [0] NCCL INFO ncclCommInitRankConfig comm 0x55f538570380 rank 0 nranks 2 cudaDev 0 nvmlDev 4 busId e0 commId 0xba3cbd4c3921417a - Init COMPLETE
yln-a800-8:2620088:2629647 [1] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 2 total 0.96 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.93, topo 0.01, graphs 0.00, connections 0.01, rest 0.00)
yln-a800-8:2620087:2629646 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 2 total 0.96 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.93, topo 0.01, graphs 0.00, connections 0.01, rest 0.00)
yln-a800-8:2620087:2629667 [0] NCCL INFO Channel 00 : 0[4] -> 1[5] via SHM/direct/direct
yln-a800-8:2620088:2629666 [1] NCCL INFO Channel 00 : 1[5] -> 0[4] via SHM/direct/direct
yln-a800-8:2620087:2629667 [0] NCCL INFO Channel 01 : 0[4] -> 1[5] via SHM/direct/direct
yln-a800-8:2620088:2629666 [1] NCCL INFO Channel 01 : 1[5] -> 0[4] via SHM/direct/direct
yln-a800-8:2620088:2629666 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
yln-a800-8:2620087:2629667 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
(Worker_TP0 pid=2620087) INFO 11-10 17:30:02 [gpu_worker.py:298] Available KV cache memory: 35.88 GiB
(Worker_TP1 pid=2620088) INFO 11-10 17:30:02 [gpu_worker.py:298] Available KV cache memory: 35.88 GiB
(EngineCore_DP0 pid=2619783) INFO 11-10 17:30:02 [kv_cache_utils.py:1087] GPU KV cache size: 1,045,072 tokens
(EngineCore_DP0 pid=2619783) INFO 11-10 17:30:02 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 50.85x
(EngineCore_DP0 pid=2619783) INFO 11-10 17:30:02 [kv_cache_utils.py:1087] GPU KV cache size: 1,045,072 tokens
(EngineCore_DP0 pid=2619783) INFO 11-10 17:30:02 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 50.85x
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|███████████████████████| 81/81 [00:24<00:00,  3.36it/s]
Capturing CUDA graphs (decode, FULL):  99%|█████████████████████████████████████████▍| 80/81 [00:27<00:00,  1.96it/s](Worker_TP1 pid=2620088) INFO 11-10 17:30:59 [gpu_model_runner.py:3480] Graph capturing finished in 54 secs, took 2.19 GiB
Capturing CUDA graphs (decode, FULL): 100%|██████████████████████████████████████████| 81/81 [00:30<00:00,  2.68it/s]
(Worker_TP0 pid=2620087) INFO 11-10 17:30:59 [gpu_model_runner.py:3480] Graph capturing finished in 55 secs, took 2.19 GiB
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671] WorkerProc hit an exception.
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671] Traceback (most recent call last):
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3217, in _dummy_sampler_run
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     sampler_output = self.sampler(logits=logits,
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     return self._call_impl(*args, **kwargs)
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     return forward_call(*args, **kwargs)
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/sample/sampler.py", line 100, in forward
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/sample/sampler.py", line 180, in sample
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     random_sampled, processed_logprobs = self.topk_topp_sampler(
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     return self._call_impl(*args, **kwargs)
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     return forward_call(*args, **kwargs)
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 90, in forward_native
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     logits = self.apply_top_k_top_p(logits, k, p)
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 183, in apply_top_k_top_p
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.30 GiB. GPU 1 has a total capacity of 79.25 GiB of which 359.94 MiB is free. Including non-PyTorch memory, this process has 78.89 GiB memory in use. Of the allocated memory 75.48 GiB is allocated by PyTorch, with 119.88 MiB allocated in private pools (e.g., CUDA Graphs), and 725.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671] 
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671] The above exception was the direct cause of the following exception:
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671] 
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671] Traceback (most recent call last):
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 666, in worker_busy_loop
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     output = func(*args, **kwargs)
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 413, in compile_or_warm_up_model
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     self.model_runner._dummy_sampler_run(
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     return func(*args, **kwargs)
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3221, in _dummy_sampler_run
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     raise RuntimeError(
(Worker_TP1 pid=2620088) ERROR 11-10 17:30:59 [multiproc_executor.py:671] RuntimeError: CUDA out of memory occurred when warming up sampler with 1024 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671] WorkerProc hit an exception.
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671] Traceback (most recent call last):
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3217, in _dummy_sampler_run
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     sampler_output = self.sampler(logits=logits,
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     return self._call_impl(*args, **kwargs)
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     return forward_call(*args, **kwargs)
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/sample/sampler.py", line 100, in forward
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/sample/sampler.py", line 180, in sample
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     random_sampled, processed_logprobs = self.topk_topp_sampler(
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     return self._call_impl(*args, **kwargs)
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     return forward_call(*args, **kwargs)
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 90, in forward_native
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     logits = self.apply_top_k_top_p(logits, k, p)
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 183, in apply_top_k_top_p
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.30 GiB. GPU 0 has a total capacity of 79.25 GiB of which 359.94 MiB is free. Including non-PyTorch memory, this process has 78.89 GiB memory in use. Of the allocated memory 75.48 GiB is allocated by PyTorch, with 119.88 MiB allocated in private pools (e.g., CUDA Graphs), and 725.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671] 
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671] The above exception was the direct cause of the following exception:
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671] 
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671] Traceback (most recent call last):
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 666, in worker_busy_loop
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     output = func(*args, **kwargs)
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 413, in compile_or_warm_up_model
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     self.model_runner._dummy_sampler_run(
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     return func(*args, **kwargs)
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3221, in _dummy_sampler_run
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671]     raise RuntimeError(
(Worker_TP0 pid=2620087) ERROR 11-10 17:30:59 [multiproc_executor.py:671] RuntimeError: CUDA out of memory occurred when warming up sampler with 1024 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
(EngineCore_DP0 pid=2619783) ERROR 11-10 17:30:59 [core.py:708] EngineCore failed to start.
(EngineCore_DP0 pid=2619783) ERROR 11-10 17:30:59 [core.py:708] Traceback (most recent call last):
(EngineCore_DP0 pid=2619783) ERROR 11-10 17:30:59 [core.py:708]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 699, in run_engine_core
(EngineCore_DP0 pid=2619783) ERROR 11-10 17:30:59 [core.py:708]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=2619783) ERROR 11-10 17:30:59 [core.py:708]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 498, in __init__
(EngineCore_DP0 pid=2619783) ERROR 11-10 17:30:59 [core.py:708]     super().__init__(vllm_config, executor_class, log_stats,
(EngineCore_DP0 pid=2619783) ERROR 11-10 17:30:59 [core.py:708]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 92, in __init__
(EngineCore_DP0 pid=2619783) ERROR 11-10 17:30:59 [core.py:708]     self._initialize_kv_caches(vllm_config)
(EngineCore_DP0 pid=2619783) ERROR 11-10 17:30:59 [core.py:708]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 207, in _initialize_kv_caches
(EngineCore_DP0 pid=2619783) ERROR 11-10 17:30:59 [core.py:708]     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=2619783) ERROR 11-10 17:30:59 [core.py:708]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 75, in initialize_from_config
(EngineCore_DP0 pid=2619783) ERROR 11-10 17:30:59 [core.py:708]     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=2619783) ERROR 11-10 17:30:59 [core.py:708]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 264, in collective_rpc
(EngineCore_DP0 pid=2619783) ERROR 11-10 17:30:59 [core.py:708]     result = get_response(w, dequeue_timeout,
(EngineCore_DP0 pid=2619783) ERROR 11-10 17:30:59 [core.py:708]   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 248, in get_response
(EngineCore_DP0 pid=2619783) ERROR 11-10 17:30:59 [core.py:708]     raise RuntimeError(
(EngineCore_DP0 pid=2619783) ERROR 11-10 17:30:59 [core.py:708] RuntimeError: Worker failed with error 'CUDA out of memory occurred when warming up sampler with 1024 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.', please check the stack trace above for the root cause
yln-a800-8:2620087:2620087 [0] NCCL INFO comm 0x55f538570380 rank 0 nranks 2 cudaDev 0 busId e0 - Destroy COMPLETE
yln-a800-8:2620088:2620088 [1] NCCL INFO comm 0x5597ff4fa040 rank 1 nranks 2 cudaDev 1 busId f0 - Destroy COMPLETE
yln-a800-8:2620087:2620388 [0] NCCL INFO [Service thread] Connection closed by localRank 1
yln-a800-8:2620087:2620338 [0] NCCL INFO [Service thread] Connection closed by localRank 1
(EngineCore_DP0 pid=2619783) ERROR 11-10 17:31:02 [multiproc_executor.py:154] Worker proc VllmWorker-1 died unexpectedly, shutting down executor.
(EngineCore_DP0 pid=2619783) Process EngineCore_DP0:
(EngineCore_DP0 pid=2619783) Traceback (most recent call last):
(EngineCore_DP0 pid=2619783)   File "/usr/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=2619783)     self.run()
(EngineCore_DP0 pid=2619783)   File "/usr/lib/python3.10/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=2619783)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=2619783)   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 712, in run_engine_core
(EngineCore_DP0 pid=2619783)     raise e
(EngineCore_DP0 pid=2619783)   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 699, in run_engine_core
(EngineCore_DP0 pid=2619783)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=2619783)   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 498, in __init__
(EngineCore_DP0 pid=2619783)     super().__init__(vllm_config, executor_class, log_stats,
(EngineCore_DP0 pid=2619783)   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 92, in __init__
(EngineCore_DP0 pid=2619783)     self._initialize_kv_caches(vllm_config)
(EngineCore_DP0 pid=2619783)   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 207, in _initialize_kv_caches
(EngineCore_DP0 pid=2619783)     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=2619783)   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 75, in initialize_from_config
(EngineCore_DP0 pid=2619783)     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=2619783)   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 264, in collective_rpc
(EngineCore_DP0 pid=2619783)     result = get_response(w, dequeue_timeout,
(EngineCore_DP0 pid=2619783)   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 248, in get_response
(EngineCore_DP0 pid=2619783)     raise RuntimeError(
(EngineCore_DP0 pid=2619783) RuntimeError: Worker failed with error 'CUDA out of memory occurred when warming up sampler with 1024 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.', please check the stack trace above for the root cause
(APIServer pid=2619420) Traceback (most recent call last):
(APIServer pid=2619420)   File "/data1/jiangyy/uvs/python310/.venv/bin/vllm", line 10, in <module>
(APIServer pid=2619420)     sys.exit(main())
(APIServer pid=2619420)   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/entrypoints/cli/main.py", line 54, in main
(APIServer pid=2619420)     args.dispatch_function(args)
(APIServer pid=2619420)   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/entrypoints/cli/serve.py", line 57, in cmd
(APIServer pid=2619420)     uvloop.run(run_server(args))
(APIServer pid=2619420)   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/uvloop/__init__.py", line 69, in run
(APIServer pid=2619420)     return loop.run_until_complete(wrapper())
(APIServer pid=2619420)   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
(APIServer pid=2619420)   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/uvloop/__init__.py", line 48, in wrapper
(APIServer pid=2619420)     return await main
(APIServer pid=2619420)   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 1884, in run_server
(APIServer pid=2619420)     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
(APIServer pid=2619420)   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 1902, in run_server_worker
(APIServer pid=2619420)     async with build_async_engine_client(
(APIServer pid=2619420)   File "/usr/lib/python3.10/contextlib.py", line 199, in __aenter__
(APIServer pid=2619420)     return await anext(self.gen)
(APIServer pid=2619420)   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 180, in build_async_engine_client
(APIServer pid=2619420)     async with build_async_engine_client_from_engine_args(
(APIServer pid=2619420)   File "/usr/lib/python3.10/contextlib.py", line 199, in __aenter__
(APIServer pid=2619420)     return await anext(self.gen)
(APIServer pid=2619420)   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 225, in build_async_engine_client_from_engine_args
(APIServer pid=2619420)     async_llm = AsyncLLM.from_vllm_config(
(APIServer pid=2619420)   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/utils/__init__.py", line 1572, in inner
(APIServer pid=2619420)     return fn(*args, **kwargs)
(APIServer pid=2619420)   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py", line 207, in from_vllm_config
(APIServer pid=2619420)     return cls(
(APIServer pid=2619420)   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py", line 134, in __init__
(APIServer pid=2619420)     self.engine_core = EngineCoreClient.make_async_mp_client(
(APIServer pid=2619420)   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 102, in make_async_mp_client
(APIServer pid=2619420)     return AsyncMPClient(*client_args)
(APIServer pid=2619420)   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 769, in __init__
(APIServer pid=2619420)     super().__init__(
(APIServer pid=2619420)   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 448, in __init__
(APIServer pid=2619420)     with launch_core_engines(vllm_config, executor_class,
(APIServer pid=2619420)   File "/usr/lib/python3.10/contextlib.py", line 142, in __exit__
(APIServer pid=2619420)     next(self.gen)
(APIServer pid=2619420)   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 732, in launch_core_engines
(APIServer pid=2619420)     wait_for_engine_startup(
(APIServer pid=2619420)   File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 785, in wait_for_engine_startup
(APIServer pid=2619420)     raise RuntimeError("Engine core initialization failed. "
(APIServer pid=2619420) RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}