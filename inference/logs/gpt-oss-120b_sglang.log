(python310) jiangyy@yln-a800-8:/data1/jiangyy/gpt-oss-120b$ CUDA_VISIBLE_DEVICES=4,5 python -m sglang.launch_server --model /data1/jiangyy/gpt-oss-120b --host 0.0.0.0 --port 12001 --tp 2 --max-running-requests 4 --mem-fraction-static 1 --context-length 512
INFO 11-10 18:20:17 [__init__.py:216] Automatically detected platform cuda.
[2025-11-10 18:20:17] WARNING server_args.py:989: Detected GPT-OSS model, enabling triton_kernels MOE kernel.
[2025-11-10 18:20:17] INFO model_config.py:876: Downcasting torch.float32 to torch.bfloat16.
[2025-11-10 18:20:17] WARNING model_config.py:715: mxfp4 quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-10 18:20:17] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-10 18:20:17] server_args=ServerArgs(model_path='/data1/jiangyy/gpt-oss-120b', tokenizer_path='/data1/jiangyy/gpt-oss-120b', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=512, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=12001, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='bfloat16', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=1.0, max_running_requests=4, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=True, radix_eviction_policy='lru', device='cuda', tp_size=2, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=164013973, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/data1/jiangyy/gpt-oss-120b', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='triton', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_moe_runner_backend=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='triton_kernel', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=256, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, decrypted_config_file=None, decrypted_draft_config_file=None)
[2025-11-10 18:20:17] Downcasting torch.float32 to torch.bfloat16.
[2025-11-10 18:20:17] mxfp4 quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-10 18:20:18] Using default HuggingFace chat template with detected content format: string
INFO 11-10 18:20:23 [__init__.py:216] Automatically detected platform cuda.
INFO 11-10 18:20:23 [__init__.py:216] Automatically detected platform cuda.
INFO 11-10 18:20:23 [__init__.py:216] Automatically detected platform cuda.
[2025-11-10 18:20:24] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-10 18:20:24] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-10 18:20:24] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-10 18:20:24 TP0] Downcasting torch.float32 to torch.bfloat16.
[2025-11-10 18:20:24 TP0] mxfp4 quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-10 18:20:24 TP1] Downcasting torch.float32 to torch.bfloat16.
[2025-11-10 18:20:24 TP1] mxfp4 quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-10 18:20:24 TP0] Downcasting torch.float32 to torch.bfloat16.
[2025-11-10 18:20:24 TP0] mxfp4 quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-10 18:20:24 TP0] Init torch distributed begin.
[2025-11-10 18:20:25 TP1] Downcasting torch.float32 to torch.bfloat16.
[2025-11-10 18:20:25 TP1] mxfp4 quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-11-10 18:20:25 TP1] Init torch distributed begin.
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[2025-11-10 18:20:25 TP0] sglang is using nccl==2.27.3
yln-a800-8:2708914:2708914 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
yln-a800-8:2708914:2708914 [0] NCCL INFO Bootstrap: Using eth0:192.168.33.176<0>
yln-a800-8:2708914:2708914 [0] NCCL INFO cudaDriverVersion 13000
yln-a800-8:2708914:2708914 [0] NCCL INFO NCCL version 2.27.3+cuda12.9
yln-a800-8:2708915:2708915 [1] NCCL INFO cudaDriverVersion 13000
yln-a800-8:2708915:2708915 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
yln-a800-8:2708915:2708915 [1] NCCL INFO Bootstrap: Using eth0:192.168.33.176<0>
yln-a800-8:2708915:2708915 [1] NCCL INFO NCCL version 2.27.3+cuda12.9
yln-a800-8:2708914:2708914 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. 
yln-a800-8:2708914:2708914 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
yln-a800-8:2708914:2708914 [0] NCCL INFO Failed to open libibverbs.so[.1]
yln-a800-8:2708914:2708914 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
yln-a800-8:2708914:2708914 [0] NCCL INFO NET/Socket : Using [0]eth0:192.168.33.176<0>
yln-a800-8:2708914:2708914 [0] NCCL INFO Initialized NET plugin Socket
yln-a800-8:2708914:2708914 [0] NCCL INFO Assigned NET plugin Socket to comm
yln-a800-8:2708914:2708914 [0] NCCL INFO Using network Socket
yln-a800-8:2708915:2708915 [1] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. 
yln-a800-8:2708915:2708915 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
yln-a800-8:2708915:2708915 [1] NCCL INFO Failed to open libibverbs.so[.1]
yln-a800-8:2708915:2708915 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
yln-a800-8:2708915:2708915 [1] NCCL INFO NET/Socket : Using [0]eth0:192.168.33.176<0>
yln-a800-8:2708915:2708915 [1] NCCL INFO Initialized NET plugin Socket
yln-a800-8:2708915:2708915 [1] NCCL INFO Assigned NET plugin Socket to comm
yln-a800-8:2708915:2708915 [1] NCCL INFO Using network Socket
yln-a800-8:2708914:2708914 [0] NCCL INFO ncclCommInitRank comm 0x555e35fb4d80 rank 0 nranks 2 cudaDev 0 nvmlDev 4 busId e0 commId 0x3bf3ed2eb0be1560 - Init START
yln-a800-8:2708915:2708915 [1] NCCL INFO ncclCommInitRank comm 0x55e46511dbb0 rank 1 nranks 2 cudaDev 1 nvmlDev 5 busId f0 commId 0x3bf3ed2eb0be1560 - Init START
yln-a800-8:2708915:2708915 [1] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
yln-a800-8:2708914:2708914 [0] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
yln-a800-8:2708914:2708914 [0] NCCL INFO Bootstrap timings total 0.003894 (create 0.000027, send 0.000107, recv 0.003458, ring 0.000024, delay 0.000001)
yln-a800-8:2708915:2708915 [1] NCCL INFO Bootstrap timings total 0.001479 (create 0.000018, send 0.000077, recv 0.001107, ring 0.000023, delay 0.000000)
yln-a800-8:2708915:2708915 [1] NCCL INFO NCCL_CUMEM_ENABLE set by environment to 0.
yln-a800-8:2708914:2708914 [0] NCCL INFO NCCL_CUMEM_ENABLE set by environment to 0.
yln-a800-8:2708914:2708914 [0] NCCL INFO NCCL_NVLS_ENABLE set by environment to 0.
yln-a800-8:2708915:2708915 [1] NCCL INFO NCCL_NVLS_ENABLE set by environment to 0.
yln-a800-8:2708915:2708915 [1] NCCL INFO comm 0x55e46511dbb0 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0
yln-a800-8:2708914:2708914 [0] NCCL INFO comm 0x555e35fb4d80 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0
yln-a800-8:2708915:2708915 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
yln-a800-8:2708915:2708915 [1] NCCL INFO P2P Chunksize set to 131072
yln-a800-8:2708914:2708914 [0] NCCL INFO Channel 00/02 : 0 1
yln-a800-8:2708914:2708914 [0] NCCL INFO Channel 01/02 : 0 1
yln-a800-8:2708914:2708914 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
yln-a800-8:2708914:2708914 [0] NCCL INFO P2P Chunksize set to 131072
yln-a800-8:2708915:2708915 [1] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
yln-a800-8:2708914:2708914 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
yln-a800-8:2708914:2708914 [0] NCCL INFO Check P2P Type isAllDirectP2p 0 directMode 0
yln-a800-8:2708915:2709583 [1] NCCL INFO [Proxy Service] Device 1 CPU core 87
yln-a800-8:2708915:2709585 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 84
yln-a800-8:2708914:2709586 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 72
yln-a800-8:2708914:2709584 [0] NCCL INFO [Proxy Service] Device 0 CPU core 84
yln-a800-8:2708914:2708914 [0] NCCL INFO Channel 00 : 0[4] -> 1[5] via SHM/direct/direct
yln-a800-8:2708914:2708914 [0] NCCL INFO Channel 01 : 0[4] -> 1[5] via SHM/direct/direct
yln-a800-8:2708915:2708915 [1] NCCL INFO Channel 00 : 1[5] -> 0[4] via SHM/direct/direct
yln-a800-8:2708915:2708915 [1] NCCL INFO Channel 01 : 1[5] -> 0[4] via SHM/direct/direct
yln-a800-8:2708914:2708914 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
yln-a800-8:2708915:2708915 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
yln-a800-8:2708914:2708914 [0] NCCL INFO Connected all trees
yln-a800-8:2708915:2708915 [1] NCCL INFO Connected all trees
yln-a800-8:2708914:2709587 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 106
yln-a800-8:2708915:2709588 [1] NCCL INFO [Proxy Progress] Device 1 CPU core 130
yln-a800-8:2708915:2708915 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
yln-a800-8:2708915:2708915 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
yln-a800-8:2708914:2708914 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
yln-a800-8:2708914:2708914 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
yln-a800-8:2708914:2708914 [0] NCCL INFO CC Off, workFifoBytes 1048576
yln-a800-8:2708914:2708914 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
yln-a800-8:2708915:2708915 [1] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
yln-a800-8:2708914:2708914 [0] NCCL INFO ncclCommInitRank comm 0x555e35fb4d80 rank 0 nranks 2 cudaDev 0 nvmlDev 4 busId e0 commId 0x3bf3ed2eb0be1560 - Init COMPLETE
yln-a800-8:2708915:2708915 [1] NCCL INFO ncclCommInitRank comm 0x55e46511dbb0 rank 1 nranks 2 cudaDev 1 nvmlDev 5 busId f0 commId 0x3bf3ed2eb0be1560 - Init COMPLETE
yln-a800-8:2708914:2708914 [0] NCCL INFO Init timings - ncclCommInitRank: rank 0 nranks 2 total 0.17 (kernels 0.08, alloc 0.01, bootstrap 0.00, allgathers 0.00, topo 0.00, graphs 0.00, connections 0.07, rest 0.00)
yln-a800-8:2708915:2708915 [1] NCCL INFO Init timings - ncclCommInitRank: rank 1 nranks 2 total 0.17 (kernels 0.08, alloc 0.01, bootstrap 0.00, allgathers 0.00, topo 0.00, graphs 0.00, connections 0.07, rest 0.00)
[2025-11-10 18:20:25 TP1] Setup Custom allreduce failed with CUDART error: peer access is not supported between these two devices. To silence this warning, specify --disable-custom-all-reduce explicitly.
[2025-11-10 18:20:25 TP0] Setup Custom allreduce failed with CUDART error: peer access is not supported between these two devices. To silence this warning, specify --disable-custom-all-reduce explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[2025-11-10 18:20:25 TP0] Init torch distributed ends. mem usage=0.20 GB
[2025-11-10 18:20:25 TP1] Init torch distributed ends. mem usage=0.20 GB
[2025-11-10 18:20:26 TP1] Load weight begin. avail mem=78.61 GB
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-11-10 18:20:26 TP0] Load weight begin. avail mem=78.61 GB
`torch_dtype` is deprecated! Use `dtype` instead!
Loading safetensors checkpoint shards:   0% Completed | 0/15 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 15/15 [00:00<00:00, 932.41it/s]

[2025-11-10 18:20:38 TP0] Load weight end. type=GptOssForCausalLM, dtype=torch.bfloat16, avail mem=47.98 GB, mem usage=30.64 GB.
[2025-11-10 18:20:38 TP1] Load weight end. type=GptOssForCausalLM, dtype=torch.bfloat16, avail mem=47.98 GB, mem usage=30.64 GB.
[2025-11-10 18:20:38 TP0] Using KV cache dtype: torch.bfloat16
[2025-11-10 18:20:38 TP0] Scheduler hit an exception: Traceback (most recent call last):
  File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py", line 2802, in run_scheduler_process
    scheduler = Scheduler(
  File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py", line 311, in __init__
    self.tp_worker = TpModelWorker(
  File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py", line 237, in __init__
    self._model_runner = ModelRunner(
  File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py", line 322, in __init__
    self.initialize(min_per_gpu_memory)
  File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py", line 471, in initialize
    self.init_memory_pool(
  File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py", line 1806, in init_memory_pool
    self.token_to_kv_pool = MHATokenToKVPool(
  File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/sglang/srt/mem_cache/memory_pool.py", line 584, in __init__
    self._create_buffers()
  File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/sglang/srt/mem_cache/memory_pool.py", line 658, in _create_buffers
    self.v_buffer = [
  File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/sglang/srt/mem_cache/memory_pool.py", line 659, in <listcomp>
    torch.zeros(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 684.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 563.12 MiB is free. Including non-PyTorch memory, this process has 78.69 GiB memory in use. Of the allocated memory 77.91 GiB is allocated by PyTorch, and 177.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2025-11-10 18:20:38] Received sigquit from a child process. It usually means the child failed.
[2025-11-10 18:20:38 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py", line 2802, in run_scheduler_process
    scheduler = Scheduler(
  File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py", line 311, in __init__
    self.tp_worker = TpModelWorker(
  File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py", line 237, in __init__
    self._model_runner = ModelRunner(
  File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py", line 322, in __init__
    self.initialize(min_per_gpu_memory)
  File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py", line 471, in initialize
    self.init_memory_pool(
  File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py", line 1806, in init_memory_pool
    self.token_to_kv_pool = MHATokenToKVPool(
  File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/sglang/srt/mem_cache/memory_pool.py", line 584, in __init__
    self._create_buffers()
  File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/sglang/srt/mem_cache/memory_pool.py", line 658, in _create_buffers
    self.v_buffer = [
  File "/data1/jiangyy/uvs/python310/.venv/lib/python3.10/site-packages/sglang/srt/mem_cache/memory_pool.py", line 659, in <listcomp>
    torch.zeros(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 684.00 MiB. GPU 1 has a total capacity of 79.25 GiB of which 563.12 MiB is free. Including non-PyTorch memory, this process has 78.69 GiB memory in use. Of the allocated memory 77.91 GiB is allocated by PyTorch, and 177.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2025-11-10 18:20:38] Received sigquit from a child process. It usually means the child failed.
Killed