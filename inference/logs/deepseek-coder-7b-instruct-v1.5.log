INFO 08-06 23:27:09 [__init__.py:243] Automatically detected platform cuda.
INFO 08-06 23:27:13 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 08-06 23:27:13 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 08-06 23:27:13 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 08-06 23:27:15 [api_server.py:1289] vLLM API server version 0.9.0.1
INFO 08-06 23:27:16 [cli_args.py:300] non-default args: {'port': 12000, 'api_key': 'empty', 'trust_remote_code': True, 'gpu_memory_utilization': 0.97}
INFO 08-06 23:27:27 [config.py:793] This model supports multiple tasks: {'reward', 'generate', 'classify', 'embed', 'score'}. Defaulting to 'generate'.
INFO 08-06 23:27:27 [api_server.py:257] Started engine process with PID 3670637
INFO 08-06 23:27:32 [__init__.py:243] Automatically detected platform cuda.
INFO 08-06 23:27:36 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 08-06 23:27:36 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 08-06 23:27:36 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 08-06 23:27:36 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.0.1) with config: model='/data/jiangyy/models/deepseek-coder-7b-instruct-v1.5', speculative_config=None, tokenizer='/data/jiangyy/models/deepseek-coder-7b-instruct-v1.5', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/jiangyy/models/deepseek-coder-7b-instruct-v1.5, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={"compile_sizes": [], "inductor_compile_config": {"enable_auto_functionalized_v2": false}, "cudagraph_capture_sizes": [256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], "max_capture_size": 256}, use_cached_outputs=True, 
INFO 08-06 23:27:36 [cuda.py:292] Using Flash Attention backend.
INFO 08-06 23:27:57 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 08-06 23:27:57 [model_runner.py:1170] Starting to load model /data/jiangyy/models/deepseek-coder-7b-instruct-v1.5...
INFO 08-06 23:29:05 [default_loader.py:280] Loading weights took 68.31 seconds
INFO 08-06 23:29:06 [model_runner.py:1202] Model loading took 12.8726 GiB and 68.488299 seconds
INFO 08-06 23:29:07 [worker.py:291] Memory profiling takes 0.86 seconds
INFO 08-06 23:29:07 [worker.py:291] the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.97) = 38.20GiB
INFO 08-06 23:29:07 [worker.py:291] model weights take 12.87GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 0.97GiB; the rest of the memory reserved for KV Cache is 24.27GiB.
INFO 08-06 23:29:07 [executor_base.py:112] # cuda blocks: 3313, # CPU blocks: 546
INFO 08-06 23:29:07 [executor_base.py:117] Maximum concurrency for 4096 tokens per request: 12.94x
INFO 08-06 23:29:11 [model_runner.py:1512] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 08-06 23:29:30 [model_runner.py:1670] Graph capturing finished in 19 secs, took 0.22 GiB
INFO 08-06 23:29:30 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 24.27 seconds
INFO 08-06 23:29:30 [api_server.py:1336] Starting vLLM API server on http://0.0.0.0:12000
INFO 08-06 23:29:30 [launcher.py:28] Available routes are:
INFO 08-06 23:29:30 [launcher.py:36] Route: /openapi.json, Methods: GET, HEAD
INFO 08-06 23:29:30 [launcher.py:36] Route: /docs, Methods: GET, HEAD
INFO 08-06 23:29:30 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 08-06 23:29:30 [launcher.py:36] Route: /redoc, Methods: GET, HEAD
INFO 08-06 23:29:30 [launcher.py:36] Route: /health, Methods: GET
INFO 08-06 23:29:30 [launcher.py:36] Route: /load, Methods: GET
INFO 08-06 23:29:30 [launcher.py:36] Route: /ping, Methods: POST
INFO 08-06 23:29:30 [launcher.py:36] Route: /ping, Methods: GET
INFO 08-06 23:29:30 [launcher.py:36] Route: /tokenize, Methods: POST
INFO 08-06 23:29:30 [launcher.py:36] Route: /detokenize, Methods: POST
INFO 08-06 23:29:30 [launcher.py:36] Route: /v1/models, Methods: GET
INFO 08-06 23:29:30 [launcher.py:36] Route: /version, Methods: GET
INFO 08-06 23:29:30 [launcher.py:36] Route: /v1/chat/completions, Methods: POST
INFO 08-06 23:29:30 [launcher.py:36] Route: /v1/completions, Methods: POST
INFO 08-06 23:29:30 [launcher.py:36] Route: /v1/embeddings, Methods: POST
INFO 08-06 23:29:30 [launcher.py:36] Route: /pooling, Methods: POST
INFO 08-06 23:29:30 [launcher.py:36] Route: /classify, Methods: POST
INFO 08-06 23:29:30 [launcher.py:36] Route: /score, Methods: POST
INFO 08-06 23:29:30 [launcher.py:36] Route: /v1/score, Methods: POST
INFO 08-06 23:29:30 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST
INFO 08-06 23:29:30 [launcher.py:36] Route: /rerank, Methods: POST
INFO 08-06 23:29:30 [launcher.py:36] Route: /v1/rerank, Methods: POST
INFO 08-06 23:29:30 [launcher.py:36] Route: /v2/rerank, Methods: POST
INFO 08-06 23:29:30 [launcher.py:36] Route: /invocations, Methods: POST
INFO 08-06 23:29:30 [launcher.py:36] Route: /metrics, Methods: GET
INFO:     172.27.221.3:43694 - "GET /v1/models HTTP/1.1" 200 OK
INFO 08-06 23:32:24 [chat_utils.py:419] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
ERROR 08-06 23:32:24 [serving_chat.py:199] Error in preprocessing prompt inputs
ERROR 08-06 23:32:24 [serving_chat.py:199] Traceback (most recent call last):
ERROR 08-06 23:32:24 [serving_chat.py:199]   File "/data/jiangyy/miniconda3/lib/python3.12/site-packages/vllm/entrypoints/openai/serving_chat.py", line 182, in create_chat_completion
ERROR 08-06 23:32:24 [serving_chat.py:199]     ) = await self._preprocess_chat(
ERROR 08-06 23:32:24 [serving_chat.py:199]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 08-06 23:32:24 [serving_chat.py:199]   File "/data/jiangyy/miniconda3/lib/python3.12/site-packages/vllm/entrypoints/openai/serving_engine.py", line 834, in _preprocess_chat
ERROR 08-06 23:32:24 [serving_chat.py:199]     prompt_inputs = await self._tokenize_prompt_input_async(
ERROR 08-06 23:32:24 [serving_chat.py:199]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 08-06 23:32:24 [serving_chat.py:199]   File "/data/jiangyy/miniconda3/lib/python3.12/concurrent/futures/thread.py", line 59, in run
ERROR 08-06 23:32:24 [serving_chat.py:199]     result = self.fn(*self.args, **self.kwargs)
ERROR 08-06 23:32:24 [serving_chat.py:199]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 08-06 23:32:24 [serving_chat.py:199]   File "/data/jiangyy/miniconda3/lib/python3.12/site-packages/vllm/entrypoints/openai/serving_engine.py", line 589, in _tokenize_prompt_input
ERROR 08-06 23:32:24 [serving_chat.py:199]     return next(
ERROR 08-06 23:32:24 [serving_chat.py:199]            ^^^^^
ERROR 08-06 23:32:24 [serving_chat.py:199]   File "/data/jiangyy/miniconda3/lib/python3.12/site-packages/vllm/entrypoints/openai/serving_engine.py", line 613, in _tokenize_prompt_inputs
ERROR 08-06 23:32:24 [serving_chat.py:199]     yield self._normalize_prompt_text_to_input(
ERROR 08-06 23:32:24 [serving_chat.py:199]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 08-06 23:32:24 [serving_chat.py:199]   File "/data/jiangyy/miniconda3/lib/python3.12/site-packages/vllm/entrypoints/openai/serving_engine.py", line 498, in _normalize_prompt_text_to_input
ERROR 08-06 23:32:24 [serving_chat.py:199]     return self._validate_input(request, input_ids, input_text)
ERROR 08-06 23:32:24 [serving_chat.py:199]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 08-06 23:32:24 [serving_chat.py:199]   File "/data/jiangyy/miniconda3/lib/python3.12/site-packages/vllm/entrypoints/openai/serving_engine.py", line 566, in _validate_input
ERROR 08-06 23:32:24 [serving_chat.py:199]     raise ValueError(
ERROR 08-06 23:32:24 [serving_chat.py:199] ValueError: This model's maximum context length is 4096 tokens. However, you requested 4162 tokens (66 in the messages, 4096 in the completion). Please reduce the length of the messages or completion.
INFO:     172.27.221.3:43694 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
INFO:     172.27.221.3:44428 - "GET /v1/models HTTP/1.1" 200 OK
INFO 08-06 23:32:40 [logger.py:42] Received request chatcmpl-3b1beee28240426eafa62d65822ae259: prompt: '<｜begin▁of▁sentence｜>You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\n### Instruction:\nhi\n### Response:\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 08-06 23:32:40 [engine.py:316] Added request chatcmpl-3b1beee28240426eafa62d65822ae259.
INFO 08-06 23:32:40 [metrics.py:486] Avg prompt throughput: 6.7 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO:     172.27.221.3:44428 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 08-06 23:32:51 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.7 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-06 23:33:01 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
