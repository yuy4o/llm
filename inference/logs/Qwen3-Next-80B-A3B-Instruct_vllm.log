root@youln-a100-4:/dockpath/Qwen3-Next-80B-A3B-Instruct# CUDA_VISIBLE_DEVICES=0,1,2,3 vllm serve /dockpath/Qwen3-Next-80B-A3B-Instruct --dtype auto --api-key empty --port 12000 --trust-remote-code --tensor-parallel-size 4 --pipeline-parallel-size 1 --served-model-name Qwen3next --max-model-len 4096 --gpu_memory_utilization 0.8



INFO 11-30 01:28:18 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=2048.
(APIServer pid=32) INFO 11-30 01:28:18 [api_server.py:1977] vLLM API server version 0.11.2
(APIServer pid=32) INFO 11-30 01:28:18 [utils.py:253] non-default args: {'model_tag': '/dockpath/Qwen3-Next-80B-A3B-Instruct', 'port': 12000, 'api_key': ['empty'], 'model': '/dockpath/Qwen3-Next-80B-A3B-Instruct', 'trust_remote_code': True, 'max_model_len': 4096, 'served_model_name': ['Qwen3next'], 'tensor_parallel_size': 4, 'gpu_memory_utilization': 0.8}
(APIServer pid=32) The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
(APIServer pid=32) INFO 11-30 01:28:30 [model.py:631] Resolved architecture: Qwen3NextForCausalLM
(APIServer pid=32) INFO 11-30 01:28:30 [model.py:1745] Using max model len 4096
(APIServer pid=32) INFO 11-30 01:28:30 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=2048.
(APIServer pid=32) INFO 11-30 01:28:30 [config.py:308] Disabling cascade attention since it is not supported for hybrid models.
(APIServer pid=32) INFO 11-30 01:28:30 [config.py:432] Setting attention block size to 272 tokens to ensure that attention page size is >= mamba page size.
(APIServer pid=32) INFO 11-30 01:28:30 [config.py:456] Padding mamba page size by 1.49% to ensure that mamba page size and attention page size are exactly equal.
(EngineCore_DP0 pid=241) INFO 11-30 01:28:38 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='/dockpath/Qwen3-Next-80B-A3B-Instruct', speculative_config=None, tokenizer='/dockpath/Qwen3-Next-80B-A3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen3next, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
(EngineCore_DP0 pid=241) WARNING 11-30 01:28:38 [multiproc_executor.py:869] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 11-30 01:28:49 [parallel_state.py:1208] world_size=4 rank=3 local_rank=3 distributed_init_method=tcp://127.0.0.1:51031 backend=nccl
INFO 11-30 01:28:49 [parallel_state.py:1208] world_size=4 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:51031 backend=nccl
INFO 11-30 01:28:49 [parallel_state.py:1208] world_size=4 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:51031 backend=nccl
INFO 11-30 01:28:49 [parallel_state.py:1208] world_size=4 rank=2 local_rank=2 distributed_init_method=tcp://127.0.0.1:51031 backend=nccl
[rank2]:[W1130 01:28:50.914044393 ProcessGroupGloo.cpp:516] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())
[rank0]:[W1130 01:28:50.914394259 ProcessGroupGloo.cpp:516] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())
[rank1]:[W1130 01:28:50.914427388 ProcessGroupGloo.cpp:516] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())
[rank3]:[W1130 01:28:50.914815866 ProcessGroupGloo.cpp:516] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 11-30 01:28:50 [pynccl.py:111] vLLM is using nccl==2.27.5
WARNING 11-30 01:28:50 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
WARNING 11-30 01:28:50 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
WARNING 11-30 01:28:50 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
WARNING 11-30 01:28:50 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
WARNING 11-30 01:28:50 [custom_all_reduce.py:154] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 11-30 01:28:50 [custom_all_reduce.py:154] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 11-30 01:28:50 [custom_all_reduce.py:154] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 11-30 01:28:50 [custom_all_reduce.py:154] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 11-30 01:28:52 [parallel_state.py:1394] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
INFO 11-30 01:28:52 [parallel_state.py:1394] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
INFO 11-30 01:28:52 [parallel_state.py:1394] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 11-30 01:28:52 [parallel_state.py:1394] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(Worker_TP1 pid=343) INFO 11-30 01:28:52 [layer.py:342] Enabled separate cuda stream for MoE shared_experts
(Worker_TP3 pid=345) INFO 11-30 01:28:52 [layer.py:342] Enabled separate cuda stream for MoE shared_experts
(Worker_TP0 pid=342) INFO 11-30 01:28:52 [gpu_model_runner.py:3259] Starting to load model /dockpath/Qwen3-Next-80B-A3B-Instruct...
(Worker_TP2 pid=344) INFO 11-30 01:28:52 [layer.py:342] Enabled separate cuda stream for MoE shared_experts
(Worker_TP0 pid=342) INFO 11-30 01:28:53 [layer.py:342] Enabled separate cuda stream for MoE shared_experts
(Worker_TP3 pid=345) INFO 11-30 01:29:14 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
(Worker_TP3 pid=345) INFO 11-30 01:29:14 [cuda.py:427] Using FLASH_ATTN backend.
(Worker_TP1 pid=343) INFO 11-30 01:29:14 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
(Worker_TP1 pid=343) INFO 11-30 01:29:14 [cuda.py:427] Using FLASH_ATTN backend.
(Worker_TP0 pid=342) INFO 11-30 01:29:14 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
(Worker_TP0 pid=342) INFO 11-30 01:29:14 [cuda.py:427] Using FLASH_ATTN backend.
(Worker_TP2 pid=344) INFO 11-30 01:29:14 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
(Worker_TP2 pid=344) INFO 11-30 01:29:14 [cuda.py:427] Using FLASH_ATTN backend.
Loading safetensors checkpoint shards:   0% Completed | 0/41 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   2% Completed | 1/41 [00:18<12:04, 18.12s/it]
Loading safetensors checkpoint shards:   5% Completed | 2/41 [00:18<05:08,  7.91s/it]
Loading safetensors checkpoint shards:   7% Completed | 3/41 [00:37<08:02, 12.69s/it]
Loading safetensors checkpoint shards:  10% Completed | 4/41 [00:56<09:23, 15.24s/it]
Loading safetensors checkpoint shards:  12% Completed | 5/41 [01:15<09:58, 16.64s/it]
Loading safetensors checkpoint shards:  15% Completed | 6/41 [01:34<10:12, 17.49s/it]
Loading safetensors checkpoint shards:  17% Completed | 7/41 [01:35<06:48, 12.00s/it]
Loading safetensors checkpoint shards:  20% Completed | 8/41 [01:53<07:43, 14.05s/it]
Loading safetensors checkpoint shards:  22% Completed | 9/41 [01:54<05:17,  9.91s/it]
Loading safetensors checkpoint shards:  24% Completed | 10/41 [02:12<06:27, 12.50s/it]
Loading safetensors checkpoint shards:  27% Completed | 11/41 [02:14<04:32,  9.08s/it]
Loading safetensors checkpoint shards:  29% Completed | 12/41 [02:32<05:46, 11.96s/it]
Loading safetensors checkpoint shards:  32% Completed | 13/41 [02:33<04:00,  8.60s/it]
Loading safetensors checkpoint shards:  34% Completed | 14/41 [02:34<02:47,  6.21s/it]
Loading safetensors checkpoint shards:  37% Completed | 15/41 [02:35<01:58,  4.55s/it]
Loading safetensors checkpoint shards:  39% Completed | 16/41 [02:49<03:11,  7.66s/it]
Loading safetensors checkpoint shards:  41% Completed | 17/41 [02:50<02:13,  5.57s/it]
Loading safetensors checkpoint shards:  44% Completed | 18/41 [03:09<03:37,  9.44s/it]
Loading safetensors checkpoint shards:  46% Completed | 19/41 [03:28<04:31, 12.34s/it]
Loading safetensors checkpoint shards:  49% Completed | 20/41 [03:28<03:05,  8.84s/it]
Loading safetensors checkpoint shards:  51% Completed | 21/41 [03:47<03:54, 11.73s/it]
Loading safetensors checkpoint shards:  54% Completed | 22/41 [04:06<04:25, 13.96s/it]
Loading safetensors checkpoint shards:  56% Completed | 23/41 [04:25<04:39, 15.51s/it]
Loading safetensors checkpoint shards:  59% Completed | 24/41 [04:26<03:08, 11.08s/it]
Loading safetensors checkpoint shards:  61% Completed | 25/41 [04:27<02:07,  7.97s/it]
Loading safetensors checkpoint shards:  66% Completed | 27/41 [04:27<01:02,  4.48s/it]
Loading safetensors checkpoint shards:  68% Completed | 28/41 [04:45<01:41,  7.79s/it]
Loading safetensors checkpoint shards:  71% Completed | 29/41 [04:46<01:11,  5.96s/it]
Loading safetensors checkpoint shards:  73% Completed | 30/41 [04:47<00:50,  4.55s/it]
Loading safetensors checkpoint shards:  76% Completed | 31/41 [04:47<00:34,  3.47s/it]
Loading safetensors checkpoint shards:  78% Completed | 32/41 [04:48<00:23,  2.66s/it]
Loading safetensors checkpoint shards:  80% Completed | 33/41 [05:06<00:57,  7.15s/it]
Loading safetensors checkpoint shards:  83% Completed | 34/41 [05:07<00:37,  5.30s/it]
Loading safetensors checkpoint shards:  85% Completed | 35/41 [05:08<00:23,  3.97s/it]
Loading safetensors checkpoint shards:  88% Completed | 36/41 [05:26<00:40,  8.14s/it]
Loading safetensors checkpoint shards:  90% Completed | 37/41 [05:45<00:45, 11.40s/it]
Loading safetensors checkpoint shards:  93% Completed | 38/41 [06:04<00:41, 13.71s/it]
Loading safetensors checkpoint shards:  95% Completed | 39/41 [06:05<00:19,  9.87s/it]
Loading safetensors checkpoint shards:  98% Completed | 40/41 [06:23<00:12, 12.39s/it]
Loading safetensors checkpoint shards: 100% Completed | 41/41 [06:24<00:00,  8.91s/it]
Loading safetensors checkpoint shards: 100% Completed | 41/41 [06:24<00:00,  9.38s/it]
(Worker_TP0 pid=342) 
(Worker_TP0 pid=342) INFO 11-30 01:35:39 [default_loader.py:314] Loading weights took 384.69 seconds
(Worker_TP0 pid=342) INFO 11-30 01:35:40 [gpu_model_runner.py:3338] Model loading took 37.2151 GiB memory and 406.736575 seconds
(Worker_TP0 pid=342) INFO 11-30 01:35:50 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/e23beeb529/rank_0_0/backbone for vLLM's torch.compile
(Worker_TP0 pid=342) INFO 11-30 01:35:50 [backends.py:647] Dynamo bytecode transform time: 9.21 s
(Worker_TP0 pid=342) INFO 11-30 01:35:57 [backends.py:251] Cache the graph for dynamic shape for later use
(EngineCore_DP0 pid=241) INFO 11-30 01:36:40 [shm_broadcast.py:501] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
(Worker_TP0 pid=342) INFO 11-30 01:37:16 [backends.py:282] Compiling a graph for dynamic shape takes 84.55 s
(Worker_TP0 pid=342) WARNING 11-30 01:37:18 [fused_moe.py:886] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_A100-PCIE-40GB.json']
(Worker_TP3 pid=345) WARNING 11-30 01:37:18 [fused_moe.py:886] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_A100-PCIE-40GB.json']
(Worker_TP2 pid=344) WARNING 11-30 01:37:18 [fused_moe.py:886] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_A100-PCIE-40GB.json']
(Worker_TP1 pid=343) WARNING 11-30 01:37:18 [fused_moe.py:886] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_A100-PCIE-40GB.json']
(Worker_TP0 pid=342) INFO 11-30 01:37:20 [monitor.py:34] torch.compile takes 93.75 s in total
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815] WorkerProc hit an exception.
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815] Traceback (most recent call last):
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 3938, in _dummy_sampler_run
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     sampler_output = self.sampler(
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                      ^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return self._call_impl(*args, **kwargs)
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return forward_call(*args, **kwargs)
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/sampler.py", line 93, in forward
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/sampler.py", line 184, in sample
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     random_sampled, processed_logprobs = self.topk_topp_sampler(
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815] WorkerProc hit an exception.
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                                          ^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815] Traceback (most recent call last):
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return self._call_impl(*args, **kwargs)
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 3938, in _dummy_sampler_run
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     sampler_output = self.sampler(
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                      ^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return forward_call(*args, **kwargs)
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 75, in forward_native
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return self._call_impl(*args, **kwargs)
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     logits = self.apply_top_k_top_p(logits, k, p)
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 171, in apply_top_k_top_p
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return forward_call(*args, **kwargs)
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/sampler.py", line 93, in forward
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 446.00 MiB. GPU 1 has a total capacity of 39.38 GiB of which 345.38 MiB is free. Process 3720030 has 39.04 GiB memory in use. Of the allocated memory 38.18 GiB is allocated by PyTorch, and 199.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815] 
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/sampler.py", line 184, in sample
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815] The above exception was the direct cause of the following exception:
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     random_sampled, processed_logprobs = self.topk_topp_sampler(
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815] 
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                                          ^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815] Traceback (most recent call last):
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 810, in worker_busy_loop
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return self._call_impl(*args, **kwargs)
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     output = func(*args, **kwargs)
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]              ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return forward_call(*args, **kwargs)
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return func(*args, **kwargs)
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 75, in forward_native
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     logits = self.apply_top_k_top_p(logits, k, p)
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     self.model_runner.profile_run()
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 4142, in profile_run
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 171, in apply_top_k_top_p
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     output = self._dummy_sampler_run(last_hidden_states)
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return func(*args, **kwargs)
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 446.00 MiB. GPU 3 has a total capacity of 39.38 GiB of which 345.38 MiB is free. Process 3720032 has 39.04 GiB memory in use. Of the allocated memory 38.18 GiB is allocated by PyTorch, and 199.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815] 
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 3943, in _dummy_sampler_run
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815] The above exception was the direct cause of the following exception:
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     raise RuntimeError(
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815] 
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815] RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815] Traceback (most recent call last):
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815] Traceback (most recent call last):
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 810, in worker_busy_loop
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 3938, in _dummy_sampler_run
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     output = func(*args, **kwargs)
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     sampler_output = self.sampler(
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]              ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                      ^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return func(*args, **kwargs)
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return self._call_impl(*args, **kwargs)
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     self.model_runner.profile_run()
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return forward_call(*args, **kwargs)
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 4142, in profile_run
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     output = self._dummy_sampler_run(last_hidden_states)
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/sampler.py", line 93, in forward
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return func(*args, **kwargs)
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/sampler.py", line 184, in sample
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     random_sampled, processed_logprobs = self.topk_topp_sampler(
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 3943, in _dummy_sampler_run
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                                          ^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     raise RuntimeError(
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815] RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return self._call_impl(*args, **kwargs)
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815] Traceback (most recent call last):
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 3938, in _dummy_sampler_run
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     sampler_output = self.sampler(
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return forward_call(*args, **kwargs)
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                      ^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 75, in forward_native
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return self._call_impl(*args, **kwargs)
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     logits = self.apply_top_k_top_p(logits, k, p)
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 171, in apply_top_k_top_p
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return forward_call(*args, **kwargs)
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/sampler.py", line 93, in forward
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 446.00 MiB. GPU 1 has a total capacity of 39.38 GiB of which 345.38 MiB is free. Process 3720030 has 39.04 GiB memory in use. Of the allocated memory 38.18 GiB is allocated by PyTorch, and 199.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815] 
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815] The above exception was the direct cause of the following exception:
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/sampler.py", line 184, in sample
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815] 
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     random_sampled, processed_logprobs = self.topk_topp_sampler(
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815] Traceback (most recent call last):
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                                          ^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815] WorkerProc hit an exception.
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 810, in worker_busy_loop
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     output = func(*args, **kwargs)
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return self._call_impl(*args, **kwargs)
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]              ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815] Traceback (most recent call last):
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return func(*args, **kwargs)
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return forward_call(*args, **kwargs)
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 3938, in _dummy_sampler_run
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     sampler_output = self.sampler(
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 75, in forward_native
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                      ^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     self.model_runner.profile_run()
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     logits = self.apply_top_k_top_p(logits, k, p)
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 4142, in profile_run
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return self._call_impl(*args, **kwargs)
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     output = self._dummy_sampler_run(last_hidden_states)
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 171, in apply_top_k_top_p
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return forward_call(*args, **kwargs)
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return func(*args, **kwargs)
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 446.00 MiB. GPU 3 has a total capacity of 39.38 GiB of which 345.38 MiB is free. Process 3720032 has 39.04 GiB memory in use. Of the allocated memory 38.18 GiB is allocated by PyTorch, and 199.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815] 
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/sampler.py", line 93, in forward
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 3943, in _dummy_sampler_run
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815] The above exception was the direct cause of the following exception:
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     raise RuntimeError(
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815] 
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815] RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815] Traceback (most recent call last):
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/sampler.py", line 184, in sample
(Worker_TP1 pid=343) ERROR 11-30 01:37:20 [multiproc_executor.py:815] 
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 810, in worker_busy_loop
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     random_sampled, processed_logprobs = self.topk_topp_sampler(
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     output = func(*args, **kwargs)
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                                          ^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]              ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return self._call_impl(*args, **kwargs)
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return func(*args, **kwargs)
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return forward_call(*args, **kwargs)
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     self.model_runner.profile_run()
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 4142, in profile_run
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 75, in forward_native
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     output = self._dummy_sampler_run(last_hidden_states)
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     logits = self.apply_top_k_top_p(logits, k, p)
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 171, in apply_top_k_top_p
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return func(*args, **kwargs)
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 3943, in _dummy_sampler_run
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 446.00 MiB. GPU 0 has a total capacity of 39.38 GiB of which 345.38 MiB is free. Process 3720029 has 39.04 GiB memory in use. Of the allocated memory 38.18 GiB is allocated by PyTorch, and 199.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     raise RuntimeError(
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815] 
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815] RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815] The above exception was the direct cause of the following exception:
(Worker_TP3 pid=345) ERROR 11-30 01:37:20 [multiproc_executor.py:815] 
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815] 
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815] Traceback (most recent call last):
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 810, in worker_busy_loop
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     output = func(*args, **kwargs)
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]              ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return func(*args, **kwargs)
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     self.model_runner.profile_run()
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 4142, in profile_run
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     output = self._dummy_sampler_run(last_hidden_states)
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return func(*args, **kwargs)
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 3943, in _dummy_sampler_run
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     raise RuntimeError(
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815] RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815] Traceback (most recent call last):
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 3938, in _dummy_sampler_run
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     sampler_output = self.sampler(
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                      ^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return self._call_impl(*args, **kwargs)
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return forward_call(*args, **kwargs)
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/sampler.py", line 93, in forward
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/sampler.py", line 184, in sample
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     random_sampled, processed_logprobs = self.topk_topp_sampler(
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                                          ^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return self._call_impl(*args, **kwargs)
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return forward_call(*args, **kwargs)
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 75, in forward_native
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     logits = self.apply_top_k_top_p(logits, k, p)
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 171, in apply_top_k_top_p
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 446.00 MiB. GPU 0 has a total capacity of 39.38 GiB of which 345.38 MiB is free. Process 3720029 has 39.04 GiB memory in use. Of the allocated memory 38.18 GiB is allocated by PyTorch, and 199.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815] 
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815] The above exception was the direct cause of the following exception:
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815] 
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815] Traceback (most recent call last):
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 810, in worker_busy_loop
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     output = func(*args, **kwargs)
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]              ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return func(*args, **kwargs)
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     self.model_runner.profile_run()
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 4142, in profile_run
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     output = self._dummy_sampler_run(last_hidden_states)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815] WorkerProc hit an exception.
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return func(*args, **kwargs)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815] Traceback (most recent call last):
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 3938, in _dummy_sampler_run
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 3943, in _dummy_sampler_run
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     sampler_output = self.sampler(
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     raise RuntimeError(
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                      ^^^^^^^^^^^^^
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815] RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(Worker_TP0 pid=342) ERROR 11-30 01:37:20 [multiproc_executor.py:815] 
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return self._call_impl(*args, **kwargs)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return forward_call(*args, **kwargs)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/sampler.py", line 93, in forward
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/sampler.py", line 184, in sample
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     random_sampled, processed_logprobs = self.topk_topp_sampler(
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                                          ^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return self._call_impl(*args, **kwargs)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return forward_call(*args, **kwargs)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 75, in forward_native
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     logits = self.apply_top_k_top_p(logits, k, p)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 171, in apply_top_k_top_p
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 446.00 MiB. GPU 2 has a total capacity of 39.38 GiB of which 345.38 MiB is free. Process 3720031 has 39.04 GiB memory in use. Of the allocated memory 38.18 GiB is allocated by PyTorch, and 199.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815] 
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815] The above exception was the direct cause of the following exception:
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815] 
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815] Traceback (most recent call last):
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 810, in worker_busy_loop
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     output = func(*args, **kwargs)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]              ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return func(*args, **kwargs)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     self.model_runner.profile_run()
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 4142, in profile_run
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     output = self._dummy_sampler_run(last_hidden_states)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return func(*args, **kwargs)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 3943, in _dummy_sampler_run
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     raise RuntimeError(
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815] RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815] Traceback (most recent call last):
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 3938, in _dummy_sampler_run
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     sampler_output = self.sampler(
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                      ^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return self._call_impl(*args, **kwargs)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return forward_call(*args, **kwargs)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/sampler.py", line 93, in forward
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/sampler.py", line 184, in sample
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     random_sampled, processed_logprobs = self.topk_topp_sampler(
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                                          ^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return self._call_impl(*args, **kwargs)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return forward_call(*args, **kwargs)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 75, in forward_native
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     logits = self.apply_top_k_top_p(logits, k, p)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 171, in apply_top_k_top_p
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 446.00 MiB. GPU 2 has a total capacity of 39.38 GiB of which 345.38 MiB is free. Process 3720031 has 39.04 GiB memory in use. Of the allocated memory 38.18 GiB is allocated by PyTorch, and 199.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815] 
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815] The above exception was the direct cause of the following exception:
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815] 
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815] Traceback (most recent call last):
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 810, in worker_busy_loop
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     output = func(*args, **kwargs)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]              ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return func(*args, **kwargs)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     self.model_runner.profile_run()
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 4142, in profile_run
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     output = self._dummy_sampler_run(last_hidden_states)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     return func(*args, **kwargs)
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]            ^^^^^^^^^^^^^^^^^^^^^
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 3943, in _dummy_sampler_run
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815]     raise RuntimeError(
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815] RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
(Worker_TP2 pid=344) ERROR 11-30 01:37:20 [multiproc_executor.py:815] 
(EngineCore_DP0 pid=241) ERROR 11-30 01:37:20 [core.py:842] EngineCore failed to start.
(EngineCore_DP0 pid=241) ERROR 11-30 01:37:20 [core.py:842] Traceback (most recent call last):
(EngineCore_DP0 pid=241) ERROR 11-30 01:37:20 [core.py:842]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
(EngineCore_DP0 pid=241) ERROR 11-30 01:37:20 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=241) ERROR 11-30 01:37:20 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=241) ERROR 11-30 01:37:20 [core.py:842]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 606, in __init__
(EngineCore_DP0 pid=241) ERROR 11-30 01:37:20 [core.py:842]     super().__init__(
(EngineCore_DP0 pid=241) ERROR 11-30 01:37:20 [core.py:842]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=241) ERROR 11-30 01:37:20 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=241) ERROR 11-30 01:37:20 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=241) ERROR 11-30 01:37:20 [core.py:842]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
(EngineCore_DP0 pid=241) ERROR 11-30 01:37:20 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=241) ERROR 11-30 01:37:20 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=241) ERROR 11-30 01:37:20 [core.py:842]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=241) ERROR 11-30 01:37:20 [core.py:842]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=241) ERROR 11-30 01:37:20 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=241) ERROR 11-30 01:37:20 [core.py:842]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 358, in collective_rpc
(EngineCore_DP0 pid=241) ERROR 11-30 01:37:20 [core.py:842]     return aggregate(get_response())
(EngineCore_DP0 pid=241) ERROR 11-30 01:37:20 [core.py:842]                      ^^^^^^^^^^^^^^
(EngineCore_DP0 pid=241) ERROR 11-30 01:37:20 [core.py:842]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 341, in get_response
(EngineCore_DP0 pid=241) ERROR 11-30 01:37:20 [core.py:842]     raise RuntimeError(
(EngineCore_DP0 pid=241) ERROR 11-30 01:37:20 [core.py:842] RuntimeError: Worker failed with error 'CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.', please check the stack trace above for the root cause
(EngineCore_DP0 pid=241) ERROR 11-30 01:37:23 [multiproc_executor.py:230] Worker proc VllmWorker-2 died unexpectedly, shutting down executor.
(EngineCore_DP0 pid=241) Process EngineCore_DP0:
(EngineCore_DP0 pid=241) Traceback (most recent call last):
(EngineCore_DP0 pid=241)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=241)     self.run()
(EngineCore_DP0 pid=241)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=241)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=241)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
(EngineCore_DP0 pid=241)     raise e
(EngineCore_DP0 pid=241)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
(EngineCore_DP0 pid=241)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=241)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=241)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 606, in __init__
(EngineCore_DP0 pid=241)     super().__init__(
(EngineCore_DP0 pid=241)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=241)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=241)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=241)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
(EngineCore_DP0 pid=241)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=241)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=241)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=241)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=241)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=241)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 358, in collective_rpc
(EngineCore_DP0 pid=241)     return aggregate(get_response())
(EngineCore_DP0 pid=241)                      ^^^^^^^^^^^^^^
(EngineCore_DP0 pid=241)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 341, in get_response
(EngineCore_DP0 pid=241)     raise RuntimeError(
(EngineCore_DP0 pid=241) RuntimeError: Worker failed with error 'CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.', please check the stack trace above for the root cause
(APIServer pid=32) Traceback (most recent call last):
(APIServer pid=32)   File "/usr/local/bin/vllm", line 10, in <module>
(APIServer pid=32)     sys.exit(main())
(APIServer pid=32)              ^^^^^^
(APIServer pid=32)   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/cli/main.py", line 73, in main
(APIServer pid=32)     args.dispatch_function(args)
(APIServer pid=32)   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
(APIServer pid=32)     uvloop.run(run_server(args))
(APIServer pid=32)   File "/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py", line 96, in run
(APIServer pid=32)     return __asyncio.run(
(APIServer pid=32)            ^^^^^^^^^^^^^^
(APIServer pid=32)   File "/usr/lib/python3.12/asyncio/runners.py", line 195, in run
(APIServer pid=32)     return runner.run(main)
(APIServer pid=32)            ^^^^^^^^^^^^^^^^
(APIServer pid=32)   File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
(APIServer pid=32)     return self._loop.run_until_complete(task)
(APIServer pid=32)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=32)   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
(APIServer pid=32)   File "/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py", line 48, in wrapper
(APIServer pid=32)     return await main
(APIServer pid=32)            ^^^^^^^^^^
(APIServer pid=32)   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", line 2024, in run_server
(APIServer pid=32)     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
(APIServer pid=32)   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", line 2043, in run_server_worker
(APIServer pid=32)     async with build_async_engine_client(
(APIServer pid=32)                ^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=32)   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
(APIServer pid=32)     return await anext(self.gen)
(APIServer pid=32)            ^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=32)   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", line 195, in build_async_engine_client
(APIServer pid=32)     async with build_async_engine_client_from_engine_args(
(APIServer pid=32)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=32)   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
(APIServer pid=32)     return await anext(self.gen)
(APIServer pid=32)            ^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=32)   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", line 236, in build_async_engine_client_from_engine_args
(APIServer pid=32)     async_llm = AsyncLLM.from_vllm_config(
(APIServer pid=32)                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=32)   File "/usr/local/lib/python3.12/dist-packages/vllm/utils/func_utils.py", line 116, in inner
(APIServer pid=32)     return fn(*args, **kwargs)
(APIServer pid=32)            ^^^^^^^^^^^^^^^^^^^
(APIServer pid=32)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 203, in from_vllm_config
(APIServer pid=32)     return cls(
(APIServer pid=32)            ^^^^
(APIServer pid=32)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 133, in __init__
(APIServer pid=32)     self.engine_core = EngineCoreClient.make_async_mp_client(
(APIServer pid=32)                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=32)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
(APIServer pid=32)     return AsyncMPClient(*client_args)
(APIServer pid=32)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=32)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 808, in __init__
(APIServer pid=32)     super().__init__(
(APIServer pid=32)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 469, in __init__
(APIServer pid=32)     with launch_core_engines(vllm_config, executor_class, log_stats) as (
(APIServer pid=32)          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=32)   File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
(APIServer pid=32)     next(self.gen)
(APIServer pid=32)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 907, in launch_core_engines
(APIServer pid=32)     wait_for_engine_startup(
(APIServer pid=32)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 964, in wait_for_engine_startup
(APIServer pid=32)     raise RuntimeError(
(APIServer pid=32) RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
root@youln-a100-4:/dockpath/Qwen3-Next-80B-A3B-Instruct# 