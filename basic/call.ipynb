{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f197230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# import torch\n",
    "\n",
    "# MODEL_DIR = \"/data/jiangyy/phi-4/phi-4\"\n",
    "\n",
    "# # åŠ è½½tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "\n",
    "# # åŠ è½½æ¨¡å‹ï¼ˆç¡®ä¿æ¨¡å‹ä»¥ .safetensors æ ¼å¼ä¿å­˜ï¼‰\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR, trust_remote_code=True)\n",
    "# model.eval()\n",
    "# text = \"This is an example sentence to test the model.\"\n",
    "# inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "\n",
    "# logits = outputs.logits\n",
    "# predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "# print(f\"Input text: {text}\")\n",
    "# print(f\"Predicted class: {predicted_class}\")\n",
    "\n",
    "\n",
    "# from transformers import Qwen2ForCausalLM, Qwen2TokenizerFast\n",
    "\n",
    "# model_name_or_path = \"/data/jiangyy/Qwen2.5-Coder-7B-Instruct\"\n",
    "# tokenizer = Qwen2TokenizerFast.from_pretrained(model_name_or_path)\n",
    "# print(tokenizer.get_chat_template())\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/data/jiangyy/lftrain2/LLaMA-Factory-main/saves/qwen25-7b/lora/sft2/checkpoint-2220\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"ä»Šå¤©ä¼¦æ•¦çš„å¤©æ°”æ€ä¹ˆæ ·\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "\n",
    "tools = [\n",
    "        {\n",
    "            \"name\": \"get_current_temperature\",\n",
    "            \"description\": \"Get current temperature at a location.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\": \"string\", \"description\": \"The location to get the temperature for.\"},\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The unit to return the temperature in.\"}\n",
    "                },\n",
    "                \"required\": [\"location\"]\n",
    "            }\n",
    "        }\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tools=tools, \n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2bd357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_name = \"/data/jiangyy/Qwen2.5-Math-RM-72B\"\n",
    "device = \"auto\" # the device to load the model onto\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "        model_name, \n",
    "        device_map=device, \n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        ).eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "chat = [\n",
    "        {\"role\": \"system\", \"content\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"To determine how much Janet makes from selling the duck eggs at the farmers' market, we need to follow these steps:\\n\\n1. Calculate the total number of eggs laid by the ducks each day.\\n2. Determine how many eggs Janet eats and bakes for herself each day.\\n3. Find out how many eggs are left to be sold.\\n4. Calculate the revenue from selling the remaining eggs at $2 per egg.\\n\\nLet's start with the first step:\\n\\n1. Janet's ducks lay 16 eggs per day.\\n\\nNext, we calculate how many eggs Janet eats and bakes for herself each day:\\n\\n2. Janet eats 3 eggs for breakfast every morning.\\n3. Janet bakes 4 eggs for her friends every day.\\n\\nSo, the total number of eggs Janet eats and bakes for herself each day is:\\n\\\\[ 3 + 4 = 7 \\\\text{ eggs} \\\\]\\n\\nNow, we find out how many eggs are left to be sold:\\n\\\\[ 16 - 7 = 9 \\\\text{ eggs} \\\\]\\n\\nFinally, we calculate the revenue from selling the remaining eggs at $2 per egg:\\n\\\\[ 9 \\\\times 2 = 18 \\\\text{ dollars} \\\\]\\n\\nTherefore, Janet makes boxed18 dollars every day at the farmers' market.\"}] # 3.75\n",
    "\n",
    "conversation_str = tokenizer.apply_chat_template(\n",
    "                    chat, \n",
    "                    tokenize=False, \n",
    "                    add_generation_prompt=False\n",
    "                    )\n",
    "\n",
    "input_ids = tokenizer.encode(\n",
    "            conversation_str, \n",
    "            return_tensors=\"pt\", \n",
    "            add_special_tokens=False\n",
    "            ).to(model.device)\n",
    "\n",
    "outputs = model(input_ids=input_ids)\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b363c051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data1/jiangyy/Qwen3-14B/\n",
      "<think>\n",
      "Okay, the user said \"hi\". I need to respond appropriately. Since they're greeting me, I should greet them back and offer help. Maybe say something like \"Hello! How can I assist you today?\" That's friendly and open-ended. Let me check if there's any other context I should consider. The user hasn't provided any specific details, so keeping it general is best. Alright, that should work.\n",
      "</think>\n",
      "\n",
      "Hello! How can I assist you today? ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import concurrent.futures\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:12000/v1\",\n",
    "    api_key=\"empty\",\n",
    "    # api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "def call(prompt, question, history):\n",
    "    models = client.models.list()\n",
    "    model = models.data[0].id\n",
    "    print(model)\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    for item in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": list(item.keys())[0]})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": list(item.values())[0]})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": question})\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=1024,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None,\n",
    "        # extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "# num_threads = 80\n",
    "# while True:\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "#         futures = [executor.submit(call) for _ in range(num_threads)]\n",
    "#         concurrent.futures.wait(futures)\n",
    "        \n",
    "msg = '''\n",
    "###æ¬¾é¡¹åç§°å®šä¹‰å¦‚ä¸‹ï¼š\n",
    "è¿›åº¦æ¬¾-ä¸šåŠ¡è¿›åº¦ï¼šä¸ä¸šåŠ¡å®æ–½è¿›åº¦æœ‰å…³\n",
    "è¿›åº¦æ¬¾-æ—¶é—´è¿›åº¦ï¼šä»…ä¸å›ºå®šæ—¶é—´æœ‰å…³\n",
    "è¿›åº¦æ¬¾-ç»´ä¿è¿›åº¦ï¼šä¸ç»´æŠ¤ã€ç»´ä¿®æœ‰å…³\n",
    "åˆ°è´§æ¬¾ï¼šè´§ç‰©åˆ°è´§ã€ç­¾æ”¶\n",
    "åˆéªŒæ¬¾ï¼šåˆéªŒåˆæ ¼å\n",
    "ç»ˆéªŒæ¬¾ï¼šéªŒæ”¶å’Œç»ˆéªŒéƒ½å¯¹åº”ç»ˆéªŒæ¬¾\n",
    "å®¡è®¡æ¬¾ï¼šå®¡è®¡å\n",
    "è´¨ä¿æ¬¾ï¼šè´¨ä¿æœŸæ»¡å\n",
    "\n",
    "###åˆåŒå†…å®¹\n",
    "1.åˆåŒç­¾è®¢å30æ—¥å†…æ”¯ä»˜ç­¾çº¦åˆåŒä»·85%çš„10%ã€‚\n",
    "2.å†œæ°‘å·¥å·¥èµ„æ¬¾é¡¹è¿›å…¥å†œæ°‘å·¥ä¸“æˆ·ã€‚\n",
    "3.åœ°ä¸‹å®¤é¡¶æ¿æµ‡ç­‘ç»“æŸä¸”åŸºç¡€éªŒæ”¶å®Œæˆå30æ—¥å†…,æ”¯ä»˜ç­¾çº¦åˆåŒä»·85%çš„10%ã€‚\n",
    "4.ä¸»ä½“éªŒæ”¶åˆæ ¼å30æ—¥å†…,æ”¯ä»˜ç­¾çº¦åˆåŒä»·85%çš„10%ã€‚\n",
    "5.æ‰€æœ‰ç³»ç»Ÿè°ƒè¯•å®Œå30æ—¥å†…ä»˜ç­¾çº¦åˆåŒä»·85%çš„10%ã€‚\n",
    "6.å·¥ç¨‹ç«£å·¥éªŒæ”¶åˆæ ¼å30æ—¥å†…,å·¥ç¨‹æ¬¾ä»˜è‡³åˆåŒä»·çš„65%(å«å†œæ°‘å·¥å·¥èµ„)ã€‚\n",
    "7.å·¥ç¨‹ç§»äº¤å®Œæ•´çš„è®¡ç®—èµ„æ–™å30æ—¥å†…,å·¥ç¨‹æ¬¾ä»˜è‡³åˆåŒä»·çš„70%(å«å†œæ°‘å·¥å·¥èµ„)ã€‚\n",
    "8.å·¥ç¨‹ç»“ç®—ç»å‘åŒ…äººå§”æ‰˜çš„å…·æœ‰ç›¸åº”èµ„è´¨çš„é€ ä»·å’¨è¯¢æœºæ„å®¡å®šåå30æ—¥å†…,å‘åˆ†åŒ…äººæ”¯ä»˜æœ€ç»ˆå®‰è£…ä¸“ä¸šå®¡å®šç»“ç®—ä»·çš„97%(æä¾›å…¨é¢å‘ç¥¨)ã€‚\n",
    "9.3%ä¿ä¿®é‡‘æŒ‰ã€Šå·¥ç¨‹è´¨é‡ä¿ä¿®ä¹¦ã€‹çš„çº¦å®šæ‰§è¡Œ\n",
    "â‘ è´¨é‡ä¿ä¿®é‡‘ä¸ºç«£å·¥ç»“ç®—ä»·çš„3%\n",
    "â‘¡å‘åŒ…äººåœ¨å·¥ç¨‹ç«£å·¥éªŒæ”¶æ»¡2å¹´å30å¤©å†…ä»˜ä¿ä¿®é‡‘çš„60%,æ»¡5å¹´å30å¤©å†…ä»˜ä¿ä¿®é‡‘çš„40%ã€‚\n",
    "\n",
    "è¯·æ ¹æ®###åˆåŒå†…å®¹ï¼Œç»™å‡ºåˆåŒå•æ¬¡å±äºå“ªä¸€ç±»æ¬¾é¡¹åç§°ï¼ŒåŒæ—¶è®¡ç®—å‡ºè¯¥åˆåŒå•æ¬¡çš„æ”¯ä»˜é‡‘é¢åœ¨æ•´ä¸ªç­¾çº¦åˆåŒä»·çš„å æ¯”ï¼Œæ³¨æ„åªéœ€è¦ç™¾åˆ†æ¯”ï¼Œä¸è¦ç»™æˆ‘é‡‘é¢æ•°å­—ï¼Œæ³¨æ„æ‰€æœ‰åˆåŒå•æ¬¡çš„å æ¯”æ€»å’Œä¸º100%ï¼Œæ³¨æ„ å¦‚æœæ˜¯â€œç­¾çº¦åˆåŒä»·85%çš„10%â€ï¼Œåˆ™å æ¯”ä¸º8.5%\n",
    "'''\n",
    "\n",
    "print(call(\"\",\"hi\",[]))\n",
    "\n",
    "\n",
    "# nohup python continueReq.py >> continueReq.log 2>&1 & echo $! > continueReq.pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40178f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œæ˜¯é˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„é€šä¹‰å®éªŒå®¤ç ”å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘è¢«è®¾è®¡ç”¨æ¥å›ç­”å„ç§é—®é¢˜ã€åˆ›ä½œæ–‡å­—ã€è¿›è¡Œé€»è¾‘æ¨ç†ã€ç¼–ç¨‹ç­‰ä»»åŠ¡ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"http://172.27.221.3:12000/v1\",\n",
    "    api_key=\"empty\",\n",
    ")\n",
    "completion = client.chat.completions.create(\n",
    "    model = client.models.list().data[0].id,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"ä½ æ˜¯è°\"}],\n",
    "    extra_body={\"stop_token_ids\": [151329, 151336, 151338], \"chat_template_kwargs\": {\"enable_thinking\": False}},\n",
    "    stream=True,\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        # print(chunk.choices[0].delta.content)\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd3e4be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "å¥½çš„ï¼Œç”¨æˆ·æ‰“æ‹›å‘¼è¯´â€œä½ å¥½â€ï¼Œæˆ‘éœ€è¦å‹å¥½å›åº”ã€‚å…ˆç¡®è®¤æ˜¯å¦æœ‰ç‰¹æ®Šéœ€æ±‚ï¼Œä½†çœ‹èµ·æ¥åªæ˜¯æ™®é€šé—®å€™ã€‚ä¿æŒè‡ªç„¶ï¼Œç”¨ä¸­æ–‡å›å¤ï¼Œä¸ç”¨å¤æ‚ç»“æ„ã€‚å¯ä»¥åŠ ä¸ªè¡¨æƒ…ç¬¦å·è®©è¯­æ°”æ›´äº²åˆ‡ã€‚ç¡®ä¿æ²¡æœ‰éšè—ä»»åŠ¡æˆ–é—®é¢˜ï¼Œç›´æ¥å›åº”å³å¯ã€‚\n",
      "</think>\n",
      "\n",
      "ä½ å¥½å‘€ï¼ğŸ˜Š æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:12000/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer empty\"\n",
    "}\n",
    "data = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"ä½ å¥½\"}\n",
    "    ],\n",
    "    \"model\": \"/data1/jiangyy/Qwen3-14B/\",  # æ³¨æ„ï¼šä¸èƒ½å†™ç»å¯¹è·¯å¾„\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "with requests.post(url, json=data, headers=headers, stream=True) as r:\n",
    "    for line in r.iter_lines():\n",
    "        if line:\n",
    "            line = line.decode(\"utf-8\")\n",
    "            if line.startswith(\"data: \"):\n",
    "                payload = line[len(\"data: \"):]\n",
    "                if payload.strip() == \"[DONE]\":\n",
    "                    break\n",
    "                data_json = json.loads(payload)\n",
    "                delta = data_json[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n",
    "                print(delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe79a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pprint\n",
    "import requests\n",
    "\n",
    "def post_http_request(prompt: dict, api_url: str) -> requests.Response:\n",
    "    headers = {\"User-Agent\": \"Test Client\"}\n",
    "    response = requests.post(api_url, headers=headers, json=prompt)\n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--host\", type=str, default=\"localhost\")\n",
    "    parser.add_argument(\"--port\", type=int, default=8081)\n",
    "    parser.add_argument(\"--model\",\n",
    "                        type=str,\n",
    "                        default=\"/data/jiangyy/Qwen2.5-Math-RM-72B\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    api_url = f\"http://{args.host}:{args.port}/pooling\"\n",
    "    model_name = args.model\n",
    "\n",
    "    # # Input like Completions API\n",
    "    # prompt = {\"model\": model_name, \"input\": \"vLLM is great!\"}\n",
    "    # pooling_response = post_http_request(prompt=prompt, api_url=api_url)\n",
    "    # print(\"Pooling Response:\")\n",
    "    # pprint.pprint(pooling_response.json())\n",
    "\n",
    "    # chat = [\n",
    "    #         {\"role\": \"system\", \"content\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\"},\n",
    "    #             {\"role\": \"user\", \"content\": \"Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\"},\n",
    "    #                 {\"role\": \"assistant\", \"content\": \"To determine how much Janet makes from selling the duck eggs at the farmers' market, we need to follow these steps:\\n\\n1. Calculate the total number of eggs laid by the ducks each day.\\n2. Determine how many eggs Janet eats and bakes for herself each day.\\n3. Find out how many eggs are left to be sold.\\n4. Calculate the revenue from selling the remaining eggs at $2 per egg.\\n\\nLet's start with the first step:\\n\\n1. Janet's ducks lay 16 eggs per day.\\n\\nNext, we calculate how many eggs Janet eats and bakes for herself each day:\\n\\n2. Janet eats 3 eggs for breakfast every morning.\\n3. Janet bakes 4 eggs for her friends every day.\\n\\nSo, the total number of eggs Janet eats and bakes for herself each day is:\\n\\\\[ 3 + 4 = 7 \\\\text{ eggs} \\\\]\\n\\nNow, we find out how many eggs are left to be sold:\\n\\\\[ 16 - 7 = 9 \\\\text{ eggs} \\\\]\\n\\nFinally, we calculate the revenue from selling the remaining eggs at $2 per egg:\\n\\\\[ 9 \\\\times 2 = 18 \\\\text{ dollars} \\\\]\\n\\nTherefore, Janet makes boxed18 dollars every day at the farmers' market.\"}] # 3.75\n",
    "\n",
    "\n",
    "    # Input like Chat API\n",
    "    prompt = {\n",
    "        \"model\":\n",
    "        model_name,\n",
    "        \"messages\": [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\"\n",
    "            }],\n",
    "        }]\n",
    "    }\n",
    "    pooling_response = post_http_request(prompt=prompt, api_url=api_url)\n",
    "    print(\"Pooling Response:\")\n",
    "    pprint.pprint(pooling_response.json())\n",
    "\n",
    "\n",
    "\n",
    "# from openai import OpenAI\n",
    "\n",
    "# # Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "# openai_api_key = \"empty\"\n",
    "# openai_api_base = \"http://localhost:8081/v1\"\n",
    "\n",
    "# client = OpenAI(\n",
    "#     api_key=openai_api_key,\n",
    "#     base_url=openai_api_base,\n",
    "# )\n",
    "\n",
    "# models = client.models.list()\n",
    "# model = models.data[0].id\n",
    "\n",
    "# responses = client.embeddings.create(\n",
    "#     input=\"Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\",\n",
    "#     model=model,\n",
    "# )\n",
    "\n",
    "# for data in responses.data:\n",
    "#     print(data.embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb48a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please install OpenAI SDK first: `pip3 install openai`\n",
    "\n",
    "# from openai import OpenAI\n",
    "\n",
    "# client = OpenAI(api_key=\"sk-4494bc7a7510462da2f58e52e5c90560\", base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "# question = '''\n",
    "# There is a collection of $25$ indistinguishable white chips and $25$ indistinguishable black chips. Find the number of ways to place some of these chips in the $25$ unit cells of a $5\\times5$ grid such that:\n",
    "\n",
    "# each cell contains at most one chip\n",
    "# all chips in the same row and all chips in the same column have the same colour\n",
    "# any additional chip placed on the grid would violate one or more of the previous two conditions.\n",
    "# '''\n",
    "\n",
    "# response = client.chat.completions.create(\n",
    "#     model=\"deepseek-reasoner\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"Your job is to solve the math problem step by step.\"},\n",
    "#         {\"role\": \"user\", \"content\": question},\n",
    "#     ],\n",
    "#     stream=False\n",
    "# )\n",
    "\n",
    "# print(response.choices[0].message.content)\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"sk-4494bc7a7510462da2f58e52e5c90560\", base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": '1+2='}]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reasoning_content = \"\"\n",
    "content = \"\"\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk)\n",
    "    if chunk.choices[0].delta.reasoning_content:\n",
    "        reasoning_content += chunk.choices[0].delta.reasoning_content\n",
    "    else:\n",
    "        content += chunk.choices[0].delta.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516dcebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from time import sleep\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://172.27.221.3:3443/oscap/llm/d10/v1\",  # åŸºç¡€ URLï¼Œä¸åŒ…å« /chat/completions\n",
    "    api_key=\"empty\",  # å¦‚æœä¸éœ€è¦è®¤è¯ï¼Œå°è¯•ç”¨ \"empty\"ï¼Œå¦åˆ™æ¢æˆæ­£ç¡®çš„ API Key\n",
    ")\n",
    "\n",
    "\n",
    "def call():\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"d10\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"å¸®æˆ‘å†™ä¸ª500å­—å°ä½œæ–‡\"}],\n",
    "        temperature=0.7,\n",
    "        max_tokens=500\n",
    "    )\n",
    "\n",
    "while True:\n",
    "    print(call())\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e68ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "#åœ¨playgroundé‡Œé¢éªŒè¯æˆåŠŸçš„æ—¶å€™ï¼Œç‚¹æŸ¥çœ‹ä»£ç èƒ½çœ‹åˆ°ä¸‹é¢çš„è¿™4è¡Œã€‚api_versioné çŒœæ˜¯å¾ˆéš¾çŒœå‡ºæ¥çš„\n",
    "client = AzureOpenAI(\n",
    "    api_key=\"187d20a319d4422198fc35ed62028e31\",  \n",
    "    api_version=\"2024-02-01\",\n",
    "    azure_endpoint=\"https://llmsecnew.openai.azure.com/\"\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"gpt4turbo0409\",      #è¿™ä¸ªåœ°æ–¹æ˜¯è¦ä½ è®¾å®šçš„deployment_nameï¼Œä¸æ˜¯å…·ä½“çš„æ¨¡å‹åç§°ï¼Œä¹Ÿå¯ä»¥æ˜¯åŸºäºgpt4åˆ›å»ºçš„éƒ¨ç½²å\n",
    "    messages=[        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"x=3, x+yç­‰äºå¤šå°‘\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"è¯·å‘Šè¯‰æˆ‘yæ˜¯å¤šå°‘\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"yæ˜¯100ï¼Œé‚£ç»“æœæ˜¯å¤šå°‘\"\n",
    "        }],\n",
    "    # stream=True,\n",
    "    # extra_body={\n",
    "    #     \"data_sources\": [\n",
    "    #         {\n",
    "    #             \"type\": \"azure_search\",\n",
    "    #             \"parameters\": {\n",
    "    #                 \"endpoint\": search_endpoint,\n",
    "    #                 \"index_name\": search_index,\n",
    "    #                 \"authentication\": {\n",
    "    #                     \"type\": \"system_assigned_managed_identity\"\n",
    "    #                 }\n",
    "    #             }\n",
    "    #         }\n",
    "    #     ]\n",
    "    # }\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a4e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # gpt-35-turbo-instruct\n",
    "# from openai import AzureOpenAI\n",
    "\n",
    "# client = AzureOpenAI(\n",
    "#     api_key=\"e7d9310f4325408b8351712c27c7e4bb\",\n",
    "#     api_version=\"2024-02-15-preview\",\n",
    "#     azure_endpoint = \"https://llmsec.openai.azure.com/\"\n",
    "#     )\n",
    "    \n",
    "# deployment_name=\"1030-1\"#This will correspond to the custom name you chose for your deployment when you deployed a model. Use a gpt-35-turbo-instruct deployment. \n",
    "    \n",
    "# # Send a completion call to generate an answer\n",
    "# print('Sending a test completion job')\n",
    "# start_phrase = 'ç¾å›½çš„é¦–éƒ½æ˜¯ä»€ä¹ˆ'\n",
    "# response = client.completions.create(model=deployment_name, prompt=start_phrase, max_tokens=10)\n",
    "# print(start_phrase+response.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6683bae",
   "metadata": {},
   "source": [
    "### qwen api\n",
    "\n",
    "https://help.aliyun.com/zh/dashscope/developer-reference/api-details#9c5fdab94csp4\n",
    "\n",
    "### openai api\n",
    "https://platform.openai.com/docs/api-reference/chat/create"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
