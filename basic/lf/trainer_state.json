{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 27272.727272727272,
  "eval_steps": 500,
  "global_step": 150000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 1.510888934135437,
      "learning_rate": 6.666666666666666e-10,
      "loss": 2.4027,
      "step": 10
    },
    {
      "epoch": 3.6363636363636362,
      "grad_norm": 1.3543083667755127,
      "learning_rate": 1.3333333333333333e-09,
      "loss": 2.393,
      "step": 20
    },
    {
      "epoch": 5.454545454545454,
      "grad_norm": 1.5337156057357788,
      "learning_rate": 2e-09,
      "loss": 2.3867,
      "step": 30
    },
    {
      "epoch": 7.2727272727272725,
      "grad_norm": 1.6681904792785645,
      "learning_rate": 2.6666666666666666e-09,
      "loss": 2.3863,
      "step": 40
    },
    {
      "epoch": 9.090909090909092,
      "grad_norm": 1.7133139371871948,
      "learning_rate": 3.3333333333333334e-09,
      "loss": 2.4133,
      "step": 50
    },
    {
      "epoch": 10.909090909090908,
      "grad_norm": 1.6405086517333984,
      "learning_rate": 4e-09,
      "loss": 2.388,
      "step": 60
    },
    {
      "epoch": 12.727272727272727,
      "grad_norm": 1.4936535358428955,
      "learning_rate": 4.666666666666667e-09,
      "loss": 2.3728,
      "step": 70
    },
    {
      "epoch": 14.545454545454545,
      "grad_norm": 1.2650034427642822,
      "learning_rate": 5.333333333333333e-09,
      "loss": 2.417,
      "step": 80
    },
    {
      "epoch": 16.363636363636363,
      "grad_norm": 1.2555614709854126,
      "learning_rate": 6e-09,
      "loss": 2.4083,
      "step": 90
    },
    {
      "epoch": 18.181818181818183,
      "grad_norm": 1.3603262901306152,
      "learning_rate": 6.666666666666667e-09,
      "loss": 2.3701,
      "step": 100
    },
    {
      "epoch": 20.0,
      "grad_norm": 1.3660393953323364,
      "learning_rate": 7.333333333333333e-09,
      "loss": 2.3939,
      "step": 110
    },
    {
      "epoch": 21.818181818181817,
      "grad_norm": 1.2307370901107788,
      "learning_rate": 8e-09,
      "loss": 2.387,
      "step": 120
    },
    {
      "epoch": 23.636363636363637,
      "grad_norm": 1.507379174232483,
      "learning_rate": 8.666666666666667e-09,
      "loss": 2.4253,
      "step": 130
    },
    {
      "epoch": 25.454545454545453,
      "grad_norm": 1.2210479974746704,
      "learning_rate": 9.333333333333334e-09,
      "loss": 2.3745,
      "step": 140
    },
    {
      "epoch": 27.272727272727273,
      "grad_norm": 1.1921006441116333,
      "learning_rate": 1e-08,
      "loss": 2.4182,
      "step": 150
    },
    {
      "epoch": 29.09090909090909,
      "grad_norm": 1.6919455528259277,
      "learning_rate": 1.0666666666666666e-08,
      "loss": 2.4001,
      "step": 160
    },
    {
      "epoch": 30.90909090909091,
      "grad_norm": 1.6019827127456665,
      "learning_rate": 1.1333333333333334e-08,
      "loss": 2.3911,
      "step": 170
    },
    {
      "epoch": 32.72727272727273,
      "grad_norm": 1.632535457611084,
      "learning_rate": 1.2e-08,
      "loss": 2.3982,
      "step": 180
    },
    {
      "epoch": 34.54545454545455,
      "grad_norm": 1.469927191734314,
      "learning_rate": 1.2666666666666666e-08,
      "loss": 2.3968,
      "step": 190
    },
    {
      "epoch": 36.36363636363637,
      "grad_norm": 1.1969904899597168,
      "learning_rate": 1.3333333333333334e-08,
      "loss": 2.392,
      "step": 200
    },
    {
      "epoch": 38.18181818181818,
      "grad_norm": 1.4174176454544067,
      "learning_rate": 1.4e-08,
      "loss": 2.4021,
      "step": 210
    },
    {
      "epoch": 40.0,
      "grad_norm": 1.1710057258605957,
      "learning_rate": 1.4666666666666666e-08,
      "loss": 2.3889,
      "step": 220
    },
    {
      "epoch": 41.81818181818182,
      "grad_norm": 1.324893832206726,
      "learning_rate": 1.5333333333333332e-08,
      "loss": 2.3907,
      "step": 230
    },
    {
      "epoch": 43.63636363636363,
      "grad_norm": 1.2016375064849854,
      "learning_rate": 1.6e-08,
      "loss": 2.4223,
      "step": 240
    },
    {
      "epoch": 45.45454545454545,
      "grad_norm": 1.5910824537277222,
      "learning_rate": 1.6666666666666667e-08,
      "loss": 2.3933,
      "step": 250
    },
    {
      "epoch": 47.27272727272727,
      "grad_norm": 1.455525279045105,
      "learning_rate": 1.7333333333333333e-08,
      "loss": 2.3816,
      "step": 260
    },
    {
      "epoch": 49.09090909090909,
      "grad_norm": 1.445336937904358,
      "learning_rate": 1.8e-08,
      "loss": 2.4024,
      "step": 270
    },
    {
      "epoch": 50.90909090909091,
      "grad_norm": 1.8414394855499268,
      "learning_rate": 1.866666666666667e-08,
      "loss": 2.38,
      "step": 280
    },
    {
      "epoch": 52.72727272727273,
      "grad_norm": 1.2329691648483276,
      "learning_rate": 1.9333333333333334e-08,
      "loss": 2.4028,
      "step": 290
    },
    {
      "epoch": 54.54545454545455,
      "grad_norm": 1.719754934310913,
      "learning_rate": 2e-08,
      "loss": 2.412,
      "step": 300
    },
    {
      "epoch": 56.36363636363637,
      "grad_norm": 1.6102443933486938,
      "learning_rate": 2.0666666666666666e-08,
      "loss": 2.3949,
      "step": 310
    },
    {
      "epoch": 58.18181818181818,
      "grad_norm": 1.503041386604309,
      "learning_rate": 2.1333333333333332e-08,
      "loss": 2.407,
      "step": 320
    },
    {
      "epoch": 60.0,
      "grad_norm": 1.4904022216796875,
      "learning_rate": 2.2e-08,
      "loss": 2.3804,
      "step": 330
    },
    {
      "epoch": 61.81818181818182,
      "grad_norm": 1.3604676723480225,
      "learning_rate": 2.2666666666666668e-08,
      "loss": 2.378,
      "step": 340
    },
    {
      "epoch": 63.63636363636363,
      "grad_norm": 1.6055017709732056,
      "learning_rate": 2.3333333333333334e-08,
      "loss": 2.4115,
      "step": 350
    },
    {
      "epoch": 65.45454545454545,
      "grad_norm": 1.42942476272583,
      "learning_rate": 2.4e-08,
      "loss": 2.3802,
      "step": 360
    },
    {
      "epoch": 67.27272727272727,
      "grad_norm": 1.306193232536316,
      "learning_rate": 2.4666666666666666e-08,
      "loss": 2.423,
      "step": 370
    },
    {
      "epoch": 69.0909090909091,
      "grad_norm": 1.5743250846862793,
      "learning_rate": 2.5333333333333332e-08,
      "loss": 2.3849,
      "step": 380
    },
    {
      "epoch": 70.9090909090909,
      "grad_norm": 1.4900968074798584,
      "learning_rate": 2.5999999999999998e-08,
      "loss": 2.397,
      "step": 390
    },
    {
      "epoch": 72.72727272727273,
      "grad_norm": 1.253603219985962,
      "learning_rate": 2.6666666666666667e-08,
      "loss": 2.3692,
      "step": 400
    },
    {
      "epoch": 74.54545454545455,
      "grad_norm": 1.385490894317627,
      "learning_rate": 2.7333333333333333e-08,
      "loss": 2.4027,
      "step": 410
    },
    {
      "epoch": 76.36363636363636,
      "grad_norm": 1.429490089416504,
      "learning_rate": 2.8e-08,
      "loss": 2.4011,
      "step": 420
    },
    {
      "epoch": 78.18181818181819,
      "grad_norm": 1.3837212324142456,
      "learning_rate": 2.8666666666666665e-08,
      "loss": 2.4119,
      "step": 430
    },
    {
      "epoch": 80.0,
      "grad_norm": 1.7931424379348755,
      "learning_rate": 2.933333333333333e-08,
      "loss": 2.3795,
      "step": 440
    },
    {
      "epoch": 81.81818181818181,
      "grad_norm": 1.2783983945846558,
      "learning_rate": 3e-08,
      "loss": 2.3906,
      "step": 450
    },
    {
      "epoch": 83.63636363636364,
      "grad_norm": 1.3788219690322876,
      "learning_rate": 3.0666666666666663e-08,
      "loss": 2.3793,
      "step": 460
    },
    {
      "epoch": 85.45454545454545,
      "grad_norm": 1.316633701324463,
      "learning_rate": 3.133333333333333e-08,
      "loss": 2.411,
      "step": 470
    },
    {
      "epoch": 87.27272727272727,
      "grad_norm": 1.6444238424301147,
      "learning_rate": 3.2e-08,
      "loss": 2.4135,
      "step": 480
    },
    {
      "epoch": 89.0909090909091,
      "grad_norm": 1.653151035308838,
      "learning_rate": 3.266666666666666e-08,
      "loss": 2.3777,
      "step": 490
    },
    {
      "epoch": 90.9090909090909,
      "grad_norm": 1.3475329875946045,
      "learning_rate": 3.3333333333333334e-08,
      "loss": 2.3966,
      "step": 500
    },
    {
      "epoch": 90.9090909090909,
      "eval_loss": 2.47072434425354,
      "eval_runtime": 0.9497,
      "eval_samples_per_second": 10.529,
      "eval_steps_per_second": 5.265,
      "step": 500
    },
    {
      "epoch": 92.72727272727273,
      "grad_norm": 1.6277661323547363,
      "learning_rate": 3.4e-08,
      "loss": 2.3906,
      "step": 510
    },
    {
      "epoch": 94.54545454545455,
      "grad_norm": 1.5446758270263672,
      "learning_rate": 3.4666666666666666e-08,
      "loss": 2.3913,
      "step": 520
    },
    {
      "epoch": 96.36363636363636,
      "grad_norm": 1.5253698825836182,
      "learning_rate": 3.533333333333333e-08,
      "loss": 2.3666,
      "step": 530
    },
    {
      "epoch": 98.18181818181819,
      "grad_norm": 1.3555521965026855,
      "learning_rate": 3.6e-08,
      "loss": 2.4231,
      "step": 540
    },
    {
      "epoch": 100.0,
      "grad_norm": 1.437173843383789,
      "learning_rate": 3.6666666666666664e-08,
      "loss": 2.409,
      "step": 550
    },
    {
      "epoch": 101.81818181818181,
      "grad_norm": 1.8914296627044678,
      "learning_rate": 3.733333333333334e-08,
      "loss": 2.3811,
      "step": 560
    },
    {
      "epoch": 103.63636363636364,
      "grad_norm": 1.4289414882659912,
      "learning_rate": 3.7999999999999996e-08,
      "loss": 2.4159,
      "step": 570
    },
    {
      "epoch": 105.45454545454545,
      "grad_norm": 1.2949469089508057,
      "learning_rate": 3.866666666666667e-08,
      "loss": 2.3615,
      "step": 580
    },
    {
      "epoch": 107.27272727272727,
      "grad_norm": 1.7768099308013916,
      "learning_rate": 3.933333333333333e-08,
      "loss": 2.4417,
      "step": 590
    },
    {
      "epoch": 109.0909090909091,
      "grad_norm": 1.4331855773925781,
      "learning_rate": 4e-08,
      "loss": 2.3835,
      "step": 600
    },
    {
      "epoch": 110.9090909090909,
      "grad_norm": 1.7205474376678467,
      "learning_rate": 4.066666666666666e-08,
      "loss": 2.3926,
      "step": 610
    },
    {
      "epoch": 112.72727272727273,
      "grad_norm": 1.2983508110046387,
      "learning_rate": 4.133333333333333e-08,
      "loss": 2.3766,
      "step": 620
    },
    {
      "epoch": 114.54545454545455,
      "grad_norm": 1.6636369228363037,
      "learning_rate": 4.2e-08,
      "loss": 2.4086,
      "step": 630
    },
    {
      "epoch": 116.36363636363636,
      "grad_norm": 1.5167691707611084,
      "learning_rate": 4.2666666666666665e-08,
      "loss": 2.3898,
      "step": 640
    },
    {
      "epoch": 118.18181818181819,
      "grad_norm": 1.3247148990631104,
      "learning_rate": 4.333333333333333e-08,
      "loss": 2.4081,
      "step": 650
    },
    {
      "epoch": 120.0,
      "grad_norm": 1.3131072521209717,
      "learning_rate": 4.4e-08,
      "loss": 2.3883,
      "step": 660
    },
    {
      "epoch": 121.81818181818181,
      "grad_norm": 1.1697832345962524,
      "learning_rate": 4.466666666666666e-08,
      "loss": 2.3952,
      "step": 670
    },
    {
      "epoch": 123.63636363636364,
      "grad_norm": 1.366551399230957,
      "learning_rate": 4.5333333333333336e-08,
      "loss": 2.3825,
      "step": 680
    },
    {
      "epoch": 125.45454545454545,
      "grad_norm": 1.5517138242721558,
      "learning_rate": 4.5999999999999995e-08,
      "loss": 2.4048,
      "step": 690
    },
    {
      "epoch": 127.27272727272727,
      "grad_norm": 1.3490773439407349,
      "learning_rate": 4.666666666666667e-08,
      "loss": 2.4043,
      "step": 700
    },
    {
      "epoch": 129.0909090909091,
      "grad_norm": 1.2574214935302734,
      "learning_rate": 4.733333333333333e-08,
      "loss": 2.3761,
      "step": 710
    },
    {
      "epoch": 130.9090909090909,
      "grad_norm": 1.1362781524658203,
      "learning_rate": 4.8e-08,
      "loss": 2.3932,
      "step": 720
    },
    {
      "epoch": 132.72727272727272,
      "grad_norm": 1.2129873037338257,
      "learning_rate": 4.866666666666666e-08,
      "loss": 2.397,
      "step": 730
    },
    {
      "epoch": 134.54545454545453,
      "grad_norm": 1.4452476501464844,
      "learning_rate": 4.933333333333333e-08,
      "loss": 2.3946,
      "step": 740
    },
    {
      "epoch": 136.36363636363637,
      "grad_norm": 1.7093831300735474,
      "learning_rate": 5e-08,
      "loss": 2.4068,
      "step": 750
    },
    {
      "epoch": 138.1818181818182,
      "grad_norm": 1.2440969944000244,
      "learning_rate": 5.0666666666666664e-08,
      "loss": 2.3629,
      "step": 760
    },
    {
      "epoch": 140.0,
      "grad_norm": 1.5978487730026245,
      "learning_rate": 5.133333333333333e-08,
      "loss": 2.3974,
      "step": 770
    },
    {
      "epoch": 141.8181818181818,
      "grad_norm": 1.2530217170715332,
      "learning_rate": 5.1999999999999996e-08,
      "loss": 2.389,
      "step": 780
    },
    {
      "epoch": 143.63636363636363,
      "grad_norm": 1.3353838920593262,
      "learning_rate": 5.266666666666666e-08,
      "loss": 2.3977,
      "step": 790
    },
    {
      "epoch": 145.45454545454547,
      "grad_norm": 1.3301904201507568,
      "learning_rate": 5.3333333333333334e-08,
      "loss": 2.385,
      "step": 800
    },
    {
      "epoch": 147.27272727272728,
      "grad_norm": 1.7992465496063232,
      "learning_rate": 5.3999999999999994e-08,
      "loss": 2.3959,
      "step": 810
    },
    {
      "epoch": 149.0909090909091,
      "grad_norm": 1.3690358400344849,
      "learning_rate": 5.4666666666666666e-08,
      "loss": 2.3919,
      "step": 820
    },
    {
      "epoch": 150.9090909090909,
      "grad_norm": 1.6269781589508057,
      "learning_rate": 5.5333333333333326e-08,
      "loss": 2.3906,
      "step": 830
    },
    {
      "epoch": 152.72727272727272,
      "grad_norm": 1.7725750207901,
      "learning_rate": 5.6e-08,
      "loss": 2.3919,
      "step": 840
    },
    {
      "epoch": 154.54545454545453,
      "grad_norm": 1.2109277248382568,
      "learning_rate": 5.666666666666666e-08,
      "loss": 2.3636,
      "step": 850
    },
    {
      "epoch": 156.36363636363637,
      "grad_norm": 1.4756860733032227,
      "learning_rate": 5.733333333333333e-08,
      "loss": 2.4266,
      "step": 860
    },
    {
      "epoch": 158.1818181818182,
      "grad_norm": 1.598594307899475,
      "learning_rate": 5.8e-08,
      "loss": 2.4042,
      "step": 870
    },
    {
      "epoch": 160.0,
      "grad_norm": 1.5453541278839111,
      "learning_rate": 5.866666666666666e-08,
      "loss": 2.3933,
      "step": 880
    },
    {
      "epoch": 161.8181818181818,
      "grad_norm": 1.4407578706741333,
      "learning_rate": 5.9333333333333335e-08,
      "loss": 2.3945,
      "step": 890
    },
    {
      "epoch": 163.63636363636363,
      "grad_norm": 1.3889923095703125,
      "learning_rate": 6e-08,
      "loss": 2.3711,
      "step": 900
    },
    {
      "epoch": 165.45454545454547,
      "grad_norm": 1.5083180665969849,
      "learning_rate": 6.066666666666667e-08,
      "loss": 2.4211,
      "step": 910
    },
    {
      "epoch": 167.27272727272728,
      "grad_norm": 1.416438341140747,
      "learning_rate": 6.133333333333333e-08,
      "loss": 2.3771,
      "step": 920
    },
    {
      "epoch": 169.0909090909091,
      "grad_norm": 1.1054760217666626,
      "learning_rate": 6.2e-08,
      "loss": 2.389,
      "step": 930
    },
    {
      "epoch": 170.9090909090909,
      "grad_norm": 1.677217960357666,
      "learning_rate": 6.266666666666666e-08,
      "loss": 2.3958,
      "step": 940
    },
    {
      "epoch": 172.72727272727272,
      "grad_norm": 1.5270670652389526,
      "learning_rate": 6.333333333333333e-08,
      "loss": 2.3948,
      "step": 950
    },
    {
      "epoch": 174.54545454545453,
      "grad_norm": 1.621392011642456,
      "learning_rate": 6.4e-08,
      "loss": 2.3966,
      "step": 960
    },
    {
      "epoch": 176.36363636363637,
      "grad_norm": 1.393598198890686,
      "learning_rate": 6.466666666666666e-08,
      "loss": 2.39,
      "step": 970
    },
    {
      "epoch": 178.1818181818182,
      "grad_norm": 1.544985294342041,
      "learning_rate": 6.533333333333332e-08,
      "loss": 2.4033,
      "step": 980
    },
    {
      "epoch": 180.0,
      "grad_norm": 1.2509533166885376,
      "learning_rate": 6.6e-08,
      "loss": 2.3728,
      "step": 990
    },
    {
      "epoch": 181.8181818181818,
      "grad_norm": 1.558000087738037,
      "learning_rate": 6.666666666666667e-08,
      "loss": 2.3921,
      "step": 1000
    },
    {
      "epoch": 181.8181818181818,
      "eval_loss": 2.471266269683838,
      "eval_runtime": 0.9523,
      "eval_samples_per_second": 10.501,
      "eval_steps_per_second": 5.25,
      "step": 1000
    },
    {
      "epoch": 183.63636363636363,
      "grad_norm": 1.6612566709518433,
      "learning_rate": 6.733333333333333e-08,
      "loss": 2.3984,
      "step": 1010
    },
    {
      "epoch": 185.45454545454547,
      "grad_norm": 1.5464674234390259,
      "learning_rate": 6.8e-08,
      "loss": 2.3995,
      "step": 1020
    },
    {
      "epoch": 187.27272727272728,
      "grad_norm": 1.851069688796997,
      "learning_rate": 6.866666666666666e-08,
      "loss": 2.3716,
      "step": 1030
    },
    {
      "epoch": 189.0909090909091,
      "grad_norm": 1.3786755800247192,
      "learning_rate": 6.933333333333333e-08,
      "loss": 2.3887,
      "step": 1040
    },
    {
      "epoch": 190.9090909090909,
      "grad_norm": 1.2723561525344849,
      "learning_rate": 7e-08,
      "loss": 2.3864,
      "step": 1050
    },
    {
      "epoch": 192.72727272727272,
      "grad_norm": 1.4635953903198242,
      "learning_rate": 7.066666666666666e-08,
      "loss": 2.3695,
      "step": 1060
    },
    {
      "epoch": 194.54545454545453,
      "grad_norm": 1.6819931268692017,
      "learning_rate": 7.133333333333332e-08,
      "loss": 2.4009,
      "step": 1070
    },
    {
      "epoch": 196.36363636363637,
      "grad_norm": 1.2779507637023926,
      "learning_rate": 7.2e-08,
      "loss": 2.421,
      "step": 1080
    },
    {
      "epoch": 198.1818181818182,
      "grad_norm": 1.4699126482009888,
      "learning_rate": 7.266666666666667e-08,
      "loss": 2.347,
      "step": 1090
    },
    {
      "epoch": 200.0,
      "grad_norm": 1.4669184684753418,
      "learning_rate": 7.333333333333333e-08,
      "loss": 2.409,
      "step": 1100
    },
    {
      "epoch": 201.8181818181818,
      "grad_norm": 1.3158910274505615,
      "learning_rate": 7.399999999999999e-08,
      "loss": 2.3845,
      "step": 1110
    },
    {
      "epoch": 203.63636363636363,
      "grad_norm": 1.5388613939285278,
      "learning_rate": 7.466666666666667e-08,
      "loss": 2.3973,
      "step": 1120
    },
    {
      "epoch": 205.45454545454547,
      "grad_norm": 1.9227581024169922,
      "learning_rate": 7.533333333333333e-08,
      "loss": 2.3778,
      "step": 1130
    },
    {
      "epoch": 207.27272727272728,
      "grad_norm": 1.6016099452972412,
      "learning_rate": 7.599999999999999e-08,
      "loss": 2.3909,
      "step": 1140
    },
    {
      "epoch": 209.0909090909091,
      "grad_norm": 1.712689757347107,
      "learning_rate": 7.666666666666665e-08,
      "loss": 2.3846,
      "step": 1150
    },
    {
      "epoch": 210.9090909090909,
      "grad_norm": 1.4090297222137451,
      "learning_rate": 7.733333333333334e-08,
      "loss": 2.3919,
      "step": 1160
    },
    {
      "epoch": 212.72727272727272,
      "grad_norm": 1.2017278671264648,
      "learning_rate": 7.8e-08,
      "loss": 2.3829,
      "step": 1170
    },
    {
      "epoch": 214.54545454545453,
      "grad_norm": 1.4033358097076416,
      "learning_rate": 7.866666666666666e-08,
      "loss": 2.3652,
      "step": 1180
    },
    {
      "epoch": 216.36363636363637,
      "grad_norm": 1.6794488430023193,
      "learning_rate": 7.933333333333333e-08,
      "loss": 2.4098,
      "step": 1190
    },
    {
      "epoch": 218.1818181818182,
      "grad_norm": 1.7561427354812622,
      "learning_rate": 8e-08,
      "loss": 2.3771,
      "step": 1200
    },
    {
      "epoch": 220.0,
      "grad_norm": 1.5108001232147217,
      "learning_rate": 8.066666666666666e-08,
      "loss": 2.3826,
      "step": 1210
    },
    {
      "epoch": 221.8181818181818,
      "grad_norm": 1.5865880250930786,
      "learning_rate": 8.133333333333332e-08,
      "loss": 2.3881,
      "step": 1220
    },
    {
      "epoch": 223.63636363636363,
      "grad_norm": 1.9800273180007935,
      "learning_rate": 8.2e-08,
      "loss": 2.3671,
      "step": 1230
    },
    {
      "epoch": 225.45454545454547,
      "grad_norm": 2.0008153915405273,
      "learning_rate": 8.266666666666667e-08,
      "loss": 2.4379,
      "step": 1240
    },
    {
      "epoch": 227.27272727272728,
      "grad_norm": 1.7029448747634888,
      "learning_rate": 8.333333333333333e-08,
      "loss": 2.3771,
      "step": 1250
    },
    {
      "epoch": 229.0909090909091,
      "grad_norm": 1.596948266029358,
      "learning_rate": 8.4e-08,
      "loss": 2.3558,
      "step": 1260
    },
    {
      "epoch": 230.9090909090909,
      "grad_norm": 1.623246669769287,
      "learning_rate": 8.466666666666667e-08,
      "loss": 2.3922,
      "step": 1270
    },
    {
      "epoch": 232.72727272727272,
      "grad_norm": 1.786337971687317,
      "learning_rate": 8.533333333333333e-08,
      "loss": 2.3938,
      "step": 1280
    },
    {
      "epoch": 234.54545454545453,
      "grad_norm": 1.8052291870117188,
      "learning_rate": 8.599999999999999e-08,
      "loss": 2.3705,
      "step": 1290
    },
    {
      "epoch": 236.36363636363637,
      "grad_norm": 1.1320291757583618,
      "learning_rate": 8.666666666666666e-08,
      "loss": 2.3825,
      "step": 1300
    },
    {
      "epoch": 238.1818181818182,
      "grad_norm": 1.2686430215835571,
      "learning_rate": 8.733333333333333e-08,
      "loss": 2.3876,
      "step": 1310
    },
    {
      "epoch": 240.0,
      "grad_norm": 2.023237943649292,
      "learning_rate": 8.8e-08,
      "loss": 2.3799,
      "step": 1320
    },
    {
      "epoch": 241.8181818181818,
      "grad_norm": 1.2688969373703003,
      "learning_rate": 8.866666666666667e-08,
      "loss": 2.3668,
      "step": 1330
    },
    {
      "epoch": 243.63636363636363,
      "grad_norm": 1.5088731050491333,
      "learning_rate": 8.933333333333333e-08,
      "loss": 2.3935,
      "step": 1340
    },
    {
      "epoch": 245.45454545454547,
      "grad_norm": 1.8744010925292969,
      "learning_rate": 9e-08,
      "loss": 2.3804,
      "step": 1350
    },
    {
      "epoch": 247.27272727272728,
      "grad_norm": 1.6129480600357056,
      "learning_rate": 9.066666666666667e-08,
      "loss": 2.3683,
      "step": 1360
    },
    {
      "epoch": 249.0909090909091,
      "grad_norm": 1.6696223020553589,
      "learning_rate": 9.133333333333333e-08,
      "loss": 2.4078,
      "step": 1370
    },
    {
      "epoch": 250.9090909090909,
      "grad_norm": 1.5446281433105469,
      "learning_rate": 9.199999999999999e-08,
      "loss": 2.3839,
      "step": 1380
    },
    {
      "epoch": 252.72727272727272,
      "grad_norm": 1.389046311378479,
      "learning_rate": 9.266666666666666e-08,
      "loss": 2.3731,
      "step": 1390
    },
    {
      "epoch": 254.54545454545453,
      "grad_norm": 1.5500547885894775,
      "learning_rate": 9.333333333333334e-08,
      "loss": 2.3877,
      "step": 1400
    },
    {
      "epoch": 256.3636363636364,
      "grad_norm": 1.988437294960022,
      "learning_rate": 9.4e-08,
      "loss": 2.3668,
      "step": 1410
    },
    {
      "epoch": 258.1818181818182,
      "grad_norm": 1.9305648803710938,
      "learning_rate": 9.466666666666665e-08,
      "loss": 2.3799,
      "step": 1420
    },
    {
      "epoch": 260.0,
      "grad_norm": 1.420863151550293,
      "learning_rate": 9.533333333333334e-08,
      "loss": 2.3763,
      "step": 1430
    },
    {
      "epoch": 261.8181818181818,
      "grad_norm": 1.8152979612350464,
      "learning_rate": 9.6e-08,
      "loss": 2.3693,
      "step": 1440
    },
    {
      "epoch": 263.6363636363636,
      "grad_norm": 1.4725797176361084,
      "learning_rate": 9.666666666666666e-08,
      "loss": 2.3581,
      "step": 1450
    },
    {
      "epoch": 265.45454545454544,
      "grad_norm": 1.535287618637085,
      "learning_rate": 9.733333333333332e-08,
      "loss": 2.4316,
      "step": 1460
    },
    {
      "epoch": 267.27272727272725,
      "grad_norm": 1.585459589958191,
      "learning_rate": 9.8e-08,
      "loss": 2.3614,
      "step": 1470
    },
    {
      "epoch": 269.09090909090907,
      "grad_norm": 1.9461133480072021,
      "learning_rate": 9.866666666666666e-08,
      "loss": 2.364,
      "step": 1480
    },
    {
      "epoch": 270.90909090909093,
      "grad_norm": 1.6226534843444824,
      "learning_rate": 9.933333333333332e-08,
      "loss": 2.3864,
      "step": 1490
    },
    {
      "epoch": 272.72727272727275,
      "grad_norm": 1.4213062524795532,
      "learning_rate": 1e-07,
      "loss": 2.36,
      "step": 1500
    },
    {
      "epoch": 272.72727272727275,
      "eval_loss": 2.449153423309326,
      "eval_runtime": 0.9481,
      "eval_samples_per_second": 10.548,
      "eval_steps_per_second": 5.274,
      "step": 1500
    },
    {
      "epoch": 274.54545454545456,
      "grad_norm": 1.704300880432129,
      "learning_rate": 1.0066666666666667e-07,
      "loss": 2.3498,
      "step": 1510
    },
    {
      "epoch": 276.3636363636364,
      "grad_norm": 2.0985538959503174,
      "learning_rate": 1.0133333333333333e-07,
      "loss": 2.3987,
      "step": 1520
    },
    {
      "epoch": 278.1818181818182,
      "grad_norm": 1.7467042207717896,
      "learning_rate": 1.0199999999999999e-07,
      "loss": 2.3503,
      "step": 1530
    },
    {
      "epoch": 280.0,
      "grad_norm": 1.790603518486023,
      "learning_rate": 1.0266666666666666e-07,
      "loss": 2.3856,
      "step": 1540
    },
    {
      "epoch": 281.8181818181818,
      "grad_norm": 1.2274773120880127,
      "learning_rate": 1.0333333333333333e-07,
      "loss": 2.3587,
      "step": 1550
    },
    {
      "epoch": 283.6363636363636,
      "grad_norm": 1.779795527458191,
      "learning_rate": 1.0399999999999999e-07,
      "loss": 2.3921,
      "step": 1560
    },
    {
      "epoch": 285.45454545454544,
      "grad_norm": 1.9701493978500366,
      "learning_rate": 1.0466666666666666e-07,
      "loss": 2.3652,
      "step": 1570
    },
    {
      "epoch": 287.27272727272725,
      "grad_norm": 1.4926820993423462,
      "learning_rate": 1.0533333333333332e-07,
      "loss": 2.3458,
      "step": 1580
    },
    {
      "epoch": 289.09090909090907,
      "grad_norm": 1.5197879076004028,
      "learning_rate": 1.06e-07,
      "loss": 2.3673,
      "step": 1590
    },
    {
      "epoch": 290.90909090909093,
      "grad_norm": 1.4194996356964111,
      "learning_rate": 1.0666666666666667e-07,
      "loss": 2.3626,
      "step": 1600
    },
    {
      "epoch": 292.72727272727275,
      "grad_norm": 1.7574396133422852,
      "learning_rate": 1.0733333333333333e-07,
      "loss": 2.3531,
      "step": 1610
    },
    {
      "epoch": 294.54545454545456,
      "grad_norm": 1.4181206226348877,
      "learning_rate": 1.0799999999999999e-07,
      "loss": 2.3784,
      "step": 1620
    },
    {
      "epoch": 296.3636363636364,
      "grad_norm": 1.3775440454483032,
      "learning_rate": 1.0866666666666666e-07,
      "loss": 2.3359,
      "step": 1630
    },
    {
      "epoch": 298.1818181818182,
      "grad_norm": 2.207637310028076,
      "learning_rate": 1.0933333333333333e-07,
      "loss": 2.4009,
      "step": 1640
    },
    {
      "epoch": 300.0,
      "grad_norm": 1.4415698051452637,
      "learning_rate": 1.0999999999999999e-07,
      "loss": 2.3584,
      "step": 1650
    },
    {
      "epoch": 301.8181818181818,
      "grad_norm": 1.545926809310913,
      "learning_rate": 1.1066666666666665e-07,
      "loss": 2.3473,
      "step": 1660
    },
    {
      "epoch": 303.6363636363636,
      "grad_norm": 1.8671412467956543,
      "learning_rate": 1.1133333333333334e-07,
      "loss": 2.3784,
      "step": 1670
    },
    {
      "epoch": 305.45454545454544,
      "grad_norm": 1.386712908744812,
      "learning_rate": 1.12e-07,
      "loss": 2.3239,
      "step": 1680
    },
    {
      "epoch": 307.27272727272725,
      "grad_norm": 1.3846004009246826,
      "learning_rate": 1.1266666666666666e-07,
      "loss": 2.3874,
      "step": 1690
    },
    {
      "epoch": 309.09090909090907,
      "grad_norm": 1.3746671676635742,
      "learning_rate": 1.1333333333333332e-07,
      "loss": 2.3619,
      "step": 1700
    },
    {
      "epoch": 310.90909090909093,
      "grad_norm": 1.7025161981582642,
      "learning_rate": 1.14e-07,
      "loss": 2.3471,
      "step": 1710
    },
    {
      "epoch": 312.72727272727275,
      "grad_norm": 1.5581746101379395,
      "learning_rate": 1.1466666666666666e-07,
      "loss": 2.3551,
      "step": 1720
    },
    {
      "epoch": 314.54545454545456,
      "grad_norm": 1.940748929977417,
      "learning_rate": 1.1533333333333332e-07,
      "loss": 2.3672,
      "step": 1730
    },
    {
      "epoch": 316.3636363636364,
      "grad_norm": 1.3791749477386475,
      "learning_rate": 1.16e-07,
      "loss": 2.3551,
      "step": 1740
    },
    {
      "epoch": 318.1818181818182,
      "grad_norm": 1.8428305387496948,
      "learning_rate": 1.1666666666666667e-07,
      "loss": 2.3826,
      "step": 1750
    },
    {
      "epoch": 320.0,
      "grad_norm": 1.6281598806381226,
      "learning_rate": 1.1733333333333333e-07,
      "loss": 2.3354,
      "step": 1760
    },
    {
      "epoch": 321.8181818181818,
      "grad_norm": 1.5688340663909912,
      "learning_rate": 1.1799999999999998e-07,
      "loss": 2.3586,
      "step": 1770
    },
    {
      "epoch": 323.6363636363636,
      "grad_norm": 2.255450487136841,
      "learning_rate": 1.1866666666666667e-07,
      "loss": 2.3398,
      "step": 1780
    },
    {
      "epoch": 325.45454545454544,
      "grad_norm": 1.7073713541030884,
      "learning_rate": 1.1933333333333332e-07,
      "loss": 2.3199,
      "step": 1790
    },
    {
      "epoch": 327.27272727272725,
      "grad_norm": 1.363407015800476,
      "learning_rate": 1.2e-07,
      "loss": 2.3617,
      "step": 1800
    },
    {
      "epoch": 329.09090909090907,
      "grad_norm": 1.4719527959823608,
      "learning_rate": 1.2066666666666666e-07,
      "loss": 2.3321,
      "step": 1810
    },
    {
      "epoch": 330.90909090909093,
      "grad_norm": 1.7558990716934204,
      "learning_rate": 1.2133333333333333e-07,
      "loss": 2.3529,
      "step": 1820
    },
    {
      "epoch": 332.72727272727275,
      "grad_norm": 1.8509646654129028,
      "learning_rate": 1.2199999999999998e-07,
      "loss": 2.3405,
      "step": 1830
    },
    {
      "epoch": 334.54545454545456,
      "grad_norm": 2.2267086505889893,
      "learning_rate": 1.2266666666666665e-07,
      "loss": 2.3593,
      "step": 1840
    },
    {
      "epoch": 336.3636363636364,
      "grad_norm": 1.4667272567749023,
      "learning_rate": 1.2333333333333333e-07,
      "loss": 2.3269,
      "step": 1850
    },
    {
      "epoch": 338.1818181818182,
      "grad_norm": 1.6102102994918823,
      "learning_rate": 1.24e-07,
      "loss": 2.3354,
      "step": 1860
    },
    {
      "epoch": 340.0,
      "grad_norm": 1.7770148515701294,
      "learning_rate": 1.2466666666666664e-07,
      "loss": 2.3412,
      "step": 1870
    },
    {
      "epoch": 341.8181818181818,
      "grad_norm": 1.5799450874328613,
      "learning_rate": 1.2533333333333332e-07,
      "loss": 2.337,
      "step": 1880
    },
    {
      "epoch": 343.6363636363636,
      "grad_norm": 1.4181302785873413,
      "learning_rate": 1.26e-07,
      "loss": 2.3393,
      "step": 1890
    },
    {
      "epoch": 345.45454545454544,
      "grad_norm": 1.7826048135757446,
      "learning_rate": 1.2666666666666666e-07,
      "loss": 2.346,
      "step": 1900
    },
    {
      "epoch": 347.27272727272725,
      "grad_norm": 1.8714261054992676,
      "learning_rate": 1.273333333333333e-07,
      "loss": 2.3287,
      "step": 1910
    },
    {
      "epoch": 349.09090909090907,
      "grad_norm": 1.703657865524292,
      "learning_rate": 1.28e-07,
      "loss": 2.3225,
      "step": 1920
    },
    {
      "epoch": 350.90909090909093,
      "grad_norm": 1.708814263343811,
      "learning_rate": 1.2866666666666668e-07,
      "loss": 2.33,
      "step": 1930
    },
    {
      "epoch": 352.72727272727275,
      "grad_norm": 1.3068270683288574,
      "learning_rate": 1.2933333333333333e-07,
      "loss": 2.3097,
      "step": 1940
    },
    {
      "epoch": 354.54545454545456,
      "grad_norm": 1.8247318267822266,
      "learning_rate": 1.3e-07,
      "loss": 2.3621,
      "step": 1950
    },
    {
      "epoch": 356.3636363636364,
      "grad_norm": 1.8633302450180054,
      "learning_rate": 1.3066666666666665e-07,
      "loss": 2.3021,
      "step": 1960
    },
    {
      "epoch": 358.1818181818182,
      "grad_norm": 1.9215102195739746,
      "learning_rate": 1.3133333333333332e-07,
      "loss": 2.3242,
      "step": 1970
    },
    {
      "epoch": 360.0,
      "grad_norm": 1.7011680603027344,
      "learning_rate": 1.32e-07,
      "loss": 2.3176,
      "step": 1980
    },
    {
      "epoch": 361.8181818181818,
      "grad_norm": 2.3160555362701416,
      "learning_rate": 1.3266666666666664e-07,
      "loss": 2.3195,
      "step": 1990
    },
    {
      "epoch": 363.6363636363636,
      "grad_norm": 1.6204876899719238,
      "learning_rate": 1.3333333333333334e-07,
      "loss": 2.3069,
      "step": 2000
    },
    {
      "epoch": 363.6363636363636,
      "eval_loss": 2.401912212371826,
      "eval_runtime": 0.9545,
      "eval_samples_per_second": 10.477,
      "eval_steps_per_second": 5.238,
      "step": 2000
    },
    {
      "epoch": 365.45454545454544,
      "grad_norm": 1.4492160081863403,
      "learning_rate": 1.34e-07,
      "loss": 2.2995,
      "step": 2010
    },
    {
      "epoch": 367.27272727272725,
      "grad_norm": 1.3710591793060303,
      "learning_rate": 1.3466666666666665e-07,
      "loss": 2.329,
      "step": 2020
    },
    {
      "epoch": 369.09090909090907,
      "grad_norm": 1.7505592107772827,
      "learning_rate": 1.3533333333333333e-07,
      "loss": 2.3302,
      "step": 2030
    },
    {
      "epoch": 370.90909090909093,
      "grad_norm": 1.5692933797836304,
      "learning_rate": 1.36e-07,
      "loss": 2.3078,
      "step": 2040
    },
    {
      "epoch": 372.72727272727275,
      "grad_norm": 2.177692174911499,
      "learning_rate": 1.3666666666666665e-07,
      "loss": 2.3209,
      "step": 2050
    },
    {
      "epoch": 374.54545454545456,
      "grad_norm": 1.3393151760101318,
      "learning_rate": 1.3733333333333332e-07,
      "loss": 2.266,
      "step": 2060
    },
    {
      "epoch": 376.3636363636364,
      "grad_norm": 1.573171854019165,
      "learning_rate": 1.3800000000000002e-07,
      "loss": 2.33,
      "step": 2070
    },
    {
      "epoch": 378.1818181818182,
      "grad_norm": 1.3614850044250488,
      "learning_rate": 1.3866666666666666e-07,
      "loss": 2.2968,
      "step": 2080
    },
    {
      "epoch": 380.0,
      "grad_norm": 1.7702163457870483,
      "learning_rate": 1.3933333333333334e-07,
      "loss": 2.323,
      "step": 2090
    },
    {
      "epoch": 381.8181818181818,
      "grad_norm": 1.3754695653915405,
      "learning_rate": 1.4e-07,
      "loss": 2.2844,
      "step": 2100
    },
    {
      "epoch": 383.6363636363636,
      "grad_norm": 1.58744478225708,
      "learning_rate": 1.4066666666666666e-07,
      "loss": 2.3211,
      "step": 2110
    },
    {
      "epoch": 385.45454545454544,
      "grad_norm": 1.4629226922988892,
      "learning_rate": 1.4133333333333333e-07,
      "loss": 2.2994,
      "step": 2120
    },
    {
      "epoch": 387.27272727272725,
      "grad_norm": 1.7193220853805542,
      "learning_rate": 1.4199999999999997e-07,
      "loss": 2.3126,
      "step": 2130
    },
    {
      "epoch": 389.09090909090907,
      "grad_norm": 1.44765043258667,
      "learning_rate": 1.4266666666666665e-07,
      "loss": 2.2764,
      "step": 2140
    },
    {
      "epoch": 390.90909090909093,
      "grad_norm": 1.1813099384307861,
      "learning_rate": 1.4333333333333335e-07,
      "loss": 2.2993,
      "step": 2150
    },
    {
      "epoch": 392.72727272727275,
      "grad_norm": 1.6126525402069092,
      "learning_rate": 1.44e-07,
      "loss": 2.301,
      "step": 2160
    },
    {
      "epoch": 394.54545454545456,
      "grad_norm": 1.7471959590911865,
      "learning_rate": 1.4466666666666667e-07,
      "loss": 2.2657,
      "step": 2170
    },
    {
      "epoch": 396.3636363636364,
      "grad_norm": 2.0765187740325928,
      "learning_rate": 1.4533333333333334e-07,
      "loss": 2.315,
      "step": 2180
    },
    {
      "epoch": 398.1818181818182,
      "grad_norm": 1.8535792827606201,
      "learning_rate": 1.4599999999999998e-07,
      "loss": 2.2662,
      "step": 2190
    },
    {
      "epoch": 400.0,
      "grad_norm": 1.5963470935821533,
      "learning_rate": 1.4666666666666666e-07,
      "loss": 2.2864,
      "step": 2200
    },
    {
      "epoch": 401.8181818181818,
      "grad_norm": 1.4816982746124268,
      "learning_rate": 1.4733333333333333e-07,
      "loss": 2.2855,
      "step": 2210
    },
    {
      "epoch": 403.6363636363636,
      "grad_norm": 1.723251223564148,
      "learning_rate": 1.4799999999999998e-07,
      "loss": 2.2784,
      "step": 2220
    },
    {
      "epoch": 405.45454545454544,
      "grad_norm": 1.6464098691940308,
      "learning_rate": 1.4866666666666667e-07,
      "loss": 2.2888,
      "step": 2230
    },
    {
      "epoch": 407.27272727272725,
      "grad_norm": 1.6274899244308472,
      "learning_rate": 1.4933333333333335e-07,
      "loss": 2.2841,
      "step": 2240
    },
    {
      "epoch": 409.09090909090907,
      "grad_norm": 1.2756370306015015,
      "learning_rate": 1.5e-07,
      "loss": 2.2571,
      "step": 2250
    },
    {
      "epoch": 410.90909090909093,
      "grad_norm": 1.7191399335861206,
      "learning_rate": 1.5066666666666667e-07,
      "loss": 2.2713,
      "step": 2260
    },
    {
      "epoch": 412.72727272727275,
      "grad_norm": 1.2318531274795532,
      "learning_rate": 1.513333333333333e-07,
      "loss": 2.2561,
      "step": 2270
    },
    {
      "epoch": 414.54545454545456,
      "grad_norm": 1.4976357221603394,
      "learning_rate": 1.5199999999999998e-07,
      "loss": 2.2768,
      "step": 2280
    },
    {
      "epoch": 416.3636363636364,
      "grad_norm": 1.5036550760269165,
      "learning_rate": 1.5266666666666666e-07,
      "loss": 2.2462,
      "step": 2290
    },
    {
      "epoch": 418.1818181818182,
      "grad_norm": 1.793499231338501,
      "learning_rate": 1.533333333333333e-07,
      "loss": 2.2665,
      "step": 2300
    },
    {
      "epoch": 420.0,
      "grad_norm": 1.5594441890716553,
      "learning_rate": 1.54e-07,
      "loss": 2.2604,
      "step": 2310
    },
    {
      "epoch": 421.8181818181818,
      "grad_norm": 2.1006970405578613,
      "learning_rate": 1.5466666666666668e-07,
      "loss": 2.2381,
      "step": 2320
    },
    {
      "epoch": 423.6363636363636,
      "grad_norm": 1.6567150354385376,
      "learning_rate": 1.5533333333333332e-07,
      "loss": 2.2743,
      "step": 2330
    },
    {
      "epoch": 425.45454545454544,
      "grad_norm": 1.4788269996643066,
      "learning_rate": 1.56e-07,
      "loss": 2.2427,
      "step": 2340
    },
    {
      "epoch": 427.27272727272725,
      "grad_norm": 1.6088298559188843,
      "learning_rate": 1.5666666666666667e-07,
      "loss": 2.2434,
      "step": 2350
    },
    {
      "epoch": 429.09090909090907,
      "grad_norm": 1.5758287906646729,
      "learning_rate": 1.573333333333333e-07,
      "loss": 2.2393,
      "step": 2360
    },
    {
      "epoch": 430.90909090909093,
      "grad_norm": 1.595683217048645,
      "learning_rate": 1.5799999999999999e-07,
      "loss": 2.2489,
      "step": 2370
    },
    {
      "epoch": 432.72727272727275,
      "grad_norm": 1.4548791646957397,
      "learning_rate": 1.5866666666666666e-07,
      "loss": 2.2324,
      "step": 2380
    },
    {
      "epoch": 434.54545454545456,
      "grad_norm": 1.443975567817688,
      "learning_rate": 1.5933333333333333e-07,
      "loss": 2.225,
      "step": 2390
    },
    {
      "epoch": 436.3636363636364,
      "grad_norm": 1.6294835805892944,
      "learning_rate": 1.6e-07,
      "loss": 2.2465,
      "step": 2400
    },
    {
      "epoch": 438.1818181818182,
      "grad_norm": 2.201154947280884,
      "learning_rate": 1.6066666666666668e-07,
      "loss": 2.2385,
      "step": 2410
    },
    {
      "epoch": 440.0,
      "grad_norm": 1.5761778354644775,
      "learning_rate": 1.6133333333333332e-07,
      "loss": 2.2166,
      "step": 2420
    },
    {
      "epoch": 441.8181818181818,
      "grad_norm": 1.9168082475662231,
      "learning_rate": 1.62e-07,
      "loss": 2.2227,
      "step": 2430
    },
    {
      "epoch": 443.6363636363636,
      "grad_norm": 1.7130643129348755,
      "learning_rate": 1.6266666666666664e-07,
      "loss": 2.2186,
      "step": 2440
    },
    {
      "epoch": 445.45454545454544,
      "grad_norm": 1.4736870527267456,
      "learning_rate": 1.6333333333333331e-07,
      "loss": 2.2165,
      "step": 2450
    },
    {
      "epoch": 447.27272727272725,
      "grad_norm": 1.2759037017822266,
      "learning_rate": 1.64e-07,
      "loss": 2.2137,
      "step": 2460
    },
    {
      "epoch": 449.09090909090907,
      "grad_norm": 1.9888278245925903,
      "learning_rate": 1.6466666666666666e-07,
      "loss": 2.2123,
      "step": 2470
    },
    {
      "epoch": 450.90909090909093,
      "grad_norm": 1.5317585468292236,
      "learning_rate": 1.6533333333333333e-07,
      "loss": 2.2089,
      "step": 2480
    },
    {
      "epoch": 452.72727272727275,
      "grad_norm": 1.6071430444717407,
      "learning_rate": 1.66e-07,
      "loss": 2.2235,
      "step": 2490
    },
    {
      "epoch": 454.54545454545456,
      "grad_norm": 1.5418431758880615,
      "learning_rate": 1.6666666666666665e-07,
      "loss": 2.2029,
      "step": 2500
    },
    {
      "epoch": 454.54545454545456,
      "eval_loss": 2.292327880859375,
      "eval_runtime": 0.9471,
      "eval_samples_per_second": 10.558,
      "eval_steps_per_second": 5.279,
      "step": 2500
    },
    {
      "epoch": 456.3636363636364,
      "grad_norm": 1.9994642734527588,
      "learning_rate": 1.6733333333333332e-07,
      "loss": 2.1988,
      "step": 2510
    },
    {
      "epoch": 458.1818181818182,
      "grad_norm": 1.7587884664535522,
      "learning_rate": 1.68e-07,
      "loss": 2.2021,
      "step": 2520
    },
    {
      "epoch": 460.0,
      "grad_norm": 1.4326796531677246,
      "learning_rate": 1.6866666666666664e-07,
      "loss": 2.193,
      "step": 2530
    },
    {
      "epoch": 461.8181818181818,
      "grad_norm": 1.5821927785873413,
      "learning_rate": 1.6933333333333334e-07,
      "loss": 2.1889,
      "step": 2540
    },
    {
      "epoch": 463.6363636363636,
      "grad_norm": 1.4955360889434814,
      "learning_rate": 1.7000000000000001e-07,
      "loss": 2.2019,
      "step": 2550
    },
    {
      "epoch": 465.45454545454544,
      "grad_norm": 1.7172043323516846,
      "learning_rate": 1.7066666666666666e-07,
      "loss": 2.1842,
      "step": 2560
    },
    {
      "epoch": 467.27272727272725,
      "grad_norm": 1.8037198781967163,
      "learning_rate": 1.7133333333333333e-07,
      "loss": 2.1961,
      "step": 2570
    },
    {
      "epoch": 469.09090909090907,
      "grad_norm": 1.3995773792266846,
      "learning_rate": 1.7199999999999998e-07,
      "loss": 2.1762,
      "step": 2580
    },
    {
      "epoch": 470.90909090909093,
      "grad_norm": 1.6166579723358154,
      "learning_rate": 1.7266666666666665e-07,
      "loss": 2.1785,
      "step": 2590
    },
    {
      "epoch": 472.72727272727275,
      "grad_norm": 1.4368268251419067,
      "learning_rate": 1.7333333333333332e-07,
      "loss": 2.1981,
      "step": 2600
    },
    {
      "epoch": 474.54545454545456,
      "grad_norm": 1.5151476860046387,
      "learning_rate": 1.7399999999999997e-07,
      "loss": 2.1583,
      "step": 2610
    },
    {
      "epoch": 476.3636363636364,
      "grad_norm": 1.7643221616744995,
      "learning_rate": 1.7466666666666667e-07,
      "loss": 2.1867,
      "step": 2620
    },
    {
      "epoch": 478.1818181818182,
      "grad_norm": 1.4916256666183472,
      "learning_rate": 1.7533333333333334e-07,
      "loss": 2.1684,
      "step": 2630
    },
    {
      "epoch": 480.0,
      "grad_norm": 1.6954560279846191,
      "learning_rate": 1.76e-07,
      "loss": 2.1448,
      "step": 2640
    },
    {
      "epoch": 481.8181818181818,
      "grad_norm": 1.6058764457702637,
      "learning_rate": 1.7666666666666666e-07,
      "loss": 2.1641,
      "step": 2650
    },
    {
      "epoch": 483.6363636363636,
      "grad_norm": 1.5483840703964233,
      "learning_rate": 1.7733333333333333e-07,
      "loss": 2.1439,
      "step": 2660
    },
    {
      "epoch": 485.45454545454544,
      "grad_norm": 1.6644024848937988,
      "learning_rate": 1.7799999999999998e-07,
      "loss": 2.1758,
      "step": 2670
    },
    {
      "epoch": 487.27272727272725,
      "grad_norm": 1.5709447860717773,
      "learning_rate": 1.7866666666666665e-07,
      "loss": 2.1278,
      "step": 2680
    },
    {
      "epoch": 489.09090909090907,
      "grad_norm": 1.5325875282287598,
      "learning_rate": 1.7933333333333332e-07,
      "loss": 2.1672,
      "step": 2690
    },
    {
      "epoch": 490.90909090909093,
      "grad_norm": 1.7847055196762085,
      "learning_rate": 1.8e-07,
      "loss": 2.1503,
      "step": 2700
    },
    {
      "epoch": 492.72727272727275,
      "grad_norm": 1.651473045349121,
      "learning_rate": 1.8066666666666667e-07,
      "loss": 2.1438,
      "step": 2710
    },
    {
      "epoch": 494.54545454545456,
      "grad_norm": 1.6080174446105957,
      "learning_rate": 1.8133333333333334e-07,
      "loss": 2.1447,
      "step": 2720
    },
    {
      "epoch": 496.3636363636364,
      "grad_norm": 1.39878511428833,
      "learning_rate": 1.82e-07,
      "loss": 2.1265,
      "step": 2730
    },
    {
      "epoch": 498.1818181818182,
      "grad_norm": 1.547816514968872,
      "learning_rate": 1.8266666666666666e-07,
      "loss": 2.1304,
      "step": 2740
    },
    {
      "epoch": 500.0,
      "grad_norm": 1.573121428489685,
      "learning_rate": 1.833333333333333e-07,
      "loss": 2.1294,
      "step": 2750
    },
    {
      "epoch": 501.8181818181818,
      "grad_norm": 1.7086464166641235,
      "learning_rate": 1.8399999999999998e-07,
      "loss": 2.1185,
      "step": 2760
    },
    {
      "epoch": 503.6363636363636,
      "grad_norm": 1.3301540613174438,
      "learning_rate": 1.8466666666666665e-07,
      "loss": 2.1277,
      "step": 2770
    },
    {
      "epoch": 505.45454545454544,
      "grad_norm": 1.4931327104568481,
      "learning_rate": 1.8533333333333333e-07,
      "loss": 2.1328,
      "step": 2780
    },
    {
      "epoch": 507.27272727272725,
      "grad_norm": 1.6644424200057983,
      "learning_rate": 1.86e-07,
      "loss": 2.0775,
      "step": 2790
    },
    {
      "epoch": 509.09090909090907,
      "grad_norm": 1.5639934539794922,
      "learning_rate": 1.8666666666666667e-07,
      "loss": 2.1282,
      "step": 2800
    },
    {
      "epoch": 510.90909090909093,
      "grad_norm": 1.4171723127365112,
      "learning_rate": 1.8733333333333332e-07,
      "loss": 2.096,
      "step": 2810
    },
    {
      "epoch": 512.7272727272727,
      "grad_norm": 1.6027395725250244,
      "learning_rate": 1.88e-07,
      "loss": 2.1024,
      "step": 2820
    },
    {
      "epoch": 514.5454545454545,
      "grad_norm": 1.5202898979187012,
      "learning_rate": 1.8866666666666666e-07,
      "loss": 2.1271,
      "step": 2830
    },
    {
      "epoch": 516.3636363636364,
      "grad_norm": 1.5958433151245117,
      "learning_rate": 1.893333333333333e-07,
      "loss": 2.0614,
      "step": 2840
    },
    {
      "epoch": 518.1818181818181,
      "grad_norm": 1.4467209577560425,
      "learning_rate": 1.8999999999999998e-07,
      "loss": 2.1088,
      "step": 2850
    },
    {
      "epoch": 520.0,
      "grad_norm": 1.5085948705673218,
      "learning_rate": 1.9066666666666668e-07,
      "loss": 2.09,
      "step": 2860
    },
    {
      "epoch": 521.8181818181819,
      "grad_norm": 1.5143966674804688,
      "learning_rate": 1.9133333333333333e-07,
      "loss": 2.086,
      "step": 2870
    },
    {
      "epoch": 523.6363636363636,
      "grad_norm": 1.599343180656433,
      "learning_rate": 1.92e-07,
      "loss": 2.0904,
      "step": 2880
    },
    {
      "epoch": 525.4545454545455,
      "grad_norm": 1.682073950767517,
      "learning_rate": 1.9266666666666667e-07,
      "loss": 2.0592,
      "step": 2890
    },
    {
      "epoch": 527.2727272727273,
      "grad_norm": 1.337059497833252,
      "learning_rate": 1.9333333333333332e-07,
      "loss": 2.0761,
      "step": 2900
    },
    {
      "epoch": 529.0909090909091,
      "grad_norm": 1.3893910646438599,
      "learning_rate": 1.94e-07,
      "loss": 2.0717,
      "step": 2910
    },
    {
      "epoch": 530.9090909090909,
      "grad_norm": 1.6118555068969727,
      "learning_rate": 1.9466666666666664e-07,
      "loss": 2.0579,
      "step": 2920
    },
    {
      "epoch": 532.7272727272727,
      "grad_norm": 1.515028476715088,
      "learning_rate": 1.953333333333333e-07,
      "loss": 2.0636,
      "step": 2930
    },
    {
      "epoch": 534.5454545454545,
      "grad_norm": 1.3569667339324951,
      "learning_rate": 1.96e-07,
      "loss": 2.073,
      "step": 2940
    },
    {
      "epoch": 536.3636363636364,
      "grad_norm": 1.5349373817443848,
      "learning_rate": 1.9666666666666665e-07,
      "loss": 2.0369,
      "step": 2950
    },
    {
      "epoch": 538.1818181818181,
      "grad_norm": 1.715294599533081,
      "learning_rate": 1.9733333333333333e-07,
      "loss": 2.0686,
      "step": 2960
    },
    {
      "epoch": 540.0,
      "grad_norm": 1.6754807233810425,
      "learning_rate": 1.98e-07,
      "loss": 2.0469,
      "step": 2970
    },
    {
      "epoch": 541.8181818181819,
      "grad_norm": 1.6832650899887085,
      "learning_rate": 1.9866666666666665e-07,
      "loss": 2.0486,
      "step": 2980
    },
    {
      "epoch": 543.6363636363636,
      "grad_norm": 1.5942463874816895,
      "learning_rate": 1.9933333333333332e-07,
      "loss": 2.0365,
      "step": 2990
    },
    {
      "epoch": 545.4545454545455,
      "grad_norm": 1.2691282033920288,
      "learning_rate": 2e-07,
      "loss": 2.0274,
      "step": 3000
    },
    {
      "epoch": 545.4545454545455,
      "eval_loss": 2.1446189880371094,
      "eval_runtime": 0.9495,
      "eval_samples_per_second": 10.532,
      "eval_steps_per_second": 5.266,
      "step": 3000
    },
    {
      "epoch": 547.2727272727273,
      "grad_norm": 1.432078242301941,
      "learning_rate": 2.0066666666666666e-07,
      "loss": 2.0201,
      "step": 3010
    },
    {
      "epoch": 549.0909090909091,
      "grad_norm": 1.529570460319519,
      "learning_rate": 2.0133333333333334e-07,
      "loss": 2.0448,
      "step": 3020
    },
    {
      "epoch": 550.9090909090909,
      "grad_norm": 1.4555869102478027,
      "learning_rate": 2.02e-07,
      "loss": 2.0256,
      "step": 3030
    },
    {
      "epoch": 552.7272727272727,
      "grad_norm": 1.7702977657318115,
      "learning_rate": 2.0266666666666666e-07,
      "loss": 2.0265,
      "step": 3040
    },
    {
      "epoch": 554.5454545454545,
      "grad_norm": 1.2834511995315552,
      "learning_rate": 2.0333333333333333e-07,
      "loss": 2.016,
      "step": 3050
    },
    {
      "epoch": 556.3636363636364,
      "grad_norm": 1.4233101606369019,
      "learning_rate": 2.0399999999999997e-07,
      "loss": 2.0373,
      "step": 3060
    },
    {
      "epoch": 558.1818181818181,
      "grad_norm": 1.6787066459655762,
      "learning_rate": 2.0466666666666665e-07,
      "loss": 2.0055,
      "step": 3070
    },
    {
      "epoch": 560.0,
      "grad_norm": 1.9125111103057861,
      "learning_rate": 2.0533333333333332e-07,
      "loss": 2.0165,
      "step": 3080
    },
    {
      "epoch": 561.8181818181819,
      "grad_norm": 1.5529147386550903,
      "learning_rate": 2.06e-07,
      "loss": 2.0176,
      "step": 3090
    },
    {
      "epoch": 563.6363636363636,
      "grad_norm": 1.998192548751831,
      "learning_rate": 2.0666666666666666e-07,
      "loss": 1.997,
      "step": 3100
    },
    {
      "epoch": 565.4545454545455,
      "grad_norm": 1.6774709224700928,
      "learning_rate": 2.0733333333333334e-07,
      "loss": 1.9751,
      "step": 3110
    },
    {
      "epoch": 567.2727272727273,
      "grad_norm": 1.6396846771240234,
      "learning_rate": 2.0799999999999998e-07,
      "loss": 1.9998,
      "step": 3120
    },
    {
      "epoch": 569.0909090909091,
      "grad_norm": 2.1901254653930664,
      "learning_rate": 2.0866666666666666e-07,
      "loss": 1.9973,
      "step": 3130
    },
    {
      "epoch": 570.9090909090909,
      "grad_norm": 1.7277460098266602,
      "learning_rate": 2.0933333333333333e-07,
      "loss": 2.0016,
      "step": 3140
    },
    {
      "epoch": 572.7272727272727,
      "grad_norm": 1.6809532642364502,
      "learning_rate": 2.0999999999999997e-07,
      "loss": 1.9688,
      "step": 3150
    },
    {
      "epoch": 574.5454545454545,
      "grad_norm": 1.5338085889816284,
      "learning_rate": 2.1066666666666665e-07,
      "loss": 1.992,
      "step": 3160
    },
    {
      "epoch": 576.3636363636364,
      "grad_norm": 1.434778094291687,
      "learning_rate": 2.1133333333333335e-07,
      "loss": 1.9698,
      "step": 3170
    },
    {
      "epoch": 578.1818181818181,
      "grad_norm": 1.4968163967132568,
      "learning_rate": 2.12e-07,
      "loss": 1.9736,
      "step": 3180
    },
    {
      "epoch": 580.0,
      "grad_norm": 1.7612887620925903,
      "learning_rate": 2.1266666666666667e-07,
      "loss": 1.9804,
      "step": 3190
    },
    {
      "epoch": 581.8181818181819,
      "grad_norm": 1.2365436553955078,
      "learning_rate": 2.1333333333333334e-07,
      "loss": 1.9769,
      "step": 3200
    },
    {
      "epoch": 583.6363636363636,
      "grad_norm": 1.2687195539474487,
      "learning_rate": 2.1399999999999998e-07,
      "loss": 1.9469,
      "step": 3210
    },
    {
      "epoch": 585.4545454545455,
      "grad_norm": 1.281680941581726,
      "learning_rate": 2.1466666666666666e-07,
      "loss": 1.9551,
      "step": 3220
    },
    {
      "epoch": 587.2727272727273,
      "grad_norm": 1.4446289539337158,
      "learning_rate": 2.153333333333333e-07,
      "loss": 1.946,
      "step": 3230
    },
    {
      "epoch": 589.0909090909091,
      "grad_norm": 1.6316888332366943,
      "learning_rate": 2.1599999999999998e-07,
      "loss": 1.9553,
      "step": 3240
    },
    {
      "epoch": 590.9090909090909,
      "grad_norm": 1.5763999223709106,
      "learning_rate": 2.1666666666666667e-07,
      "loss": 1.9476,
      "step": 3250
    },
    {
      "epoch": 592.7272727272727,
      "grad_norm": 1.672714114189148,
      "learning_rate": 2.1733333333333332e-07,
      "loss": 1.9598,
      "step": 3260
    },
    {
      "epoch": 594.5454545454545,
      "grad_norm": 1.6052463054656982,
      "learning_rate": 2.18e-07,
      "loss": 1.9292,
      "step": 3270
    },
    {
      "epoch": 596.3636363636364,
      "grad_norm": 1.740121603012085,
      "learning_rate": 2.1866666666666667e-07,
      "loss": 1.9166,
      "step": 3280
    },
    {
      "epoch": 598.1818181818181,
      "grad_norm": 1.4473053216934204,
      "learning_rate": 2.193333333333333e-07,
      "loss": 1.9531,
      "step": 3290
    },
    {
      "epoch": 600.0,
      "grad_norm": 1.6820487976074219,
      "learning_rate": 2.1999999999999998e-07,
      "loss": 1.9265,
      "step": 3300
    },
    {
      "epoch": 601.8181818181819,
      "grad_norm": 1.5969486236572266,
      "learning_rate": 2.2066666666666666e-07,
      "loss": 1.9242,
      "step": 3310
    },
    {
      "epoch": 603.6363636363636,
      "grad_norm": 1.4471749067306519,
      "learning_rate": 2.213333333333333e-07,
      "loss": 1.9041,
      "step": 3320
    },
    {
      "epoch": 605.4545454545455,
      "grad_norm": 1.3856618404388428,
      "learning_rate": 2.22e-07,
      "loss": 1.9153,
      "step": 3330
    },
    {
      "epoch": 607.2727272727273,
      "grad_norm": 1.4901083707809448,
      "learning_rate": 2.2266666666666668e-07,
      "loss": 1.9299,
      "step": 3340
    },
    {
      "epoch": 609.0909090909091,
      "grad_norm": 1.561728835105896,
      "learning_rate": 2.2333333333333332e-07,
      "loss": 1.8986,
      "step": 3350
    },
    {
      "epoch": 610.9090909090909,
      "grad_norm": 1.3846110105514526,
      "learning_rate": 2.24e-07,
      "loss": 1.9075,
      "step": 3360
    },
    {
      "epoch": 612.7272727272727,
      "grad_norm": 1.5054677724838257,
      "learning_rate": 2.2466666666666664e-07,
      "loss": 1.8961,
      "step": 3370
    },
    {
      "epoch": 614.5454545454545,
      "grad_norm": 1.78083074092865,
      "learning_rate": 2.253333333333333e-07,
      "loss": 1.8977,
      "step": 3380
    },
    {
      "epoch": 616.3636363636364,
      "grad_norm": 1.3467854261398315,
      "learning_rate": 2.2599999999999999e-07,
      "loss": 1.8929,
      "step": 3390
    },
    {
      "epoch": 618.1818181818181,
      "grad_norm": 1.6435141563415527,
      "learning_rate": 2.2666666666666663e-07,
      "loss": 1.8654,
      "step": 3400
    },
    {
      "epoch": 620.0,
      "grad_norm": 1.6602411270141602,
      "learning_rate": 2.2733333333333333e-07,
      "loss": 1.8935,
      "step": 3410
    },
    {
      "epoch": 621.8181818181819,
      "grad_norm": 1.4731909036636353,
      "learning_rate": 2.28e-07,
      "loss": 1.8782,
      "step": 3420
    },
    {
      "epoch": 623.6363636363636,
      "grad_norm": 1.5324068069458008,
      "learning_rate": 2.2866666666666665e-07,
      "loss": 1.8528,
      "step": 3430
    },
    {
      "epoch": 625.4545454545455,
      "grad_norm": 1.7622885704040527,
      "learning_rate": 2.2933333333333332e-07,
      "loss": 1.9001,
      "step": 3440
    },
    {
      "epoch": 627.2727272727273,
      "grad_norm": 1.662812352180481,
      "learning_rate": 2.3e-07,
      "loss": 1.8629,
      "step": 3450
    },
    {
      "epoch": 629.0909090909091,
      "grad_norm": 1.731172800064087,
      "learning_rate": 2.3066666666666664e-07,
      "loss": 1.8686,
      "step": 3460
    },
    {
      "epoch": 630.9090909090909,
      "grad_norm": 1.6350733041763306,
      "learning_rate": 2.3133333333333331e-07,
      "loss": 1.8604,
      "step": 3470
    },
    {
      "epoch": 632.7272727272727,
      "grad_norm": 1.6106675863265991,
      "learning_rate": 2.32e-07,
      "loss": 1.8591,
      "step": 3480
    },
    {
      "epoch": 634.5454545454545,
      "grad_norm": 1.6233443021774292,
      "learning_rate": 2.3266666666666666e-07,
      "loss": 1.8347,
      "step": 3490
    },
    {
      "epoch": 636.3636363636364,
      "grad_norm": 1.5136297941207886,
      "learning_rate": 2.3333333333333333e-07,
      "loss": 1.8604,
      "step": 3500
    },
    {
      "epoch": 636.3636363636364,
      "eval_loss": 1.9831393957138062,
      "eval_runtime": 0.9536,
      "eval_samples_per_second": 10.486,
      "eval_steps_per_second": 5.243,
      "step": 3500
    },
    {
      "epoch": 638.1818181818181,
      "grad_norm": 1.5434433221817017,
      "learning_rate": 2.34e-07,
      "loss": 1.8354,
      "step": 3510
    },
    {
      "epoch": 640.0,
      "grad_norm": 1.4861671924591064,
      "learning_rate": 2.3466666666666665e-07,
      "loss": 1.8529,
      "step": 3520
    },
    {
      "epoch": 641.8181818181819,
      "grad_norm": 1.6611334085464478,
      "learning_rate": 2.3533333333333332e-07,
      "loss": 1.8386,
      "step": 3530
    },
    {
      "epoch": 643.6363636363636,
      "grad_norm": 1.4402685165405273,
      "learning_rate": 2.3599999999999997e-07,
      "loss": 1.8162,
      "step": 3540
    },
    {
      "epoch": 645.4545454545455,
      "grad_norm": 1.5519144535064697,
      "learning_rate": 2.3666666666666664e-07,
      "loss": 1.8443,
      "step": 3550
    },
    {
      "epoch": 647.2727272727273,
      "grad_norm": 1.6059768199920654,
      "learning_rate": 2.3733333333333334e-07,
      "loss": 1.854,
      "step": 3560
    },
    {
      "epoch": 649.0909090909091,
      "grad_norm": 1.8887954950332642,
      "learning_rate": 2.38e-07,
      "loss": 1.7905,
      "step": 3570
    },
    {
      "epoch": 650.9090909090909,
      "grad_norm": 1.6079933643341064,
      "learning_rate": 2.3866666666666663e-07,
      "loss": 1.8151,
      "step": 3580
    },
    {
      "epoch": 652.7272727272727,
      "grad_norm": 1.6226170063018799,
      "learning_rate": 2.3933333333333333e-07,
      "loss": 1.7982,
      "step": 3590
    },
    {
      "epoch": 654.5454545454545,
      "grad_norm": 1.3776941299438477,
      "learning_rate": 2.4e-07,
      "loss": 1.8122,
      "step": 3600
    },
    {
      "epoch": 656.3636363636364,
      "grad_norm": 1.6208046674728394,
      "learning_rate": 2.406666666666667e-07,
      "loss": 1.8162,
      "step": 3610
    },
    {
      "epoch": 658.1818181818181,
      "grad_norm": 1.5675896406173706,
      "learning_rate": 2.413333333333333e-07,
      "loss": 1.7928,
      "step": 3620
    },
    {
      "epoch": 660.0,
      "grad_norm": 1.6680591106414795,
      "learning_rate": 2.4199999999999997e-07,
      "loss": 1.8119,
      "step": 3630
    },
    {
      "epoch": 661.8181818181819,
      "grad_norm": 1.9806156158447266,
      "learning_rate": 2.4266666666666667e-07,
      "loss": 1.7857,
      "step": 3640
    },
    {
      "epoch": 663.6363636363636,
      "grad_norm": 1.2828269004821777,
      "learning_rate": 2.433333333333333e-07,
      "loss": 1.7895,
      "step": 3650
    },
    {
      "epoch": 665.4545454545455,
      "grad_norm": 1.6921743154525757,
      "learning_rate": 2.4399999999999996e-07,
      "loss": 1.8031,
      "step": 3660
    },
    {
      "epoch": 667.2727272727273,
      "grad_norm": 1.872429609298706,
      "learning_rate": 2.4466666666666666e-07,
      "loss": 1.7632,
      "step": 3670
    },
    {
      "epoch": 669.0909090909091,
      "grad_norm": 1.5000371932983398,
      "learning_rate": 2.453333333333333e-07,
      "loss": 1.7766,
      "step": 3680
    },
    {
      "epoch": 670.9090909090909,
      "grad_norm": 1.589717149734497,
      "learning_rate": 2.46e-07,
      "loss": 1.7649,
      "step": 3690
    },
    {
      "epoch": 672.7272727272727,
      "grad_norm": 1.7901936769485474,
      "learning_rate": 2.4666666666666665e-07,
      "loss": 1.7589,
      "step": 3700
    },
    {
      "epoch": 674.5454545454545,
      "grad_norm": 1.467668890953064,
      "learning_rate": 2.473333333333333e-07,
      "loss": 1.7785,
      "step": 3710
    },
    {
      "epoch": 676.3636363636364,
      "grad_norm": 1.8285839557647705,
      "learning_rate": 2.48e-07,
      "loss": 1.7681,
      "step": 3720
    },
    {
      "epoch": 678.1818181818181,
      "grad_norm": 1.564446210861206,
      "learning_rate": 2.4866666666666664e-07,
      "loss": 1.7608,
      "step": 3730
    },
    {
      "epoch": 680.0,
      "grad_norm": 1.677510142326355,
      "learning_rate": 2.493333333333333e-07,
      "loss": 1.7496,
      "step": 3740
    },
    {
      "epoch": 681.8181818181819,
      "grad_norm": 1.5726068019866943,
      "learning_rate": 2.5e-07,
      "loss": 1.7405,
      "step": 3750
    },
    {
      "epoch": 683.6363636363636,
      "grad_norm": 1.514096736907959,
      "learning_rate": 2.5066666666666663e-07,
      "loss": 1.7522,
      "step": 3760
    },
    {
      "epoch": 685.4545454545455,
      "grad_norm": 1.712019681930542,
      "learning_rate": 2.5133333333333333e-07,
      "loss": 1.721,
      "step": 3770
    },
    {
      "epoch": 687.2727272727273,
      "grad_norm": 1.540371298789978,
      "learning_rate": 2.52e-07,
      "loss": 1.7544,
      "step": 3780
    },
    {
      "epoch": 689.0909090909091,
      "grad_norm": 2.238741874694824,
      "learning_rate": 2.526666666666666e-07,
      "loss": 1.7116,
      "step": 3790
    },
    {
      "epoch": 690.9090909090909,
      "grad_norm": 1.5712857246398926,
      "learning_rate": 2.533333333333333e-07,
      "loss": 1.7329,
      "step": 3800
    },
    {
      "epoch": 692.7272727272727,
      "grad_norm": 1.4921706914901733,
      "learning_rate": 2.5399999999999997e-07,
      "loss": 1.7211,
      "step": 3810
    },
    {
      "epoch": 694.5454545454545,
      "grad_norm": 2.020486831665039,
      "learning_rate": 2.546666666666666e-07,
      "loss": 1.6891,
      "step": 3820
    },
    {
      "epoch": 696.3636363636364,
      "grad_norm": 1.8852728605270386,
      "learning_rate": 2.5533333333333337e-07,
      "loss": 1.7304,
      "step": 3830
    },
    {
      "epoch": 698.1818181818181,
      "grad_norm": 1.7248808145523071,
      "learning_rate": 2.56e-07,
      "loss": 1.6988,
      "step": 3840
    },
    {
      "epoch": 700.0,
      "grad_norm": 1.5236037969589233,
      "learning_rate": 2.5666666666666666e-07,
      "loss": 1.702,
      "step": 3850
    },
    {
      "epoch": 701.8181818181819,
      "grad_norm": 1.5529788732528687,
      "learning_rate": 2.5733333333333336e-07,
      "loss": 1.6954,
      "step": 3860
    },
    {
      "epoch": 703.6363636363636,
      "grad_norm": 1.621830701828003,
      "learning_rate": 2.58e-07,
      "loss": 1.7041,
      "step": 3870
    },
    {
      "epoch": 705.4545454545455,
      "grad_norm": 1.6857744455337524,
      "learning_rate": 2.5866666666666665e-07,
      "loss": 1.6631,
      "step": 3880
    },
    {
      "epoch": 707.2727272727273,
      "grad_norm": 1.5575264692306519,
      "learning_rate": 2.5933333333333335e-07,
      "loss": 1.6796,
      "step": 3890
    },
    {
      "epoch": 709.0909090909091,
      "grad_norm": 1.6020718812942505,
      "learning_rate": 2.6e-07,
      "loss": 1.6839,
      "step": 3900
    },
    {
      "epoch": 710.9090909090909,
      "grad_norm": 1.703516960144043,
      "learning_rate": 2.6066666666666664e-07,
      "loss": 1.6948,
      "step": 3910
    },
    {
      "epoch": 712.7272727272727,
      "grad_norm": 1.6289594173431396,
      "learning_rate": 2.613333333333333e-07,
      "loss": 1.6689,
      "step": 3920
    },
    {
      "epoch": 714.5454545454545,
      "grad_norm": 1.3853042125701904,
      "learning_rate": 2.62e-07,
      "loss": 1.6646,
      "step": 3930
    },
    {
      "epoch": 716.3636363636364,
      "grad_norm": 1.7300697565078735,
      "learning_rate": 2.6266666666666664e-07,
      "loss": 1.6551,
      "step": 3940
    },
    {
      "epoch": 718.1818181818181,
      "grad_norm": 1.791831374168396,
      "learning_rate": 2.633333333333333e-07,
      "loss": 1.6513,
      "step": 3950
    },
    {
      "epoch": 720.0,
      "grad_norm": 1.4746253490447998,
      "learning_rate": 2.64e-07,
      "loss": 1.6563,
      "step": 3960
    },
    {
      "epoch": 721.8181818181819,
      "grad_norm": 1.8141391277313232,
      "learning_rate": 2.6466666666666663e-07,
      "loss": 1.6497,
      "step": 3970
    },
    {
      "epoch": 723.6363636363636,
      "grad_norm": 1.4567906856536865,
      "learning_rate": 2.653333333333333e-07,
      "loss": 1.6326,
      "step": 3980
    },
    {
      "epoch": 725.4545454545455,
      "grad_norm": 1.7804415225982666,
      "learning_rate": 2.66e-07,
      "loss": 1.6406,
      "step": 3990
    },
    {
      "epoch": 727.2727272727273,
      "grad_norm": 1.5646090507507324,
      "learning_rate": 2.6666666666666667e-07,
      "loss": 1.6335,
      "step": 4000
    },
    {
      "epoch": 727.2727272727273,
      "eval_loss": 1.7883598804473877,
      "eval_runtime": 0.9508,
      "eval_samples_per_second": 10.518,
      "eval_steps_per_second": 5.259,
      "step": 4000
    },
    {
      "epoch": 729.0909090909091,
      "grad_norm": 1.6935144662857056,
      "learning_rate": 2.673333333333333e-07,
      "loss": 1.613,
      "step": 4010
    },
    {
      "epoch": 730.9090909090909,
      "grad_norm": 1.5327136516571045,
      "learning_rate": 2.68e-07,
      "loss": 1.6489,
      "step": 4020
    },
    {
      "epoch": 732.7272727272727,
      "grad_norm": 1.4733402729034424,
      "learning_rate": 2.6866666666666666e-07,
      "loss": 1.5961,
      "step": 4030
    },
    {
      "epoch": 734.5454545454545,
      "grad_norm": 1.9657827615737915,
      "learning_rate": 2.693333333333333e-07,
      "loss": 1.6352,
      "step": 4040
    },
    {
      "epoch": 736.3636363636364,
      "grad_norm": 1.8400249481201172,
      "learning_rate": 2.7e-07,
      "loss": 1.5897,
      "step": 4050
    },
    {
      "epoch": 738.1818181818181,
      "grad_norm": 1.736219882965088,
      "learning_rate": 2.7066666666666666e-07,
      "loss": 1.6112,
      "step": 4060
    },
    {
      "epoch": 740.0,
      "grad_norm": 1.6296974420547485,
      "learning_rate": 2.713333333333333e-07,
      "loss": 1.6061,
      "step": 4070
    },
    {
      "epoch": 741.8181818181819,
      "grad_norm": 1.7390908002853394,
      "learning_rate": 2.72e-07,
      "loss": 1.599,
      "step": 4080
    },
    {
      "epoch": 743.6363636363636,
      "grad_norm": 1.5156580209732056,
      "learning_rate": 2.7266666666666665e-07,
      "loss": 1.5845,
      "step": 4090
    },
    {
      "epoch": 745.4545454545455,
      "grad_norm": 1.9427767992019653,
      "learning_rate": 2.733333333333333e-07,
      "loss": 1.5736,
      "step": 4100
    },
    {
      "epoch": 747.2727272727273,
      "grad_norm": 1.865661382675171,
      "learning_rate": 2.74e-07,
      "loss": 1.6019,
      "step": 4110
    },
    {
      "epoch": 749.0909090909091,
      "grad_norm": 1.5543873310089111,
      "learning_rate": 2.7466666666666664e-07,
      "loss": 1.5841,
      "step": 4120
    },
    {
      "epoch": 750.9090909090909,
      "grad_norm": 1.5714480876922607,
      "learning_rate": 2.753333333333333e-07,
      "loss": 1.5698,
      "step": 4130
    },
    {
      "epoch": 752.7272727272727,
      "grad_norm": 1.5280849933624268,
      "learning_rate": 2.7600000000000004e-07,
      "loss": 1.5459,
      "step": 4140
    },
    {
      "epoch": 754.5454545454545,
      "grad_norm": 1.5567679405212402,
      "learning_rate": 2.766666666666667e-07,
      "loss": 1.6063,
      "step": 4150
    },
    {
      "epoch": 756.3636363636364,
      "grad_norm": 1.6821800470352173,
      "learning_rate": 2.7733333333333333e-07,
      "loss": 1.5311,
      "step": 4160
    },
    {
      "epoch": 758.1818181818181,
      "grad_norm": 1.8452264070510864,
      "learning_rate": 2.7800000000000003e-07,
      "loss": 1.5526,
      "step": 4170
    },
    {
      "epoch": 760.0,
      "grad_norm": 1.7175532579421997,
      "learning_rate": 2.786666666666667e-07,
      "loss": 1.5588,
      "step": 4180
    },
    {
      "epoch": 761.8181818181819,
      "grad_norm": 1.7406929731369019,
      "learning_rate": 2.793333333333333e-07,
      "loss": 1.5519,
      "step": 4190
    },
    {
      "epoch": 763.6363636363636,
      "grad_norm": 1.54957115650177,
      "learning_rate": 2.8e-07,
      "loss": 1.5524,
      "step": 4200
    },
    {
      "epoch": 765.4545454545455,
      "grad_norm": 1.79135262966156,
      "learning_rate": 2.8066666666666667e-07,
      "loss": 1.5172,
      "step": 4210
    },
    {
      "epoch": 767.2727272727273,
      "grad_norm": 1.7867939472198486,
      "learning_rate": 2.813333333333333e-07,
      "loss": 1.5254,
      "step": 4220
    },
    {
      "epoch": 769.0909090909091,
      "grad_norm": 1.756966233253479,
      "learning_rate": 2.8199999999999996e-07,
      "loss": 1.5392,
      "step": 4230
    },
    {
      "epoch": 770.9090909090909,
      "grad_norm": 1.722519040107727,
      "learning_rate": 2.8266666666666666e-07,
      "loss": 1.505,
      "step": 4240
    },
    {
      "epoch": 772.7272727272727,
      "grad_norm": 1.695894718170166,
      "learning_rate": 2.833333333333333e-07,
      "loss": 1.5368,
      "step": 4250
    },
    {
      "epoch": 774.5454545454545,
      "grad_norm": 1.7829127311706543,
      "learning_rate": 2.8399999999999995e-07,
      "loss": 1.5016,
      "step": 4260
    },
    {
      "epoch": 776.3636363636364,
      "grad_norm": 1.684888243675232,
      "learning_rate": 2.8466666666666665e-07,
      "loss": 1.4897,
      "step": 4270
    },
    {
      "epoch": 778.1818181818181,
      "grad_norm": 1.677933692932129,
      "learning_rate": 2.853333333333333e-07,
      "loss": 1.5003,
      "step": 4280
    },
    {
      "epoch": 780.0,
      "grad_norm": 1.6997674703598022,
      "learning_rate": 2.8599999999999994e-07,
      "loss": 1.506,
      "step": 4290
    },
    {
      "epoch": 781.8181818181819,
      "grad_norm": 1.7868560552597046,
      "learning_rate": 2.866666666666667e-07,
      "loss": 1.4788,
      "step": 4300
    },
    {
      "epoch": 783.6363636363636,
      "grad_norm": 1.967398762702942,
      "learning_rate": 2.8733333333333334e-07,
      "loss": 1.4803,
      "step": 4310
    },
    {
      "epoch": 785.4545454545455,
      "grad_norm": 1.6654828786849976,
      "learning_rate": 2.88e-07,
      "loss": 1.5154,
      "step": 4320
    },
    {
      "epoch": 787.2727272727273,
      "grad_norm": 1.8002817630767822,
      "learning_rate": 2.886666666666667e-07,
      "loss": 1.4467,
      "step": 4330
    },
    {
      "epoch": 789.0909090909091,
      "grad_norm": 1.9090973138809204,
      "learning_rate": 2.8933333333333333e-07,
      "loss": 1.4752,
      "step": 4340
    },
    {
      "epoch": 790.9090909090909,
      "grad_norm": 1.8544310331344604,
      "learning_rate": 2.9e-07,
      "loss": 1.483,
      "step": 4350
    },
    {
      "epoch": 792.7272727272727,
      "grad_norm": 1.6908529996871948,
      "learning_rate": 2.906666666666667e-07,
      "loss": 1.4543,
      "step": 4360
    },
    {
      "epoch": 794.5454545454545,
      "grad_norm": 1.9053895473480225,
      "learning_rate": 2.913333333333333e-07,
      "loss": 1.4427,
      "step": 4370
    },
    {
      "epoch": 796.3636363636364,
      "grad_norm": 1.7683804035186768,
      "learning_rate": 2.9199999999999997e-07,
      "loss": 1.4701,
      "step": 4380
    },
    {
      "epoch": 798.1818181818181,
      "grad_norm": 1.4864236116409302,
      "learning_rate": 2.9266666666666667e-07,
      "loss": 1.4411,
      "step": 4390
    },
    {
      "epoch": 800.0,
      "grad_norm": 1.6474109888076782,
      "learning_rate": 2.933333333333333e-07,
      "loss": 1.4261,
      "step": 4400
    },
    {
      "epoch": 801.8181818181819,
      "grad_norm": 1.835069179534912,
      "learning_rate": 2.9399999999999996e-07,
      "loss": 1.4251,
      "step": 4410
    },
    {
      "epoch": 803.6363636363636,
      "grad_norm": 1.5838223695755005,
      "learning_rate": 2.9466666666666666e-07,
      "loss": 1.4486,
      "step": 4420
    },
    {
      "epoch": 805.4545454545455,
      "grad_norm": 1.5537333488464355,
      "learning_rate": 2.953333333333333e-07,
      "loss": 1.4021,
      "step": 4430
    },
    {
      "epoch": 807.2727272727273,
      "grad_norm": 1.7476909160614014,
      "learning_rate": 2.9599999999999995e-07,
      "loss": 1.4337,
      "step": 4440
    },
    {
      "epoch": 809.0909090909091,
      "grad_norm": 1.6083753108978271,
      "learning_rate": 2.966666666666667e-07,
      "loss": 1.4288,
      "step": 4450
    },
    {
      "epoch": 810.9090909090909,
      "grad_norm": 1.914032220840454,
      "learning_rate": 2.9733333333333335e-07,
      "loss": 1.4358,
      "step": 4460
    },
    {
      "epoch": 812.7272727272727,
      "grad_norm": 1.684127926826477,
      "learning_rate": 2.98e-07,
      "loss": 1.4035,
      "step": 4470
    },
    {
      "epoch": 814.5454545454545,
      "grad_norm": 1.6072998046875,
      "learning_rate": 2.986666666666667e-07,
      "loss": 1.3834,
      "step": 4480
    },
    {
      "epoch": 816.3636363636364,
      "grad_norm": 1.6508171558380127,
      "learning_rate": 2.9933333333333334e-07,
      "loss": 1.4015,
      "step": 4490
    },
    {
      "epoch": 818.1818181818181,
      "grad_norm": 1.638367772102356,
      "learning_rate": 3e-07,
      "loss": 1.4098,
      "step": 4500
    },
    {
      "epoch": 818.1818181818181,
      "eval_loss": 1.565694808959961,
      "eval_runtime": 0.9544,
      "eval_samples_per_second": 10.478,
      "eval_steps_per_second": 5.239,
      "step": 4500
    },
    {
      "epoch": 820.0,
      "grad_norm": 1.5054240226745605,
      "learning_rate": 3.006666666666667e-07,
      "loss": 1.3987,
      "step": 4510
    },
    {
      "epoch": 821.8181818181819,
      "grad_norm": 1.8475672006607056,
      "learning_rate": 3.0133333333333333e-07,
      "loss": 1.3817,
      "step": 4520
    },
    {
      "epoch": 823.6363636363636,
      "grad_norm": 1.469794750213623,
      "learning_rate": 3.02e-07,
      "loss": 1.3619,
      "step": 4530
    },
    {
      "epoch": 825.4545454545455,
      "grad_norm": 1.790298342704773,
      "learning_rate": 3.026666666666666e-07,
      "loss": 1.4112,
      "step": 4540
    },
    {
      "epoch": 827.2727272727273,
      "grad_norm": 1.6739113330841064,
      "learning_rate": 3.033333333333333e-07,
      "loss": 1.353,
      "step": 4550
    },
    {
      "epoch": 829.0909090909091,
      "grad_norm": 1.96330726146698,
      "learning_rate": 3.0399999999999997e-07,
      "loss": 1.3709,
      "step": 4560
    },
    {
      "epoch": 830.9090909090909,
      "grad_norm": 1.6306071281433105,
      "learning_rate": 3.046666666666666e-07,
      "loss": 1.3825,
      "step": 4570
    },
    {
      "epoch": 832.7272727272727,
      "grad_norm": 1.6618930101394653,
      "learning_rate": 3.053333333333333e-07,
      "loss": 1.3494,
      "step": 4580
    },
    {
      "epoch": 834.5454545454545,
      "grad_norm": 1.820796251296997,
      "learning_rate": 3.0599999999999996e-07,
      "loss": 1.3435,
      "step": 4590
    },
    {
      "epoch": 836.3636363636364,
      "grad_norm": 1.7538174390792847,
      "learning_rate": 3.066666666666666e-07,
      "loss": 1.3523,
      "step": 4600
    },
    {
      "epoch": 838.1818181818181,
      "grad_norm": 1.3697588443756104,
      "learning_rate": 3.0733333333333336e-07,
      "loss": 1.3797,
      "step": 4610
    },
    {
      "epoch": 840.0,
      "grad_norm": 1.3293830156326294,
      "learning_rate": 3.08e-07,
      "loss": 1.3332,
      "step": 4620
    },
    {
      "epoch": 841.8181818181819,
      "grad_norm": 1.2273175716400146,
      "learning_rate": 3.0866666666666665e-07,
      "loss": 1.3608,
      "step": 4630
    },
    {
      "epoch": 843.6363636363636,
      "grad_norm": 1.5206278562545776,
      "learning_rate": 3.0933333333333335e-07,
      "loss": 1.304,
      "step": 4640
    },
    {
      "epoch": 845.4545454545455,
      "grad_norm": 1.7586361169815063,
      "learning_rate": 3.1e-07,
      "loss": 1.3289,
      "step": 4650
    },
    {
      "epoch": 847.2727272727273,
      "grad_norm": 1.5265291929244995,
      "learning_rate": 3.1066666666666664e-07,
      "loss": 1.3381,
      "step": 4660
    },
    {
      "epoch": 849.0909090909091,
      "grad_norm": 1.6653039455413818,
      "learning_rate": 3.1133333333333334e-07,
      "loss": 1.3177,
      "step": 4670
    },
    {
      "epoch": 850.9090909090909,
      "grad_norm": 1.5154094696044922,
      "learning_rate": 3.12e-07,
      "loss": 1.3213,
      "step": 4680
    },
    {
      "epoch": 852.7272727272727,
      "grad_norm": 1.7256505489349365,
      "learning_rate": 3.1266666666666663e-07,
      "loss": 1.3252,
      "step": 4690
    },
    {
      "epoch": 854.5454545454545,
      "grad_norm": 1.6247444152832031,
      "learning_rate": 3.1333333333333333e-07,
      "loss": 1.2971,
      "step": 4700
    },
    {
      "epoch": 856.3636363636364,
      "grad_norm": 1.4296090602874756,
      "learning_rate": 3.14e-07,
      "loss": 1.292,
      "step": 4710
    },
    {
      "epoch": 858.1818181818181,
      "grad_norm": 1.3487277030944824,
      "learning_rate": 3.146666666666666e-07,
      "loss": 1.3228,
      "step": 4720
    },
    {
      "epoch": 860.0,
      "grad_norm": 1.3449163436889648,
      "learning_rate": 3.153333333333333e-07,
      "loss": 1.282,
      "step": 4730
    },
    {
      "epoch": 861.8181818181819,
      "grad_norm": 1.5308866500854492,
      "learning_rate": 3.1599999999999997e-07,
      "loss": 1.2898,
      "step": 4740
    },
    {
      "epoch": 863.6363636363636,
      "grad_norm": 1.5180584192276,
      "learning_rate": 3.166666666666666e-07,
      "loss": 1.3071,
      "step": 4750
    },
    {
      "epoch": 865.4545454545455,
      "grad_norm": 1.3579294681549072,
      "learning_rate": 3.173333333333333e-07,
      "loss": 1.287,
      "step": 4760
    },
    {
      "epoch": 867.2727272727273,
      "grad_norm": 1.5822529792785645,
      "learning_rate": 3.18e-07,
      "loss": 1.2618,
      "step": 4770
    },
    {
      "epoch": 869.0909090909091,
      "grad_norm": 1.4560478925704956,
      "learning_rate": 3.1866666666666666e-07,
      "loss": 1.2844,
      "step": 4780
    },
    {
      "epoch": 870.9090909090909,
      "grad_norm": 1.5568621158599854,
      "learning_rate": 3.1933333333333336e-07,
      "loss": 1.2878,
      "step": 4790
    },
    {
      "epoch": 872.7272727272727,
      "grad_norm": 1.4080936908721924,
      "learning_rate": 3.2e-07,
      "loss": 1.2569,
      "step": 4800
    },
    {
      "epoch": 874.5454545454545,
      "grad_norm": 1.2296874523162842,
      "learning_rate": 3.2066666666666665e-07,
      "loss": 1.285,
      "step": 4810
    },
    {
      "epoch": 876.3636363636364,
      "grad_norm": 1.508251667022705,
      "learning_rate": 3.2133333333333335e-07,
      "loss": 1.2645,
      "step": 4820
    },
    {
      "epoch": 878.1818181818181,
      "grad_norm": 1.2223490476608276,
      "learning_rate": 3.22e-07,
      "loss": 1.2682,
      "step": 4830
    },
    {
      "epoch": 880.0,
      "grad_norm": 1.6025744676589966,
      "learning_rate": 3.2266666666666664e-07,
      "loss": 1.2429,
      "step": 4840
    },
    {
      "epoch": 881.8181818181819,
      "grad_norm": 1.3502150774002075,
      "learning_rate": 3.233333333333333e-07,
      "loss": 1.2716,
      "step": 4850
    },
    {
      "epoch": 883.6363636363636,
      "grad_norm": 1.60727059841156,
      "learning_rate": 3.24e-07,
      "loss": 1.2404,
      "step": 4860
    },
    {
      "epoch": 885.4545454545455,
      "grad_norm": 1.3670538663864136,
      "learning_rate": 3.2466666666666664e-07,
      "loss": 1.2389,
      "step": 4870
    },
    {
      "epoch": 887.2727272727273,
      "grad_norm": 1.2798593044281006,
      "learning_rate": 3.253333333333333e-07,
      "loss": 1.2454,
      "step": 4880
    },
    {
      "epoch": 889.0909090909091,
      "grad_norm": 1.55330228805542,
      "learning_rate": 3.26e-07,
      "loss": 1.2223,
      "step": 4890
    },
    {
      "epoch": 890.9090909090909,
      "grad_norm": 1.5025821924209595,
      "learning_rate": 3.2666666666666663e-07,
      "loss": 1.2478,
      "step": 4900
    },
    {
      "epoch": 892.7272727272727,
      "grad_norm": 1.387189269065857,
      "learning_rate": 3.2733333333333327e-07,
      "loss": 1.238,
      "step": 4910
    },
    {
      "epoch": 894.5454545454545,
      "grad_norm": 1.3436709642410278,
      "learning_rate": 3.28e-07,
      "loss": 1.2042,
      "step": 4920
    },
    {
      "epoch": 896.3636363636364,
      "grad_norm": 1.3701910972595215,
      "learning_rate": 3.2866666666666667e-07,
      "loss": 1.2308,
      "step": 4930
    },
    {
      "epoch": 898.1818181818181,
      "grad_norm": 1.3237464427947998,
      "learning_rate": 3.293333333333333e-07,
      "loss": 1.2408,
      "step": 4940
    },
    {
      "epoch": 900.0,
      "grad_norm": 1.2443355321884155,
      "learning_rate": 3.3e-07,
      "loss": 1.2148,
      "step": 4950
    },
    {
      "epoch": 901.8181818181819,
      "grad_norm": 1.412661075592041,
      "learning_rate": 3.3066666666666666e-07,
      "loss": 1.2127,
      "step": 4960
    },
    {
      "epoch": 903.6363636363636,
      "grad_norm": 1.441908359527588,
      "learning_rate": 3.313333333333333e-07,
      "loss": 1.213,
      "step": 4970
    },
    {
      "epoch": 905.4545454545455,
      "grad_norm": 1.253012776374817,
      "learning_rate": 3.32e-07,
      "loss": 1.2128,
      "step": 4980
    },
    {
      "epoch": 907.2727272727273,
      "grad_norm": 1.4214426279067993,
      "learning_rate": 3.3266666666666665e-07,
      "loss": 1.1846,
      "step": 4990
    },
    {
      "epoch": 909.0909090909091,
      "grad_norm": 1.4703054428100586,
      "learning_rate": 3.333333333333333e-07,
      "loss": 1.2203,
      "step": 5000
    },
    {
      "epoch": 909.0909090909091,
      "eval_loss": 1.4128847122192383,
      "eval_runtime": 0.9532,
      "eval_samples_per_second": 10.49,
      "eval_steps_per_second": 5.245,
      "step": 5000
    },
    {
      "epoch": 910.9090909090909,
      "grad_norm": 1.3790068626403809,
      "learning_rate": 3.34e-07,
      "loss": 1.1732,
      "step": 5010
    },
    {
      "epoch": 912.7272727272727,
      "grad_norm": 1.3052978515625,
      "learning_rate": 3.3466666666666665e-07,
      "loss": 1.2029,
      "step": 5020
    },
    {
      "epoch": 914.5454545454545,
      "grad_norm": 1.2235702276229858,
      "learning_rate": 3.353333333333333e-07,
      "loss": 1.1928,
      "step": 5030
    },
    {
      "epoch": 916.3636363636364,
      "grad_norm": 1.247785210609436,
      "learning_rate": 3.36e-07,
      "loss": 1.1776,
      "step": 5040
    },
    {
      "epoch": 918.1818181818181,
      "grad_norm": 1.59340500831604,
      "learning_rate": 3.3666666666666664e-07,
      "loss": 1.2043,
      "step": 5050
    },
    {
      "epoch": 920.0,
      "grad_norm": 1.4434839487075806,
      "learning_rate": 3.373333333333333e-07,
      "loss": 1.1799,
      "step": 5060
    },
    {
      "epoch": 921.8181818181819,
      "grad_norm": 1.1871370077133179,
      "learning_rate": 3.38e-07,
      "loss": 1.1705,
      "step": 5070
    },
    {
      "epoch": 923.6363636363636,
      "grad_norm": 1.4971693754196167,
      "learning_rate": 3.386666666666667e-07,
      "loss": 1.1838,
      "step": 5080
    },
    {
      "epoch": 925.4545454545455,
      "grad_norm": 1.8545368909835815,
      "learning_rate": 3.3933333333333333e-07,
      "loss": 1.1847,
      "step": 5090
    },
    {
      "epoch": 927.2727272727273,
      "grad_norm": 1.3627116680145264,
      "learning_rate": 3.4000000000000003e-07,
      "loss": 1.1755,
      "step": 5100
    },
    {
      "epoch": 929.0909090909091,
      "grad_norm": 1.4424703121185303,
      "learning_rate": 3.4066666666666667e-07,
      "loss": 1.1633,
      "step": 5110
    },
    {
      "epoch": 930.9090909090909,
      "grad_norm": 1.3832200765609741,
      "learning_rate": 3.413333333333333e-07,
      "loss": 1.1485,
      "step": 5120
    },
    {
      "epoch": 932.7272727272727,
      "grad_norm": 1.3688663244247437,
      "learning_rate": 3.42e-07,
      "loss": 1.1855,
      "step": 5130
    },
    {
      "epoch": 934.5454545454545,
      "grad_norm": 1.261146903038025,
      "learning_rate": 3.4266666666666666e-07,
      "loss": 1.1456,
      "step": 5140
    },
    {
      "epoch": 936.3636363636364,
      "grad_norm": 1.273314356803894,
      "learning_rate": 3.433333333333333e-07,
      "loss": 1.1861,
      "step": 5150
    },
    {
      "epoch": 938.1818181818181,
      "grad_norm": 1.3565126657485962,
      "learning_rate": 3.4399999999999996e-07,
      "loss": 1.1343,
      "step": 5160
    },
    {
      "epoch": 940.0,
      "grad_norm": 1.4812687635421753,
      "learning_rate": 3.4466666666666666e-07,
      "loss": 1.1529,
      "step": 5170
    },
    {
      "epoch": 941.8181818181819,
      "grad_norm": 1.3484407663345337,
      "learning_rate": 3.453333333333333e-07,
      "loss": 1.1216,
      "step": 5180
    },
    {
      "epoch": 943.6363636363636,
      "grad_norm": 1.4055310487747192,
      "learning_rate": 3.4599999999999995e-07,
      "loss": 1.1519,
      "step": 5190
    },
    {
      "epoch": 945.4545454545455,
      "grad_norm": 1.4728691577911377,
      "learning_rate": 3.4666666666666665e-07,
      "loss": 1.1509,
      "step": 5200
    },
    {
      "epoch": 947.2727272727273,
      "grad_norm": 1.3399076461791992,
      "learning_rate": 3.473333333333333e-07,
      "loss": 1.1511,
      "step": 5210
    },
    {
      "epoch": 949.0909090909091,
      "grad_norm": 1.2443170547485352,
      "learning_rate": 3.4799999999999994e-07,
      "loss": 1.1559,
      "step": 5220
    },
    {
      "epoch": 950.9090909090909,
      "grad_norm": 1.2211833000183105,
      "learning_rate": 3.4866666666666664e-07,
      "loss": 1.1461,
      "step": 5230
    },
    {
      "epoch": 952.7272727272727,
      "grad_norm": 1.5236725807189941,
      "learning_rate": 3.4933333333333334e-07,
      "loss": 1.0969,
      "step": 5240
    },
    {
      "epoch": 954.5454545454545,
      "grad_norm": 1.3108165264129639,
      "learning_rate": 3.5e-07,
      "loss": 1.1079,
      "step": 5250
    },
    {
      "epoch": 956.3636363636364,
      "grad_norm": 1.4499270915985107,
      "learning_rate": 3.506666666666667e-07,
      "loss": 1.195,
      "step": 5260
    },
    {
      "epoch": 958.1818181818181,
      "grad_norm": 1.341651439666748,
      "learning_rate": 3.5133333333333333e-07,
      "loss": 1.099,
      "step": 5270
    },
    {
      "epoch": 960.0,
      "grad_norm": 1.4866071939468384,
      "learning_rate": 3.52e-07,
      "loss": 1.1271,
      "step": 5280
    },
    {
      "epoch": 961.8181818181819,
      "grad_norm": 1.3283075094223022,
      "learning_rate": 3.526666666666667e-07,
      "loss": 1.1294,
      "step": 5290
    },
    {
      "epoch": 963.6363636363636,
      "grad_norm": 1.282545566558838,
      "learning_rate": 3.533333333333333e-07,
      "loss": 1.099,
      "step": 5300
    },
    {
      "epoch": 965.4545454545455,
      "grad_norm": 1.275543212890625,
      "learning_rate": 3.5399999999999997e-07,
      "loss": 1.1156,
      "step": 5310
    },
    {
      "epoch": 967.2727272727273,
      "grad_norm": 1.2308323383331299,
      "learning_rate": 3.5466666666666667e-07,
      "loss": 1.1339,
      "step": 5320
    },
    {
      "epoch": 969.0909090909091,
      "grad_norm": 1.3455935716629028,
      "learning_rate": 3.553333333333333e-07,
      "loss": 1.1313,
      "step": 5330
    },
    {
      "epoch": 970.9090909090909,
      "grad_norm": 1.2562224864959717,
      "learning_rate": 3.5599999999999996e-07,
      "loss": 1.0809,
      "step": 5340
    },
    {
      "epoch": 972.7272727272727,
      "grad_norm": 1.2826247215270996,
      "learning_rate": 3.5666666666666666e-07,
      "loss": 1.1161,
      "step": 5350
    },
    {
      "epoch": 974.5454545454545,
      "grad_norm": 1.325972080230713,
      "learning_rate": 3.573333333333333e-07,
      "loss": 1.1377,
      "step": 5360
    },
    {
      "epoch": 976.3636363636364,
      "grad_norm": 1.3550385236740112,
      "learning_rate": 3.5799999999999995e-07,
      "loss": 1.1055,
      "step": 5370
    },
    {
      "epoch": 978.1818181818181,
      "grad_norm": 1.2024813890457153,
      "learning_rate": 3.5866666666666665e-07,
      "loss": 1.1031,
      "step": 5380
    },
    {
      "epoch": 980.0,
      "grad_norm": 1.141525149345398,
      "learning_rate": 3.5933333333333335e-07,
      "loss": 1.0883,
      "step": 5390
    },
    {
      "epoch": 981.8181818181819,
      "grad_norm": 1.3663965463638306,
      "learning_rate": 3.6e-07,
      "loss": 1.0811,
      "step": 5400
    },
    {
      "epoch": 983.6363636363636,
      "grad_norm": 1.5425255298614502,
      "learning_rate": 3.606666666666667e-07,
      "loss": 1.0577,
      "step": 5410
    },
    {
      "epoch": 985.4545454545455,
      "grad_norm": 1.2204487323760986,
      "learning_rate": 3.6133333333333334e-07,
      "loss": 1.1315,
      "step": 5420
    },
    {
      "epoch": 987.2727272727273,
      "grad_norm": 1.4171373844146729,
      "learning_rate": 3.62e-07,
      "loss": 1.0971,
      "step": 5430
    },
    {
      "epoch": 989.0909090909091,
      "grad_norm": 1.3774205446243286,
      "learning_rate": 3.626666666666667e-07,
      "loss": 1.0814,
      "step": 5440
    },
    {
      "epoch": 990.9090909090909,
      "grad_norm": 1.4074076414108276,
      "learning_rate": 3.6333333333333333e-07,
      "loss": 1.1058,
      "step": 5450
    },
    {
      "epoch": 992.7272727272727,
      "grad_norm": 1.135764241218567,
      "learning_rate": 3.64e-07,
      "loss": 1.0403,
      "step": 5460
    },
    {
      "epoch": 994.5454545454545,
      "grad_norm": 1.3857966661453247,
      "learning_rate": 3.646666666666666e-07,
      "loss": 1.1153,
      "step": 5470
    },
    {
      "epoch": 996.3636363636364,
      "grad_norm": 1.5438176393508911,
      "learning_rate": 3.653333333333333e-07,
      "loss": 1.0727,
      "step": 5480
    },
    {
      "epoch": 998.1818181818181,
      "grad_norm": 1.2029201984405518,
      "learning_rate": 3.6599999999999997e-07,
      "loss": 1.0905,
      "step": 5490
    },
    {
      "epoch": 1000.0,
      "grad_norm": 1.3346047401428223,
      "learning_rate": 3.666666666666666e-07,
      "loss": 1.0676,
      "step": 5500
    },
    {
      "epoch": 1000.0,
      "eval_loss": 1.3462306261062622,
      "eval_runtime": 0.9517,
      "eval_samples_per_second": 10.508,
      "eval_steps_per_second": 5.254,
      "step": 5500
    },
    {
      "epoch": 1001.8181818181819,
      "grad_norm": 1.3304412364959717,
      "learning_rate": 3.673333333333333e-07,
      "loss": 1.044,
      "step": 5510
    },
    {
      "epoch": 1003.6363636363636,
      "grad_norm": 1.298790454864502,
      "learning_rate": 3.6799999999999996e-07,
      "loss": 1.0843,
      "step": 5520
    },
    {
      "epoch": 1005.4545454545455,
      "grad_norm": 1.3537054061889648,
      "learning_rate": 3.686666666666666e-07,
      "loss": 1.0506,
      "step": 5530
    },
    {
      "epoch": 1007.2727272727273,
      "grad_norm": 1.2657089233398438,
      "learning_rate": 3.693333333333333e-07,
      "loss": 1.0833,
      "step": 5540
    },
    {
      "epoch": 1009.0909090909091,
      "grad_norm": 1.3918687105178833,
      "learning_rate": 3.7e-07,
      "loss": 1.0543,
      "step": 5550
    },
    {
      "epoch": 1010.9090909090909,
      "grad_norm": 1.4295992851257324,
      "learning_rate": 3.7066666666666665e-07,
      "loss": 1.069,
      "step": 5560
    },
    {
      "epoch": 1012.7272727272727,
      "grad_norm": 1.6673856973648071,
      "learning_rate": 3.7133333333333335e-07,
      "loss": 1.0368,
      "step": 5570
    },
    {
      "epoch": 1014.5454545454545,
      "grad_norm": 1.3862154483795166,
      "learning_rate": 3.72e-07,
      "loss": 1.0434,
      "step": 5580
    },
    {
      "epoch": 1016.3636363636364,
      "grad_norm": 1.5501264333724976,
      "learning_rate": 3.7266666666666664e-07,
      "loss": 1.0729,
      "step": 5590
    },
    {
      "epoch": 1018.1818181818181,
      "grad_norm": 1.3041919469833374,
      "learning_rate": 3.7333333333333334e-07,
      "loss": 1.0417,
      "step": 5600
    },
    {
      "epoch": 1020.0,
      "grad_norm": 1.3285092115402222,
      "learning_rate": 3.74e-07,
      "loss": 1.0426,
      "step": 5610
    },
    {
      "epoch": 1021.8181818181819,
      "grad_norm": 1.344909429550171,
      "learning_rate": 3.7466666666666663e-07,
      "loss": 1.0371,
      "step": 5620
    },
    {
      "epoch": 1023.6363636363636,
      "grad_norm": 1.4226034879684448,
      "learning_rate": 3.7533333333333333e-07,
      "loss": 1.0357,
      "step": 5630
    },
    {
      "epoch": 1025.4545454545455,
      "grad_norm": 1.441498875617981,
      "learning_rate": 3.76e-07,
      "loss": 1.0494,
      "step": 5640
    },
    {
      "epoch": 1027.2727272727273,
      "grad_norm": 1.3398057222366333,
      "learning_rate": 3.766666666666666e-07,
      "loss": 1.0108,
      "step": 5650
    },
    {
      "epoch": 1029.090909090909,
      "grad_norm": 1.363767385482788,
      "learning_rate": 3.773333333333333e-07,
      "loss": 1.031,
      "step": 5660
    },
    {
      "epoch": 1030.909090909091,
      "grad_norm": 1.2723124027252197,
      "learning_rate": 3.7799999999999997e-07,
      "loss": 1.0349,
      "step": 5670
    },
    {
      "epoch": 1032.7272727272727,
      "grad_norm": 1.266229510307312,
      "learning_rate": 3.786666666666666e-07,
      "loss": 1.0052,
      "step": 5680
    },
    {
      "epoch": 1034.5454545454545,
      "grad_norm": 1.4878370761871338,
      "learning_rate": 3.793333333333333e-07,
      "loss": 1.0655,
      "step": 5690
    },
    {
      "epoch": 1036.3636363636363,
      "grad_norm": 1.3572548627853394,
      "learning_rate": 3.7999999999999996e-07,
      "loss": 1.0155,
      "step": 5700
    },
    {
      "epoch": 1038.1818181818182,
      "grad_norm": 1.4701292514801025,
      "learning_rate": 3.8066666666666666e-07,
      "loss": 1.0175,
      "step": 5710
    },
    {
      "epoch": 1040.0,
      "grad_norm": 1.4412386417388916,
      "learning_rate": 3.8133333333333336e-07,
      "loss": 1.0227,
      "step": 5720
    },
    {
      "epoch": 1041.8181818181818,
      "grad_norm": 1.3904752731323242,
      "learning_rate": 3.82e-07,
      "loss": 1.0286,
      "step": 5730
    },
    {
      "epoch": 1043.6363636363637,
      "grad_norm": 1.432132363319397,
      "learning_rate": 3.8266666666666665e-07,
      "loss": 1.0068,
      "step": 5740
    },
    {
      "epoch": 1045.4545454545455,
      "grad_norm": 1.4684776067733765,
      "learning_rate": 3.8333333333333335e-07,
      "loss": 1.0067,
      "step": 5750
    },
    {
      "epoch": 1047.2727272727273,
      "grad_norm": 1.1846470832824707,
      "learning_rate": 3.84e-07,
      "loss": 1.0169,
      "step": 5760
    },
    {
      "epoch": 1049.090909090909,
      "grad_norm": 1.3416780233383179,
      "learning_rate": 3.8466666666666664e-07,
      "loss": 0.9996,
      "step": 5770
    },
    {
      "epoch": 1050.909090909091,
      "grad_norm": 1.2453824281692505,
      "learning_rate": 3.8533333333333334e-07,
      "loss": 1.017,
      "step": 5780
    },
    {
      "epoch": 1052.7272727272727,
      "grad_norm": 1.5091649293899536,
      "learning_rate": 3.86e-07,
      "loss": 0.9998,
      "step": 5790
    },
    {
      "epoch": 1054.5454545454545,
      "grad_norm": 1.3647558689117432,
      "learning_rate": 3.8666666666666664e-07,
      "loss": 1.0226,
      "step": 5800
    },
    {
      "epoch": 1056.3636363636363,
      "grad_norm": 1.470717191696167,
      "learning_rate": 3.873333333333333e-07,
      "loss": 1.0179,
      "step": 5810
    },
    {
      "epoch": 1058.1818181818182,
      "grad_norm": 1.4047902822494507,
      "learning_rate": 3.88e-07,
      "loss": 0.977,
      "step": 5820
    },
    {
      "epoch": 1060.0,
      "grad_norm": 1.4461793899536133,
      "learning_rate": 3.8866666666666663e-07,
      "loss": 1.0004,
      "step": 5830
    },
    {
      "epoch": 1061.8181818181818,
      "grad_norm": 1.5699692964553833,
      "learning_rate": 3.8933333333333327e-07,
      "loss": 0.9958,
      "step": 5840
    },
    {
      "epoch": 1063.6363636363637,
      "grad_norm": 1.467279076576233,
      "learning_rate": 3.8999999999999997e-07,
      "loss": 0.9652,
      "step": 5850
    },
    {
      "epoch": 1065.4545454545455,
      "grad_norm": 1.4261043071746826,
      "learning_rate": 3.906666666666666e-07,
      "loss": 1.0311,
      "step": 5860
    },
    {
      "epoch": 1067.2727272727273,
      "grad_norm": 1.3169513940811157,
      "learning_rate": 3.913333333333333e-07,
      "loss": 0.9932,
      "step": 5870
    },
    {
      "epoch": 1069.090909090909,
      "grad_norm": 1.5316740274429321,
      "learning_rate": 3.92e-07,
      "loss": 0.9691,
      "step": 5880
    },
    {
      "epoch": 1070.909090909091,
      "grad_norm": 1.5663467645645142,
      "learning_rate": 3.9266666666666666e-07,
      "loss": 0.983,
      "step": 5890
    },
    {
      "epoch": 1072.7272727272727,
      "grad_norm": 1.2610584497451782,
      "learning_rate": 3.933333333333333e-07,
      "loss": 0.9937,
      "step": 5900
    },
    {
      "epoch": 1074.5454545454545,
      "grad_norm": 1.4744166135787964,
      "learning_rate": 3.94e-07,
      "loss": 0.9667,
      "step": 5910
    },
    {
      "epoch": 1076.3636363636363,
      "grad_norm": 1.5395832061767578,
      "learning_rate": 3.9466666666666665e-07,
      "loss": 0.9668,
      "step": 5920
    },
    {
      "epoch": 1078.1818181818182,
      "grad_norm": 1.344592809677124,
      "learning_rate": 3.953333333333333e-07,
      "loss": 0.943,
      "step": 5930
    },
    {
      "epoch": 1080.0,
      "grad_norm": 1.3554693460464478,
      "learning_rate": 3.96e-07,
      "loss": 0.9928,
      "step": 5940
    },
    {
      "epoch": 1081.8181818181818,
      "grad_norm": 1.5150583982467651,
      "learning_rate": 3.9666666666666665e-07,
      "loss": 0.9663,
      "step": 5950
    },
    {
      "epoch": 1083.6363636363637,
      "grad_norm": 1.5407480001449585,
      "learning_rate": 3.973333333333333e-07,
      "loss": 0.9777,
      "step": 5960
    },
    {
      "epoch": 1085.4545454545455,
      "grad_norm": 1.5109823942184448,
      "learning_rate": 3.98e-07,
      "loss": 0.9553,
      "step": 5970
    },
    {
      "epoch": 1087.2727272727273,
      "grad_norm": 1.8861560821533203,
      "learning_rate": 3.9866666666666664e-07,
      "loss": 0.9881,
      "step": 5980
    },
    {
      "epoch": 1089.090909090909,
      "grad_norm": 1.2048299312591553,
      "learning_rate": 3.993333333333333e-07,
      "loss": 0.9331,
      "step": 5990
    },
    {
      "epoch": 1090.909090909091,
      "grad_norm": 1.5113413333892822,
      "learning_rate": 4e-07,
      "loss": 0.9742,
      "step": 6000
    },
    {
      "epoch": 1090.909090909091,
      "eval_loss": 1.3155276775360107,
      "eval_runtime": 0.9514,
      "eval_samples_per_second": 10.511,
      "eval_steps_per_second": 5.256,
      "step": 6000
    },
    {
      "epoch": 1092.7272727272727,
      "grad_norm": 1.5008517503738403,
      "learning_rate": 4.0066666666666663e-07,
      "loss": 0.9486,
      "step": 6010
    },
    {
      "epoch": 1094.5454545454545,
      "grad_norm": 1.5418834686279297,
      "learning_rate": 4.0133333333333333e-07,
      "loss": 0.9427,
      "step": 6020
    },
    {
      "epoch": 1096.3636363636363,
      "grad_norm": 1.5900077819824219,
      "learning_rate": 4.02e-07,
      "loss": 0.9647,
      "step": 6030
    },
    {
      "epoch": 1098.1818181818182,
      "grad_norm": 1.5776609182357788,
      "learning_rate": 4.0266666666666667e-07,
      "loss": 0.9477,
      "step": 6040
    },
    {
      "epoch": 1100.0,
      "grad_norm": 1.4288498163223267,
      "learning_rate": 4.033333333333333e-07,
      "loss": 0.9325,
      "step": 6050
    },
    {
      "epoch": 1101.8181818181818,
      "grad_norm": 1.4113235473632812,
      "learning_rate": 4.04e-07,
      "loss": 0.9552,
      "step": 6060
    },
    {
      "epoch": 1103.6363636363637,
      "grad_norm": 1.5975077152252197,
      "learning_rate": 4.0466666666666666e-07,
      "loss": 0.9201,
      "step": 6070
    },
    {
      "epoch": 1105.4545454545455,
      "grad_norm": 1.4708911180496216,
      "learning_rate": 4.053333333333333e-07,
      "loss": 0.9312,
      "step": 6080
    },
    {
      "epoch": 1107.2727272727273,
      "grad_norm": 1.6126216650009155,
      "learning_rate": 4.06e-07,
      "loss": 0.9708,
      "step": 6090
    },
    {
      "epoch": 1109.090909090909,
      "grad_norm": 1.6836178302764893,
      "learning_rate": 4.0666666666666666e-07,
      "loss": 0.9294,
      "step": 6100
    },
    {
      "epoch": 1110.909090909091,
      "grad_norm": 1.4609194993972778,
      "learning_rate": 4.073333333333333e-07,
      "loss": 0.9012,
      "step": 6110
    },
    {
      "epoch": 1112.7272727272727,
      "grad_norm": 1.740419626235962,
      "learning_rate": 4.0799999999999995e-07,
      "loss": 0.9348,
      "step": 6120
    },
    {
      "epoch": 1114.5454545454545,
      "grad_norm": 1.3199267387390137,
      "learning_rate": 4.0866666666666665e-07,
      "loss": 0.9547,
      "step": 6130
    },
    {
      "epoch": 1116.3636363636363,
      "grad_norm": 1.539379358291626,
      "learning_rate": 4.093333333333333e-07,
      "loss": 0.9364,
      "step": 6140
    },
    {
      "epoch": 1118.1818181818182,
      "grad_norm": 1.306080937385559,
      "learning_rate": 4.0999999999999994e-07,
      "loss": 0.9073,
      "step": 6150
    },
    {
      "epoch": 1120.0,
      "grad_norm": 1.494415521621704,
      "learning_rate": 4.1066666666666664e-07,
      "loss": 0.9215,
      "step": 6160
    },
    {
      "epoch": 1121.8181818181818,
      "grad_norm": 1.585739016532898,
      "learning_rate": 4.113333333333333e-07,
      "loss": 0.9128,
      "step": 6170
    },
    {
      "epoch": 1123.6363636363637,
      "grad_norm": 1.7619051933288574,
      "learning_rate": 4.12e-07,
      "loss": 0.9305,
      "step": 6180
    },
    {
      "epoch": 1125.4545454545455,
      "grad_norm": 1.6007328033447266,
      "learning_rate": 4.126666666666667e-07,
      "loss": 0.9165,
      "step": 6190
    },
    {
      "epoch": 1127.2727272727273,
      "grad_norm": 1.6364960670471191,
      "learning_rate": 4.1333333333333333e-07,
      "loss": 0.9245,
      "step": 6200
    },
    {
      "epoch": 1129.090909090909,
      "grad_norm": 1.5269947052001953,
      "learning_rate": 4.14e-07,
      "loss": 0.8936,
      "step": 6210
    },
    {
      "epoch": 1130.909090909091,
      "grad_norm": 1.8920910358428955,
      "learning_rate": 4.146666666666667e-07,
      "loss": 0.9203,
      "step": 6220
    },
    {
      "epoch": 1132.7272727272727,
      "grad_norm": 1.644985556602478,
      "learning_rate": 4.153333333333333e-07,
      "loss": 0.9102,
      "step": 6230
    },
    {
      "epoch": 1134.5454545454545,
      "grad_norm": 1.4481048583984375,
      "learning_rate": 4.1599999999999997e-07,
      "loss": 0.8935,
      "step": 6240
    },
    {
      "epoch": 1136.3636363636363,
      "grad_norm": 1.674146294593811,
      "learning_rate": 4.1666666666666667e-07,
      "loss": 0.8991,
      "step": 6250
    },
    {
      "epoch": 1138.1818181818182,
      "grad_norm": 1.9279513359069824,
      "learning_rate": 4.173333333333333e-07,
      "loss": 0.8942,
      "step": 6260
    },
    {
      "epoch": 1140.0,
      "grad_norm": 1.9510678052902222,
      "learning_rate": 4.1799999999999996e-07,
      "loss": 0.9015,
      "step": 6270
    },
    {
      "epoch": 1141.8181818181818,
      "grad_norm": 1.5338923931121826,
      "learning_rate": 4.1866666666666666e-07,
      "loss": 0.8793,
      "step": 6280
    },
    {
      "epoch": 1143.6363636363637,
      "grad_norm": 1.5920689105987549,
      "learning_rate": 4.193333333333333e-07,
      "loss": 0.8968,
      "step": 6290
    },
    {
      "epoch": 1145.4545454545455,
      "grad_norm": 1.6620564460754395,
      "learning_rate": 4.1999999999999995e-07,
      "loss": 0.8889,
      "step": 6300
    },
    {
      "epoch": 1147.2727272727273,
      "grad_norm": 1.7272725105285645,
      "learning_rate": 4.2066666666666665e-07,
      "loss": 0.9205,
      "step": 6310
    },
    {
      "epoch": 1149.090909090909,
      "grad_norm": 1.4736430644989014,
      "learning_rate": 4.213333333333333e-07,
      "loss": 0.8853,
      "step": 6320
    },
    {
      "epoch": 1150.909090909091,
      "grad_norm": 1.6683064699172974,
      "learning_rate": 4.2199999999999994e-07,
      "loss": 0.8869,
      "step": 6330
    },
    {
      "epoch": 1152.7272727272727,
      "grad_norm": 1.8961735963821411,
      "learning_rate": 4.226666666666667e-07,
      "loss": 0.8739,
      "step": 6340
    },
    {
      "epoch": 1154.5454545454545,
      "grad_norm": 1.7246273756027222,
      "learning_rate": 4.2333333333333334e-07,
      "loss": 0.8644,
      "step": 6350
    },
    {
      "epoch": 1156.3636363636363,
      "grad_norm": 1.5482051372528076,
      "learning_rate": 4.24e-07,
      "loss": 0.9083,
      "step": 6360
    },
    {
      "epoch": 1158.1818181818182,
      "grad_norm": 1.6879292726516724,
      "learning_rate": 4.246666666666667e-07,
      "loss": 0.8883,
      "step": 6370
    },
    {
      "epoch": 1160.0,
      "grad_norm": 1.5385345220565796,
      "learning_rate": 4.2533333333333333e-07,
      "loss": 0.8573,
      "step": 6380
    },
    {
      "epoch": 1161.8181818181818,
      "grad_norm": 1.6768691539764404,
      "learning_rate": 4.26e-07,
      "loss": 0.8595,
      "step": 6390
    },
    {
      "epoch": 1163.6363636363637,
      "grad_norm": 1.6555732488632202,
      "learning_rate": 4.266666666666667e-07,
      "loss": 0.8575,
      "step": 6400
    },
    {
      "epoch": 1165.4545454545455,
      "grad_norm": 1.9159711599349976,
      "learning_rate": 4.273333333333333e-07,
      "loss": 0.8591,
      "step": 6410
    },
    {
      "epoch": 1167.2727272727273,
      "grad_norm": 1.8508391380310059,
      "learning_rate": 4.2799999999999997e-07,
      "loss": 0.9024,
      "step": 6420
    },
    {
      "epoch": 1169.090909090909,
      "grad_norm": 1.6956967115402222,
      "learning_rate": 4.286666666666666e-07,
      "loss": 0.8462,
      "step": 6430
    },
    {
      "epoch": 1170.909090909091,
      "grad_norm": 1.8979730606079102,
      "learning_rate": 4.293333333333333e-07,
      "loss": 0.8773,
      "step": 6440
    },
    {
      "epoch": 1172.7272727272727,
      "grad_norm": 1.9453089237213135,
      "learning_rate": 4.2999999999999996e-07,
      "loss": 0.8425,
      "step": 6450
    },
    {
      "epoch": 1174.5454545454545,
      "grad_norm": 1.7502444982528687,
      "learning_rate": 4.306666666666666e-07,
      "loss": 0.8774,
      "step": 6460
    },
    {
      "epoch": 1176.3636363636363,
      "grad_norm": 1.7986527681350708,
      "learning_rate": 4.313333333333333e-07,
      "loss": 0.8161,
      "step": 6470
    },
    {
      "epoch": 1178.1818181818182,
      "grad_norm": 1.814249873161316,
      "learning_rate": 4.3199999999999995e-07,
      "loss": 0.8831,
      "step": 6480
    },
    {
      "epoch": 1180.0,
      "grad_norm": 1.5121233463287354,
      "learning_rate": 4.3266666666666665e-07,
      "loss": 0.8365,
      "step": 6490
    },
    {
      "epoch": 1181.8181818181818,
      "grad_norm": 1.7220656871795654,
      "learning_rate": 4.3333333333333335e-07,
      "loss": 0.8415,
      "step": 6500
    },
    {
      "epoch": 1181.8181818181818,
      "eval_loss": 1.3091508150100708,
      "eval_runtime": 0.9558,
      "eval_samples_per_second": 10.462,
      "eval_steps_per_second": 5.231,
      "step": 6500
    },
    {
      "epoch": 1183.6363636363637,
      "grad_norm": 1.5097339153289795,
      "learning_rate": 4.34e-07,
      "loss": 0.8634,
      "step": 6510
    },
    {
      "epoch": 1185.4545454545455,
      "grad_norm": 1.7155959606170654,
      "learning_rate": 4.3466666666666664e-07,
      "loss": 0.8319,
      "step": 6520
    },
    {
      "epoch": 1187.2727272727273,
      "grad_norm": 1.7421071529388428,
      "learning_rate": 4.3533333333333334e-07,
      "loss": 0.8534,
      "step": 6530
    },
    {
      "epoch": 1189.090909090909,
      "grad_norm": 1.7407433986663818,
      "learning_rate": 4.36e-07,
      "loss": 0.8453,
      "step": 6540
    },
    {
      "epoch": 1190.909090909091,
      "grad_norm": 1.8172460794448853,
      "learning_rate": 4.3666666666666663e-07,
      "loss": 0.8315,
      "step": 6550
    },
    {
      "epoch": 1192.7272727272727,
      "grad_norm": 1.6322731971740723,
      "learning_rate": 4.3733333333333333e-07,
      "loss": 0.843,
      "step": 6560
    },
    {
      "epoch": 1194.5454545454545,
      "grad_norm": 1.7958985567092896,
      "learning_rate": 4.38e-07,
      "loss": 0.8376,
      "step": 6570
    },
    {
      "epoch": 1196.3636363636363,
      "grad_norm": 1.8493605852127075,
      "learning_rate": 4.386666666666666e-07,
      "loss": 0.8353,
      "step": 6580
    },
    {
      "epoch": 1198.1818181818182,
      "grad_norm": 1.864612340927124,
      "learning_rate": 4.393333333333333e-07,
      "loss": 0.802,
      "step": 6590
    },
    {
      "epoch": 1200.0,
      "grad_norm": 1.7939327955245972,
      "learning_rate": 4.3999999999999997e-07,
      "loss": 0.8433,
      "step": 6600
    },
    {
      "epoch": 1201.8181818181818,
      "grad_norm": 1.6745333671569824,
      "learning_rate": 4.406666666666666e-07,
      "loss": 0.8165,
      "step": 6610
    },
    {
      "epoch": 1203.6363636363637,
      "grad_norm": 2.073707103729248,
      "learning_rate": 4.413333333333333e-07,
      "loss": 0.8176,
      "step": 6620
    },
    {
      "epoch": 1205.4545454545455,
      "grad_norm": 1.7431622743606567,
      "learning_rate": 4.4199999999999996e-07,
      "loss": 0.8256,
      "step": 6630
    },
    {
      "epoch": 1207.2727272727273,
      "grad_norm": 1.9121677875518799,
      "learning_rate": 4.426666666666666e-07,
      "loss": 0.8254,
      "step": 6640
    },
    {
      "epoch": 1209.090909090909,
      "grad_norm": 2.0117061138153076,
      "learning_rate": 4.4333333333333336e-07,
      "loss": 0.8312,
      "step": 6650
    },
    {
      "epoch": 1210.909090909091,
      "grad_norm": 1.885439157485962,
      "learning_rate": 4.44e-07,
      "loss": 0.7976,
      "step": 6660
    },
    {
      "epoch": 1212.7272727272727,
      "grad_norm": 1.6875569820404053,
      "learning_rate": 4.4466666666666665e-07,
      "loss": 0.7973,
      "step": 6670
    },
    {
      "epoch": 1214.5454545454545,
      "grad_norm": 1.72884202003479,
      "learning_rate": 4.4533333333333335e-07,
      "loss": 0.8353,
      "step": 6680
    },
    {
      "epoch": 1216.3636363636363,
      "grad_norm": 1.7927031517028809,
      "learning_rate": 4.46e-07,
      "loss": 0.7918,
      "step": 6690
    },
    {
      "epoch": 1218.1818181818182,
      "grad_norm": 1.7466367483139038,
      "learning_rate": 4.4666666666666664e-07,
      "loss": 0.7981,
      "step": 6700
    },
    {
      "epoch": 1220.0,
      "grad_norm": 1.8589617013931274,
      "learning_rate": 4.4733333333333334e-07,
      "loss": 0.8191,
      "step": 6710
    },
    {
      "epoch": 1221.8181818181818,
      "grad_norm": 2.069027900695801,
      "learning_rate": 4.48e-07,
      "loss": 0.7997,
      "step": 6720
    },
    {
      "epoch": 1223.6363636363637,
      "grad_norm": 1.7490270137786865,
      "learning_rate": 4.4866666666666663e-07,
      "loss": 0.7836,
      "step": 6730
    },
    {
      "epoch": 1225.4545454545455,
      "grad_norm": 1.9134955406188965,
      "learning_rate": 4.493333333333333e-07,
      "loss": 0.8008,
      "step": 6740
    },
    {
      "epoch": 1227.2727272727273,
      "grad_norm": 1.6350442171096802,
      "learning_rate": 4.5e-07,
      "loss": 0.7966,
      "step": 6750
    },
    {
      "epoch": 1229.090909090909,
      "grad_norm": 1.6557687520980835,
      "learning_rate": 4.506666666666666e-07,
      "loss": 0.7965,
      "step": 6760
    },
    {
      "epoch": 1230.909090909091,
      "grad_norm": 1.7351477146148682,
      "learning_rate": 4.5133333333333327e-07,
      "loss": 0.7902,
      "step": 6770
    },
    {
      "epoch": 1232.7272727272727,
      "grad_norm": 1.8658753633499146,
      "learning_rate": 4.5199999999999997e-07,
      "loss": 0.8104,
      "step": 6780
    },
    {
      "epoch": 1234.5454545454545,
      "grad_norm": 1.838688850402832,
      "learning_rate": 4.526666666666666e-07,
      "loss": 0.7654,
      "step": 6790
    },
    {
      "epoch": 1236.3636363636363,
      "grad_norm": 1.620508074760437,
      "learning_rate": 4.5333333333333326e-07,
      "loss": 0.7937,
      "step": 6800
    },
    {
      "epoch": 1238.1818181818182,
      "grad_norm": 1.9294604063034058,
      "learning_rate": 4.54e-07,
      "loss": 0.7733,
      "step": 6810
    },
    {
      "epoch": 1240.0,
      "grad_norm": 2.0914928913116455,
      "learning_rate": 4.5466666666666666e-07,
      "loss": 0.7798,
      "step": 6820
    },
    {
      "epoch": 1241.8181818181818,
      "grad_norm": 1.6733585596084595,
      "learning_rate": 4.553333333333333e-07,
      "loss": 0.7759,
      "step": 6830
    },
    {
      "epoch": 1243.6363636363637,
      "grad_norm": 1.7235674858093262,
      "learning_rate": 4.56e-07,
      "loss": 0.7931,
      "step": 6840
    },
    {
      "epoch": 1245.4545454545455,
      "grad_norm": 1.9370135068893433,
      "learning_rate": 4.5666666666666665e-07,
      "loss": 0.6943,
      "step": 6850
    },
    {
      "epoch": 1247.2727272727273,
      "grad_norm": 1.7162625789642334,
      "learning_rate": 4.573333333333333e-07,
      "loss": 0.814,
      "step": 6860
    },
    {
      "epoch": 1249.090909090909,
      "grad_norm": 2.134242534637451,
      "learning_rate": 4.58e-07,
      "loss": 0.7968,
      "step": 6870
    },
    {
      "epoch": 1250.909090909091,
      "grad_norm": 1.9932725429534912,
      "learning_rate": 4.5866666666666664e-07,
      "loss": 0.7528,
      "step": 6880
    },
    {
      "epoch": 1252.7272727272727,
      "grad_norm": 1.8108917474746704,
      "learning_rate": 4.593333333333333e-07,
      "loss": 0.773,
      "step": 6890
    },
    {
      "epoch": 1254.5454545454545,
      "grad_norm": 1.8030439615249634,
      "learning_rate": 4.6e-07,
      "loss": 0.7799,
      "step": 6900
    },
    {
      "epoch": 1256.3636363636363,
      "grad_norm": 1.92072331905365,
      "learning_rate": 4.6066666666666664e-07,
      "loss": 0.7478,
      "step": 6910
    },
    {
      "epoch": 1258.1818181818182,
      "grad_norm": 1.9255001544952393,
      "learning_rate": 4.613333333333333e-07,
      "loss": 0.7425,
      "step": 6920
    },
    {
      "epoch": 1260.0,
      "grad_norm": 2.0153613090515137,
      "learning_rate": 4.62e-07,
      "loss": 0.7681,
      "step": 6930
    },
    {
      "epoch": 1261.8181818181818,
      "grad_norm": 1.9780194759368896,
      "learning_rate": 4.6266666666666663e-07,
      "loss": 0.7555,
      "step": 6940
    },
    {
      "epoch": 1263.6363636363637,
      "grad_norm": 1.6981016397476196,
      "learning_rate": 4.633333333333333e-07,
      "loss": 0.7559,
      "step": 6950
    },
    {
      "epoch": 1265.4545454545455,
      "grad_norm": 1.8841423988342285,
      "learning_rate": 4.64e-07,
      "loss": 0.7468,
      "step": 6960
    },
    {
      "epoch": 1267.2727272727273,
      "grad_norm": 1.8242863416671753,
      "learning_rate": 4.6466666666666667e-07,
      "loss": 0.7708,
      "step": 6970
    },
    {
      "epoch": 1269.090909090909,
      "grad_norm": 2.0682144165039062,
      "learning_rate": 4.653333333333333e-07,
      "loss": 0.7183,
      "step": 6980
    },
    {
      "epoch": 1270.909090909091,
      "grad_norm": 2.1686599254608154,
      "learning_rate": 4.66e-07,
      "loss": 0.7579,
      "step": 6990
    },
    {
      "epoch": 1272.7272727272727,
      "grad_norm": 2.2548508644104004,
      "learning_rate": 4.6666666666666666e-07,
      "loss": 0.7326,
      "step": 7000
    },
    {
      "epoch": 1272.7272727272727,
      "eval_loss": 1.3452931642532349,
      "eval_runtime": 0.954,
      "eval_samples_per_second": 10.482,
      "eval_steps_per_second": 5.241,
      "step": 7000
    },
    {
      "epoch": 1274.5454545454545,
      "grad_norm": 1.91908597946167,
      "learning_rate": 4.673333333333333e-07,
      "loss": 0.744,
      "step": 7010
    },
    {
      "epoch": 1276.3636363636363,
      "grad_norm": 2.1635873317718506,
      "learning_rate": 4.68e-07,
      "loss": 0.739,
      "step": 7020
    },
    {
      "epoch": 1278.1818181818182,
      "grad_norm": 1.804146409034729,
      "learning_rate": 4.6866666666666665e-07,
      "loss": 0.7346,
      "step": 7030
    },
    {
      "epoch": 1280.0,
      "grad_norm": 2.0477800369262695,
      "learning_rate": 4.693333333333333e-07,
      "loss": 0.7298,
      "step": 7040
    },
    {
      "epoch": 1281.8181818181818,
      "grad_norm": 2.1609175205230713,
      "learning_rate": 4.6999999999999995e-07,
      "loss": 0.7387,
      "step": 7050
    },
    {
      "epoch": 1283.6363636363637,
      "grad_norm": 1.99752938747406,
      "learning_rate": 4.7066666666666665e-07,
      "loss": 0.6978,
      "step": 7060
    },
    {
      "epoch": 1285.4545454545455,
      "grad_norm": 1.8712424039840698,
      "learning_rate": 4.713333333333333e-07,
      "loss": 0.7481,
      "step": 7070
    },
    {
      "epoch": 1287.2727272727273,
      "grad_norm": 2.1430909633636475,
      "learning_rate": 4.7199999999999994e-07,
      "loss": 0.7434,
      "step": 7080
    },
    {
      "epoch": 1289.090909090909,
      "grad_norm": 2.011435031890869,
      "learning_rate": 4.7266666666666664e-07,
      "loss": 0.7145,
      "step": 7090
    },
    {
      "epoch": 1290.909090909091,
      "grad_norm": 2.2666001319885254,
      "learning_rate": 4.733333333333333e-07,
      "loss": 0.7095,
      "step": 7100
    },
    {
      "epoch": 1292.7272727272727,
      "grad_norm": 2.148012161254883,
      "learning_rate": 4.7399999999999993e-07,
      "loss": 0.7391,
      "step": 7110
    },
    {
      "epoch": 1294.5454545454545,
      "grad_norm": 2.0288662910461426,
      "learning_rate": 4.746666666666667e-07,
      "loss": 0.6817,
      "step": 7120
    },
    {
      "epoch": 1296.3636363636363,
      "grad_norm": 2.0171961784362793,
      "learning_rate": 4.7533333333333333e-07,
      "loss": 0.7207,
      "step": 7130
    },
    {
      "epoch": 1298.1818181818182,
      "grad_norm": 1.854345440864563,
      "learning_rate": 4.76e-07,
      "loss": 0.6939,
      "step": 7140
    },
    {
      "epoch": 1300.0,
      "grad_norm": 1.9496082067489624,
      "learning_rate": 4.7666666666666667e-07,
      "loss": 0.7321,
      "step": 7150
    },
    {
      "epoch": 1301.8181818181818,
      "grad_norm": 2.4143805503845215,
      "learning_rate": 4.773333333333333e-07,
      "loss": 0.7137,
      "step": 7160
    },
    {
      "epoch": 1303.6363636363637,
      "grad_norm": 2.137540817260742,
      "learning_rate": 4.779999999999999e-07,
      "loss": 0.6814,
      "step": 7170
    },
    {
      "epoch": 1305.4545454545455,
      "grad_norm": 1.8083341121673584,
      "learning_rate": 4.786666666666667e-07,
      "loss": 0.7102,
      "step": 7180
    },
    {
      "epoch": 1307.2727272727273,
      "grad_norm": 2.091285467147827,
      "learning_rate": 4.793333333333333e-07,
      "loss": 0.6918,
      "step": 7190
    },
    {
      "epoch": 1309.090909090909,
      "grad_norm": 2.578460693359375,
      "learning_rate": 4.8e-07,
      "loss": 0.7014,
      "step": 7200
    },
    {
      "epoch": 1310.909090909091,
      "grad_norm": 1.9887486696243286,
      "learning_rate": 4.806666666666667e-07,
      "loss": 0.6975,
      "step": 7210
    },
    {
      "epoch": 1312.7272727272727,
      "grad_norm": 2.0860838890075684,
      "learning_rate": 4.813333333333334e-07,
      "loss": 0.7,
      "step": 7220
    },
    {
      "epoch": 1314.5454545454545,
      "grad_norm": 2.1295106410980225,
      "learning_rate": 4.82e-07,
      "loss": 0.6848,
      "step": 7230
    },
    {
      "epoch": 1316.3636363636363,
      "grad_norm": 1.8778151273727417,
      "learning_rate": 4.826666666666666e-07,
      "loss": 0.7212,
      "step": 7240
    },
    {
      "epoch": 1318.1818181818182,
      "grad_norm": 2.213719606399536,
      "learning_rate": 4.833333333333333e-07,
      "loss": 0.6473,
      "step": 7250
    },
    {
      "epoch": 1320.0,
      "grad_norm": 2.3890373706817627,
      "learning_rate": 4.839999999999999e-07,
      "loss": 0.6969,
      "step": 7260
    },
    {
      "epoch": 1321.8181818181818,
      "grad_norm": 1.9804365634918213,
      "learning_rate": 4.846666666666667e-07,
      "loss": 0.6949,
      "step": 7270
    },
    {
      "epoch": 1323.6363636363637,
      "grad_norm": 2.0703601837158203,
      "learning_rate": 4.853333333333333e-07,
      "loss": 0.6915,
      "step": 7280
    },
    {
      "epoch": 1325.4545454545455,
      "grad_norm": 2.152437210083008,
      "learning_rate": 4.86e-07,
      "loss": 0.6494,
      "step": 7290
    },
    {
      "epoch": 1327.2727272727273,
      "grad_norm": 2.0996384620666504,
      "learning_rate": 4.866666666666666e-07,
      "loss": 0.6795,
      "step": 7300
    },
    {
      "epoch": 1329.090909090909,
      "grad_norm": 2.295306921005249,
      "learning_rate": 4.873333333333333e-07,
      "loss": 0.6795,
      "step": 7310
    },
    {
      "epoch": 1330.909090909091,
      "grad_norm": 2.3133373260498047,
      "learning_rate": 4.879999999999999e-07,
      "loss": 0.677,
      "step": 7320
    },
    {
      "epoch": 1332.7272727272727,
      "grad_norm": 2.4013009071350098,
      "learning_rate": 4.886666666666667e-07,
      "loss": 0.6637,
      "step": 7330
    },
    {
      "epoch": 1334.5454545454545,
      "grad_norm": 2.3708415031433105,
      "learning_rate": 4.893333333333333e-07,
      "loss": 0.6603,
      "step": 7340
    },
    {
      "epoch": 1336.3636363636363,
      "grad_norm": 2.009429931640625,
      "learning_rate": 4.9e-07,
      "loss": 0.6505,
      "step": 7350
    },
    {
      "epoch": 1338.1818181818182,
      "grad_norm": 2.2697086334228516,
      "learning_rate": 4.906666666666666e-07,
      "loss": 0.6835,
      "step": 7360
    },
    {
      "epoch": 1340.0,
      "grad_norm": 2.2665457725524902,
      "learning_rate": 4.913333333333334e-07,
      "loss": 0.6608,
      "step": 7370
    },
    {
      "epoch": 1341.8181818181818,
      "grad_norm": 2.231205940246582,
      "learning_rate": 4.92e-07,
      "loss": 0.6456,
      "step": 7380
    },
    {
      "epoch": 1343.6363636363637,
      "grad_norm": 2.322068691253662,
      "learning_rate": 4.926666666666667e-07,
      "loss": 0.667,
      "step": 7390
    },
    {
      "epoch": 1345.4545454545455,
      "grad_norm": 1.8201347589492798,
      "learning_rate": 4.933333333333333e-07,
      "loss": 0.6938,
      "step": 7400
    },
    {
      "epoch": 1347.2727272727273,
      "grad_norm": 2.5046095848083496,
      "learning_rate": 4.94e-07,
      "loss": 0.6196,
      "step": 7410
    },
    {
      "epoch": 1349.090909090909,
      "grad_norm": 2.5943005084991455,
      "learning_rate": 4.946666666666666e-07,
      "loss": 0.6661,
      "step": 7420
    },
    {
      "epoch": 1350.909090909091,
      "grad_norm": 2.0220558643341064,
      "learning_rate": 4.953333333333333e-07,
      "loss": 0.635,
      "step": 7430
    },
    {
      "epoch": 1352.7272727272727,
      "grad_norm": 2.015592575073242,
      "learning_rate": 4.96e-07,
      "loss": 0.6517,
      "step": 7440
    },
    {
      "epoch": 1354.5454545454545,
      "grad_norm": 2.3991057872772217,
      "learning_rate": 4.966666666666666e-07,
      "loss": 0.6426,
      "step": 7450
    },
    {
      "epoch": 1356.3636363636363,
      "grad_norm": 2.457735300064087,
      "learning_rate": 4.973333333333333e-07,
      "loss": 0.6307,
      "step": 7460
    },
    {
      "epoch": 1358.1818181818182,
      "grad_norm": 2.6684253215789795,
      "learning_rate": 4.979999999999999e-07,
      "loss": 0.6528,
      "step": 7470
    },
    {
      "epoch": 1360.0,
      "grad_norm": 1.9926061630249023,
      "learning_rate": 4.986666666666666e-07,
      "loss": 0.6447,
      "step": 7480
    },
    {
      "epoch": 1361.8181818181818,
      "grad_norm": 2.416400909423828,
      "learning_rate": 4.993333333333333e-07,
      "loss": 0.644,
      "step": 7490
    },
    {
      "epoch": 1363.6363636363637,
      "grad_norm": 1.9049819707870483,
      "learning_rate": 5e-07,
      "loss": 0.65,
      "step": 7500
    },
    {
      "epoch": 1363.6363636363637,
      "eval_loss": 1.394803762435913,
      "eval_runtime": 0.9508,
      "eval_samples_per_second": 10.517,
      "eval_steps_per_second": 5.259,
      "step": 7500
    },
    {
      "epoch": 1365.4545454545455,
      "grad_norm": 2.064253807067871,
      "learning_rate": 5.006666666666667e-07,
      "loss": 0.6231,
      "step": 7510
    },
    {
      "epoch": 1367.2727272727273,
      "grad_norm": 2.397549629211426,
      "learning_rate": 5.013333333333333e-07,
      "loss": 0.6412,
      "step": 7520
    },
    {
      "epoch": 1369.090909090909,
      "grad_norm": 2.599519729614258,
      "learning_rate": 5.02e-07,
      "loss": 0.6279,
      "step": 7530
    },
    {
      "epoch": 1370.909090909091,
      "grad_norm": 2.4246835708618164,
      "learning_rate": 5.026666666666667e-07,
      "loss": 0.6197,
      "step": 7540
    },
    {
      "epoch": 1372.7272727272727,
      "grad_norm": 2.4878108501434326,
      "learning_rate": 5.033333333333333e-07,
      "loss": 0.6399,
      "step": 7550
    },
    {
      "epoch": 1374.5454545454545,
      "grad_norm": 2.2441654205322266,
      "learning_rate": 5.04e-07,
      "loss": 0.5946,
      "step": 7560
    },
    {
      "epoch": 1376.3636363636363,
      "grad_norm": 2.331012725830078,
      "learning_rate": 5.046666666666667e-07,
      "loss": 0.6211,
      "step": 7570
    },
    {
      "epoch": 1378.1818181818182,
      "grad_norm": 2.3858513832092285,
      "learning_rate": 5.053333333333333e-07,
      "loss": 0.6172,
      "step": 7580
    },
    {
      "epoch": 1380.0,
      "grad_norm": 2.2918763160705566,
      "learning_rate": 5.06e-07,
      "loss": 0.6038,
      "step": 7590
    },
    {
      "epoch": 1381.8181818181818,
      "grad_norm": 2.2448108196258545,
      "learning_rate": 5.066666666666667e-07,
      "loss": 0.5986,
      "step": 7600
    },
    {
      "epoch": 1383.6363636363637,
      "grad_norm": 2.530604839324951,
      "learning_rate": 5.073333333333333e-07,
      "loss": 0.6332,
      "step": 7610
    },
    {
      "epoch": 1385.4545454545455,
      "grad_norm": 2.3394274711608887,
      "learning_rate": 5.079999999999999e-07,
      "loss": 0.5987,
      "step": 7620
    },
    {
      "epoch": 1387.2727272727273,
      "grad_norm": 2.481337070465088,
      "learning_rate": 5.086666666666667e-07,
      "loss": 0.6074,
      "step": 7630
    },
    {
      "epoch": 1389.090909090909,
      "grad_norm": 2.8032150268554688,
      "learning_rate": 5.093333333333332e-07,
      "loss": 0.5992,
      "step": 7640
    },
    {
      "epoch": 1390.909090909091,
      "grad_norm": 2.470306396484375,
      "learning_rate": 5.1e-07,
      "loss": 0.5977,
      "step": 7650
    },
    {
      "epoch": 1392.7272727272727,
      "grad_norm": 2.404108762741089,
      "learning_rate": 5.106666666666667e-07,
      "loss": 0.5791,
      "step": 7660
    },
    {
      "epoch": 1394.5454545454545,
      "grad_norm": 2.697213888168335,
      "learning_rate": 5.113333333333333e-07,
      "loss": 0.6261,
      "step": 7670
    },
    {
      "epoch": 1396.3636363636363,
      "grad_norm": 2.1127915382385254,
      "learning_rate": 5.12e-07,
      "loss": 0.5898,
      "step": 7680
    },
    {
      "epoch": 1398.1818181818182,
      "grad_norm": 2.620652437210083,
      "learning_rate": 5.126666666666667e-07,
      "loss": 0.5966,
      "step": 7690
    },
    {
      "epoch": 1400.0,
      "grad_norm": 2.5950961112976074,
      "learning_rate": 5.133333333333333e-07,
      "loss": 0.5933,
      "step": 7700
    },
    {
      "epoch": 1401.8181818181818,
      "grad_norm": 2.9109435081481934,
      "learning_rate": 5.14e-07,
      "loss": 0.5757,
      "step": 7710
    },
    {
      "epoch": 1403.6363636363637,
      "grad_norm": 2.4718472957611084,
      "learning_rate": 5.146666666666667e-07,
      "loss": 0.5868,
      "step": 7720
    },
    {
      "epoch": 1405.4545454545455,
      "grad_norm": 2.4520111083984375,
      "learning_rate": 5.153333333333333e-07,
      "loss": 0.5566,
      "step": 7730
    },
    {
      "epoch": 1407.2727272727273,
      "grad_norm": 1.9652421474456787,
      "learning_rate": 5.16e-07,
      "loss": 0.5976,
      "step": 7740
    },
    {
      "epoch": 1409.090909090909,
      "grad_norm": 2.699679374694824,
      "learning_rate": 5.166666666666667e-07,
      "loss": 0.5993,
      "step": 7750
    },
    {
      "epoch": 1410.909090909091,
      "grad_norm": 2.719834804534912,
      "learning_rate": 5.173333333333333e-07,
      "loss": 0.5795,
      "step": 7760
    },
    {
      "epoch": 1412.7272727272727,
      "grad_norm": 3.267129421234131,
      "learning_rate": 5.18e-07,
      "loss": 0.581,
      "step": 7770
    },
    {
      "epoch": 1414.5454545454545,
      "grad_norm": 2.5679855346679688,
      "learning_rate": 5.186666666666667e-07,
      "loss": 0.5538,
      "step": 7780
    },
    {
      "epoch": 1416.3636363636363,
      "grad_norm": 2.463726758956909,
      "learning_rate": 5.193333333333332e-07,
      "loss": 0.5914,
      "step": 7790
    },
    {
      "epoch": 1418.1818181818182,
      "grad_norm": 3.0395708084106445,
      "learning_rate": 5.2e-07,
      "loss": 0.571,
      "step": 7800
    },
    {
      "epoch": 1420.0,
      "grad_norm": 2.8414790630340576,
      "learning_rate": 5.206666666666667e-07,
      "loss": 0.564,
      "step": 7810
    },
    {
      "epoch": 1421.8181818181818,
      "grad_norm": 2.6655869483947754,
      "learning_rate": 5.213333333333333e-07,
      "loss": 0.5584,
      "step": 7820
    },
    {
      "epoch": 1423.6363636363637,
      "grad_norm": 3.044402837753296,
      "learning_rate": 5.22e-07,
      "loss": 0.5815,
      "step": 7830
    },
    {
      "epoch": 1425.4545454545455,
      "grad_norm": 2.49562406539917,
      "learning_rate": 5.226666666666666e-07,
      "loss": 0.5361,
      "step": 7840
    },
    {
      "epoch": 1427.2727272727273,
      "grad_norm": 3.1595678329467773,
      "learning_rate": 5.233333333333333e-07,
      "loss": 0.5666,
      "step": 7850
    },
    {
      "epoch": 1429.090909090909,
      "grad_norm": 2.3533074855804443,
      "learning_rate": 5.24e-07,
      "loss": 0.5437,
      "step": 7860
    },
    {
      "epoch": 1430.909090909091,
      "grad_norm": 2.581085681915283,
      "learning_rate": 5.246666666666666e-07,
      "loss": 0.5592,
      "step": 7870
    },
    {
      "epoch": 1432.7272727272727,
      "grad_norm": 2.6028494834899902,
      "learning_rate": 5.253333333333333e-07,
      "loss": 0.5574,
      "step": 7880
    },
    {
      "epoch": 1434.5454545454545,
      "grad_norm": 2.289011240005493,
      "learning_rate": 5.26e-07,
      "loss": 0.5565,
      "step": 7890
    },
    {
      "epoch": 1436.3636363636363,
      "grad_norm": 2.5692837238311768,
      "learning_rate": 5.266666666666666e-07,
      "loss": 0.5218,
      "step": 7900
    },
    {
      "epoch": 1438.1818181818182,
      "grad_norm": 2.4455013275146484,
      "learning_rate": 5.273333333333333e-07,
      "loss": 0.553,
      "step": 7910
    },
    {
      "epoch": 1440.0,
      "grad_norm": 2.9633126258850098,
      "learning_rate": 5.28e-07,
      "loss": 0.5256,
      "step": 7920
    },
    {
      "epoch": 1441.8181818181818,
      "grad_norm": 2.8188230991363525,
      "learning_rate": 5.286666666666666e-07,
      "loss": 0.5275,
      "step": 7930
    },
    {
      "epoch": 1443.6363636363637,
      "grad_norm": 2.6176159381866455,
      "learning_rate": 5.293333333333333e-07,
      "loss": 0.5674,
      "step": 7940
    },
    {
      "epoch": 1445.4545454545455,
      "grad_norm": 2.9995148181915283,
      "learning_rate": 5.3e-07,
      "loss": 0.4903,
      "step": 7950
    },
    {
      "epoch": 1447.2727272727273,
      "grad_norm": 2.758824110031128,
      "learning_rate": 5.306666666666665e-07,
      "loss": 0.5573,
      "step": 7960
    },
    {
      "epoch": 1449.090909090909,
      "grad_norm": 3.7099719047546387,
      "learning_rate": 5.313333333333333e-07,
      "loss": 0.5094,
      "step": 7970
    },
    {
      "epoch": 1450.909090909091,
      "grad_norm": 2.5302205085754395,
      "learning_rate": 5.32e-07,
      "loss": 0.5278,
      "step": 7980
    },
    {
      "epoch": 1452.7272727272727,
      "grad_norm": 2.8791537284851074,
      "learning_rate": 5.326666666666666e-07,
      "loss": 0.5114,
      "step": 7990
    },
    {
      "epoch": 1454.5454545454545,
      "grad_norm": 2.865116834640503,
      "learning_rate": 5.333333333333333e-07,
      "loss": 0.5304,
      "step": 8000
    },
    {
      "epoch": 1454.5454545454545,
      "eval_loss": 1.4667210578918457,
      "eval_runtime": 0.9551,
      "eval_samples_per_second": 10.47,
      "eval_steps_per_second": 5.235,
      "step": 8000
    },
    {
      "epoch": 1456.3636363636363,
      "grad_norm": 2.603318214416504,
      "learning_rate": 5.34e-07,
      "loss": 0.5291,
      "step": 8010
    },
    {
      "epoch": 1458.1818181818182,
      "grad_norm": 2.9607455730438232,
      "learning_rate": 5.346666666666666e-07,
      "loss": 0.5043,
      "step": 8020
    },
    {
      "epoch": 1460.0,
      "grad_norm": 2.5570242404937744,
      "learning_rate": 5.353333333333333e-07,
      "loss": 0.522,
      "step": 8030
    },
    {
      "epoch": 1461.8181818181818,
      "grad_norm": 2.8811745643615723,
      "learning_rate": 5.36e-07,
      "loss": 0.5133,
      "step": 8040
    },
    {
      "epoch": 1463.6363636363637,
      "grad_norm": 2.5781030654907227,
      "learning_rate": 5.366666666666666e-07,
      "loss": 0.5176,
      "step": 8050
    },
    {
      "epoch": 1465.4545454545455,
      "grad_norm": 2.579080581665039,
      "learning_rate": 5.373333333333333e-07,
      "loss": 0.4988,
      "step": 8060
    },
    {
      "epoch": 1467.2727272727273,
      "grad_norm": 2.695896863937378,
      "learning_rate": 5.38e-07,
      "loss": 0.5138,
      "step": 8070
    },
    {
      "epoch": 1469.090909090909,
      "grad_norm": 2.8226592540740967,
      "learning_rate": 5.386666666666666e-07,
      "loss": 0.5063,
      "step": 8080
    },
    {
      "epoch": 1470.909090909091,
      "grad_norm": 2.9198739528656006,
      "learning_rate": 5.393333333333333e-07,
      "loss": 0.4951,
      "step": 8090
    },
    {
      "epoch": 1472.7272727272727,
      "grad_norm": 3.1468589305877686,
      "learning_rate": 5.4e-07,
      "loss": 0.5002,
      "step": 8100
    },
    {
      "epoch": 1474.5454545454545,
      "grad_norm": 3.2522032260894775,
      "learning_rate": 5.406666666666666e-07,
      "loss": 0.5127,
      "step": 8110
    },
    {
      "epoch": 1476.3636363636363,
      "grad_norm": 3.364250898361206,
      "learning_rate": 5.413333333333333e-07,
      "loss": 0.5022,
      "step": 8120
    },
    {
      "epoch": 1478.1818181818182,
      "grad_norm": 3.0626018047332764,
      "learning_rate": 5.420000000000001e-07,
      "loss": 0.4589,
      "step": 8130
    },
    {
      "epoch": 1480.0,
      "grad_norm": 2.5147552490234375,
      "learning_rate": 5.426666666666666e-07,
      "loss": 0.5027,
      "step": 8140
    },
    {
      "epoch": 1481.8181818181818,
      "grad_norm": 2.676621437072754,
      "learning_rate": 5.433333333333334e-07,
      "loss": 0.5039,
      "step": 8150
    },
    {
      "epoch": 1483.6363636363637,
      "grad_norm": 2.5273118019104004,
      "learning_rate": 5.44e-07,
      "loss": 0.4701,
      "step": 8160
    },
    {
      "epoch": 1485.4545454545455,
      "grad_norm": 2.4840474128723145,
      "learning_rate": 5.446666666666666e-07,
      "loss": 0.4721,
      "step": 8170
    },
    {
      "epoch": 1487.2727272727273,
      "grad_norm": 3.0541839599609375,
      "learning_rate": 5.453333333333333e-07,
      "loss": 0.4678,
      "step": 8180
    },
    {
      "epoch": 1489.090909090909,
      "grad_norm": 2.8932406902313232,
      "learning_rate": 5.46e-07,
      "loss": 0.5015,
      "step": 8190
    },
    {
      "epoch": 1490.909090909091,
      "grad_norm": 2.338932514190674,
      "learning_rate": 5.466666666666666e-07,
      "loss": 0.463,
      "step": 8200
    },
    {
      "epoch": 1492.7272727272727,
      "grad_norm": 2.4538185596466064,
      "learning_rate": 5.473333333333333e-07,
      "loss": 0.4892,
      "step": 8210
    },
    {
      "epoch": 1494.5454545454545,
      "grad_norm": 2.9354734420776367,
      "learning_rate": 5.48e-07,
      "loss": 0.4897,
      "step": 8220
    },
    {
      "epoch": 1496.3636363636363,
      "grad_norm": 2.8793768882751465,
      "learning_rate": 5.486666666666666e-07,
      "loss": 0.4244,
      "step": 8230
    },
    {
      "epoch": 1498.1818181818182,
      "grad_norm": 2.9814038276672363,
      "learning_rate": 5.493333333333333e-07,
      "loss": 0.4978,
      "step": 8240
    },
    {
      "epoch": 1500.0,
      "grad_norm": 3.292731523513794,
      "learning_rate": 5.5e-07,
      "loss": 0.4543,
      "step": 8250
    },
    {
      "epoch": 1501.8181818181818,
      "grad_norm": 3.3122358322143555,
      "learning_rate": 5.506666666666666e-07,
      "loss": 0.4728,
      "step": 8260
    },
    {
      "epoch": 1503.6363636363637,
      "grad_norm": 7.332683086395264,
      "learning_rate": 5.513333333333333e-07,
      "loss": 0.4454,
      "step": 8270
    },
    {
      "epoch": 1505.4545454545455,
      "grad_norm": 2.5568389892578125,
      "learning_rate": 5.520000000000001e-07,
      "loss": 0.4742,
      "step": 8280
    },
    {
      "epoch": 1507.2727272727273,
      "grad_norm": 2.469768762588501,
      "learning_rate": 5.526666666666666e-07,
      "loss": 0.4478,
      "step": 8290
    },
    {
      "epoch": 1509.090909090909,
      "grad_norm": 2.6555016040802,
      "learning_rate": 5.533333333333334e-07,
      "loss": 0.4351,
      "step": 8300
    },
    {
      "epoch": 1510.909090909091,
      "grad_norm": 2.776387929916382,
      "learning_rate": 5.54e-07,
      "loss": 0.4617,
      "step": 8310
    },
    {
      "epoch": 1512.7272727272727,
      "grad_norm": 2.618194818496704,
      "learning_rate": 5.546666666666667e-07,
      "loss": 0.4499,
      "step": 8320
    },
    {
      "epoch": 1514.5454545454545,
      "grad_norm": 4.7555251121521,
      "learning_rate": 5.553333333333333e-07,
      "loss": 0.4398,
      "step": 8330
    },
    {
      "epoch": 1516.3636363636363,
      "grad_norm": 3.585129737854004,
      "learning_rate": 5.560000000000001e-07,
      "loss": 0.4683,
      "step": 8340
    },
    {
      "epoch": 1518.1818181818182,
      "grad_norm": 2.8052356243133545,
      "learning_rate": 5.566666666666666e-07,
      "loss": 0.4285,
      "step": 8350
    },
    {
      "epoch": 1520.0,
      "grad_norm": 2.9477686882019043,
      "learning_rate": 5.573333333333333e-07,
      "loss": 0.4317,
      "step": 8360
    },
    {
      "epoch": 1521.8181818181818,
      "grad_norm": 3.4123404026031494,
      "learning_rate": 5.58e-07,
      "loss": 0.4381,
      "step": 8370
    },
    {
      "epoch": 1523.6363636363637,
      "grad_norm": 3.2569050788879395,
      "learning_rate": 5.586666666666666e-07,
      "loss": 0.4391,
      "step": 8380
    },
    {
      "epoch": 1525.4545454545455,
      "grad_norm": 2.7200393676757812,
      "learning_rate": 5.593333333333333e-07,
      "loss": 0.4435,
      "step": 8390
    },
    {
      "epoch": 1527.2727272727273,
      "grad_norm": 3.691983938217163,
      "learning_rate": 5.6e-07,
      "loss": 0.4252,
      "step": 8400
    },
    {
      "epoch": 1529.090909090909,
      "grad_norm": 3.2255430221557617,
      "learning_rate": 5.606666666666666e-07,
      "loss": 0.4377,
      "step": 8410
    },
    {
      "epoch": 1530.909090909091,
      "grad_norm": 2.9283125400543213,
      "learning_rate": 5.613333333333333e-07,
      "loss": 0.4267,
      "step": 8420
    },
    {
      "epoch": 1532.7272727272727,
      "grad_norm": 2.697779417037964,
      "learning_rate": 5.620000000000001e-07,
      "loss": 0.4189,
      "step": 8430
    },
    {
      "epoch": 1534.5454545454545,
      "grad_norm": 4.052005767822266,
      "learning_rate": 5.626666666666666e-07,
      "loss": 0.4137,
      "step": 8440
    },
    {
      "epoch": 1536.3636363636363,
      "grad_norm": 3.1920089721679688,
      "learning_rate": 5.633333333333334e-07,
      "loss": 0.4155,
      "step": 8450
    },
    {
      "epoch": 1538.1818181818182,
      "grad_norm": 3.119431495666504,
      "learning_rate": 5.639999999999999e-07,
      "loss": 0.4221,
      "step": 8460
    },
    {
      "epoch": 1540.0,
      "grad_norm": 3.3211262226104736,
      "learning_rate": 5.646666666666667e-07,
      "loss": 0.4173,
      "step": 8470
    },
    {
      "epoch": 1541.8181818181818,
      "grad_norm": 3.048973321914673,
      "learning_rate": 5.653333333333333e-07,
      "loss": 0.4267,
      "step": 8480
    },
    {
      "epoch": 1543.6363636363637,
      "grad_norm": 2.767284393310547,
      "learning_rate": 5.66e-07,
      "loss": 0.4017,
      "step": 8490
    },
    {
      "epoch": 1545.4545454545455,
      "grad_norm": 3.8262345790863037,
      "learning_rate": 5.666666666666666e-07,
      "loss": 0.4084,
      "step": 8500
    },
    {
      "epoch": 1545.4545454545455,
      "eval_loss": 1.5822458267211914,
      "eval_runtime": 0.9494,
      "eval_samples_per_second": 10.533,
      "eval_steps_per_second": 5.266,
      "step": 8500
    },
    {
      "epoch": 1547.2727272727273,
      "grad_norm": 3.372830629348755,
      "learning_rate": 5.673333333333334e-07,
      "loss": 0.4313,
      "step": 8510
    },
    {
      "epoch": 1549.090909090909,
      "grad_norm": 3.699538469314575,
      "learning_rate": 5.679999999999999e-07,
      "loss": 0.3917,
      "step": 8520
    },
    {
      "epoch": 1550.909090909091,
      "grad_norm": 3.010486602783203,
      "learning_rate": 5.686666666666667e-07,
      "loss": 0.4022,
      "step": 8530
    },
    {
      "epoch": 1552.7272727272727,
      "grad_norm": 2.6179702281951904,
      "learning_rate": 5.693333333333333e-07,
      "loss": 0.4029,
      "step": 8540
    },
    {
      "epoch": 1554.5454545454545,
      "grad_norm": 5.061021327972412,
      "learning_rate": 5.699999999999999e-07,
      "loss": 0.4084,
      "step": 8550
    },
    {
      "epoch": 1556.3636363636363,
      "grad_norm": 3.269577980041504,
      "learning_rate": 5.706666666666666e-07,
      "loss": 0.4101,
      "step": 8560
    },
    {
      "epoch": 1558.1818181818182,
      "grad_norm": 3.5857863426208496,
      "learning_rate": 5.713333333333333e-07,
      "loss": 0.3684,
      "step": 8570
    },
    {
      "epoch": 1560.0,
      "grad_norm": 2.727865695953369,
      "learning_rate": 5.719999999999999e-07,
      "loss": 0.4046,
      "step": 8580
    },
    {
      "epoch": 1561.8181818181818,
      "grad_norm": 2.546781063079834,
      "learning_rate": 5.726666666666666e-07,
      "loss": 0.4057,
      "step": 8590
    },
    {
      "epoch": 1563.6363636363637,
      "grad_norm": 3.5635156631469727,
      "learning_rate": 5.733333333333334e-07,
      "loss": 0.3678,
      "step": 8600
    },
    {
      "epoch": 1565.4545454545455,
      "grad_norm": 2.7047088146209717,
      "learning_rate": 5.739999999999999e-07,
      "loss": 0.4027,
      "step": 8610
    },
    {
      "epoch": 1567.2727272727273,
      "grad_norm": 2.970088243484497,
      "learning_rate": 5.746666666666667e-07,
      "loss": 0.4065,
      "step": 8620
    },
    {
      "epoch": 1569.090909090909,
      "grad_norm": 2.8854446411132812,
      "learning_rate": 5.753333333333333e-07,
      "loss": 0.3581,
      "step": 8630
    },
    {
      "epoch": 1570.909090909091,
      "grad_norm": 11.273502349853516,
      "learning_rate": 5.76e-07,
      "loss": 0.3895,
      "step": 8640
    },
    {
      "epoch": 1572.7272727272727,
      "grad_norm": 3.7800402641296387,
      "learning_rate": 5.766666666666666e-07,
      "loss": 0.3946,
      "step": 8650
    },
    {
      "epoch": 1574.5454545454545,
      "grad_norm": 2.3915345668792725,
      "learning_rate": 5.773333333333334e-07,
      "loss": 0.3507,
      "step": 8660
    },
    {
      "epoch": 1576.3636363636363,
      "grad_norm": 3.7667393684387207,
      "learning_rate": 5.779999999999999e-07,
      "loss": 0.3713,
      "step": 8670
    },
    {
      "epoch": 1578.1818181818182,
      "grad_norm": 2.859992742538452,
      "learning_rate": 5.786666666666667e-07,
      "loss": 0.3917,
      "step": 8680
    },
    {
      "epoch": 1580.0,
      "grad_norm": 3.7137441635131836,
      "learning_rate": 5.793333333333333e-07,
      "loss": 0.3643,
      "step": 8690
    },
    {
      "epoch": 1581.8181818181818,
      "grad_norm": 4.000030994415283,
      "learning_rate": 5.8e-07,
      "loss": 0.3742,
      "step": 8700
    },
    {
      "epoch": 1583.6363636363637,
      "grad_norm": 2.7163455486297607,
      "learning_rate": 5.806666666666666e-07,
      "loss": 0.3693,
      "step": 8710
    },
    {
      "epoch": 1585.4545454545455,
      "grad_norm": 2.4419193267822266,
      "learning_rate": 5.813333333333334e-07,
      "loss": 0.3425,
      "step": 8720
    },
    {
      "epoch": 1587.2727272727273,
      "grad_norm": 3.2612335681915283,
      "learning_rate": 5.819999999999999e-07,
      "loss": 0.348,
      "step": 8730
    },
    {
      "epoch": 1589.090909090909,
      "grad_norm": 3.5170700550079346,
      "learning_rate": 5.826666666666666e-07,
      "loss": 0.3794,
      "step": 8740
    },
    {
      "epoch": 1590.909090909091,
      "grad_norm": 3.9732728004455566,
      "learning_rate": 5.833333333333334e-07,
      "loss": 0.356,
      "step": 8750
    },
    {
      "epoch": 1592.7272727272727,
      "grad_norm": 3.4939472675323486,
      "learning_rate": 5.839999999999999e-07,
      "loss": 0.3538,
      "step": 8760
    },
    {
      "epoch": 1594.5454545454545,
      "grad_norm": 3.6977486610412598,
      "learning_rate": 5.846666666666667e-07,
      "loss": 0.3706,
      "step": 8770
    },
    {
      "epoch": 1596.3636363636363,
      "grad_norm": 2.6531522274017334,
      "learning_rate": 5.853333333333333e-07,
      "loss": 0.343,
      "step": 8780
    },
    {
      "epoch": 1598.1818181818182,
      "grad_norm": 3.565629243850708,
      "learning_rate": 5.86e-07,
      "loss": 0.3507,
      "step": 8790
    },
    {
      "epoch": 1600.0,
      "grad_norm": 3.731163501739502,
      "learning_rate": 5.866666666666666e-07,
      "loss": 0.3477,
      "step": 8800
    },
    {
      "epoch": 1601.8181818181818,
      "grad_norm": 3.565645933151245,
      "learning_rate": 5.873333333333334e-07,
      "loss": 0.3432,
      "step": 8810
    },
    {
      "epoch": 1603.6363636363637,
      "grad_norm": 3.409813404083252,
      "learning_rate": 5.879999999999999e-07,
      "loss": 0.3211,
      "step": 8820
    },
    {
      "epoch": 1605.4545454545455,
      "grad_norm": 2.997925281524658,
      "learning_rate": 5.886666666666667e-07,
      "loss": 0.363,
      "step": 8830
    },
    {
      "epoch": 1607.2727272727273,
      "grad_norm": 2.626049041748047,
      "learning_rate": 5.893333333333333e-07,
      "loss": 0.3527,
      "step": 8840
    },
    {
      "epoch": 1609.090909090909,
      "grad_norm": 3.10400652885437,
      "learning_rate": 5.9e-07,
      "loss": 0.3236,
      "step": 8850
    },
    {
      "epoch": 1610.909090909091,
      "grad_norm": 3.4885237216949463,
      "learning_rate": 5.906666666666666e-07,
      "loss": 0.3473,
      "step": 8860
    },
    {
      "epoch": 1612.7272727272727,
      "grad_norm": 4.504655838012695,
      "learning_rate": 5.913333333333334e-07,
      "loss": 0.3106,
      "step": 8870
    },
    {
      "epoch": 1614.5454545454545,
      "grad_norm": 2.5902578830718994,
      "learning_rate": 5.919999999999999e-07,
      "loss": 0.3558,
      "step": 8880
    },
    {
      "epoch": 1616.3636363636363,
      "grad_norm": 4.063049793243408,
      "learning_rate": 5.926666666666667e-07,
      "loss": 0.3112,
      "step": 8890
    },
    {
      "epoch": 1618.1818181818182,
      "grad_norm": 3.011932849884033,
      "learning_rate": 5.933333333333334e-07,
      "loss": 0.3404,
      "step": 8900
    },
    {
      "epoch": 1620.0,
      "grad_norm": 2.5625150203704834,
      "learning_rate": 5.939999999999999e-07,
      "loss": 0.3366,
      "step": 8910
    },
    {
      "epoch": 1621.8181818181818,
      "grad_norm": 4.176286220550537,
      "learning_rate": 5.946666666666667e-07,
      "loss": 0.3203,
      "step": 8920
    },
    {
      "epoch": 1623.6363636363637,
      "grad_norm": 3.651848077774048,
      "learning_rate": 5.953333333333333e-07,
      "loss": 0.3213,
      "step": 8930
    },
    {
      "epoch": 1625.4545454545455,
      "grad_norm": 2.7506682872772217,
      "learning_rate": 5.96e-07,
      "loss": 0.3268,
      "step": 8940
    },
    {
      "epoch": 1627.2727272727273,
      "grad_norm": 3.085392951965332,
      "learning_rate": 5.966666666666666e-07,
      "loss": 0.3315,
      "step": 8950
    },
    {
      "epoch": 1629.090909090909,
      "grad_norm": 2.897125244140625,
      "learning_rate": 5.973333333333334e-07,
      "loss": 0.3025,
      "step": 8960
    },
    {
      "epoch": 1630.909090909091,
      "grad_norm": 3.7746052742004395,
      "learning_rate": 5.979999999999999e-07,
      "loss": 0.3142,
      "step": 8970
    },
    {
      "epoch": 1632.7272727272727,
      "grad_norm": 2.4999005794525146,
      "learning_rate": 5.986666666666667e-07,
      "loss": 0.3014,
      "step": 8980
    },
    {
      "epoch": 1634.5454545454545,
      "grad_norm": 2.8394501209259033,
      "learning_rate": 5.993333333333333e-07,
      "loss": 0.3195,
      "step": 8990
    },
    {
      "epoch": 1636.3636363636363,
      "grad_norm": 3.288637161254883,
      "learning_rate": 6e-07,
      "loss": 0.2977,
      "step": 9000
    },
    {
      "epoch": 1636.3636363636363,
      "eval_loss": 1.816873550415039,
      "eval_runtime": 0.9531,
      "eval_samples_per_second": 10.492,
      "eval_steps_per_second": 5.246,
      "step": 9000
    },
    {
      "epoch": 1638.1818181818182,
      "grad_norm": 3.3973915576934814,
      "learning_rate": 6.006666666666666e-07,
      "loss": 0.3149,
      "step": 9010
    },
    {
      "epoch": 1640.0,
      "grad_norm": 2.3796162605285645,
      "learning_rate": 6.013333333333334e-07,
      "loss": 0.305,
      "step": 9020
    },
    {
      "epoch": 1641.8181818181818,
      "grad_norm": 3.458630323410034,
      "learning_rate": 6.019999999999999e-07,
      "loss": 0.3131,
      "step": 9030
    },
    {
      "epoch": 1643.6363636363637,
      "grad_norm": 2.7521979808807373,
      "learning_rate": 6.026666666666667e-07,
      "loss": 0.2989,
      "step": 9040
    },
    {
      "epoch": 1645.4545454545455,
      "grad_norm": 3.6517155170440674,
      "learning_rate": 6.033333333333333e-07,
      "loss": 0.3271,
      "step": 9050
    },
    {
      "epoch": 1647.2727272727273,
      "grad_norm": 3.209892511367798,
      "learning_rate": 6.04e-07,
      "loss": 0.2836,
      "step": 9060
    },
    {
      "epoch": 1649.090909090909,
      "grad_norm": 2.905672788619995,
      "learning_rate": 6.046666666666667e-07,
      "loss": 0.2951,
      "step": 9070
    },
    {
      "epoch": 1650.909090909091,
      "grad_norm": 3.8649442195892334,
      "learning_rate": 6.053333333333332e-07,
      "loss": 0.2976,
      "step": 9080
    },
    {
      "epoch": 1652.7272727272727,
      "grad_norm": 3.36348032951355,
      "learning_rate": 6.06e-07,
      "loss": 0.2726,
      "step": 9090
    },
    {
      "epoch": 1654.5454545454545,
      "grad_norm": 4.996466159820557,
      "learning_rate": 6.066666666666666e-07,
      "loss": 0.326,
      "step": 9100
    },
    {
      "epoch": 1656.3636363636363,
      "grad_norm": 4.386314868927002,
      "learning_rate": 6.073333333333333e-07,
      "loss": 0.2452,
      "step": 9110
    },
    {
      "epoch": 1658.1818181818182,
      "grad_norm": 3.2449049949645996,
      "learning_rate": 6.079999999999999e-07,
      "loss": 0.2954,
      "step": 9120
    },
    {
      "epoch": 1660.0,
      "grad_norm": 2.8548495769500732,
      "learning_rate": 6.086666666666667e-07,
      "loss": 0.2751,
      "step": 9130
    },
    {
      "epoch": 1661.8181818181818,
      "grad_norm": 3.4581077098846436,
      "learning_rate": 6.093333333333332e-07,
      "loss": 0.2776,
      "step": 9140
    },
    {
      "epoch": 1663.6363636363637,
      "grad_norm": 2.6051223278045654,
      "learning_rate": 6.1e-07,
      "loss": 0.2756,
      "step": 9150
    },
    {
      "epoch": 1665.4545454545455,
      "grad_norm": 3.039853572845459,
      "learning_rate": 6.106666666666666e-07,
      "loss": 0.2961,
      "step": 9160
    },
    {
      "epoch": 1667.2727272727273,
      "grad_norm": 2.482903003692627,
      "learning_rate": 6.113333333333333e-07,
      "loss": 0.2708,
      "step": 9170
    },
    {
      "epoch": 1669.090909090909,
      "grad_norm": 3.900857448577881,
      "learning_rate": 6.119999999999999e-07,
      "loss": 0.2615,
      "step": 9180
    },
    {
      "epoch": 1670.909090909091,
      "grad_norm": 3.290616750717163,
      "learning_rate": 6.126666666666667e-07,
      "loss": 0.2814,
      "step": 9190
    },
    {
      "epoch": 1672.7272727272727,
      "grad_norm": 5.485236167907715,
      "learning_rate": 6.133333333333332e-07,
      "loss": 0.2498,
      "step": 9200
    },
    {
      "epoch": 1674.5454545454545,
      "grad_norm": 3.345407724380493,
      "learning_rate": 6.14e-07,
      "loss": 0.2855,
      "step": 9210
    },
    {
      "epoch": 1676.3636363636363,
      "grad_norm": 4.012807846069336,
      "learning_rate": 6.146666666666667e-07,
      "loss": 0.258,
      "step": 9220
    },
    {
      "epoch": 1678.1818181818182,
      "grad_norm": 2.7758963108062744,
      "learning_rate": 6.153333333333333e-07,
      "loss": 0.2751,
      "step": 9230
    },
    {
      "epoch": 1680.0,
      "grad_norm": 3.7055423259735107,
      "learning_rate": 6.16e-07,
      "loss": 0.2681,
      "step": 9240
    },
    {
      "epoch": 1681.8181818181818,
      "grad_norm": 4.088123321533203,
      "learning_rate": 6.166666666666667e-07,
      "loss": 0.2818,
      "step": 9250
    },
    {
      "epoch": 1683.6363636363637,
      "grad_norm": 3.216113328933716,
      "learning_rate": 6.173333333333333e-07,
      "loss": 0.2389,
      "step": 9260
    },
    {
      "epoch": 1685.4545454545455,
      "grad_norm": 4.628371238708496,
      "learning_rate": 6.18e-07,
      "loss": 0.2539,
      "step": 9270
    },
    {
      "epoch": 1687.2727272727273,
      "grad_norm": 2.6736490726470947,
      "learning_rate": 6.186666666666667e-07,
      "loss": 0.2425,
      "step": 9280
    },
    {
      "epoch": 1689.090909090909,
      "grad_norm": 4.523526668548584,
      "learning_rate": 6.193333333333332e-07,
      "loss": 0.2866,
      "step": 9290
    },
    {
      "epoch": 1690.909090909091,
      "grad_norm": 4.310502052307129,
      "learning_rate": 6.2e-07,
      "loss": 0.2382,
      "step": 9300
    },
    {
      "epoch": 1692.7272727272727,
      "grad_norm": 3.105936288833618,
      "learning_rate": 6.206666666666666e-07,
      "loss": 0.2475,
      "step": 9310
    },
    {
      "epoch": 1694.5454545454545,
      "grad_norm": 2.3573691844940186,
      "learning_rate": 6.213333333333333e-07,
      "loss": 0.2717,
      "step": 9320
    },
    {
      "epoch": 1696.3636363636363,
      "grad_norm": 2.600654125213623,
      "learning_rate": 6.219999999999999e-07,
      "loss": 0.2184,
      "step": 9330
    },
    {
      "epoch": 1698.1818181818182,
      "grad_norm": 2.3356947898864746,
      "learning_rate": 6.226666666666667e-07,
      "loss": 0.256,
      "step": 9340
    },
    {
      "epoch": 1700.0,
      "grad_norm": 2.476325273513794,
      "learning_rate": 6.233333333333332e-07,
      "loss": 0.2457,
      "step": 9350
    },
    {
      "epoch": 1701.8181818181818,
      "grad_norm": 3.1660313606262207,
      "learning_rate": 6.24e-07,
      "loss": 0.2387,
      "step": 9360
    },
    {
      "epoch": 1703.6363636363637,
      "grad_norm": 2.8394696712493896,
      "learning_rate": 6.246666666666667e-07,
      "loss": 0.2499,
      "step": 9370
    },
    {
      "epoch": 1705.4545454545455,
      "grad_norm": 2.122433662414551,
      "learning_rate": 6.253333333333333e-07,
      "loss": 0.2226,
      "step": 9380
    },
    {
      "epoch": 1707.2727272727273,
      "grad_norm": 4.8123602867126465,
      "learning_rate": 6.26e-07,
      "loss": 0.2775,
      "step": 9390
    },
    {
      "epoch": 1709.090909090909,
      "grad_norm": 3.1209757328033447,
      "learning_rate": 6.266666666666667e-07,
      "loss": 0.2144,
      "step": 9400
    },
    {
      "epoch": 1710.909090909091,
      "grad_norm": 2.479590654373169,
      "learning_rate": 6.273333333333333e-07,
      "loss": 0.231,
      "step": 9410
    },
    {
      "epoch": 1712.7272727272727,
      "grad_norm": 2.4843509197235107,
      "learning_rate": 6.28e-07,
      "loss": 0.2372,
      "step": 9420
    },
    {
      "epoch": 1714.5454545454545,
      "grad_norm": 3.8837080001831055,
      "learning_rate": 6.286666666666667e-07,
      "loss": 0.2325,
      "step": 9430
    },
    {
      "epoch": 1716.3636363636363,
      "grad_norm": 3.2065236568450928,
      "learning_rate": 6.293333333333333e-07,
      "loss": 0.2495,
      "step": 9440
    },
    {
      "epoch": 1718.1818181818182,
      "grad_norm": 2.6569149494171143,
      "learning_rate": 6.3e-07,
      "loss": 0.2503,
      "step": 9450
    },
    {
      "epoch": 1720.0,
      "grad_norm": 4.330385684967041,
      "learning_rate": 6.306666666666666e-07,
      "loss": 0.2041,
      "step": 9460
    },
    {
      "epoch": 1721.8181818181818,
      "grad_norm": 3.355861186981201,
      "learning_rate": 6.313333333333333e-07,
      "loss": 0.2368,
      "step": 9470
    },
    {
      "epoch": 1723.6363636363637,
      "grad_norm": 3.4373691082000732,
      "learning_rate": 6.319999999999999e-07,
      "loss": 0.206,
      "step": 9480
    },
    {
      "epoch": 1725.4545454545455,
      "grad_norm": 2.272416830062866,
      "learning_rate": 6.326666666666667e-07,
      "loss": 0.2442,
      "step": 9490
    },
    {
      "epoch": 1727.2727272727273,
      "grad_norm": 2.6532089710235596,
      "learning_rate": 6.333333333333332e-07,
      "loss": 0.2156,
      "step": 9500
    },
    {
      "epoch": 1727.2727272727273,
      "eval_loss": 2.0497217178344727,
      "eval_runtime": 0.9522,
      "eval_samples_per_second": 10.503,
      "eval_steps_per_second": 5.251,
      "step": 9500
    },
    {
      "epoch": 1729.090909090909,
      "grad_norm": 3.5146477222442627,
      "learning_rate": 6.34e-07,
      "loss": 0.2097,
      "step": 9510
    },
    {
      "epoch": 1730.909090909091,
      "grad_norm": 2.1929516792297363,
      "learning_rate": 6.346666666666666e-07,
      "loss": 0.2183,
      "step": 9520
    },
    {
      "epoch": 1732.7272727272727,
      "grad_norm": 2.9794750213623047,
      "learning_rate": 6.353333333333333e-07,
      "loss": 0.2365,
      "step": 9530
    },
    {
      "epoch": 1734.5454545454545,
      "grad_norm": 3.743382215499878,
      "learning_rate": 6.36e-07,
      "loss": 0.2172,
      "step": 9540
    },
    {
      "epoch": 1736.3636363636363,
      "grad_norm": 2.1267600059509277,
      "learning_rate": 6.366666666666667e-07,
      "loss": 0.2193,
      "step": 9550
    },
    {
      "epoch": 1738.1818181818182,
      "grad_norm": 2.946805477142334,
      "learning_rate": 6.373333333333333e-07,
      "loss": 0.2209,
      "step": 9560
    },
    {
      "epoch": 1740.0,
      "grad_norm": 7.148573875427246,
      "learning_rate": 6.38e-07,
      "loss": 0.2075,
      "step": 9570
    },
    {
      "epoch": 1741.8181818181818,
      "grad_norm": 3.2454776763916016,
      "learning_rate": 6.386666666666667e-07,
      "loss": 0.21,
      "step": 9580
    },
    {
      "epoch": 1743.6363636363637,
      "grad_norm": 3.177826404571533,
      "learning_rate": 6.393333333333333e-07,
      "loss": 0.2094,
      "step": 9590
    },
    {
      "epoch": 1745.4545454545455,
      "grad_norm": 5.182718753814697,
      "learning_rate": 6.4e-07,
      "loss": 0.2259,
      "step": 9600
    },
    {
      "epoch": 1747.2727272727273,
      "grad_norm": 3.069746494293213,
      "learning_rate": 6.406666666666667e-07,
      "loss": 0.2095,
      "step": 9610
    },
    {
      "epoch": 1749.090909090909,
      "grad_norm": 3.197618246078491,
      "learning_rate": 6.413333333333333e-07,
      "loss": 0.2123,
      "step": 9620
    },
    {
      "epoch": 1750.909090909091,
      "grad_norm": 3.3959646224975586,
      "learning_rate": 6.42e-07,
      "loss": 0.2071,
      "step": 9630
    },
    {
      "epoch": 1752.7272727272727,
      "grad_norm": 2.841904878616333,
      "learning_rate": 6.426666666666667e-07,
      "loss": 0.1963,
      "step": 9640
    },
    {
      "epoch": 1754.5454545454545,
      "grad_norm": 6.360623836517334,
      "learning_rate": 6.433333333333332e-07,
      "loss": 0.2218,
      "step": 9650
    },
    {
      "epoch": 1756.3636363636363,
      "grad_norm": 7.206640243530273,
      "learning_rate": 6.44e-07,
      "loss": 0.1804,
      "step": 9660
    },
    {
      "epoch": 1758.1818181818182,
      "grad_norm": 2.9637203216552734,
      "learning_rate": 6.446666666666666e-07,
      "loss": 0.2038,
      "step": 9670
    },
    {
      "epoch": 1760.0,
      "grad_norm": 4.212893962860107,
      "learning_rate": 6.453333333333333e-07,
      "loss": 0.2023,
      "step": 9680
    },
    {
      "epoch": 1761.8181818181818,
      "grad_norm": 5.246697425842285,
      "learning_rate": 6.46e-07,
      "loss": 0.2113,
      "step": 9690
    },
    {
      "epoch": 1763.6363636363637,
      "grad_norm": 3.993438720703125,
      "learning_rate": 6.466666666666666e-07,
      "loss": 0.1784,
      "step": 9700
    },
    {
      "epoch": 1765.4545454545455,
      "grad_norm": 2.019742488861084,
      "learning_rate": 6.473333333333333e-07,
      "loss": 0.2092,
      "step": 9710
    },
    {
      "epoch": 1767.2727272727273,
      "grad_norm": 5.08542013168335,
      "learning_rate": 6.48e-07,
      "loss": 0.1802,
      "step": 9720
    },
    {
      "epoch": 1769.090909090909,
      "grad_norm": 5.2071003913879395,
      "learning_rate": 6.486666666666666e-07,
      "loss": 0.1948,
      "step": 9730
    },
    {
      "epoch": 1770.909090909091,
      "grad_norm": 4.56139612197876,
      "learning_rate": 6.493333333333333e-07,
      "loss": 0.1951,
      "step": 9740
    },
    {
      "epoch": 1772.7272727272727,
      "grad_norm": 2.8923466205596924,
      "learning_rate": 6.5e-07,
      "loss": 0.2042,
      "step": 9750
    },
    {
      "epoch": 1774.5454545454545,
      "grad_norm": 1.8616390228271484,
      "learning_rate": 6.506666666666666e-07,
      "loss": 0.1618,
      "step": 9760
    },
    {
      "epoch": 1776.3636363636363,
      "grad_norm": 2.5498900413513184,
      "learning_rate": 6.513333333333333e-07,
      "loss": 0.2175,
      "step": 9770
    },
    {
      "epoch": 1778.1818181818182,
      "grad_norm": 5.729847431182861,
      "learning_rate": 6.52e-07,
      "loss": 0.1731,
      "step": 9780
    },
    {
      "epoch": 1780.0,
      "grad_norm": 5.2604780197143555,
      "learning_rate": 6.526666666666666e-07,
      "loss": 0.1948,
      "step": 9790
    },
    {
      "epoch": 1781.8181818181818,
      "grad_norm": 3.1967380046844482,
      "learning_rate": 6.533333333333333e-07,
      "loss": 0.1899,
      "step": 9800
    },
    {
      "epoch": 1783.6363636363637,
      "grad_norm": 4.039566516876221,
      "learning_rate": 6.54e-07,
      "loss": 0.1793,
      "step": 9810
    },
    {
      "epoch": 1785.4545454545455,
      "grad_norm": 3.8617684841156006,
      "learning_rate": 6.546666666666665e-07,
      "loss": 0.1836,
      "step": 9820
    },
    {
      "epoch": 1787.2727272727273,
      "grad_norm": 2.856391429901123,
      "learning_rate": 6.553333333333333e-07,
      "loss": 0.1968,
      "step": 9830
    },
    {
      "epoch": 1789.090909090909,
      "grad_norm": 3.7530581951141357,
      "learning_rate": 6.56e-07,
      "loss": 0.1736,
      "step": 9840
    },
    {
      "epoch": 1790.909090909091,
      "grad_norm": 1.559743046760559,
      "learning_rate": 6.566666666666666e-07,
      "loss": 0.1794,
      "step": 9850
    },
    {
      "epoch": 1792.7272727272727,
      "grad_norm": 4.761434555053711,
      "learning_rate": 6.573333333333333e-07,
      "loss": 0.2042,
      "step": 9860
    },
    {
      "epoch": 1794.5454545454545,
      "grad_norm": 4.124763011932373,
      "learning_rate": 6.58e-07,
      "loss": 0.1837,
      "step": 9870
    },
    {
      "epoch": 1796.3636363636363,
      "grad_norm": 2.4594719409942627,
      "learning_rate": 6.586666666666666e-07,
      "loss": 0.1635,
      "step": 9880
    },
    {
      "epoch": 1798.1818181818182,
      "grad_norm": 4.25388240814209,
      "learning_rate": 6.593333333333333e-07,
      "loss": 0.1971,
      "step": 9890
    },
    {
      "epoch": 1800.0,
      "grad_norm": 3.698904514312744,
      "learning_rate": 6.6e-07,
      "loss": 0.1714,
      "step": 9900
    },
    {
      "epoch": 1801.8181818181818,
      "grad_norm": 2.013470411300659,
      "learning_rate": 6.606666666666666e-07,
      "loss": 0.1766,
      "step": 9910
    },
    {
      "epoch": 1803.6363636363637,
      "grad_norm": 1.793922781944275,
      "learning_rate": 6.613333333333333e-07,
      "loss": 0.1519,
      "step": 9920
    },
    {
      "epoch": 1805.4545454545455,
      "grad_norm": 2.5124807357788086,
      "learning_rate": 6.62e-07,
      "loss": 0.2062,
      "step": 9930
    },
    {
      "epoch": 1807.2727272727273,
      "grad_norm": 3.2585670948028564,
      "learning_rate": 6.626666666666666e-07,
      "loss": 0.1454,
      "step": 9940
    },
    {
      "epoch": 1809.090909090909,
      "grad_norm": 5.6378350257873535,
      "learning_rate": 6.633333333333333e-07,
      "loss": 0.1619,
      "step": 9950
    },
    {
      "epoch": 1810.909090909091,
      "grad_norm": 1.8874459266662598,
      "learning_rate": 6.64e-07,
      "loss": 0.1546,
      "step": 9960
    },
    {
      "epoch": 1812.7272727272727,
      "grad_norm": 2.4263951778411865,
      "learning_rate": 6.646666666666666e-07,
      "loss": 0.1789,
      "step": 9970
    },
    {
      "epoch": 1814.5454545454545,
      "grad_norm": 3.7133586406707764,
      "learning_rate": 6.653333333333333e-07,
      "loss": 0.1586,
      "step": 9980
    },
    {
      "epoch": 1816.3636363636363,
      "grad_norm": 5.61927604675293,
      "learning_rate": 6.66e-07,
      "loss": 0.1586,
      "step": 9990
    },
    {
      "epoch": 1818.1818181818182,
      "grad_norm": 7.272953510284424,
      "learning_rate": 6.666666666666666e-07,
      "loss": 0.1659,
      "step": 10000
    },
    {
      "epoch": 1818.1818181818182,
      "eval_loss": 2.3718907833099365,
      "eval_runtime": 0.9509,
      "eval_samples_per_second": 10.516,
      "eval_steps_per_second": 5.258,
      "step": 10000
    },
    {
      "epoch": 1820.0,
      "grad_norm": 5.264566421508789,
      "learning_rate": 6.673333333333334e-07,
      "loss": 0.1688,
      "step": 10010
    },
    {
      "epoch": 1821.8181818181818,
      "grad_norm": 3.939560651779175,
      "learning_rate": 6.68e-07,
      "loss": 0.1597,
      "step": 10020
    },
    {
      "epoch": 1823.6363636363637,
      "grad_norm": 2.706970691680908,
      "learning_rate": 6.686666666666666e-07,
      "loss": 0.1773,
      "step": 10030
    },
    {
      "epoch": 1825.4545454545455,
      "grad_norm": 4.94699764251709,
      "learning_rate": 6.693333333333333e-07,
      "loss": 0.1381,
      "step": 10040
    },
    {
      "epoch": 1827.2727272727273,
      "grad_norm": 4.757089138031006,
      "learning_rate": 6.7e-07,
      "loss": 0.1596,
      "step": 10050
    },
    {
      "epoch": 1829.090909090909,
      "grad_norm": 1.8673187494277954,
      "learning_rate": 6.706666666666666e-07,
      "loss": 0.1803,
      "step": 10060
    },
    {
      "epoch": 1830.909090909091,
      "grad_norm": 2.4144115447998047,
      "learning_rate": 6.713333333333333e-07,
      "loss": 0.1416,
      "step": 10070
    },
    {
      "epoch": 1832.7272727272727,
      "grad_norm": 1.262791395187378,
      "learning_rate": 6.72e-07,
      "loss": 0.1492,
      "step": 10080
    },
    {
      "epoch": 1834.5454545454545,
      "grad_norm": 2.2447311878204346,
      "learning_rate": 6.726666666666666e-07,
      "loss": 0.171,
      "step": 10090
    },
    {
      "epoch": 1836.3636363636363,
      "grad_norm": 1.7734382152557373,
      "learning_rate": 6.733333333333333e-07,
      "loss": 0.1327,
      "step": 10100
    },
    {
      "epoch": 1838.1818181818182,
      "grad_norm": 2.570019245147705,
      "learning_rate": 6.74e-07,
      "loss": 0.1559,
      "step": 10110
    },
    {
      "epoch": 1840.0,
      "grad_norm": 2.326282024383545,
      "learning_rate": 6.746666666666666e-07,
      "loss": 0.1468,
      "step": 10120
    },
    {
      "epoch": 1841.8181818181818,
      "grad_norm": 3.956286668777466,
      "learning_rate": 6.753333333333333e-07,
      "loss": 0.155,
      "step": 10130
    },
    {
      "epoch": 1843.6363636363637,
      "grad_norm": 3.2219526767730713,
      "learning_rate": 6.76e-07,
      "loss": 0.1485,
      "step": 10140
    },
    {
      "epoch": 1845.4545454545455,
      "grad_norm": 1.6410213708877563,
      "learning_rate": 6.766666666666666e-07,
      "loss": 0.1345,
      "step": 10150
    },
    {
      "epoch": 1847.2727272727273,
      "grad_norm": 6.531579971313477,
      "learning_rate": 6.773333333333334e-07,
      "loss": 0.1638,
      "step": 10160
    },
    {
      "epoch": 1849.090909090909,
      "grad_norm": 3.0676352977752686,
      "learning_rate": 6.78e-07,
      "loss": 0.1608,
      "step": 10170
    },
    {
      "epoch": 1850.909090909091,
      "grad_norm": 1.8803236484527588,
      "learning_rate": 6.786666666666667e-07,
      "loss": 0.1301,
      "step": 10180
    },
    {
      "epoch": 1852.7272727272727,
      "grad_norm": 6.562470436096191,
      "learning_rate": 6.793333333333333e-07,
      "loss": 0.1502,
      "step": 10190
    },
    {
      "epoch": 1854.5454545454545,
      "grad_norm": 2.8140387535095215,
      "learning_rate": 6.800000000000001e-07,
      "loss": 0.1257,
      "step": 10200
    },
    {
      "epoch": 1856.3636363636363,
      "grad_norm": 2.923583507537842,
      "learning_rate": 6.806666666666666e-07,
      "loss": 0.154,
      "step": 10210
    },
    {
      "epoch": 1858.1818181818182,
      "grad_norm": 3.8412508964538574,
      "learning_rate": 6.813333333333333e-07,
      "loss": 0.1616,
      "step": 10220
    },
    {
      "epoch": 1860.0,
      "grad_norm": 2.43855357170105,
      "learning_rate": 6.82e-07,
      "loss": 0.1219,
      "step": 10230
    },
    {
      "epoch": 1861.8181818181818,
      "grad_norm": 4.469852447509766,
      "learning_rate": 6.826666666666666e-07,
      "loss": 0.1325,
      "step": 10240
    },
    {
      "epoch": 1863.6363636363637,
      "grad_norm": 2.0059866905212402,
      "learning_rate": 6.833333333333333e-07,
      "loss": 0.1441,
      "step": 10250
    },
    {
      "epoch": 1865.4545454545455,
      "grad_norm": 6.663400650024414,
      "learning_rate": 6.84e-07,
      "loss": 0.1535,
      "step": 10260
    },
    {
      "epoch": 1867.2727272727273,
      "grad_norm": 8.174712181091309,
      "learning_rate": 6.846666666666666e-07,
      "loss": 0.1353,
      "step": 10270
    },
    {
      "epoch": 1869.090909090909,
      "grad_norm": 7.668515205383301,
      "learning_rate": 6.853333333333333e-07,
      "loss": 0.1496,
      "step": 10280
    },
    {
      "epoch": 1870.909090909091,
      "grad_norm": 3.541612148284912,
      "learning_rate": 6.86e-07,
      "loss": 0.1222,
      "step": 10290
    },
    {
      "epoch": 1872.7272727272727,
      "grad_norm": 6.002834320068359,
      "learning_rate": 6.866666666666666e-07,
      "loss": 0.1293,
      "step": 10300
    },
    {
      "epoch": 1874.5454545454545,
      "grad_norm": 2.5072951316833496,
      "learning_rate": 6.873333333333334e-07,
      "loss": 0.1277,
      "step": 10310
    },
    {
      "epoch": 1876.3636363636363,
      "grad_norm": 1.9763792753219604,
      "learning_rate": 6.879999999999999e-07,
      "loss": 0.1425,
      "step": 10320
    },
    {
      "epoch": 1878.1818181818182,
      "grad_norm": 1.755245327949524,
      "learning_rate": 6.886666666666667e-07,
      "loss": 0.1477,
      "step": 10330
    },
    {
      "epoch": 1880.0,
      "grad_norm": 4.290982246398926,
      "learning_rate": 6.893333333333333e-07,
      "loss": 0.1242,
      "step": 10340
    },
    {
      "epoch": 1881.8181818181818,
      "grad_norm": 3.607173204421997,
      "learning_rate": 6.9e-07,
      "loss": 0.1239,
      "step": 10350
    },
    {
      "epoch": 1883.6363636363637,
      "grad_norm": 5.457785606384277,
      "learning_rate": 6.906666666666666e-07,
      "loss": 0.1257,
      "step": 10360
    },
    {
      "epoch": 1885.4545454545455,
      "grad_norm": 1.6286392211914062,
      "learning_rate": 6.913333333333334e-07,
      "loss": 0.1423,
      "step": 10370
    },
    {
      "epoch": 1887.2727272727273,
      "grad_norm": 4.350770473480225,
      "learning_rate": 6.919999999999999e-07,
      "loss": 0.1151,
      "step": 10380
    },
    {
      "epoch": 1889.090909090909,
      "grad_norm": 3.0152816772460938,
      "learning_rate": 6.926666666666666e-07,
      "loss": 0.1311,
      "step": 10390
    },
    {
      "epoch": 1890.909090909091,
      "grad_norm": 4.750040531158447,
      "learning_rate": 6.933333333333333e-07,
      "loss": 0.121,
      "step": 10400
    },
    {
      "epoch": 1892.7272727272727,
      "grad_norm": 1.5560071468353271,
      "learning_rate": 6.939999999999999e-07,
      "loss": 0.1128,
      "step": 10410
    },
    {
      "epoch": 1894.5454545454545,
      "grad_norm": 3.0896856784820557,
      "learning_rate": 6.946666666666666e-07,
      "loss": 0.1186,
      "step": 10420
    },
    {
      "epoch": 1896.3636363636363,
      "grad_norm": 2.530348062515259,
      "learning_rate": 6.953333333333333e-07,
      "loss": 0.1292,
      "step": 10430
    },
    {
      "epoch": 1898.1818181818182,
      "grad_norm": 4.800229072570801,
      "learning_rate": 6.959999999999999e-07,
      "loss": 0.1182,
      "step": 10440
    },
    {
      "epoch": 1900.0,
      "grad_norm": 2.1247878074645996,
      "learning_rate": 6.966666666666666e-07,
      "loss": 0.1242,
      "step": 10450
    },
    {
      "epoch": 1901.8181818181818,
      "grad_norm": 4.952657222747803,
      "learning_rate": 6.973333333333333e-07,
      "loss": 0.1275,
      "step": 10460
    },
    {
      "epoch": 1903.6363636363637,
      "grad_norm": 4.008337020874023,
      "learning_rate": 6.979999999999999e-07,
      "loss": 0.1064,
      "step": 10470
    },
    {
      "epoch": 1905.4545454545455,
      "grad_norm": 6.610115051269531,
      "learning_rate": 6.986666666666667e-07,
      "loss": 0.1183,
      "step": 10480
    },
    {
      "epoch": 1907.2727272727273,
      "grad_norm": 1.5695501565933228,
      "learning_rate": 6.993333333333333e-07,
      "loss": 0.1405,
      "step": 10490
    },
    {
      "epoch": 1909.090909090909,
      "grad_norm": 1.9632625579833984,
      "learning_rate": 7e-07,
      "loss": 0.1003,
      "step": 10500
    },
    {
      "epoch": 1909.090909090909,
      "eval_loss": 2.6941370964050293,
      "eval_runtime": 0.9498,
      "eval_samples_per_second": 10.529,
      "eval_steps_per_second": 5.264,
      "step": 10500
    },
    {
      "epoch": 1910.909090909091,
      "grad_norm": 2.0482308864593506,
      "learning_rate": 7.006666666666666e-07,
      "loss": 0.1192,
      "step": 10510
    },
    {
      "epoch": 1912.7272727272727,
      "grad_norm": 4.859166622161865,
      "learning_rate": 7.013333333333334e-07,
      "loss": 0.1119,
      "step": 10520
    },
    {
      "epoch": 1914.5454545454545,
      "grad_norm": 3.342564582824707,
      "learning_rate": 7.019999999999999e-07,
      "loss": 0.1048,
      "step": 10530
    },
    {
      "epoch": 1916.3636363636363,
      "grad_norm": 6.0323076248168945,
      "learning_rate": 7.026666666666667e-07,
      "loss": 0.1121,
      "step": 10540
    },
    {
      "epoch": 1918.1818181818182,
      "grad_norm": 1.0970302820205688,
      "learning_rate": 7.033333333333333e-07,
      "loss": 0.1229,
      "step": 10550
    },
    {
      "epoch": 1920.0,
      "grad_norm": 1.3986111879348755,
      "learning_rate": 7.04e-07,
      "loss": 0.1088,
      "step": 10560
    },
    {
      "epoch": 1921.8181818181818,
      "grad_norm": 12.639037132263184,
      "learning_rate": 7.046666666666666e-07,
      "loss": 0.1014,
      "step": 10570
    },
    {
      "epoch": 1923.6363636363637,
      "grad_norm": 2.239503860473633,
      "learning_rate": 7.053333333333333e-07,
      "loss": 0.1236,
      "step": 10580
    },
    {
      "epoch": 1925.4545454545455,
      "grad_norm": 3.4770443439483643,
      "learning_rate": 7.059999999999999e-07,
      "loss": 0.1034,
      "step": 10590
    },
    {
      "epoch": 1927.2727272727273,
      "grad_norm": 1.4906131029129028,
      "learning_rate": 7.066666666666666e-07,
      "loss": 0.0944,
      "step": 10600
    },
    {
      "epoch": 1929.090909090909,
      "grad_norm": 2.5299148559570312,
      "learning_rate": 7.073333333333333e-07,
      "loss": 0.1137,
      "step": 10610
    },
    {
      "epoch": 1930.909090909091,
      "grad_norm": 8.023274421691895,
      "learning_rate": 7.079999999999999e-07,
      "loss": 0.1113,
      "step": 10620
    },
    {
      "epoch": 1932.7272727272727,
      "grad_norm": 5.358431816101074,
      "learning_rate": 7.086666666666667e-07,
      "loss": 0.1147,
      "step": 10630
    },
    {
      "epoch": 1934.5454545454545,
      "grad_norm": 2.53755521774292,
      "learning_rate": 7.093333333333333e-07,
      "loss": 0.0808,
      "step": 10640
    },
    {
      "epoch": 1936.3636363636363,
      "grad_norm": 3.1488969326019287,
      "learning_rate": 7.1e-07,
      "loss": 0.115,
      "step": 10650
    },
    {
      "epoch": 1938.1818181818182,
      "grad_norm": 1.63693368434906,
      "learning_rate": 7.106666666666666e-07,
      "loss": 0.1106,
      "step": 10660
    },
    {
      "epoch": 1940.0,
      "grad_norm": 2.6025047302246094,
      "learning_rate": 7.113333333333334e-07,
      "loss": 0.0953,
      "step": 10670
    },
    {
      "epoch": 1941.8181818181818,
      "grad_norm": 43.726318359375,
      "learning_rate": 7.119999999999999e-07,
      "loss": 0.102,
      "step": 10680
    },
    {
      "epoch": 1943.6363636363637,
      "grad_norm": 4.197887897491455,
      "learning_rate": 7.126666666666667e-07,
      "loss": 0.091,
      "step": 10690
    },
    {
      "epoch": 1945.4545454545455,
      "grad_norm": 6.476441860198975,
      "learning_rate": 7.133333333333333e-07,
      "loss": 0.1156,
      "step": 10700
    },
    {
      "epoch": 1947.2727272727273,
      "grad_norm": 1.085918664932251,
      "learning_rate": 7.14e-07,
      "loss": 0.0904,
      "step": 10710
    },
    {
      "epoch": 1949.090909090909,
      "grad_norm": 2.3253612518310547,
      "learning_rate": 7.146666666666666e-07,
      "loss": 0.1053,
      "step": 10720
    },
    {
      "epoch": 1950.909090909091,
      "grad_norm": 4.696340560913086,
      "learning_rate": 7.153333333333334e-07,
      "loss": 0.0999,
      "step": 10730
    },
    {
      "epoch": 1952.7272727272727,
      "grad_norm": 1.1155081987380981,
      "learning_rate": 7.159999999999999e-07,
      "loss": 0.0987,
      "step": 10740
    },
    {
      "epoch": 1954.5454545454545,
      "grad_norm": 4.43233585357666,
      "learning_rate": 7.166666666666667e-07,
      "loss": 0.0926,
      "step": 10750
    },
    {
      "epoch": 1956.3636363636363,
      "grad_norm": 2.149513006210327,
      "learning_rate": 7.173333333333333e-07,
      "loss": 0.0943,
      "step": 10760
    },
    {
      "epoch": 1958.1818181818182,
      "grad_norm": 3.183069944381714,
      "learning_rate": 7.179999999999999e-07,
      "loss": 0.0902,
      "step": 10770
    },
    {
      "epoch": 1960.0,
      "grad_norm": 4.594140529632568,
      "learning_rate": 7.186666666666667e-07,
      "loss": 0.099,
      "step": 10780
    },
    {
      "epoch": 1961.8181818181818,
      "grad_norm": 5.7783122062683105,
      "learning_rate": 7.193333333333333e-07,
      "loss": 0.093,
      "step": 10790
    },
    {
      "epoch": 1963.6363636363637,
      "grad_norm": 5.729181289672852,
      "learning_rate": 7.2e-07,
      "loss": 0.0832,
      "step": 10800
    },
    {
      "epoch": 1965.4545454545455,
      "grad_norm": 9.522210121154785,
      "learning_rate": 7.206666666666666e-07,
      "loss": 0.1066,
      "step": 10810
    },
    {
      "epoch": 1967.2727272727273,
      "grad_norm": 1.926993489265442,
      "learning_rate": 7.213333333333334e-07,
      "loss": 0.0817,
      "step": 10820
    },
    {
      "epoch": 1969.090909090909,
      "grad_norm": 2.376220941543579,
      "learning_rate": 7.219999999999999e-07,
      "loss": 0.094,
      "step": 10830
    },
    {
      "epoch": 1970.909090909091,
      "grad_norm": 6.296722888946533,
      "learning_rate": 7.226666666666667e-07,
      "loss": 0.0977,
      "step": 10840
    },
    {
      "epoch": 1972.7272727272727,
      "grad_norm": 3.5615971088409424,
      "learning_rate": 7.233333333333333e-07,
      "loss": 0.0737,
      "step": 10850
    },
    {
      "epoch": 1974.5454545454545,
      "grad_norm": 5.4637346267700195,
      "learning_rate": 7.24e-07,
      "loss": 0.1046,
      "step": 10860
    },
    {
      "epoch": 1976.3636363636363,
      "grad_norm": 1.0240906476974487,
      "learning_rate": 7.246666666666666e-07,
      "loss": 0.0817,
      "step": 10870
    },
    {
      "epoch": 1978.1818181818182,
      "grad_norm": 0.956744372844696,
      "learning_rate": 7.253333333333334e-07,
      "loss": 0.098,
      "step": 10880
    },
    {
      "epoch": 1980.0,
      "grad_norm": 63.930755615234375,
      "learning_rate": 7.259999999999999e-07,
      "loss": 0.083,
      "step": 10890
    },
    {
      "epoch": 1981.8181818181818,
      "grad_norm": 5.709168434143066,
      "learning_rate": 7.266666666666667e-07,
      "loss": 0.0936,
      "step": 10900
    },
    {
      "epoch": 1983.6363636363637,
      "grad_norm": 4.076992988586426,
      "learning_rate": 7.273333333333333e-07,
      "loss": 0.0843,
      "step": 10910
    },
    {
      "epoch": 1985.4545454545455,
      "grad_norm": 5.415882110595703,
      "learning_rate": 7.28e-07,
      "loss": 0.0948,
      "step": 10920
    },
    {
      "epoch": 1987.2727272727273,
      "grad_norm": 3.943230152130127,
      "learning_rate": 7.286666666666666e-07,
      "loss": 0.0695,
      "step": 10930
    },
    {
      "epoch": 1989.090909090909,
      "grad_norm": 1.986860990524292,
      "learning_rate": 7.293333333333332e-07,
      "loss": 0.1075,
      "step": 10940
    },
    {
      "epoch": 1990.909090909091,
      "grad_norm": 3.0937938690185547,
      "learning_rate": 7.3e-07,
      "loss": 0.0875,
      "step": 10950
    },
    {
      "epoch": 1992.7272727272727,
      "grad_norm": 4.537914752960205,
      "learning_rate": 7.306666666666666e-07,
      "loss": 0.0735,
      "step": 10960
    },
    {
      "epoch": 1994.5454545454545,
      "grad_norm": 7.163854598999023,
      "learning_rate": 7.313333333333333e-07,
      "loss": 0.0875,
      "step": 10970
    },
    {
      "epoch": 1996.3636363636363,
      "grad_norm": 1.896269679069519,
      "learning_rate": 7.319999999999999e-07,
      "loss": 0.0838,
      "step": 10980
    },
    {
      "epoch": 1998.1818181818182,
      "grad_norm": 4.296438694000244,
      "learning_rate": 7.326666666666667e-07,
      "loss": 0.0811,
      "step": 10990
    },
    {
      "epoch": 2000.0,
      "grad_norm": 2.9008963108062744,
      "learning_rate": 7.333333333333332e-07,
      "loss": 0.085,
      "step": 11000
    },
    {
      "epoch": 2000.0,
      "eval_loss": 2.9184889793395996,
      "eval_runtime": 0.9553,
      "eval_samples_per_second": 10.468,
      "eval_steps_per_second": 5.234,
      "step": 11000
    },
    {
      "epoch": 2001.8181818181818,
      "grad_norm": 1.493526816368103,
      "learning_rate": 7.34e-07,
      "loss": 0.073,
      "step": 11010
    },
    {
      "epoch": 2003.6363636363637,
      "grad_norm": 3.862619161605835,
      "learning_rate": 7.346666666666666e-07,
      "loss": 0.0762,
      "step": 11020
    },
    {
      "epoch": 2005.4545454545455,
      "grad_norm": 5.5643744468688965,
      "learning_rate": 7.353333333333333e-07,
      "loss": 0.0919,
      "step": 11030
    },
    {
      "epoch": 2007.2727272727273,
      "grad_norm": 0.5869947671890259,
      "learning_rate": 7.359999999999999e-07,
      "loss": 0.0758,
      "step": 11040
    },
    {
      "epoch": 2009.090909090909,
      "grad_norm": 5.337042331695557,
      "learning_rate": 7.366666666666667e-07,
      "loss": 0.0821,
      "step": 11050
    },
    {
      "epoch": 2010.909090909091,
      "grad_norm": 1.4503482580184937,
      "learning_rate": 7.373333333333332e-07,
      "loss": 0.077,
      "step": 11060
    },
    {
      "epoch": 2012.7272727272727,
      "grad_norm": 5.257061004638672,
      "learning_rate": 7.38e-07,
      "loss": 0.0811,
      "step": 11070
    },
    {
      "epoch": 2014.5454545454545,
      "grad_norm": 1.688355803489685,
      "learning_rate": 7.386666666666666e-07,
      "loss": 0.0705,
      "step": 11080
    },
    {
      "epoch": 2016.3636363636363,
      "grad_norm": 1.3224475383758545,
      "learning_rate": 7.393333333333333e-07,
      "loss": 0.0624,
      "step": 11090
    },
    {
      "epoch": 2018.1818181818182,
      "grad_norm": 1.0498363971710205,
      "learning_rate": 7.4e-07,
      "loss": 0.0841,
      "step": 11100
    },
    {
      "epoch": 2020.0,
      "grad_norm": 1.3199251890182495,
      "learning_rate": 7.406666666666667e-07,
      "loss": 0.0755,
      "step": 11110
    },
    {
      "epoch": 2021.8181818181818,
      "grad_norm": 1.067864179611206,
      "learning_rate": 7.413333333333333e-07,
      "loss": 0.061,
      "step": 11120
    },
    {
      "epoch": 2023.6363636363637,
      "grad_norm": 8.538860321044922,
      "learning_rate": 7.42e-07,
      "loss": 0.088,
      "step": 11130
    },
    {
      "epoch": 2025.4545454545455,
      "grad_norm": 1.5023748874664307,
      "learning_rate": 7.426666666666667e-07,
      "loss": 0.0658,
      "step": 11140
    },
    {
      "epoch": 2027.2727272727273,
      "grad_norm": 0.664620578289032,
      "learning_rate": 7.433333333333332e-07,
      "loss": 0.0887,
      "step": 11150
    },
    {
      "epoch": 2029.090909090909,
      "grad_norm": 1.3486891984939575,
      "learning_rate": 7.44e-07,
      "loss": 0.0683,
      "step": 11160
    },
    {
      "epoch": 2030.909090909091,
      "grad_norm": 2.461306095123291,
      "learning_rate": 7.446666666666666e-07,
      "loss": 0.073,
      "step": 11170
    },
    {
      "epoch": 2032.7272727272727,
      "grad_norm": 1.102060317993164,
      "learning_rate": 7.453333333333333e-07,
      "loss": 0.0576,
      "step": 11180
    },
    {
      "epoch": 2034.5454545454545,
      "grad_norm": 1.572627305984497,
      "learning_rate": 7.459999999999999e-07,
      "loss": 0.0763,
      "step": 11190
    },
    {
      "epoch": 2036.3636363636363,
      "grad_norm": 0.7237846255302429,
      "learning_rate": 7.466666666666667e-07,
      "loss": 0.0626,
      "step": 11200
    },
    {
      "epoch": 2038.1818181818182,
      "grad_norm": 4.415873050689697,
      "learning_rate": 7.473333333333332e-07,
      "loss": 0.0769,
      "step": 11210
    },
    {
      "epoch": 2040.0,
      "grad_norm": 6.912598133087158,
      "learning_rate": 7.48e-07,
      "loss": 0.0714,
      "step": 11220
    },
    {
      "epoch": 2041.8181818181818,
      "grad_norm": 0.518258810043335,
      "learning_rate": 7.486666666666666e-07,
      "loss": 0.0736,
      "step": 11230
    },
    {
      "epoch": 2043.6363636363637,
      "grad_norm": 1.4930309057235718,
      "learning_rate": 7.493333333333333e-07,
      "loss": 0.043,
      "step": 11240
    },
    {
      "epoch": 2045.4545454545455,
      "grad_norm": 1.0821068286895752,
      "learning_rate": 7.5e-07,
      "loss": 0.0884,
      "step": 11250
    },
    {
      "epoch": 2047.2727272727273,
      "grad_norm": 6.298626899719238,
      "learning_rate": 7.506666666666667e-07,
      "loss": 0.0623,
      "step": 11260
    },
    {
      "epoch": 2049.090909090909,
      "grad_norm": 0.7084165811538696,
      "learning_rate": 7.513333333333333e-07,
      "loss": 0.0666,
      "step": 11270
    },
    {
      "epoch": 2050.909090909091,
      "grad_norm": 0.7957503199577332,
      "learning_rate": 7.52e-07,
      "loss": 0.066,
      "step": 11280
    },
    {
      "epoch": 2052.7272727272725,
      "grad_norm": 0.6429123282432556,
      "learning_rate": 7.526666666666667e-07,
      "loss": 0.0758,
      "step": 11290
    },
    {
      "epoch": 2054.5454545454545,
      "grad_norm": 1.0428650379180908,
      "learning_rate": 7.533333333333332e-07,
      "loss": 0.058,
      "step": 11300
    },
    {
      "epoch": 2056.3636363636365,
      "grad_norm": 5.566152095794678,
      "learning_rate": 7.54e-07,
      "loss": 0.0528,
      "step": 11310
    },
    {
      "epoch": 2058.181818181818,
      "grad_norm": 4.656488418579102,
      "learning_rate": 7.546666666666666e-07,
      "loss": 0.0721,
      "step": 11320
    },
    {
      "epoch": 2060.0,
      "grad_norm": 2.0427772998809814,
      "learning_rate": 7.553333333333333e-07,
      "loss": 0.0659,
      "step": 11330
    },
    {
      "epoch": 2061.818181818182,
      "grad_norm": 6.997532367706299,
      "learning_rate": 7.559999999999999e-07,
      "loss": 0.0594,
      "step": 11340
    },
    {
      "epoch": 2063.6363636363635,
      "grad_norm": 2.474574089050293,
      "learning_rate": 7.566666666666667e-07,
      "loss": 0.0743,
      "step": 11350
    },
    {
      "epoch": 2065.4545454545455,
      "grad_norm": 8.664898872375488,
      "learning_rate": 7.573333333333332e-07,
      "loss": 0.0565,
      "step": 11360
    },
    {
      "epoch": 2067.2727272727275,
      "grad_norm": 0.9994385838508606,
      "learning_rate": 7.58e-07,
      "loss": 0.0605,
      "step": 11370
    },
    {
      "epoch": 2069.090909090909,
      "grad_norm": 9.836149215698242,
      "learning_rate": 7.586666666666666e-07,
      "loss": 0.0612,
      "step": 11380
    },
    {
      "epoch": 2070.909090909091,
      "grad_norm": 1.079354166984558,
      "learning_rate": 7.593333333333333e-07,
      "loss": 0.0658,
      "step": 11390
    },
    {
      "epoch": 2072.7272727272725,
      "grad_norm": 2.5552749633789062,
      "learning_rate": 7.599999999999999e-07,
      "loss": 0.0626,
      "step": 11400
    },
    {
      "epoch": 2074.5454545454545,
      "grad_norm": 1.0769983530044556,
      "learning_rate": 7.606666666666667e-07,
      "loss": 0.0603,
      "step": 11410
    },
    {
      "epoch": 2076.3636363636365,
      "grad_norm": 1.0742974281311035,
      "learning_rate": 7.613333333333333e-07,
      "loss": 0.0569,
      "step": 11420
    },
    {
      "epoch": 2078.181818181818,
      "grad_norm": 0.5444413423538208,
      "learning_rate": 7.62e-07,
      "loss": 0.053,
      "step": 11430
    },
    {
      "epoch": 2080.0,
      "grad_norm": 1.2043782472610474,
      "learning_rate": 7.626666666666667e-07,
      "loss": 0.0641,
      "step": 11440
    },
    {
      "epoch": 2081.818181818182,
      "grad_norm": 2.664064884185791,
      "learning_rate": 7.633333333333333e-07,
      "loss": 0.0615,
      "step": 11450
    },
    {
      "epoch": 2083.6363636363635,
      "grad_norm": 0.45134836435317993,
      "learning_rate": 7.64e-07,
      "loss": 0.0417,
      "step": 11460
    },
    {
      "epoch": 2085.4545454545455,
      "grad_norm": 0.8058925867080688,
      "learning_rate": 7.646666666666667e-07,
      "loss": 0.073,
      "step": 11470
    },
    {
      "epoch": 2087.2727272727275,
      "grad_norm": 5.128025054931641,
      "learning_rate": 7.653333333333333e-07,
      "loss": 0.0656,
      "step": 11480
    },
    {
      "epoch": 2089.090909090909,
      "grad_norm": 0.6683903932571411,
      "learning_rate": 7.66e-07,
      "loss": 0.0446,
      "step": 11490
    },
    {
      "epoch": 2090.909090909091,
      "grad_norm": 0.897466242313385,
      "learning_rate": 7.666666666666667e-07,
      "loss": 0.0491,
      "step": 11500
    },
    {
      "epoch": 2090.909090909091,
      "eval_loss": 3.1537370681762695,
      "eval_runtime": 0.9531,
      "eval_samples_per_second": 10.492,
      "eval_steps_per_second": 5.246,
      "step": 11500
    },
    {
      "epoch": 2092.7272727272725,
      "grad_norm": 0.7094299793243408,
      "learning_rate": 7.673333333333332e-07,
      "loss": 0.0644,
      "step": 11510
    },
    {
      "epoch": 2094.5454545454545,
      "grad_norm": 1.0726292133331299,
      "learning_rate": 7.68e-07,
      "loss": 0.0621,
      "step": 11520
    },
    {
      "epoch": 2096.3636363636365,
      "grad_norm": 0.568096399307251,
      "learning_rate": 7.686666666666666e-07,
      "loss": 0.0517,
      "step": 11530
    },
    {
      "epoch": 2098.181818181818,
      "grad_norm": 4.809057235717773,
      "learning_rate": 7.693333333333333e-07,
      "loss": 0.0473,
      "step": 11540
    },
    {
      "epoch": 2100.0,
      "grad_norm": 1.3609005212783813,
      "learning_rate": 7.699999999999999e-07,
      "loss": 0.0581,
      "step": 11550
    },
    {
      "epoch": 2101.818181818182,
      "grad_norm": 1.26567804813385,
      "learning_rate": 7.706666666666667e-07,
      "loss": 0.0507,
      "step": 11560
    },
    {
      "epoch": 2103.6363636363635,
      "grad_norm": 1.5700433254241943,
      "learning_rate": 7.713333333333333e-07,
      "loss": 0.045,
      "step": 11570
    },
    {
      "epoch": 2105.4545454545455,
      "grad_norm": 1.4234695434570312,
      "learning_rate": 7.72e-07,
      "loss": 0.0544,
      "step": 11580
    },
    {
      "epoch": 2107.2727272727275,
      "grad_norm": 0.6777369976043701,
      "learning_rate": 7.726666666666666e-07,
      "loss": 0.0565,
      "step": 11590
    },
    {
      "epoch": 2109.090909090909,
      "grad_norm": 2.3095314502716064,
      "learning_rate": 7.733333333333333e-07,
      "loss": 0.0565,
      "step": 11600
    },
    {
      "epoch": 2110.909090909091,
      "grad_norm": 9.08948802947998,
      "learning_rate": 7.74e-07,
      "loss": 0.0562,
      "step": 11610
    },
    {
      "epoch": 2112.7272727272725,
      "grad_norm": 5.0526556968688965,
      "learning_rate": 7.746666666666666e-07,
      "loss": 0.0441,
      "step": 11620
    },
    {
      "epoch": 2114.5454545454545,
      "grad_norm": 3.7367048263549805,
      "learning_rate": 7.753333333333333e-07,
      "loss": 0.0658,
      "step": 11630
    },
    {
      "epoch": 2116.3636363636365,
      "grad_norm": 1.7067714929580688,
      "learning_rate": 7.76e-07,
      "loss": 0.0432,
      "step": 11640
    },
    {
      "epoch": 2118.181818181818,
      "grad_norm": 6.556347846984863,
      "learning_rate": 7.766666666666666e-07,
      "loss": 0.0584,
      "step": 11650
    },
    {
      "epoch": 2120.0,
      "grad_norm": 8.520370483398438,
      "learning_rate": 7.773333333333333e-07,
      "loss": 0.0464,
      "step": 11660
    },
    {
      "epoch": 2121.818181818182,
      "grad_norm": 0.7428786158561707,
      "learning_rate": 7.78e-07,
      "loss": 0.0542,
      "step": 11670
    },
    {
      "epoch": 2123.6363636363635,
      "grad_norm": 0.8451181054115295,
      "learning_rate": 7.786666666666665e-07,
      "loss": 0.0528,
      "step": 11680
    },
    {
      "epoch": 2125.4545454545455,
      "grad_norm": 0.9698671698570251,
      "learning_rate": 7.793333333333333e-07,
      "loss": 0.042,
      "step": 11690
    },
    {
      "epoch": 2127.2727272727275,
      "grad_norm": 5.974045753479004,
      "learning_rate": 7.799999999999999e-07,
      "loss": 0.0567,
      "step": 11700
    },
    {
      "epoch": 2129.090909090909,
      "grad_norm": 1.6949089765548706,
      "learning_rate": 7.806666666666666e-07,
      "loss": 0.0484,
      "step": 11710
    },
    {
      "epoch": 2130.909090909091,
      "grad_norm": 0.5022691488265991,
      "learning_rate": 7.813333333333332e-07,
      "loss": 0.041,
      "step": 11720
    },
    {
      "epoch": 2132.7272727272725,
      "grad_norm": 5.297654151916504,
      "learning_rate": 7.82e-07,
      "loss": 0.04,
      "step": 11730
    },
    {
      "epoch": 2134.5454545454545,
      "grad_norm": 0.8839629888534546,
      "learning_rate": 7.826666666666666e-07,
      "loss": 0.0571,
      "step": 11740
    },
    {
      "epoch": 2136.3636363636365,
      "grad_norm": 0.8303709626197815,
      "learning_rate": 7.833333333333333e-07,
      "loss": 0.0357,
      "step": 11750
    },
    {
      "epoch": 2138.181818181818,
      "grad_norm": 4.227946758270264,
      "learning_rate": 7.84e-07,
      "loss": 0.0501,
      "step": 11760
    },
    {
      "epoch": 2140.0,
      "grad_norm": 0.6962376236915588,
      "learning_rate": 7.846666666666666e-07,
      "loss": 0.0504,
      "step": 11770
    },
    {
      "epoch": 2141.818181818182,
      "grad_norm": 2.926116704940796,
      "learning_rate": 7.853333333333333e-07,
      "loss": 0.0498,
      "step": 11780
    },
    {
      "epoch": 2143.6363636363635,
      "grad_norm": 2.506401300430298,
      "learning_rate": 7.86e-07,
      "loss": 0.0468,
      "step": 11790
    },
    {
      "epoch": 2145.4545454545455,
      "grad_norm": 4.220340728759766,
      "learning_rate": 7.866666666666666e-07,
      "loss": 0.0409,
      "step": 11800
    },
    {
      "epoch": 2147.2727272727275,
      "grad_norm": 1.0612727403640747,
      "learning_rate": 7.873333333333333e-07,
      "loss": 0.0602,
      "step": 11810
    },
    {
      "epoch": 2149.090909090909,
      "grad_norm": 8.790849685668945,
      "learning_rate": 7.88e-07,
      "loss": 0.0488,
      "step": 11820
    },
    {
      "epoch": 2150.909090909091,
      "grad_norm": 0.8749176859855652,
      "learning_rate": 7.886666666666666e-07,
      "loss": 0.0419,
      "step": 11830
    },
    {
      "epoch": 2152.7272727272725,
      "grad_norm": 0.35401999950408936,
      "learning_rate": 7.893333333333333e-07,
      "loss": 0.0415,
      "step": 11840
    },
    {
      "epoch": 2154.5454545454545,
      "grad_norm": 9.177783966064453,
      "learning_rate": 7.9e-07,
      "loss": 0.0432,
      "step": 11850
    },
    {
      "epoch": 2156.3636363636365,
      "grad_norm": 0.3200089931488037,
      "learning_rate": 7.906666666666666e-07,
      "loss": 0.0382,
      "step": 11860
    },
    {
      "epoch": 2158.181818181818,
      "grad_norm": 10.44881534576416,
      "learning_rate": 7.913333333333332e-07,
      "loss": 0.0649,
      "step": 11870
    },
    {
      "epoch": 2160.0,
      "grad_norm": 9.805907249450684,
      "learning_rate": 7.92e-07,
      "loss": 0.0308,
      "step": 11880
    },
    {
      "epoch": 2161.818181818182,
      "grad_norm": 1.4400103092193604,
      "learning_rate": 7.926666666666666e-07,
      "loss": 0.043,
      "step": 11890
    },
    {
      "epoch": 2163.6363636363635,
      "grad_norm": 2.8476402759552,
      "learning_rate": 7.933333333333333e-07,
      "loss": 0.0499,
      "step": 11900
    },
    {
      "epoch": 2165.4545454545455,
      "grad_norm": 5.058468341827393,
      "learning_rate": 7.94e-07,
      "loss": 0.0403,
      "step": 11910
    },
    {
      "epoch": 2167.2727272727275,
      "grad_norm": 2.736750841140747,
      "learning_rate": 7.946666666666666e-07,
      "loss": 0.0377,
      "step": 11920
    },
    {
      "epoch": 2169.090909090909,
      "grad_norm": 0.7149469256401062,
      "learning_rate": 7.953333333333333e-07,
      "loss": 0.0422,
      "step": 11930
    },
    {
      "epoch": 2170.909090909091,
      "grad_norm": 0.7177064418792725,
      "learning_rate": 7.96e-07,
      "loss": 0.0446,
      "step": 11940
    },
    {
      "epoch": 2172.7272727272725,
      "grad_norm": 3.196824073791504,
      "learning_rate": 7.966666666666666e-07,
      "loss": 0.0393,
      "step": 11950
    },
    {
      "epoch": 2174.5454545454545,
      "grad_norm": 1.8044580221176147,
      "learning_rate": 7.973333333333333e-07,
      "loss": 0.0474,
      "step": 11960
    },
    {
      "epoch": 2176.3636363636365,
      "grad_norm": 0.89132159948349,
      "learning_rate": 7.98e-07,
      "loss": 0.028,
      "step": 11970
    },
    {
      "epoch": 2178.181818181818,
      "grad_norm": 1.4803746938705444,
      "learning_rate": 7.986666666666666e-07,
      "loss": 0.0498,
      "step": 11980
    },
    {
      "epoch": 2180.0,
      "grad_norm": 4.800841331481934,
      "learning_rate": 7.993333333333333e-07,
      "loss": 0.0389,
      "step": 11990
    },
    {
      "epoch": 2181.818181818182,
      "grad_norm": 5.785830974578857,
      "learning_rate": 8e-07,
      "loss": 0.0328,
      "step": 12000
    },
    {
      "epoch": 2181.818181818182,
      "eval_loss": 3.4084200859069824,
      "eval_runtime": 0.9558,
      "eval_samples_per_second": 10.462,
      "eval_steps_per_second": 5.231,
      "step": 12000
    },
    {
      "epoch": 2183.6363636363635,
      "grad_norm": 4.277651309967041,
      "learning_rate": 8.006666666666666e-07,
      "loss": 0.0406,
      "step": 12010
    },
    {
      "epoch": 2185.4545454545455,
      "grad_norm": 0.7587735652923584,
      "learning_rate": 8.013333333333333e-07,
      "loss": 0.0408,
      "step": 12020
    },
    {
      "epoch": 2187.2727272727275,
      "grad_norm": 0.8656778335571289,
      "learning_rate": 8.02e-07,
      "loss": 0.0444,
      "step": 12030
    },
    {
      "epoch": 2189.090909090909,
      "grad_norm": 0.37272799015045166,
      "learning_rate": 8.026666666666667e-07,
      "loss": 0.0372,
      "step": 12040
    },
    {
      "epoch": 2190.909090909091,
      "grad_norm": 4.22301721572876,
      "learning_rate": 8.033333333333333e-07,
      "loss": 0.0423,
      "step": 12050
    },
    {
      "epoch": 2192.7272727272725,
      "grad_norm": 0.3085803985595703,
      "learning_rate": 8.04e-07,
      "loss": 0.0414,
      "step": 12060
    },
    {
      "epoch": 2194.5454545454545,
      "grad_norm": 0.8441559672355652,
      "learning_rate": 8.046666666666666e-07,
      "loss": 0.031,
      "step": 12070
    },
    {
      "epoch": 2196.3636363636365,
      "grad_norm": 0.40386727452278137,
      "learning_rate": 8.053333333333333e-07,
      "loss": 0.0455,
      "step": 12080
    },
    {
      "epoch": 2198.181818181818,
      "grad_norm": 9.898367881774902,
      "learning_rate": 8.06e-07,
      "loss": 0.0505,
      "step": 12090
    },
    {
      "epoch": 2200.0,
      "grad_norm": 1.5470714569091797,
      "learning_rate": 8.066666666666666e-07,
      "loss": 0.028,
      "step": 12100
    },
    {
      "epoch": 2201.818181818182,
      "grad_norm": 4.409851551055908,
      "learning_rate": 8.073333333333333e-07,
      "loss": 0.031,
      "step": 12110
    },
    {
      "epoch": 2203.6363636363635,
      "grad_norm": 0.8532311320304871,
      "learning_rate": 8.08e-07,
      "loss": 0.035,
      "step": 12120
    },
    {
      "epoch": 2205.4545454545455,
      "grad_norm": 10.896056175231934,
      "learning_rate": 8.086666666666666e-07,
      "loss": 0.0442,
      "step": 12130
    },
    {
      "epoch": 2207.2727272727275,
      "grad_norm": 0.9201125502586365,
      "learning_rate": 8.093333333333333e-07,
      "loss": 0.0321,
      "step": 12140
    },
    {
      "epoch": 2209.090909090909,
      "grad_norm": 0.9565023183822632,
      "learning_rate": 8.1e-07,
      "loss": 0.0392,
      "step": 12150
    },
    {
      "epoch": 2210.909090909091,
      "grad_norm": 3.750732421875,
      "learning_rate": 8.106666666666666e-07,
      "loss": 0.0355,
      "step": 12160
    },
    {
      "epoch": 2212.7272727272725,
      "grad_norm": 0.4521249234676361,
      "learning_rate": 8.113333333333333e-07,
      "loss": 0.0289,
      "step": 12170
    },
    {
      "epoch": 2214.5454545454545,
      "grad_norm": 5.355258464813232,
      "learning_rate": 8.12e-07,
      "loss": 0.0463,
      "step": 12180
    },
    {
      "epoch": 2216.3636363636365,
      "grad_norm": 0.6661847233772278,
      "learning_rate": 8.126666666666666e-07,
      "loss": 0.0258,
      "step": 12190
    },
    {
      "epoch": 2218.181818181818,
      "grad_norm": 0.9948554039001465,
      "learning_rate": 8.133333333333333e-07,
      "loss": 0.0464,
      "step": 12200
    },
    {
      "epoch": 2220.0,
      "grad_norm": 0.9840245842933655,
      "learning_rate": 8.14e-07,
      "loss": 0.0285,
      "step": 12210
    },
    {
      "epoch": 2221.818181818182,
      "grad_norm": 4.447788715362549,
      "learning_rate": 8.146666666666666e-07,
      "loss": 0.037,
      "step": 12220
    },
    {
      "epoch": 2223.6363636363635,
      "grad_norm": 2.52226185798645,
      "learning_rate": 8.153333333333334e-07,
      "loss": 0.0276,
      "step": 12230
    },
    {
      "epoch": 2225.4545454545455,
      "grad_norm": 0.5794894695281982,
      "learning_rate": 8.159999999999999e-07,
      "loss": 0.0413,
      "step": 12240
    },
    {
      "epoch": 2227.2727272727275,
      "grad_norm": 1.26560640335083,
      "learning_rate": 8.166666666666666e-07,
      "loss": 0.0249,
      "step": 12250
    },
    {
      "epoch": 2229.090909090909,
      "grad_norm": 0.7590148448944092,
      "learning_rate": 8.173333333333333e-07,
      "loss": 0.0362,
      "step": 12260
    },
    {
      "epoch": 2230.909090909091,
      "grad_norm": 5.214596271514893,
      "learning_rate": 8.179999999999999e-07,
      "loss": 0.0306,
      "step": 12270
    },
    {
      "epoch": 2232.7272727272725,
      "grad_norm": 0.4969846308231354,
      "learning_rate": 8.186666666666666e-07,
      "loss": 0.0386,
      "step": 12280
    },
    {
      "epoch": 2234.5454545454545,
      "grad_norm": 10.002971649169922,
      "learning_rate": 8.193333333333333e-07,
      "loss": 0.0254,
      "step": 12290
    },
    {
      "epoch": 2236.3636363636365,
      "grad_norm": 0.9902604222297668,
      "learning_rate": 8.199999999999999e-07,
      "loss": 0.0336,
      "step": 12300
    },
    {
      "epoch": 2238.181818181818,
      "grad_norm": 0.9179568886756897,
      "learning_rate": 8.206666666666666e-07,
      "loss": 0.0335,
      "step": 12310
    },
    {
      "epoch": 2240.0,
      "grad_norm": 5.29339599609375,
      "learning_rate": 8.213333333333333e-07,
      "loss": 0.0294,
      "step": 12320
    },
    {
      "epoch": 2241.818181818182,
      "grad_norm": 4.719942569732666,
      "learning_rate": 8.219999999999999e-07,
      "loss": 0.0362,
      "step": 12330
    },
    {
      "epoch": 2243.6363636363635,
      "grad_norm": 6.923512935638428,
      "learning_rate": 8.226666666666666e-07,
      "loss": 0.0251,
      "step": 12340
    },
    {
      "epoch": 2245.4545454545455,
      "grad_norm": 1.055457353591919,
      "learning_rate": 8.233333333333333e-07,
      "loss": 0.0349,
      "step": 12350
    },
    {
      "epoch": 2247.2727272727275,
      "grad_norm": 0.7934008836746216,
      "learning_rate": 8.24e-07,
      "loss": 0.0241,
      "step": 12360
    },
    {
      "epoch": 2249.090909090909,
      "grad_norm": 0.3509369194507599,
      "learning_rate": 8.246666666666666e-07,
      "loss": 0.0319,
      "step": 12370
    },
    {
      "epoch": 2250.909090909091,
      "grad_norm": 1.0227415561676025,
      "learning_rate": 8.253333333333334e-07,
      "loss": 0.0313,
      "step": 12380
    },
    {
      "epoch": 2252.7272727272725,
      "grad_norm": 0.9560341835021973,
      "learning_rate": 8.259999999999999e-07,
      "loss": 0.0336,
      "step": 12390
    },
    {
      "epoch": 2254.5454545454545,
      "grad_norm": 13.199788093566895,
      "learning_rate": 8.266666666666667e-07,
      "loss": 0.0218,
      "step": 12400
    },
    {
      "epoch": 2256.3636363636365,
      "grad_norm": 1.2463959455490112,
      "learning_rate": 8.273333333333333e-07,
      "loss": 0.0337,
      "step": 12410
    },
    {
      "epoch": 2258.181818181818,
      "grad_norm": 0.6720073223114014,
      "learning_rate": 8.28e-07,
      "loss": 0.0351,
      "step": 12420
    },
    {
      "epoch": 2260.0,
      "grad_norm": 2.872846841812134,
      "learning_rate": 8.286666666666666e-07,
      "loss": 0.0251,
      "step": 12430
    },
    {
      "epoch": 2261.818181818182,
      "grad_norm": 0.5746945738792419,
      "learning_rate": 8.293333333333333e-07,
      "loss": 0.0229,
      "step": 12440
    },
    {
      "epoch": 2263.6363636363635,
      "grad_norm": 0.879874587059021,
      "learning_rate": 8.299999999999999e-07,
      "loss": 0.0264,
      "step": 12450
    },
    {
      "epoch": 2265.4545454545455,
      "grad_norm": 0.7911108136177063,
      "learning_rate": 8.306666666666666e-07,
      "loss": 0.0355,
      "step": 12460
    },
    {
      "epoch": 2267.2727272727275,
      "grad_norm": 0.3303190767765045,
      "learning_rate": 8.313333333333333e-07,
      "loss": 0.0228,
      "step": 12470
    },
    {
      "epoch": 2269.090909090909,
      "grad_norm": 3.634831190109253,
      "learning_rate": 8.319999999999999e-07,
      "loss": 0.0292,
      "step": 12480
    },
    {
      "epoch": 2270.909090909091,
      "grad_norm": 7.365029811859131,
      "learning_rate": 8.326666666666666e-07,
      "loss": 0.0283,
      "step": 12490
    },
    {
      "epoch": 2272.7272727272725,
      "grad_norm": 0.8473111391067505,
      "learning_rate": 8.333333333333333e-07,
      "loss": 0.029,
      "step": 12500
    },
    {
      "epoch": 2272.7272727272725,
      "eval_loss": 3.538105010986328,
      "eval_runtime": 0.9524,
      "eval_samples_per_second": 10.5,
      "eval_steps_per_second": 5.25,
      "step": 12500
    },
    {
      "epoch": 2274.5454545454545,
      "grad_norm": 6.2581024169921875,
      "learning_rate": 8.34e-07,
      "loss": 0.0206,
      "step": 12510
    },
    {
      "epoch": 2276.3636363636365,
      "grad_norm": 3.353999376296997,
      "learning_rate": 8.346666666666666e-07,
      "loss": 0.0261,
      "step": 12520
    },
    {
      "epoch": 2278.181818181818,
      "grad_norm": 0.745721161365509,
      "learning_rate": 8.353333333333334e-07,
      "loss": 0.0251,
      "step": 12530
    },
    {
      "epoch": 2280.0,
      "grad_norm": 0.9018505811691284,
      "learning_rate": 8.359999999999999e-07,
      "loss": 0.0275,
      "step": 12540
    },
    {
      "epoch": 2281.818181818182,
      "grad_norm": 5.858501434326172,
      "learning_rate": 8.366666666666667e-07,
      "loss": 0.0273,
      "step": 12550
    },
    {
      "epoch": 2283.6363636363635,
      "grad_norm": 1.9178904294967651,
      "learning_rate": 8.373333333333333e-07,
      "loss": 0.0249,
      "step": 12560
    },
    {
      "epoch": 2285.4545454545455,
      "grad_norm": 3.3755719661712646,
      "learning_rate": 8.38e-07,
      "loss": 0.0185,
      "step": 12570
    },
    {
      "epoch": 2287.2727272727275,
      "grad_norm": 3.1849043369293213,
      "learning_rate": 8.386666666666666e-07,
      "loss": 0.0274,
      "step": 12580
    },
    {
      "epoch": 2289.090909090909,
      "grad_norm": 5.471009254455566,
      "learning_rate": 8.393333333333334e-07,
      "loss": 0.0248,
      "step": 12590
    },
    {
      "epoch": 2290.909090909091,
      "grad_norm": 3.882641077041626,
      "learning_rate": 8.399999999999999e-07,
      "loss": 0.0261,
      "step": 12600
    },
    {
      "epoch": 2292.7272727272725,
      "grad_norm": 0.5048279762268066,
      "learning_rate": 8.406666666666667e-07,
      "loss": 0.0242,
      "step": 12610
    },
    {
      "epoch": 2294.5454545454545,
      "grad_norm": 8.999269485473633,
      "learning_rate": 8.413333333333333e-07,
      "loss": 0.0199,
      "step": 12620
    },
    {
      "epoch": 2296.3636363636365,
      "grad_norm": 3.299077272415161,
      "learning_rate": 8.419999999999999e-07,
      "loss": 0.0223,
      "step": 12630
    },
    {
      "epoch": 2298.181818181818,
      "grad_norm": 2.710827589035034,
      "learning_rate": 8.426666666666666e-07,
      "loss": 0.0248,
      "step": 12640
    },
    {
      "epoch": 2300.0,
      "grad_norm": 3.0935263633728027,
      "learning_rate": 8.433333333333333e-07,
      "loss": 0.0245,
      "step": 12650
    },
    {
      "epoch": 2301.818181818182,
      "grad_norm": 1.5714174509048462,
      "learning_rate": 8.439999999999999e-07,
      "loss": 0.0242,
      "step": 12660
    },
    {
      "epoch": 2303.6363636363635,
      "grad_norm": 12.249873161315918,
      "learning_rate": 8.446666666666666e-07,
      "loss": 0.0243,
      "step": 12670
    },
    {
      "epoch": 2305.4545454545455,
      "grad_norm": 0.6472087502479553,
      "learning_rate": 8.453333333333334e-07,
      "loss": 0.016,
      "step": 12680
    },
    {
      "epoch": 2307.2727272727275,
      "grad_norm": 1.1375672817230225,
      "learning_rate": 8.459999999999999e-07,
      "loss": 0.0286,
      "step": 12690
    },
    {
      "epoch": 2309.090909090909,
      "grad_norm": 0.49818989634513855,
      "learning_rate": 8.466666666666667e-07,
      "loss": 0.0165,
      "step": 12700
    },
    {
      "epoch": 2310.909090909091,
      "grad_norm": 0.3413061499595642,
      "learning_rate": 8.473333333333333e-07,
      "loss": 0.0214,
      "step": 12710
    },
    {
      "epoch": 2312.7272727272725,
      "grad_norm": 1.1871777772903442,
      "learning_rate": 8.48e-07,
      "loss": 0.0218,
      "step": 12720
    },
    {
      "epoch": 2314.5454545454545,
      "grad_norm": 0.7095982432365417,
      "learning_rate": 8.486666666666666e-07,
      "loss": 0.0161,
      "step": 12730
    },
    {
      "epoch": 2316.3636363636365,
      "grad_norm": 5.933640003204346,
      "learning_rate": 8.493333333333334e-07,
      "loss": 0.0241,
      "step": 12740
    },
    {
      "epoch": 2318.181818181818,
      "grad_norm": 1.2603646516799927,
      "learning_rate": 8.499999999999999e-07,
      "loss": 0.028,
      "step": 12750
    },
    {
      "epoch": 2320.0,
      "grad_norm": 3.588273048400879,
      "learning_rate": 8.506666666666667e-07,
      "loss": 0.016,
      "step": 12760
    },
    {
      "epoch": 2321.818181818182,
      "grad_norm": 1.279390811920166,
      "learning_rate": 8.513333333333333e-07,
      "loss": 0.0206,
      "step": 12770
    },
    {
      "epoch": 2323.6363636363635,
      "grad_norm": 6.079954624176025,
      "learning_rate": 8.52e-07,
      "loss": 0.0229,
      "step": 12780
    },
    {
      "epoch": 2325.4545454545455,
      "grad_norm": 7.499696731567383,
      "learning_rate": 8.526666666666666e-07,
      "loss": 0.0214,
      "step": 12790
    },
    {
      "epoch": 2327.2727272727275,
      "grad_norm": 1.9554989337921143,
      "learning_rate": 8.533333333333334e-07,
      "loss": 0.0195,
      "step": 12800
    },
    {
      "epoch": 2329.090909090909,
      "grad_norm": 1.0202487707138062,
      "learning_rate": 8.539999999999999e-07,
      "loss": 0.0198,
      "step": 12810
    },
    {
      "epoch": 2330.909090909091,
      "grad_norm": 0.5459213852882385,
      "learning_rate": 8.546666666666666e-07,
      "loss": 0.0194,
      "step": 12820
    },
    {
      "epoch": 2332.7272727272725,
      "grad_norm": 0.9066125154495239,
      "learning_rate": 8.553333333333333e-07,
      "loss": 0.0205,
      "step": 12830
    },
    {
      "epoch": 2334.5454545454545,
      "grad_norm": 3.3626368045806885,
      "learning_rate": 8.559999999999999e-07,
      "loss": 0.0148,
      "step": 12840
    },
    {
      "epoch": 2336.3636363636365,
      "grad_norm": 0.5578539967536926,
      "learning_rate": 8.566666666666667e-07,
      "loss": 0.0261,
      "step": 12850
    },
    {
      "epoch": 2338.181818181818,
      "grad_norm": 0.6487740278244019,
      "learning_rate": 8.573333333333332e-07,
      "loss": 0.0238,
      "step": 12860
    },
    {
      "epoch": 2340.0,
      "grad_norm": 7.156572341918945,
      "learning_rate": 8.58e-07,
      "loss": 0.0141,
      "step": 12870
    },
    {
      "epoch": 2341.818181818182,
      "grad_norm": 5.625992298126221,
      "learning_rate": 8.586666666666666e-07,
      "loss": 0.0152,
      "step": 12880
    },
    {
      "epoch": 2343.6363636363635,
      "grad_norm": 1.0639885663986206,
      "learning_rate": 8.593333333333333e-07,
      "loss": 0.0226,
      "step": 12890
    },
    {
      "epoch": 2345.4545454545455,
      "grad_norm": 3.2817726135253906,
      "learning_rate": 8.599999999999999e-07,
      "loss": 0.0208,
      "step": 12900
    },
    {
      "epoch": 2347.2727272727275,
      "grad_norm": 1.1857632398605347,
      "learning_rate": 8.606666666666667e-07,
      "loss": 0.0114,
      "step": 12910
    },
    {
      "epoch": 2349.090909090909,
      "grad_norm": 2.625553846359253,
      "learning_rate": 8.613333333333332e-07,
      "loss": 0.0197,
      "step": 12920
    },
    {
      "epoch": 2350.909090909091,
      "grad_norm": 0.6767398715019226,
      "learning_rate": 8.62e-07,
      "loss": 0.0183,
      "step": 12930
    },
    {
      "epoch": 2352.7272727272725,
      "grad_norm": 1.0331923961639404,
      "learning_rate": 8.626666666666666e-07,
      "loss": 0.0169,
      "step": 12940
    },
    {
      "epoch": 2354.5454545454545,
      "grad_norm": 1.8009990453720093,
      "learning_rate": 8.633333333333333e-07,
      "loss": 0.0141,
      "step": 12950
    },
    {
      "epoch": 2356.3636363636365,
      "grad_norm": 2.642594814300537,
      "learning_rate": 8.639999999999999e-07,
      "loss": 0.0182,
      "step": 12960
    },
    {
      "epoch": 2358.181818181818,
      "grad_norm": 0.33587855100631714,
      "learning_rate": 8.646666666666667e-07,
      "loss": 0.0166,
      "step": 12970
    },
    {
      "epoch": 2360.0,
      "grad_norm": 5.953303337097168,
      "learning_rate": 8.653333333333333e-07,
      "loss": 0.0185,
      "step": 12980
    },
    {
      "epoch": 2361.818181818182,
      "grad_norm": 0.2021065056324005,
      "learning_rate": 8.659999999999999e-07,
      "loss": 0.0174,
      "step": 12990
    },
    {
      "epoch": 2363.6363636363635,
      "grad_norm": 3.663104295730591,
      "learning_rate": 8.666666666666667e-07,
      "loss": 0.0165,
      "step": 13000
    },
    {
      "epoch": 2363.6363636363635,
      "eval_loss": 3.58807373046875,
      "eval_runtime": 0.9544,
      "eval_samples_per_second": 10.478,
      "eval_steps_per_second": 5.239,
      "step": 13000
    },
    {
      "epoch": 2365.4545454545455,
      "grad_norm": 0.1713094711303711,
      "learning_rate": 8.673333333333332e-07,
      "loss": 0.0221,
      "step": 13010
    },
    {
      "epoch": 2367.2727272727275,
      "grad_norm": 0.2298213243484497,
      "learning_rate": 8.68e-07,
      "loss": 0.0111,
      "step": 13020
    },
    {
      "epoch": 2369.090909090909,
      "grad_norm": 4.452105522155762,
      "learning_rate": 8.686666666666666e-07,
      "loss": 0.0183,
      "step": 13030
    },
    {
      "epoch": 2370.909090909091,
      "grad_norm": 3.9632728099823,
      "learning_rate": 8.693333333333333e-07,
      "loss": 0.0165,
      "step": 13040
    },
    {
      "epoch": 2372.7272727272725,
      "grad_norm": 0.3138367533683777,
      "learning_rate": 8.699999999999999e-07,
      "loss": 0.0159,
      "step": 13050
    },
    {
      "epoch": 2374.5454545454545,
      "grad_norm": 0.5968543291091919,
      "learning_rate": 8.706666666666667e-07,
      "loss": 0.0163,
      "step": 13060
    },
    {
      "epoch": 2376.3636363636365,
      "grad_norm": 2.1815342903137207,
      "learning_rate": 8.713333333333332e-07,
      "loss": 0.0113,
      "step": 13070
    },
    {
      "epoch": 2378.181818181818,
      "grad_norm": 0.8675844073295593,
      "learning_rate": 8.72e-07,
      "loss": 0.0162,
      "step": 13080
    },
    {
      "epoch": 2380.0,
      "grad_norm": 3.0391807556152344,
      "learning_rate": 8.726666666666666e-07,
      "loss": 0.0161,
      "step": 13090
    },
    {
      "epoch": 2381.818181818182,
      "grad_norm": 0.8067487478256226,
      "learning_rate": 8.733333333333333e-07,
      "loss": 0.016,
      "step": 13100
    },
    {
      "epoch": 2383.6363636363635,
      "grad_norm": 2.859531879425049,
      "learning_rate": 8.739999999999999e-07,
      "loss": 0.0155,
      "step": 13110
    },
    {
      "epoch": 2385.4545454545455,
      "grad_norm": 0.3094111382961273,
      "learning_rate": 8.746666666666667e-07,
      "loss": 0.0118,
      "step": 13120
    },
    {
      "epoch": 2387.2727272727275,
      "grad_norm": 0.47042906284332275,
      "learning_rate": 8.753333333333332e-07,
      "loss": 0.0151,
      "step": 13130
    },
    {
      "epoch": 2389.090909090909,
      "grad_norm": 1.6430153846740723,
      "learning_rate": 8.76e-07,
      "loss": 0.016,
      "step": 13140
    },
    {
      "epoch": 2390.909090909091,
      "grad_norm": 5.606552600860596,
      "learning_rate": 8.766666666666667e-07,
      "loss": 0.0159,
      "step": 13150
    },
    {
      "epoch": 2392.7272727272725,
      "grad_norm": 0.43927323818206787,
      "learning_rate": 8.773333333333332e-07,
      "loss": 0.0149,
      "step": 13160
    },
    {
      "epoch": 2394.5454545454545,
      "grad_norm": 3.7572836875915527,
      "learning_rate": 8.78e-07,
      "loss": 0.015,
      "step": 13170
    },
    {
      "epoch": 2396.3636363636365,
      "grad_norm": 0.9693599939346313,
      "learning_rate": 8.786666666666666e-07,
      "loss": 0.0134,
      "step": 13180
    },
    {
      "epoch": 2398.181818181818,
      "grad_norm": 0.8272145390510559,
      "learning_rate": 8.793333333333333e-07,
      "loss": 0.0104,
      "step": 13190
    },
    {
      "epoch": 2400.0,
      "grad_norm": 4.861095905303955,
      "learning_rate": 8.799999999999999e-07,
      "loss": 0.0145,
      "step": 13200
    },
    {
      "epoch": 2401.818181818182,
      "grad_norm": 0.4465324282646179,
      "learning_rate": 8.806666666666667e-07,
      "loss": 0.0146,
      "step": 13210
    },
    {
      "epoch": 2403.6363636363635,
      "grad_norm": 2.752887010574341,
      "learning_rate": 8.813333333333332e-07,
      "loss": 0.0095,
      "step": 13220
    },
    {
      "epoch": 2405.4545454545455,
      "grad_norm": 0.42246413230895996,
      "learning_rate": 8.82e-07,
      "loss": 0.0175,
      "step": 13230
    },
    {
      "epoch": 2407.2727272727275,
      "grad_norm": 1.995916724205017,
      "learning_rate": 8.826666666666666e-07,
      "loss": 0.01,
      "step": 13240
    },
    {
      "epoch": 2409.090909090909,
      "grad_norm": 0.20232625305652618,
      "learning_rate": 8.833333333333333e-07,
      "loss": 0.0133,
      "step": 13250
    },
    {
      "epoch": 2410.909090909091,
      "grad_norm": 0.17891888320446014,
      "learning_rate": 8.839999999999999e-07,
      "loss": 0.0092,
      "step": 13260
    },
    {
      "epoch": 2412.7272727272725,
      "grad_norm": 0.36369723081588745,
      "learning_rate": 8.846666666666667e-07,
      "loss": 0.0173,
      "step": 13270
    },
    {
      "epoch": 2414.5454545454545,
      "grad_norm": 3.445817708969116,
      "learning_rate": 8.853333333333332e-07,
      "loss": 0.0139,
      "step": 13280
    },
    {
      "epoch": 2416.3636363636365,
      "grad_norm": 5.677600860595703,
      "learning_rate": 8.86e-07,
      "loss": 0.0094,
      "step": 13290
    },
    {
      "epoch": 2418.181818181818,
      "grad_norm": 1.798106074333191,
      "learning_rate": 8.866666666666667e-07,
      "loss": 0.0176,
      "step": 13300
    },
    {
      "epoch": 2420.0,
      "grad_norm": 0.4615532159805298,
      "learning_rate": 8.873333333333333e-07,
      "loss": 0.0128,
      "step": 13310
    },
    {
      "epoch": 2421.818181818182,
      "grad_norm": 1.0196201801300049,
      "learning_rate": 8.88e-07,
      "loss": 0.0132,
      "step": 13320
    },
    {
      "epoch": 2423.6363636363635,
      "grad_norm": 0.1720924824476242,
      "learning_rate": 8.886666666666667e-07,
      "loss": 0.0117,
      "step": 13330
    },
    {
      "epoch": 2425.4545454545455,
      "grad_norm": 0.23000046610832214,
      "learning_rate": 8.893333333333333e-07,
      "loss": 0.0126,
      "step": 13340
    },
    {
      "epoch": 2427.2727272727275,
      "grad_norm": 3.058192491531372,
      "learning_rate": 8.9e-07,
      "loss": 0.0173,
      "step": 13350
    },
    {
      "epoch": 2429.090909090909,
      "grad_norm": 0.3717491924762726,
      "learning_rate": 8.906666666666667e-07,
      "loss": 0.0082,
      "step": 13360
    },
    {
      "epoch": 2430.909090909091,
      "grad_norm": 3.25669002532959,
      "learning_rate": 8.913333333333332e-07,
      "loss": 0.0124,
      "step": 13370
    },
    {
      "epoch": 2432.7272727272725,
      "grad_norm": 1.1208479404449463,
      "learning_rate": 8.92e-07,
      "loss": 0.0129,
      "step": 13380
    },
    {
      "epoch": 2434.5454545454545,
      "grad_norm": 0.26833468675613403,
      "learning_rate": 8.926666666666666e-07,
      "loss": 0.0076,
      "step": 13390
    },
    {
      "epoch": 2436.3636363636365,
      "grad_norm": 2.164289951324463,
      "learning_rate": 8.933333333333333e-07,
      "loss": 0.0162,
      "step": 13400
    },
    {
      "epoch": 2438.181818181818,
      "grad_norm": 3.1025619506835938,
      "learning_rate": 8.939999999999999e-07,
      "loss": 0.0123,
      "step": 13410
    },
    {
      "epoch": 2440.0,
      "grad_norm": 0.15669366717338562,
      "learning_rate": 8.946666666666667e-07,
      "loss": 0.0075,
      "step": 13420
    },
    {
      "epoch": 2441.818181818182,
      "grad_norm": 6.847575664520264,
      "learning_rate": 8.953333333333332e-07,
      "loss": 0.0119,
      "step": 13430
    },
    {
      "epoch": 2443.6363636363635,
      "grad_norm": 3.476266384124756,
      "learning_rate": 8.96e-07,
      "loss": 0.012,
      "step": 13440
    },
    {
      "epoch": 2445.4545454545455,
      "grad_norm": 11.88731861114502,
      "learning_rate": 8.966666666666666e-07,
      "loss": 0.0081,
      "step": 13450
    },
    {
      "epoch": 2447.2727272727275,
      "grad_norm": 2.122617721557617,
      "learning_rate": 8.973333333333333e-07,
      "loss": 0.011,
      "step": 13460
    },
    {
      "epoch": 2449.090909090909,
      "grad_norm": 1.664760708808899,
      "learning_rate": 8.98e-07,
      "loss": 0.0115,
      "step": 13470
    },
    {
      "epoch": 2450.909090909091,
      "grad_norm": 0.40730807185173035,
      "learning_rate": 8.986666666666666e-07,
      "loss": 0.0111,
      "step": 13480
    },
    {
      "epoch": 2452.7272727272725,
      "grad_norm": 1.20340096950531,
      "learning_rate": 8.993333333333333e-07,
      "loss": 0.0113,
      "step": 13490
    },
    {
      "epoch": 2454.5454545454545,
      "grad_norm": 0.7242128849029541,
      "learning_rate": 9e-07,
      "loss": 0.0111,
      "step": 13500
    },
    {
      "epoch": 2454.5454545454545,
      "eval_loss": 3.677098035812378,
      "eval_runtime": 0.9523,
      "eval_samples_per_second": 10.501,
      "eval_steps_per_second": 5.25,
      "step": 13500
    },
    {
      "epoch": 2456.3636363636365,
      "grad_norm": 0.2663363516330719,
      "learning_rate": 9.006666666666666e-07,
      "loss": 0.0073,
      "step": 13510
    },
    {
      "epoch": 2458.181818181818,
      "grad_norm": 2.190809965133667,
      "learning_rate": 9.013333333333333e-07,
      "loss": 0.0122,
      "step": 13520
    },
    {
      "epoch": 2460.0,
      "grad_norm": 0.32247990369796753,
      "learning_rate": 9.02e-07,
      "loss": 0.0098,
      "step": 13530
    },
    {
      "epoch": 2461.818181818182,
      "grad_norm": 0.16167624294757843,
      "learning_rate": 9.026666666666665e-07,
      "loss": 0.0108,
      "step": 13540
    },
    {
      "epoch": 2463.6363636363635,
      "grad_norm": 0.1334536224603653,
      "learning_rate": 9.033333333333333e-07,
      "loss": 0.0108,
      "step": 13550
    },
    {
      "epoch": 2465.4545454545455,
      "grad_norm": 5.712137222290039,
      "learning_rate": 9.039999999999999e-07,
      "loss": 0.0141,
      "step": 13560
    },
    {
      "epoch": 2467.2727272727275,
      "grad_norm": 0.29389333724975586,
      "learning_rate": 9.046666666666666e-07,
      "loss": 0.0094,
      "step": 13570
    },
    {
      "epoch": 2469.090909090909,
      "grad_norm": 0.3535096049308777,
      "learning_rate": 9.053333333333332e-07,
      "loss": 0.0073,
      "step": 13580
    },
    {
      "epoch": 2470.909090909091,
      "grad_norm": 3.6923413276672363,
      "learning_rate": 9.06e-07,
      "loss": 0.0106,
      "step": 13590
    },
    {
      "epoch": 2472.7272727272725,
      "grad_norm": 0.2526715397834778,
      "learning_rate": 9.066666666666665e-07,
      "loss": 0.0099,
      "step": 13600
    },
    {
      "epoch": 2474.5454545454545,
      "grad_norm": 0.42231613397598267,
      "learning_rate": 9.073333333333333e-07,
      "loss": 0.0111,
      "step": 13610
    },
    {
      "epoch": 2476.3636363636365,
      "grad_norm": 0.9847727417945862,
      "learning_rate": 9.08e-07,
      "loss": 0.0088,
      "step": 13620
    },
    {
      "epoch": 2478.181818181818,
      "grad_norm": 0.34025925397872925,
      "learning_rate": 9.086666666666666e-07,
      "loss": 0.0078,
      "step": 13630
    },
    {
      "epoch": 2480.0,
      "grad_norm": 0.5166429877281189,
      "learning_rate": 9.093333333333333e-07,
      "loss": 0.0096,
      "step": 13640
    },
    {
      "epoch": 2481.818181818182,
      "grad_norm": 4.220273971557617,
      "learning_rate": 9.1e-07,
      "loss": 0.0094,
      "step": 13650
    },
    {
      "epoch": 2483.6363636363635,
      "grad_norm": 1.644203543663025,
      "learning_rate": 9.106666666666666e-07,
      "loss": 0.0095,
      "step": 13660
    },
    {
      "epoch": 2485.4545454545455,
      "grad_norm": 2.9058868885040283,
      "learning_rate": 9.113333333333333e-07,
      "loss": 0.0069,
      "step": 13670
    },
    {
      "epoch": 2487.2727272727275,
      "grad_norm": 2.212106704711914,
      "learning_rate": 9.12e-07,
      "loss": 0.0094,
      "step": 13680
    },
    {
      "epoch": 2489.090909090909,
      "grad_norm": 0.13757343590259552,
      "learning_rate": 9.126666666666666e-07,
      "loss": 0.0091,
      "step": 13690
    },
    {
      "epoch": 2490.909090909091,
      "grad_norm": 0.20793184638023376,
      "learning_rate": 9.133333333333333e-07,
      "loss": 0.0089,
      "step": 13700
    },
    {
      "epoch": 2492.7272727272725,
      "grad_norm": 3.2945876121520996,
      "learning_rate": 9.14e-07,
      "loss": 0.0067,
      "step": 13710
    },
    {
      "epoch": 2494.5454545454545,
      "grad_norm": 0.6415713429450989,
      "learning_rate": 9.146666666666666e-07,
      "loss": 0.0115,
      "step": 13720
    },
    {
      "epoch": 2496.3636363636365,
      "grad_norm": 0.2381216287612915,
      "learning_rate": 9.153333333333332e-07,
      "loss": 0.0088,
      "step": 13730
    },
    {
      "epoch": 2498.181818181818,
      "grad_norm": 0.8543508648872375,
      "learning_rate": 9.16e-07,
      "loss": 0.0119,
      "step": 13740
    },
    {
      "epoch": 2500.0,
      "grad_norm": 3.6268608570098877,
      "learning_rate": 9.166666666666665e-07,
      "loss": 0.0063,
      "step": 13750
    },
    {
      "epoch": 2501.818181818182,
      "grad_norm": 2.291745662689209,
      "learning_rate": 9.173333333333333e-07,
      "loss": 0.0084,
      "step": 13760
    },
    {
      "epoch": 2503.6363636363635,
      "grad_norm": 1.036505937576294,
      "learning_rate": 9.18e-07,
      "loss": 0.006,
      "step": 13770
    },
    {
      "epoch": 2505.4545454545455,
      "grad_norm": 2.434216260910034,
      "learning_rate": 9.186666666666666e-07,
      "loss": 0.0105,
      "step": 13780
    },
    {
      "epoch": 2507.2727272727275,
      "grad_norm": 0.9927966594696045,
      "learning_rate": 9.193333333333333e-07,
      "loss": 0.0084,
      "step": 13790
    },
    {
      "epoch": 2509.090909090909,
      "grad_norm": 2.848912239074707,
      "learning_rate": 9.2e-07,
      "loss": 0.0086,
      "step": 13800
    },
    {
      "epoch": 2510.909090909091,
      "grad_norm": 1.118466854095459,
      "learning_rate": 9.206666666666666e-07,
      "loss": 0.006,
      "step": 13810
    },
    {
      "epoch": 2512.7272727272725,
      "grad_norm": 0.13921134173870087,
      "learning_rate": 9.213333333333333e-07,
      "loss": 0.0063,
      "step": 13820
    },
    {
      "epoch": 2514.5454545454545,
      "grad_norm": 7.557579040527344,
      "learning_rate": 9.22e-07,
      "loss": 0.0111,
      "step": 13830
    },
    {
      "epoch": 2516.3636363636365,
      "grad_norm": 0.13441267609596252,
      "learning_rate": 9.226666666666666e-07,
      "loss": 0.0053,
      "step": 13840
    },
    {
      "epoch": 2518.181818181818,
      "grad_norm": 1.0721800327301025,
      "learning_rate": 9.233333333333333e-07,
      "loss": 0.0082,
      "step": 13850
    },
    {
      "epoch": 2520.0,
      "grad_norm": 1.9757354259490967,
      "learning_rate": 9.24e-07,
      "loss": 0.0077,
      "step": 13860
    },
    {
      "epoch": 2521.818181818182,
      "grad_norm": 0.23927634954452515,
      "learning_rate": 9.246666666666666e-07,
      "loss": 0.0082,
      "step": 13870
    },
    {
      "epoch": 2523.6363636363635,
      "grad_norm": 0.6534085273742676,
      "learning_rate": 9.253333333333333e-07,
      "loss": 0.0053,
      "step": 13880
    },
    {
      "epoch": 2525.4545454545455,
      "grad_norm": 1.3961342573165894,
      "learning_rate": 9.26e-07,
      "loss": 0.0087,
      "step": 13890
    },
    {
      "epoch": 2527.2727272727275,
      "grad_norm": 0.3711436092853546,
      "learning_rate": 9.266666666666665e-07,
      "loss": 0.0075,
      "step": 13900
    },
    {
      "epoch": 2529.090909090909,
      "grad_norm": 0.18993709981441498,
      "learning_rate": 9.273333333333333e-07,
      "loss": 0.0075,
      "step": 13910
    },
    {
      "epoch": 2530.909090909091,
      "grad_norm": 0.3473770320415497,
      "learning_rate": 9.28e-07,
      "loss": 0.0075,
      "step": 13920
    },
    {
      "epoch": 2532.7272727272725,
      "grad_norm": 0.8931440711021423,
      "learning_rate": 9.286666666666666e-07,
      "loss": 0.0057,
      "step": 13930
    },
    {
      "epoch": 2534.5454545454545,
      "grad_norm": 4.971845626831055,
      "learning_rate": 9.293333333333333e-07,
      "loss": 0.0072,
      "step": 13940
    },
    {
      "epoch": 2536.3636363636365,
      "grad_norm": 0.16556903719902039,
      "learning_rate": 9.3e-07,
      "loss": 0.0088,
      "step": 13950
    },
    {
      "epoch": 2538.181818181818,
      "grad_norm": 2.806500196456909,
      "learning_rate": 9.306666666666666e-07,
      "loss": 0.0057,
      "step": 13960
    },
    {
      "epoch": 2540.0,
      "grad_norm": 0.08759692311286926,
      "learning_rate": 9.313333333333333e-07,
      "loss": 0.0067,
      "step": 13970
    },
    {
      "epoch": 2541.818181818182,
      "grad_norm": 0.16848795115947723,
      "learning_rate": 9.32e-07,
      "loss": 0.0068,
      "step": 13980
    },
    {
      "epoch": 2543.6363636363635,
      "grad_norm": 3.1279025077819824,
      "learning_rate": 9.326666666666666e-07,
      "loss": 0.0054,
      "step": 13990
    },
    {
      "epoch": 2545.4545454545455,
      "grad_norm": 0.5378156304359436,
      "learning_rate": 9.333333333333333e-07,
      "loss": 0.0082,
      "step": 14000
    },
    {
      "epoch": 2545.4545454545455,
      "eval_loss": 3.7457985877990723,
      "eval_runtime": 0.9519,
      "eval_samples_per_second": 10.506,
      "eval_steps_per_second": 5.253,
      "step": 14000
    },
    {
      "epoch": 2547.2727272727275,
      "grad_norm": 2.2627387046813965,
      "learning_rate": 9.34e-07,
      "loss": 0.0069,
      "step": 14010
    },
    {
      "epoch": 2549.090909090909,
      "grad_norm": 2.802029848098755,
      "learning_rate": 9.346666666666666e-07,
      "loss": 0.0047,
      "step": 14020
    },
    {
      "epoch": 2550.909090909091,
      "grad_norm": 2.667780876159668,
      "learning_rate": 9.353333333333333e-07,
      "loss": 0.0066,
      "step": 14030
    },
    {
      "epoch": 2552.7272727272725,
      "grad_norm": 0.20930281281471252,
      "learning_rate": 9.36e-07,
      "loss": 0.0061,
      "step": 14040
    },
    {
      "epoch": 2554.5454545454545,
      "grad_norm": 1.5441663265228271,
      "learning_rate": 9.366666666666666e-07,
      "loss": 0.0066,
      "step": 14050
    },
    {
      "epoch": 2556.3636363636365,
      "grad_norm": 1.6076180934906006,
      "learning_rate": 9.373333333333333e-07,
      "loss": 0.0046,
      "step": 14060
    },
    {
      "epoch": 2558.181818181818,
      "grad_norm": 1.2335706949234009,
      "learning_rate": 9.379999999999998e-07,
      "loss": 0.0077,
      "step": 14070
    },
    {
      "epoch": 2560.0,
      "grad_norm": 0.8480602502822876,
      "learning_rate": 9.386666666666666e-07,
      "loss": 0.0048,
      "step": 14080
    },
    {
      "epoch": 2561.818181818182,
      "grad_norm": 1.1054651737213135,
      "learning_rate": 9.393333333333334e-07,
      "loss": 0.0059,
      "step": 14090
    },
    {
      "epoch": 2563.6363636363635,
      "grad_norm": 0.46377032995224,
      "learning_rate": 9.399999999999999e-07,
      "loss": 0.006,
      "step": 14100
    },
    {
      "epoch": 2565.4545454545455,
      "grad_norm": 4.021841049194336,
      "learning_rate": 9.406666666666666e-07,
      "loss": 0.0058,
      "step": 14110
    },
    {
      "epoch": 2567.2727272727275,
      "grad_norm": 0.1926283836364746,
      "learning_rate": 9.413333333333333e-07,
      "loss": 0.0041,
      "step": 14120
    },
    {
      "epoch": 2569.090909090909,
      "grad_norm": 0.6199066042900085,
      "learning_rate": 9.419999999999999e-07,
      "loss": 0.0059,
      "step": 14130
    },
    {
      "epoch": 2570.909090909091,
      "grad_norm": 2.5187463760375977,
      "learning_rate": 9.426666666666666e-07,
      "loss": 0.0059,
      "step": 14140
    },
    {
      "epoch": 2572.7272727272725,
      "grad_norm": 0.6068969368934631,
      "learning_rate": 9.433333333333333e-07,
      "loss": 0.0046,
      "step": 14150
    },
    {
      "epoch": 2574.5454545454545,
      "grad_norm": 0.8893827795982361,
      "learning_rate": 9.439999999999999e-07,
      "loss": 0.0046,
      "step": 14160
    },
    {
      "epoch": 2576.3636363636365,
      "grad_norm": 4.361510753631592,
      "learning_rate": 9.446666666666666e-07,
      "loss": 0.0055,
      "step": 14170
    },
    {
      "epoch": 2578.181818181818,
      "grad_norm": 0.5807911157608032,
      "learning_rate": 9.453333333333333e-07,
      "loss": 0.005,
      "step": 14180
    },
    {
      "epoch": 2580.0,
      "grad_norm": 0.1292477548122406,
      "learning_rate": 9.459999999999999e-07,
      "loss": 0.0047,
      "step": 14190
    },
    {
      "epoch": 2581.818181818182,
      "grad_norm": 0.6621835827827454,
      "learning_rate": 9.466666666666666e-07,
      "loss": 0.0048,
      "step": 14200
    },
    {
      "epoch": 2583.6363636363635,
      "grad_norm": 2.8065576553344727,
      "learning_rate": 9.473333333333333e-07,
      "loss": 0.0044,
      "step": 14210
    },
    {
      "epoch": 2585.4545454545455,
      "grad_norm": 2.978863477706909,
      "learning_rate": 9.479999999999999e-07,
      "loss": 0.0058,
      "step": 14220
    },
    {
      "epoch": 2587.2727272727275,
      "grad_norm": 2.674572467803955,
      "learning_rate": 9.486666666666666e-07,
      "loss": 0.0044,
      "step": 14230
    },
    {
      "epoch": 2589.090909090909,
      "grad_norm": 1.0722078084945679,
      "learning_rate": 9.493333333333334e-07,
      "loss": 0.0035,
      "step": 14240
    },
    {
      "epoch": 2590.909090909091,
      "grad_norm": 1.6364216804504395,
      "learning_rate": 9.499999999999999e-07,
      "loss": 0.0047,
      "step": 14250
    },
    {
      "epoch": 2592.7272727272725,
      "grad_norm": 3.2167088985443115,
      "learning_rate": 9.506666666666667e-07,
      "loss": 0.0035,
      "step": 14260
    },
    {
      "epoch": 2594.5454545454545,
      "grad_norm": 1.0620899200439453,
      "learning_rate": 9.513333333333333e-07,
      "loss": 0.0047,
      "step": 14270
    },
    {
      "epoch": 2596.3636363636365,
      "grad_norm": 0.12060501426458359,
      "learning_rate": 9.52e-07,
      "loss": 0.0038,
      "step": 14280
    },
    {
      "epoch": 2598.181818181818,
      "grad_norm": 0.22449660301208496,
      "learning_rate": 9.526666666666666e-07,
      "loss": 0.0039,
      "step": 14290
    },
    {
      "epoch": 2600.0,
      "grad_norm": 2.6258997917175293,
      "learning_rate": 9.533333333333333e-07,
      "loss": 0.0053,
      "step": 14300
    },
    {
      "epoch": 2601.818181818182,
      "grad_norm": 0.07463859021663666,
      "learning_rate": 9.539999999999999e-07,
      "loss": 0.0038,
      "step": 14310
    },
    {
      "epoch": 2603.6363636363635,
      "grad_norm": 0.14570337533950806,
      "learning_rate": 9.546666666666665e-07,
      "loss": 0.0042,
      "step": 14320
    },
    {
      "epoch": 2605.4545454545455,
      "grad_norm": 2.67746901512146,
      "learning_rate": 9.553333333333334e-07,
      "loss": 0.0037,
      "step": 14330
    },
    {
      "epoch": 2607.2727272727275,
      "grad_norm": 1.1404025554656982,
      "learning_rate": 9.559999999999998e-07,
      "loss": 0.004,
      "step": 14340
    },
    {
      "epoch": 2609.090909090909,
      "grad_norm": 0.18337270617485046,
      "learning_rate": 9.566666666666667e-07,
      "loss": 0.0031,
      "step": 14350
    },
    {
      "epoch": 2610.909090909091,
      "grad_norm": 2.6349680423736572,
      "learning_rate": 9.573333333333333e-07,
      "loss": 0.0039,
      "step": 14360
    },
    {
      "epoch": 2612.7272727272725,
      "grad_norm": 3.299983263015747,
      "learning_rate": 9.58e-07,
      "loss": 0.0038,
      "step": 14370
    },
    {
      "epoch": 2614.5454545454545,
      "grad_norm": 1.9798542261123657,
      "learning_rate": 9.586666666666666e-07,
      "loss": 0.0035,
      "step": 14380
    },
    {
      "epoch": 2616.3636363636365,
      "grad_norm": 0.11983011662960052,
      "learning_rate": 9.593333333333333e-07,
      "loss": 0.0033,
      "step": 14390
    },
    {
      "epoch": 2618.181818181818,
      "grad_norm": 0.9296823143959045,
      "learning_rate": 9.6e-07,
      "loss": 0.0045,
      "step": 14400
    },
    {
      "epoch": 2620.0,
      "grad_norm": 1.1484671831130981,
      "learning_rate": 9.606666666666666e-07,
      "loss": 0.0035,
      "step": 14410
    },
    {
      "epoch": 2621.818181818182,
      "grad_norm": 0.12588340044021606,
      "learning_rate": 9.613333333333334e-07,
      "loss": 0.0031,
      "step": 14420
    },
    {
      "epoch": 2623.6363636363635,
      "grad_norm": 0.0866844579577446,
      "learning_rate": 9.619999999999999e-07,
      "loss": 0.0045,
      "step": 14430
    },
    {
      "epoch": 2625.4545454545455,
      "grad_norm": 1.4806373119354248,
      "learning_rate": 9.626666666666667e-07,
      "loss": 0.003,
      "step": 14440
    },
    {
      "epoch": 2627.2727272727275,
      "grad_norm": 0.6983075141906738,
      "learning_rate": 9.633333333333334e-07,
      "loss": 0.0031,
      "step": 14450
    },
    {
      "epoch": 2629.090909090909,
      "grad_norm": 0.19941559433937073,
      "learning_rate": 9.64e-07,
      "loss": 0.0035,
      "step": 14460
    },
    {
      "epoch": 2630.909090909091,
      "grad_norm": 0.7991171479225159,
      "learning_rate": 9.646666666666666e-07,
      "loss": 0.0034,
      "step": 14470
    },
    {
      "epoch": 2632.7272727272725,
      "grad_norm": 0.10287424921989441,
      "learning_rate": 9.653333333333333e-07,
      "loss": 0.0027,
      "step": 14480
    },
    {
      "epoch": 2634.5454545454545,
      "grad_norm": 0.1999015510082245,
      "learning_rate": 9.66e-07,
      "loss": 0.0031,
      "step": 14490
    },
    {
      "epoch": 2636.3636363636365,
      "grad_norm": 0.5402891039848328,
      "learning_rate": 9.666666666666666e-07,
      "loss": 0.0037,
      "step": 14500
    },
    {
      "epoch": 2636.3636363636365,
      "eval_loss": 3.7986302375793457,
      "eval_runtime": 0.9536,
      "eval_samples_per_second": 10.486,
      "eval_steps_per_second": 5.243,
      "step": 14500
    },
    {
      "epoch": 2638.181818181818,
      "grad_norm": 0.11318878084421158,
      "learning_rate": 9.673333333333332e-07,
      "loss": 0.0031,
      "step": 14510
    },
    {
      "epoch": 2640.0,
      "grad_norm": 4.152896404266357,
      "learning_rate": 9.679999999999999e-07,
      "loss": 0.0039,
      "step": 14520
    },
    {
      "epoch": 2641.818181818182,
      "grad_norm": 2.1649863719940186,
      "learning_rate": 9.686666666666667e-07,
      "loss": 0.0033,
      "step": 14530
    },
    {
      "epoch": 2643.6363636363635,
      "grad_norm": 2.499819278717041,
      "learning_rate": 9.693333333333334e-07,
      "loss": 0.0033,
      "step": 14540
    },
    {
      "epoch": 2645.4545454545455,
      "grad_norm": 0.08742077648639679,
      "learning_rate": 9.7e-07,
      "loss": 0.0028,
      "step": 14550
    },
    {
      "epoch": 2647.2727272727275,
      "grad_norm": 0.39304158091545105,
      "learning_rate": 9.706666666666667e-07,
      "loss": 0.0035,
      "step": 14560
    },
    {
      "epoch": 2649.090909090909,
      "grad_norm": 9.608927726745605,
      "learning_rate": 9.713333333333333e-07,
      "loss": 0.0028,
      "step": 14570
    },
    {
      "epoch": 2650.909090909091,
      "grad_norm": 2.1838200092315674,
      "learning_rate": 9.72e-07,
      "loss": 0.0033,
      "step": 14580
    },
    {
      "epoch": 2652.7272727272725,
      "grad_norm": 0.8472939133644104,
      "learning_rate": 9.726666666666666e-07,
      "loss": 0.0028,
      "step": 14590
    },
    {
      "epoch": 2654.5454545454545,
      "grad_norm": 1.2909479141235352,
      "learning_rate": 9.733333333333333e-07,
      "loss": 0.003,
      "step": 14600
    },
    {
      "epoch": 2656.3636363636365,
      "grad_norm": 1.1232019662857056,
      "learning_rate": 9.74e-07,
      "loss": 0.0039,
      "step": 14610
    },
    {
      "epoch": 2658.181818181818,
      "grad_norm": 2.1965017318725586,
      "learning_rate": 9.746666666666666e-07,
      "loss": 0.0028,
      "step": 14620
    },
    {
      "epoch": 2660.0,
      "grad_norm": 2.5546858310699463,
      "learning_rate": 9.753333333333334e-07,
      "loss": 0.0033,
      "step": 14630
    },
    {
      "epoch": 2661.818181818182,
      "grad_norm": 2.1256916522979736,
      "learning_rate": 9.759999999999998e-07,
      "loss": 0.003,
      "step": 14640
    },
    {
      "epoch": 2663.6363636363635,
      "grad_norm": 0.8241149187088013,
      "learning_rate": 9.766666666666667e-07,
      "loss": 0.0029,
      "step": 14650
    },
    {
      "epoch": 2665.4545454545455,
      "grad_norm": 0.7455993294715881,
      "learning_rate": 9.773333333333333e-07,
      "loss": 0.0028,
      "step": 14660
    },
    {
      "epoch": 2667.2727272727275,
      "grad_norm": 0.07289116084575653,
      "learning_rate": 9.78e-07,
      "loss": 0.0029,
      "step": 14670
    },
    {
      "epoch": 2669.090909090909,
      "grad_norm": 0.8576378226280212,
      "learning_rate": 9.786666666666666e-07,
      "loss": 0.0028,
      "step": 14680
    },
    {
      "epoch": 2670.909090909091,
      "grad_norm": 1.4027535915374756,
      "learning_rate": 9.793333333333333e-07,
      "loss": 0.0028,
      "step": 14690
    },
    {
      "epoch": 2672.7272727272725,
      "grad_norm": 0.6207737326622009,
      "learning_rate": 9.8e-07,
      "loss": 0.0038,
      "step": 14700
    },
    {
      "epoch": 2674.5454545454545,
      "grad_norm": 0.15482933819293976,
      "learning_rate": 9.806666666666666e-07,
      "loss": 0.0026,
      "step": 14710
    },
    {
      "epoch": 2676.3636363636365,
      "grad_norm": 0.07789286226034164,
      "learning_rate": 9.813333333333332e-07,
      "loss": 0.003,
      "step": 14720
    },
    {
      "epoch": 2678.181818181818,
      "grad_norm": 1.5938544273376465,
      "learning_rate": 9.819999999999999e-07,
      "loss": 0.0029,
      "step": 14730
    },
    {
      "epoch": 2680.0,
      "grad_norm": 0.49396467208862305,
      "learning_rate": 9.826666666666667e-07,
      "loss": 0.0029,
      "step": 14740
    },
    {
      "epoch": 2681.818181818182,
      "grad_norm": 0.633346438407898,
      "learning_rate": 9.833333333333332e-07,
      "loss": 0.0025,
      "step": 14750
    },
    {
      "epoch": 2683.6363636363635,
      "grad_norm": 1.4886763095855713,
      "learning_rate": 9.84e-07,
      "loss": 0.003,
      "step": 14760
    },
    {
      "epoch": 2685.4545454545455,
      "grad_norm": 0.06737854331731796,
      "learning_rate": 9.846666666666667e-07,
      "loss": 0.0027,
      "step": 14770
    },
    {
      "epoch": 2687.2727272727275,
      "grad_norm": 0.08460718393325806,
      "learning_rate": 9.853333333333333e-07,
      "loss": 0.0029,
      "step": 14780
    },
    {
      "epoch": 2689.090909090909,
      "grad_norm": 1.9827007055282593,
      "learning_rate": 9.86e-07,
      "loss": 0.0029,
      "step": 14790
    },
    {
      "epoch": 2690.909090909091,
      "grad_norm": 0.0795927345752716,
      "learning_rate": 9.866666666666666e-07,
      "loss": 0.0028,
      "step": 14800
    },
    {
      "epoch": 2692.7272727272725,
      "grad_norm": 0.10831572115421295,
      "learning_rate": 9.873333333333333e-07,
      "loss": 0.0028,
      "step": 14810
    },
    {
      "epoch": 2694.5454545454545,
      "grad_norm": 0.10477130860090256,
      "learning_rate": 9.88e-07,
      "loss": 0.0028,
      "step": 14820
    },
    {
      "epoch": 2696.3636363636365,
      "grad_norm": 0.12835247814655304,
      "learning_rate": 9.886666666666665e-07,
      "loss": 0.0026,
      "step": 14830
    },
    {
      "epoch": 2698.181818181818,
      "grad_norm": 1.7407605648040771,
      "learning_rate": 9.893333333333332e-07,
      "loss": 0.003,
      "step": 14840
    },
    {
      "epoch": 2700.0,
      "grad_norm": 2.1357953548431396,
      "learning_rate": 9.9e-07,
      "loss": 0.0025,
      "step": 14850
    },
    {
      "epoch": 2701.818181818182,
      "grad_norm": 0.10352805256843567,
      "learning_rate": 9.906666666666667e-07,
      "loss": 0.0028,
      "step": 14860
    },
    {
      "epoch": 2703.6363636363635,
      "grad_norm": 0.9565452933311462,
      "learning_rate": 9.913333333333333e-07,
      "loss": 0.0026,
      "step": 14870
    },
    {
      "epoch": 2705.4545454545455,
      "grad_norm": 1.0224006175994873,
      "learning_rate": 9.92e-07,
      "loss": 0.0029,
      "step": 14880
    },
    {
      "epoch": 2707.2727272727275,
      "grad_norm": 3.062797784805298,
      "learning_rate": 9.926666666666666e-07,
      "loss": 0.0033,
      "step": 14890
    },
    {
      "epoch": 2709.090909090909,
      "grad_norm": 0.09603041410446167,
      "learning_rate": 9.933333333333333e-07,
      "loss": 0.0023,
      "step": 14900
    },
    {
      "epoch": 2710.909090909091,
      "grad_norm": 1.3314390182495117,
      "learning_rate": 9.94e-07,
      "loss": 0.0024,
      "step": 14910
    },
    {
      "epoch": 2712.7272727272725,
      "grad_norm": 0.30240926146507263,
      "learning_rate": 9.946666666666666e-07,
      "loss": 0.003,
      "step": 14920
    },
    {
      "epoch": 2714.5454545454545,
      "grad_norm": 2.149404764175415,
      "learning_rate": 9.953333333333332e-07,
      "loss": 0.0027,
      "step": 14930
    },
    {
      "epoch": 2716.3636363636365,
      "grad_norm": 0.6816761493682861,
      "learning_rate": 9.959999999999999e-07,
      "loss": 0.0021,
      "step": 14940
    },
    {
      "epoch": 2718.181818181818,
      "grad_norm": 0.16560976207256317,
      "learning_rate": 9.966666666666667e-07,
      "loss": 0.003,
      "step": 14950
    },
    {
      "epoch": 2720.0,
      "grad_norm": 0.3607442378997803,
      "learning_rate": 9.973333333333332e-07,
      "loss": 0.0029,
      "step": 14960
    },
    {
      "epoch": 2721.818181818182,
      "grad_norm": 0.07594795525074005,
      "learning_rate": 9.98e-07,
      "loss": 0.0025,
      "step": 14970
    },
    {
      "epoch": 2723.6363636363635,
      "grad_norm": 1.1960550546646118,
      "learning_rate": 9.986666666666667e-07,
      "loss": 0.0026,
      "step": 14980
    },
    {
      "epoch": 2725.4545454545455,
      "grad_norm": 0.054113488644361496,
      "learning_rate": 9.993333333333333e-07,
      "loss": 0.0027,
      "step": 14990
    },
    {
      "epoch": 2727.2727272727275,
      "grad_norm": 2.2101805210113525,
      "learning_rate": 1e-06,
      "loss": 0.003,
      "step": 15000
    },
    {
      "epoch": 2727.2727272727275,
      "eval_loss": 3.8341832160949707,
      "eval_runtime": 0.953,
      "eval_samples_per_second": 10.493,
      "eval_steps_per_second": 5.247,
      "step": 15000
    },
    {
      "epoch": 2729.090909090909,
      "grad_norm": 0.8569076061248779,
      "learning_rate": 9.99999986461448e-07,
      "loss": 0.0025,
      "step": 15010
    },
    {
      "epoch": 2730.909090909091,
      "grad_norm": 1.707812786102295,
      "learning_rate": 9.99999945845793e-07,
      "loss": 0.0028,
      "step": 15020
    },
    {
      "epoch": 2732.7272727272725,
      "grad_norm": 0.45274221897125244,
      "learning_rate": 9.99999878153037e-07,
      "loss": 0.0027,
      "step": 15030
    },
    {
      "epoch": 2734.5454545454545,
      "grad_norm": 1.165986180305481,
      "learning_rate": 9.999997833831836e-07,
      "loss": 0.0023,
      "step": 15040
    },
    {
      "epoch": 2736.3636363636365,
      "grad_norm": 1.3502918481826782,
      "learning_rate": 9.999996615362384e-07,
      "loss": 0.0023,
      "step": 15050
    },
    {
      "epoch": 2738.181818181818,
      "grad_norm": 0.09023290127515793,
      "learning_rate": 9.999995126122076e-07,
      "loss": 0.0026,
      "step": 15060
    },
    {
      "epoch": 2740.0,
      "grad_norm": 1.1328577995300293,
      "learning_rate": 9.99999336611099e-07,
      "loss": 0.0027,
      "step": 15070
    },
    {
      "epoch": 2741.818181818182,
      "grad_norm": 0.10969430208206177,
      "learning_rate": 9.999991335329227e-07,
      "loss": 0.0022,
      "step": 15080
    },
    {
      "epoch": 2743.6363636363635,
      "grad_norm": 1.9105756282806396,
      "learning_rate": 9.999989033776898e-07,
      "loss": 0.0026,
      "step": 15090
    },
    {
      "epoch": 2745.4545454545455,
      "grad_norm": 0.13602159917354584,
      "learning_rate": 9.999986461454118e-07,
      "loss": 0.0026,
      "step": 15100
    },
    {
      "epoch": 2747.2727272727275,
      "grad_norm": 0.07885009795427322,
      "learning_rate": 9.999983618361037e-07,
      "loss": 0.0024,
      "step": 15110
    },
    {
      "epoch": 2749.090909090909,
      "grad_norm": 0.08016922324895859,
      "learning_rate": 9.999980504497803e-07,
      "loss": 0.0025,
      "step": 15120
    },
    {
      "epoch": 2750.909090909091,
      "grad_norm": 0.38724979758262634,
      "learning_rate": 9.999977119864586e-07,
      "loss": 0.0026,
      "step": 15130
    },
    {
      "epoch": 2752.7272727272725,
      "grad_norm": 0.09150176495313644,
      "learning_rate": 9.999973464461569e-07,
      "loss": 0.0024,
      "step": 15140
    },
    {
      "epoch": 2754.5454545454545,
      "grad_norm": 0.6722246408462524,
      "learning_rate": 9.999969538288952e-07,
      "loss": 0.0028,
      "step": 15150
    },
    {
      "epoch": 2756.3636363636365,
      "grad_norm": 0.07780947536230087,
      "learning_rate": 9.999965341346946e-07,
      "loss": 0.002,
      "step": 15160
    },
    {
      "epoch": 2758.181818181818,
      "grad_norm": 1.3461635112762451,
      "learning_rate": 9.999960873635775e-07,
      "loss": 0.0029,
      "step": 15170
    },
    {
      "epoch": 2760.0,
      "grad_norm": 0.11037406325340271,
      "learning_rate": 9.999956135155687e-07,
      "loss": 0.0023,
      "step": 15180
    },
    {
      "epoch": 2761.818181818182,
      "grad_norm": 0.30977287888526917,
      "learning_rate": 9.999951125906937e-07,
      "loss": 0.0027,
      "step": 15190
    },
    {
      "epoch": 2763.6363636363635,
      "grad_norm": 0.11980646848678589,
      "learning_rate": 9.999945845889793e-07,
      "loss": 0.0021,
      "step": 15200
    },
    {
      "epoch": 2765.4545454545455,
      "grad_norm": 0.09764151275157928,
      "learning_rate": 9.999940295104545e-07,
      "loss": 0.0023,
      "step": 15210
    },
    {
      "epoch": 2767.2727272727275,
      "grad_norm": 0.04955494776368141,
      "learning_rate": 9.99993447355149e-07,
      "loss": 0.0022,
      "step": 15220
    },
    {
      "epoch": 2769.090909090909,
      "grad_norm": 0.5180627107620239,
      "learning_rate": 9.999928381230944e-07,
      "loss": 0.0027,
      "step": 15230
    },
    {
      "epoch": 2770.909090909091,
      "grad_norm": 8.7691068649292,
      "learning_rate": 9.99992201814324e-07,
      "loss": 0.0021,
      "step": 15240
    },
    {
      "epoch": 2772.7272727272725,
      "grad_norm": 0.36768829822540283,
      "learning_rate": 9.99991538428872e-07,
      "loss": 0.0024,
      "step": 15250
    },
    {
      "epoch": 2774.5454545454545,
      "grad_norm": 0.6873840093612671,
      "learning_rate": 9.999908479667745e-07,
      "loss": 0.0023,
      "step": 15260
    },
    {
      "epoch": 2776.3636363636365,
      "grad_norm": 1.5011096000671387,
      "learning_rate": 9.999901304280684e-07,
      "loss": 0.0028,
      "step": 15270
    },
    {
      "epoch": 2778.181818181818,
      "grad_norm": 0.8090760111808777,
      "learning_rate": 9.999893858127932e-07,
      "loss": 0.0022,
      "step": 15280
    },
    {
      "epoch": 2780.0,
      "grad_norm": 1.7052606344223022,
      "learning_rate": 9.99988614120989e-07,
      "loss": 0.0025,
      "step": 15290
    },
    {
      "epoch": 2781.818181818182,
      "grad_norm": 0.10069302469491959,
      "learning_rate": 9.999878153526972e-07,
      "loss": 0.0024,
      "step": 15300
    },
    {
      "epoch": 2783.6363636363635,
      "grad_norm": 0.2485949546098709,
      "learning_rate": 9.999869895079617e-07,
      "loss": 0.0018,
      "step": 15310
    },
    {
      "epoch": 2785.4545454545455,
      "grad_norm": 0.13754558563232422,
      "learning_rate": 9.99986136586827e-07,
      "loss": 0.0025,
      "step": 15320
    },
    {
      "epoch": 2787.2727272727275,
      "grad_norm": 1.455209732055664,
      "learning_rate": 9.999852565893389e-07,
      "loss": 0.0024,
      "step": 15330
    },
    {
      "epoch": 2789.090909090909,
      "grad_norm": 0.19361145794391632,
      "learning_rate": 9.999843495155456e-07,
      "loss": 0.0026,
      "step": 15340
    },
    {
      "epoch": 2790.909090909091,
      "grad_norm": 0.10281253606081009,
      "learning_rate": 9.999834153654958e-07,
      "loss": 0.002,
      "step": 15350
    },
    {
      "epoch": 2792.7272727272725,
      "grad_norm": 0.9629604816436768,
      "learning_rate": 9.999824541392404e-07,
      "loss": 0.0022,
      "step": 15360
    },
    {
      "epoch": 2794.5454545454545,
      "grad_norm": 0.8114795684814453,
      "learning_rate": 9.99981465836831e-07,
      "loss": 0.0023,
      "step": 15370
    },
    {
      "epoch": 2796.3636363636365,
      "grad_norm": 0.6862804293632507,
      "learning_rate": 9.999804504583219e-07,
      "loss": 0.0022,
      "step": 15380
    },
    {
      "epoch": 2798.181818181818,
      "grad_norm": 0.3282937705516815,
      "learning_rate": 9.999794080037674e-07,
      "loss": 0.002,
      "step": 15390
    },
    {
      "epoch": 2800.0,
      "grad_norm": 1.1755027770996094,
      "learning_rate": 9.999783384732241e-07,
      "loss": 0.0023,
      "step": 15400
    },
    {
      "epoch": 2801.818181818182,
      "grad_norm": 0.07786975800991058,
      "learning_rate": 9.999772418667503e-07,
      "loss": 0.0024,
      "step": 15410
    },
    {
      "epoch": 2803.6363636363635,
      "grad_norm": 0.8728021383285522,
      "learning_rate": 9.99976118184405e-07,
      "loss": 0.0025,
      "step": 15420
    },
    {
      "epoch": 2805.4545454545455,
      "grad_norm": 0.08263649791479111,
      "learning_rate": 9.999749674262489e-07,
      "loss": 0.0023,
      "step": 15430
    },
    {
      "epoch": 2807.2727272727275,
      "grad_norm": 0.09827861189842224,
      "learning_rate": 9.999737895923446e-07,
      "loss": 0.0019,
      "step": 15440
    },
    {
      "epoch": 2809.090909090909,
      "grad_norm": 0.07563987374305725,
      "learning_rate": 9.99972584682756e-07,
      "loss": 0.0023,
      "step": 15450
    },
    {
      "epoch": 2810.909090909091,
      "grad_norm": 0.11196539551019669,
      "learning_rate": 9.999713526975482e-07,
      "loss": 0.0022,
      "step": 15460
    },
    {
      "epoch": 2812.7272727272725,
      "grad_norm": 0.06802954524755478,
      "learning_rate": 9.99970093636788e-07,
      "loss": 0.0019,
      "step": 15470
    },
    {
      "epoch": 2814.5454545454545,
      "grad_norm": 1.6881316900253296,
      "learning_rate": 9.999688075005433e-07,
      "loss": 0.0026,
      "step": 15480
    },
    {
      "epoch": 2816.3636363636365,
      "grad_norm": 1.2566709518432617,
      "learning_rate": 9.999674942888838e-07,
      "loss": 0.0019,
      "step": 15490
    },
    {
      "epoch": 2818.181818181818,
      "grad_norm": 0.2376970797777176,
      "learning_rate": 9.99966154001881e-07,
      "loss": 0.0022,
      "step": 15500
    },
    {
      "epoch": 2818.181818181818,
      "eval_loss": 3.9379398822784424,
      "eval_runtime": 0.9523,
      "eval_samples_per_second": 10.5,
      "eval_steps_per_second": 5.25,
      "step": 15500
    },
    {
      "epoch": 2820.0,
      "grad_norm": 0.07449221611022949,
      "learning_rate": 9.999647866396072e-07,
      "loss": 0.0021,
      "step": 15510
    },
    {
      "epoch": 2821.818181818182,
      "grad_norm": 0.35259726643562317,
      "learning_rate": 9.999633922021367e-07,
      "loss": 0.0023,
      "step": 15520
    },
    {
      "epoch": 2823.6363636363635,
      "grad_norm": 0.09559887647628784,
      "learning_rate": 9.999619706895446e-07,
      "loss": 0.0018,
      "step": 15530
    },
    {
      "epoch": 2825.4545454545455,
      "grad_norm": 0.7258638739585876,
      "learning_rate": 9.999605221019081e-07,
      "loss": 0.0024,
      "step": 15540
    },
    {
      "epoch": 2827.2727272727275,
      "grad_norm": 0.05662725120782852,
      "learning_rate": 9.999590464393054e-07,
      "loss": 0.0023,
      "step": 15550
    },
    {
      "epoch": 2829.090909090909,
      "grad_norm": 0.16434240341186523,
      "learning_rate": 9.99957543701817e-07,
      "loss": 0.0022,
      "step": 15560
    },
    {
      "epoch": 2830.909090909091,
      "grad_norm": 0.6775668263435364,
      "learning_rate": 9.999560138895237e-07,
      "loss": 0.0021,
      "step": 15570
    },
    {
      "epoch": 2832.7272727272725,
      "grad_norm": 0.5464466214179993,
      "learning_rate": 9.99954457002509e-07,
      "loss": 0.0024,
      "step": 15580
    },
    {
      "epoch": 2834.5454545454545,
      "grad_norm": 0.894010066986084,
      "learning_rate": 9.999528730408562e-07,
      "loss": 0.0019,
      "step": 15590
    },
    {
      "epoch": 2836.3636363636365,
      "grad_norm": 0.9395221471786499,
      "learning_rate": 9.99951262004652e-07,
      "loss": 0.0021,
      "step": 15600
    },
    {
      "epoch": 2838.181818181818,
      "grad_norm": 0.06033068522810936,
      "learning_rate": 9.999496238939832e-07,
      "loss": 0.0022,
      "step": 15610
    },
    {
      "epoch": 2840.0,
      "grad_norm": 0.07675588130950928,
      "learning_rate": 9.999479587089387e-07,
      "loss": 0.0022,
      "step": 15620
    },
    {
      "epoch": 2841.818181818182,
      "grad_norm": 0.0700821503996849,
      "learning_rate": 9.999462664496088e-07,
      "loss": 0.002,
      "step": 15630
    },
    {
      "epoch": 2843.6363636363635,
      "grad_norm": 1.703946590423584,
      "learning_rate": 9.999445471160845e-07,
      "loss": 0.0026,
      "step": 15640
    },
    {
      "epoch": 2845.4545454545455,
      "grad_norm": 0.06565585732460022,
      "learning_rate": 9.999428007084596e-07,
      "loss": 0.002,
      "step": 15650
    },
    {
      "epoch": 2847.2727272727275,
      "grad_norm": 0.058113742619752884,
      "learning_rate": 9.999410272268284e-07,
      "loss": 0.0021,
      "step": 15660
    },
    {
      "epoch": 2849.090909090909,
      "grad_norm": 0.7786396741867065,
      "learning_rate": 9.999392266712869e-07,
      "loss": 0.0025,
      "step": 15670
    },
    {
      "epoch": 2850.909090909091,
      "grad_norm": 0.06947535276412964,
      "learning_rate": 9.999373990419326e-07,
      "loss": 0.0022,
      "step": 15680
    },
    {
      "epoch": 2852.7272727272725,
      "grad_norm": 0.3006768226623535,
      "learning_rate": 9.999355443388648e-07,
      "loss": 0.0026,
      "step": 15690
    },
    {
      "epoch": 2854.5454545454545,
      "grad_norm": 0.7030467391014099,
      "learning_rate": 9.999336625621835e-07,
      "loss": 0.0019,
      "step": 15700
    },
    {
      "epoch": 2856.3636363636365,
      "grad_norm": 1.102319598197937,
      "learning_rate": 9.99931753711991e-07,
      "loss": 0.0023,
      "step": 15710
    },
    {
      "epoch": 2858.181818181818,
      "grad_norm": 0.7487431764602661,
      "learning_rate": 9.999298177883901e-07,
      "loss": 0.002,
      "step": 15720
    },
    {
      "epoch": 2860.0,
      "grad_norm": 0.25347045063972473,
      "learning_rate": 9.999278547914862e-07,
      "loss": 0.0021,
      "step": 15730
    },
    {
      "epoch": 2861.818181818182,
      "grad_norm": 1.0025187730789185,
      "learning_rate": 9.999258647213855e-07,
      "loss": 0.0023,
      "step": 15740
    },
    {
      "epoch": 2863.6363636363635,
      "grad_norm": 0.055430758744478226,
      "learning_rate": 9.999238475781957e-07,
      "loss": 0.0018,
      "step": 15750
    },
    {
      "epoch": 2865.4545454545455,
      "grad_norm": 0.04150494188070297,
      "learning_rate": 9.999218033620259e-07,
      "loss": 0.0021,
      "step": 15760
    },
    {
      "epoch": 2867.2727272727275,
      "grad_norm": 2.1405999660491943,
      "learning_rate": 9.999197320729869e-07,
      "loss": 0.0022,
      "step": 15770
    },
    {
      "epoch": 2869.090909090909,
      "grad_norm": 0.3705218732357025,
      "learning_rate": 9.999176337111907e-07,
      "loss": 0.0019,
      "step": 15780
    },
    {
      "epoch": 2870.909090909091,
      "grad_norm": 0.13335727155208588,
      "learning_rate": 9.999155082767514e-07,
      "loss": 0.002,
      "step": 15790
    },
    {
      "epoch": 2872.7272727272725,
      "grad_norm": 0.12286029756069183,
      "learning_rate": 9.999133557697838e-07,
      "loss": 0.0018,
      "step": 15800
    },
    {
      "epoch": 2874.5454545454545,
      "grad_norm": 0.2444567233324051,
      "learning_rate": 9.999111761904044e-07,
      "loss": 0.0024,
      "step": 15810
    },
    {
      "epoch": 2876.3636363636365,
      "grad_norm": 0.9925360083580017,
      "learning_rate": 9.999089695387316e-07,
      "loss": 0.0021,
      "step": 15820
    },
    {
      "epoch": 2878.181818181818,
      "grad_norm": 0.04185393825173378,
      "learning_rate": 9.999067358148844e-07,
      "loss": 0.0019,
      "step": 15830
    },
    {
      "epoch": 2880.0,
      "grad_norm": 0.06690715998411179,
      "learning_rate": 9.999044750189837e-07,
      "loss": 0.0024,
      "step": 15840
    },
    {
      "epoch": 2881.818181818182,
      "grad_norm": 0.05039828270673752,
      "learning_rate": 9.999021871511525e-07,
      "loss": 0.0025,
      "step": 15850
    },
    {
      "epoch": 2883.6363636363635,
      "grad_norm": 0.08556533604860306,
      "learning_rate": 9.998998722115146e-07,
      "loss": 0.0021,
      "step": 15860
    },
    {
      "epoch": 2885.4545454545455,
      "grad_norm": 0.034898541867733,
      "learning_rate": 9.99897530200195e-07,
      "loss": 0.0021,
      "step": 15870
    },
    {
      "epoch": 2887.2727272727275,
      "grad_norm": 0.21788690984249115,
      "learning_rate": 9.998951611173207e-07,
      "loss": 0.0023,
      "step": 15880
    },
    {
      "epoch": 2889.090909090909,
      "grad_norm": 0.4399966895580292,
      "learning_rate": 9.9989276496302e-07,
      "loss": 0.0019,
      "step": 15890
    },
    {
      "epoch": 2890.909090909091,
      "grad_norm": 0.05744246765971184,
      "learning_rate": 9.998903417374226e-07,
      "loss": 0.0022,
      "step": 15900
    },
    {
      "epoch": 2892.7272727272725,
      "grad_norm": 0.9153627157211304,
      "learning_rate": 9.9988789144066e-07,
      "loss": 0.0019,
      "step": 15910
    },
    {
      "epoch": 2894.5454545454545,
      "grad_norm": 0.2729106843471527,
      "learning_rate": 9.998854140728645e-07,
      "loss": 0.0025,
      "step": 15920
    },
    {
      "epoch": 2896.3636363636365,
      "grad_norm": 0.12522348761558533,
      "learning_rate": 9.998829096341705e-07,
      "loss": 0.0024,
      "step": 15930
    },
    {
      "epoch": 2898.181818181818,
      "grad_norm": 0.47026315331459045,
      "learning_rate": 9.998803781247137e-07,
      "loss": 0.002,
      "step": 15940
    },
    {
      "epoch": 2900.0,
      "grad_norm": 0.18641582131385803,
      "learning_rate": 9.99877819544631e-07,
      "loss": 0.0021,
      "step": 15950
    },
    {
      "epoch": 2901.818181818182,
      "grad_norm": 0.05797098949551582,
      "learning_rate": 9.99875233894061e-07,
      "loss": 0.0022,
      "step": 15960
    },
    {
      "epoch": 2903.6363636363635,
      "grad_norm": 0.7776474356651306,
      "learning_rate": 9.99872621173144e-07,
      "loss": 0.0019,
      "step": 15970
    },
    {
      "epoch": 2905.4545454545455,
      "grad_norm": 1.2294657230377197,
      "learning_rate": 9.998699813820208e-07,
      "loss": 0.0016,
      "step": 15980
    },
    {
      "epoch": 2907.2727272727275,
      "grad_norm": 0.5527902841567993,
      "learning_rate": 9.99867314520835e-07,
      "loss": 0.0019,
      "step": 15990
    },
    {
      "epoch": 2909.090909090909,
      "grad_norm": 0.901589035987854,
      "learning_rate": 9.998646205897307e-07,
      "loss": 0.0025,
      "step": 16000
    },
    {
      "epoch": 2909.090909090909,
      "eval_loss": 3.8628997802734375,
      "eval_runtime": 0.9515,
      "eval_samples_per_second": 10.51,
      "eval_steps_per_second": 5.255,
      "step": 16000
    },
    {
      "epoch": 2910.909090909091,
      "grad_norm": 0.8785535097122192,
      "learning_rate": 9.99861899588854e-07,
      "loss": 0.0018,
      "step": 16010
    },
    {
      "epoch": 2912.7272727272725,
      "grad_norm": 0.2722472548484802,
      "learning_rate": 9.998591515183523e-07,
      "loss": 0.0022,
      "step": 16020
    },
    {
      "epoch": 2914.5454545454545,
      "grad_norm": 0.05224797502160072,
      "learning_rate": 9.998563763783742e-07,
      "loss": 0.0017,
      "step": 16030
    },
    {
      "epoch": 2916.3636363636365,
      "grad_norm": 0.06002625823020935,
      "learning_rate": 9.9985357416907e-07,
      "loss": 0.002,
      "step": 16040
    },
    {
      "epoch": 2918.181818181818,
      "grad_norm": 0.95304936170578,
      "learning_rate": 9.998507448905916e-07,
      "loss": 0.0021,
      "step": 16050
    },
    {
      "epoch": 2920.0,
      "grad_norm": 1.1027816534042358,
      "learning_rate": 9.99847888543092e-07,
      "loss": 0.002,
      "step": 16060
    },
    {
      "epoch": 2921.818181818182,
      "grad_norm": 0.6413670182228088,
      "learning_rate": 9.998450051267261e-07,
      "loss": 0.0021,
      "step": 16070
    },
    {
      "epoch": 2923.6363636363635,
      "grad_norm": 0.31377214193344116,
      "learning_rate": 9.998420946416499e-07,
      "loss": 0.0017,
      "step": 16080
    },
    {
      "epoch": 2925.4545454545455,
      "grad_norm": 0.1715008169412613,
      "learning_rate": 9.998391570880211e-07,
      "loss": 0.0019,
      "step": 16090
    },
    {
      "epoch": 2927.2727272727275,
      "grad_norm": 0.6186226606369019,
      "learning_rate": 9.99836192465999e-07,
      "loss": 0.0021,
      "step": 16100
    },
    {
      "epoch": 2929.090909090909,
      "grad_norm": 0.884672224521637,
      "learning_rate": 9.998332007757436e-07,
      "loss": 0.0017,
      "step": 16110
    },
    {
      "epoch": 2930.909090909091,
      "grad_norm": 0.05904809758067131,
      "learning_rate": 9.998301820174173e-07,
      "loss": 0.0021,
      "step": 16120
    },
    {
      "epoch": 2932.7272727272725,
      "grad_norm": 0.8289334177970886,
      "learning_rate": 9.998271361911835e-07,
      "loss": 0.0016,
      "step": 16130
    },
    {
      "epoch": 2934.5454545454545,
      "grad_norm": 0.6631065607070923,
      "learning_rate": 9.998240632972072e-07,
      "loss": 0.0021,
      "step": 16140
    },
    {
      "epoch": 2936.3636363636365,
      "grad_norm": 0.8045446872711182,
      "learning_rate": 9.998209633356547e-07,
      "loss": 0.002,
      "step": 16150
    },
    {
      "epoch": 2938.181818181818,
      "grad_norm": 1.5493218898773193,
      "learning_rate": 9.99817836306694e-07,
      "loss": 0.0023,
      "step": 16160
    },
    {
      "epoch": 2940.0,
      "grad_norm": 0.051296353340148926,
      "learning_rate": 9.998146822104943e-07,
      "loss": 0.0017,
      "step": 16170
    },
    {
      "epoch": 2941.818181818182,
      "grad_norm": 0.03556618094444275,
      "learning_rate": 9.998115010472266e-07,
      "loss": 0.0018,
      "step": 16180
    },
    {
      "epoch": 2943.6363636363635,
      "grad_norm": 0.6941908597946167,
      "learning_rate": 9.99808292817063e-07,
      "loss": 0.002,
      "step": 16190
    },
    {
      "epoch": 2945.4545454545455,
      "grad_norm": 0.7952473759651184,
      "learning_rate": 9.99805057520177e-07,
      "loss": 0.0021,
      "step": 16200
    },
    {
      "epoch": 2947.2727272727275,
      "grad_norm": 0.5452625155448914,
      "learning_rate": 9.998017951567445e-07,
      "loss": 0.0021,
      "step": 16210
    },
    {
      "epoch": 2949.090909090909,
      "grad_norm": 0.7687512040138245,
      "learning_rate": 9.997985057269416e-07,
      "loss": 0.0015,
      "step": 16220
    },
    {
      "epoch": 2950.909090909091,
      "grad_norm": 0.0961313471198082,
      "learning_rate": 9.997951892309467e-07,
      "loss": 0.0018,
      "step": 16230
    },
    {
      "epoch": 2952.7272727272725,
      "grad_norm": 0.2941424548625946,
      "learning_rate": 9.99791845668939e-07,
      "loss": 0.002,
      "step": 16240
    },
    {
      "epoch": 2954.5454545454545,
      "grad_norm": 0.9618159532546997,
      "learning_rate": 9.997884750411003e-07,
      "loss": 0.0019,
      "step": 16250
    },
    {
      "epoch": 2956.3636363636365,
      "grad_norm": 1.060721755027771,
      "learning_rate": 9.997850773476125e-07,
      "loss": 0.002,
      "step": 16260
    },
    {
      "epoch": 2958.181818181818,
      "grad_norm": 0.738935112953186,
      "learning_rate": 9.997816525886598e-07,
      "loss": 0.0017,
      "step": 16270
    },
    {
      "epoch": 2960.0,
      "grad_norm": 0.8761000633239746,
      "learning_rate": 9.997782007644276e-07,
      "loss": 0.0019,
      "step": 16280
    },
    {
      "epoch": 2961.818181818182,
      "grad_norm": 1.5686755180358887,
      "learning_rate": 9.99774721875103e-07,
      "loss": 0.0019,
      "step": 16290
    },
    {
      "epoch": 2963.6363636363635,
      "grad_norm": 1.349937081336975,
      "learning_rate": 9.997712159208743e-07,
      "loss": 0.0021,
      "step": 16300
    },
    {
      "epoch": 2965.4545454545455,
      "grad_norm": 0.9744865894317627,
      "learning_rate": 9.997676829019313e-07,
      "loss": 0.002,
      "step": 16310
    },
    {
      "epoch": 2967.2727272727275,
      "grad_norm": 0.048858486115932465,
      "learning_rate": 9.997641228184655e-07,
      "loss": 0.0017,
      "step": 16320
    },
    {
      "epoch": 2969.090909090909,
      "grad_norm": 0.05408585071563721,
      "learning_rate": 9.997605356706695e-07,
      "loss": 0.0019,
      "step": 16330
    },
    {
      "epoch": 2970.909090909091,
      "grad_norm": 0.9741054773330688,
      "learning_rate": 9.997569214587376e-07,
      "loss": 0.0018,
      "step": 16340
    },
    {
      "epoch": 2972.7272727272725,
      "grad_norm": 1.2077765464782715,
      "learning_rate": 9.997532801828658e-07,
      "loss": 0.0021,
      "step": 16350
    },
    {
      "epoch": 2974.5454545454545,
      "grad_norm": 0.776311457157135,
      "learning_rate": 9.997496118432507e-07,
      "loss": 0.0019,
      "step": 16360
    },
    {
      "epoch": 2976.3636363636365,
      "grad_norm": 0.04144786298274994,
      "learning_rate": 9.997459164400917e-07,
      "loss": 0.0017,
      "step": 16370
    },
    {
      "epoch": 2978.181818181818,
      "grad_norm": 0.03873070701956749,
      "learning_rate": 9.997421939735883e-07,
      "loss": 0.0018,
      "step": 16380
    },
    {
      "epoch": 2980.0,
      "grad_norm": 1.0180985927581787,
      "learning_rate": 9.997384444439423e-07,
      "loss": 0.0019,
      "step": 16390
    },
    {
      "epoch": 2981.818181818182,
      "grad_norm": 0.05847768485546112,
      "learning_rate": 9.997346678513568e-07,
      "loss": 0.0019,
      "step": 16400
    },
    {
      "epoch": 2983.6363636363635,
      "grad_norm": 1.4017047882080078,
      "learning_rate": 9.997308641960363e-07,
      "loss": 0.0019,
      "step": 16410
    },
    {
      "epoch": 2985.4545454545455,
      "grad_norm": 0.03809072822332382,
      "learning_rate": 9.997270334781868e-07,
      "loss": 0.0018,
      "step": 16420
    },
    {
      "epoch": 2987.2727272727275,
      "grad_norm": 0.07212388515472412,
      "learning_rate": 9.997231756980157e-07,
      "loss": 0.0018,
      "step": 16430
    },
    {
      "epoch": 2989.090909090909,
      "grad_norm": 0.05830693617463112,
      "learning_rate": 9.997192908557322e-07,
      "loss": 0.0018,
      "step": 16440
    },
    {
      "epoch": 2990.909090909091,
      "grad_norm": 1.6436058282852173,
      "learning_rate": 9.99715378951546e-07,
      "loss": 0.0018,
      "step": 16450
    },
    {
      "epoch": 2992.7272727272725,
      "grad_norm": 0.07077475637197495,
      "learning_rate": 9.997114399856696e-07,
      "loss": 0.0021,
      "step": 16460
    },
    {
      "epoch": 2994.5454545454545,
      "grad_norm": 1.129591941833496,
      "learning_rate": 9.997074739583162e-07,
      "loss": 0.0016,
      "step": 16470
    },
    {
      "epoch": 2996.3636363636365,
      "grad_norm": 0.47763925790786743,
      "learning_rate": 9.997034808697002e-07,
      "loss": 0.0015,
      "step": 16480
    },
    {
      "epoch": 2998.181818181818,
      "grad_norm": 1.1085985898971558,
      "learning_rate": 9.996994607200382e-07,
      "loss": 0.0022,
      "step": 16490
    },
    {
      "epoch": 3000.0,
      "grad_norm": 1.4749208688735962,
      "learning_rate": 9.996954135095478e-07,
      "loss": 0.0017,
      "step": 16500
    },
    {
      "epoch": 3000.0,
      "eval_loss": 3.9765326976776123,
      "eval_runtime": 0.9533,
      "eval_samples_per_second": 10.49,
      "eval_steps_per_second": 5.245,
      "step": 16500
    },
    {
      "epoch": 3001.818181818182,
      "grad_norm": 0.12925532460212708,
      "learning_rate": 9.996913392384482e-07,
      "loss": 0.0017,
      "step": 16510
    },
    {
      "epoch": 3003.6363636363635,
      "grad_norm": 0.05384514480829239,
      "learning_rate": 9.9968723790696e-07,
      "loss": 0.0017,
      "step": 16520
    },
    {
      "epoch": 3005.4545454545455,
      "grad_norm": 0.7688413262367249,
      "learning_rate": 9.996831095153054e-07,
      "loss": 0.0022,
      "step": 16530
    },
    {
      "epoch": 3007.2727272727275,
      "grad_norm": 0.11805910617113113,
      "learning_rate": 9.99678954063708e-07,
      "loss": 0.0013,
      "step": 16540
    },
    {
      "epoch": 3009.090909090909,
      "grad_norm": 0.04417278617620468,
      "learning_rate": 9.996747715523923e-07,
      "loss": 0.0019,
      "step": 16550
    },
    {
      "epoch": 3010.909090909091,
      "grad_norm": 0.966852605342865,
      "learning_rate": 9.996705619815856e-07,
      "loss": 0.002,
      "step": 16560
    },
    {
      "epoch": 3012.7272727272725,
      "grad_norm": 0.054983388632535934,
      "learning_rate": 9.996663253515154e-07,
      "loss": 0.0015,
      "step": 16570
    },
    {
      "epoch": 3014.5454545454545,
      "grad_norm": 0.070198655128479,
      "learning_rate": 9.996620616624112e-07,
      "loss": 0.0022,
      "step": 16580
    },
    {
      "epoch": 3016.3636363636365,
      "grad_norm": 0.054865386337041855,
      "learning_rate": 9.996577709145038e-07,
      "loss": 0.0016,
      "step": 16590
    },
    {
      "epoch": 3018.181818181818,
      "grad_norm": 1.2101304531097412,
      "learning_rate": 9.996534531080259e-07,
      "loss": 0.0022,
      "step": 16600
    },
    {
      "epoch": 3020.0,
      "grad_norm": 0.5464284420013428,
      "learning_rate": 9.99649108243211e-07,
      "loss": 0.0016,
      "step": 16610
    },
    {
      "epoch": 3021.818181818182,
      "grad_norm": 0.039264366030693054,
      "learning_rate": 9.996447363202945e-07,
      "loss": 0.0021,
      "step": 16620
    },
    {
      "epoch": 3023.6363636363635,
      "grad_norm": 0.07016557455062866,
      "learning_rate": 9.996403373395133e-07,
      "loss": 0.0016,
      "step": 16630
    },
    {
      "epoch": 3025.4545454545455,
      "grad_norm": 1.92538583278656,
      "learning_rate": 9.996359113011055e-07,
      "loss": 0.0021,
      "step": 16640
    },
    {
      "epoch": 3027.2727272727275,
      "grad_norm": 0.7785371541976929,
      "learning_rate": 9.996314582053105e-07,
      "loss": 0.0017,
      "step": 16650
    },
    {
      "epoch": 3029.090909090909,
      "grad_norm": 0.06968943774700165,
      "learning_rate": 9.996269780523699e-07,
      "loss": 0.0017,
      "step": 16660
    },
    {
      "epoch": 3030.909090909091,
      "grad_norm": 0.13528543710708618,
      "learning_rate": 9.996224708425262e-07,
      "loss": 0.0019,
      "step": 16670
    },
    {
      "epoch": 3032.7272727272725,
      "grad_norm": 1.425970196723938,
      "learning_rate": 9.996179365760233e-07,
      "loss": 0.0019,
      "step": 16680
    },
    {
      "epoch": 3034.5454545454545,
      "grad_norm": 0.8299506306648254,
      "learning_rate": 9.99613375253107e-07,
      "loss": 0.0017,
      "step": 16690
    },
    {
      "epoch": 3036.3636363636365,
      "grad_norm": 0.805774450302124,
      "learning_rate": 9.996087868740241e-07,
      "loss": 0.0017,
      "step": 16700
    },
    {
      "epoch": 3038.181818181818,
      "grad_norm": 0.052394501864910126,
      "learning_rate": 9.996041714390232e-07,
      "loss": 0.0015,
      "step": 16710
    },
    {
      "epoch": 3040.0,
      "grad_norm": 0.1369648575782776,
      "learning_rate": 9.995995289483545e-07,
      "loss": 0.0018,
      "step": 16720
    },
    {
      "epoch": 3041.818181818182,
      "grad_norm": 0.21998420357704163,
      "learning_rate": 9.995948594022689e-07,
      "loss": 0.0017,
      "step": 16730
    },
    {
      "epoch": 3043.6363636363635,
      "grad_norm": 0.15437299013137817,
      "learning_rate": 9.995901628010194e-07,
      "loss": 0.0018,
      "step": 16740
    },
    {
      "epoch": 3045.4545454545455,
      "grad_norm": 1.1620575189590454,
      "learning_rate": 9.995854391448606e-07,
      "loss": 0.002,
      "step": 16750
    },
    {
      "epoch": 3047.2727272727275,
      "grad_norm": 0.029708173125982285,
      "learning_rate": 9.99580688434048e-07,
      "loss": 0.0015,
      "step": 16760
    },
    {
      "epoch": 3049.090909090909,
      "grad_norm": 0.06182551383972168,
      "learning_rate": 9.995759106688392e-07,
      "loss": 0.002,
      "step": 16770
    },
    {
      "epoch": 3050.909090909091,
      "grad_norm": 0.7110362648963928,
      "learning_rate": 9.995711058494928e-07,
      "loss": 0.002,
      "step": 16780
    },
    {
      "epoch": 3052.7272727272725,
      "grad_norm": 0.037497859448194504,
      "learning_rate": 9.995662739762689e-07,
      "loss": 0.0015,
      "step": 16790
    },
    {
      "epoch": 3054.5454545454545,
      "grad_norm": 0.044897034764289856,
      "learning_rate": 9.99561415049429e-07,
      "loss": 0.0019,
      "step": 16800
    },
    {
      "epoch": 3056.3636363636365,
      "grad_norm": 0.047439463436603546,
      "learning_rate": 9.995565290692368e-07,
      "loss": 0.0015,
      "step": 16810
    },
    {
      "epoch": 3058.181818181818,
      "grad_norm": 0.045257568359375,
      "learning_rate": 9.995516160359563e-07,
      "loss": 0.0019,
      "step": 16820
    },
    {
      "epoch": 3060.0,
      "grad_norm": 0.040116604417562485,
      "learning_rate": 9.99546675949854e-07,
      "loss": 0.0018,
      "step": 16830
    },
    {
      "epoch": 3061.818181818182,
      "grad_norm": 0.04256228730082512,
      "learning_rate": 9.995417088111973e-07,
      "loss": 0.0017,
      "step": 16840
    },
    {
      "epoch": 3063.6363636363635,
      "grad_norm": 0.8579531908035278,
      "learning_rate": 9.995367146202548e-07,
      "loss": 0.0019,
      "step": 16850
    },
    {
      "epoch": 3065.4545454545455,
      "grad_norm": 0.5772709846496582,
      "learning_rate": 9.995316933772977e-07,
      "loss": 0.0018,
      "step": 16860
    },
    {
      "epoch": 3067.2727272727275,
      "grad_norm": 12.971342086791992,
      "learning_rate": 9.995266450825973e-07,
      "loss": 0.0015,
      "step": 16870
    },
    {
      "epoch": 3069.090909090909,
      "grad_norm": 0.6762088537216187,
      "learning_rate": 9.995215697364273e-07,
      "loss": 0.0017,
      "step": 16880
    },
    {
      "epoch": 3070.909090909091,
      "grad_norm": 0.17480486631393433,
      "learning_rate": 9.995164673390625e-07,
      "loss": 0.0019,
      "step": 16890
    },
    {
      "epoch": 3072.7272727272725,
      "grad_norm": 0.041077300906181335,
      "learning_rate": 9.995113378907789e-07,
      "loss": 0.0013,
      "step": 16900
    },
    {
      "epoch": 3074.5454545454545,
      "grad_norm": 0.7670936584472656,
      "learning_rate": 9.995061813918546e-07,
      "loss": 0.0021,
      "step": 16910
    },
    {
      "epoch": 3076.3636363636365,
      "grad_norm": 1.0745688676834106,
      "learning_rate": 9.995009978425691e-07,
      "loss": 0.0021,
      "step": 16920
    },
    {
      "epoch": 3078.181818181818,
      "grad_norm": 0.034577567130327225,
      "learning_rate": 9.994957872432024e-07,
      "loss": 0.0016,
      "step": 16930
    },
    {
      "epoch": 3080.0,
      "grad_norm": 1.6475803852081299,
      "learning_rate": 9.994905495940374e-07,
      "loss": 0.0019,
      "step": 16940
    },
    {
      "epoch": 3081.818181818182,
      "grad_norm": 0.6774677634239197,
      "learning_rate": 9.994852848953573e-07,
      "loss": 0.0018,
      "step": 16950
    },
    {
      "epoch": 3083.6363636363635,
      "grad_norm": 0.04007880017161369,
      "learning_rate": 9.994799931474473e-07,
      "loss": 0.0014,
      "step": 16960
    },
    {
      "epoch": 3085.4545454545455,
      "grad_norm": 0.041481174528598785,
      "learning_rate": 9.99474674350594e-07,
      "loss": 0.0018,
      "step": 16970
    },
    {
      "epoch": 3087.2727272727275,
      "grad_norm": 0.9393137693405151,
      "learning_rate": 9.994693285050856e-07,
      "loss": 0.0021,
      "step": 16980
    },
    {
      "epoch": 3089.090909090909,
      "grad_norm": 0.529964804649353,
      "learning_rate": 9.994639556112113e-07,
      "loss": 0.0017,
      "step": 16990
    },
    {
      "epoch": 3090.909090909091,
      "grad_norm": 0.039714112877845764,
      "learning_rate": 9.994585556692624e-07,
      "loss": 0.0015,
      "step": 17000
    },
    {
      "epoch": 3090.909090909091,
      "eval_loss": 3.994039535522461,
      "eval_runtime": 0.9507,
      "eval_samples_per_second": 10.519,
      "eval_steps_per_second": 5.259,
      "step": 17000
    },
    {
      "epoch": 3092.7272727272725,
      "grad_norm": 0.06602022051811218,
      "learning_rate": 9.994531286795306e-07,
      "loss": 0.0017,
      "step": 17010
    },
    {
      "epoch": 3094.5454545454545,
      "grad_norm": 0.11816039681434631,
      "learning_rate": 9.994476746423109e-07,
      "loss": 0.0021,
      "step": 17020
    },
    {
      "epoch": 3096.3636363636365,
      "grad_norm": 0.7540062665939331,
      "learning_rate": 9.994421935578978e-07,
      "loss": 0.0018,
      "step": 17030
    },
    {
      "epoch": 3098.181818181818,
      "grad_norm": 0.034668758511543274,
      "learning_rate": 9.994366854265883e-07,
      "loss": 0.0014,
      "step": 17040
    },
    {
      "epoch": 3100.0,
      "grad_norm": 0.05406305193901062,
      "learning_rate": 9.994311502486813e-07,
      "loss": 0.0018,
      "step": 17050
    },
    {
      "epoch": 3101.818181818182,
      "grad_norm": 0.035867780447006226,
      "learning_rate": 9.994255880244754e-07,
      "loss": 0.0018,
      "step": 17060
    },
    {
      "epoch": 3103.6363636363635,
      "grad_norm": 0.9586777091026306,
      "learning_rate": 9.99419998754273e-07,
      "loss": 0.0019,
      "step": 17070
    },
    {
      "epoch": 3105.4545454545455,
      "grad_norm": 1.3898745775222778,
      "learning_rate": 9.99414382438376e-07,
      "loss": 0.002,
      "step": 17080
    },
    {
      "epoch": 3107.2727272727275,
      "grad_norm": 0.6851951479911804,
      "learning_rate": 9.994087390770886e-07,
      "loss": 0.0015,
      "step": 17090
    },
    {
      "epoch": 3109.090909090909,
      "grad_norm": 0.670417845249176,
      "learning_rate": 9.99403068670717e-07,
      "loss": 0.002,
      "step": 17100
    },
    {
      "epoch": 3110.909090909091,
      "grad_norm": 1.206993579864502,
      "learning_rate": 9.993973712195677e-07,
      "loss": 0.0017,
      "step": 17110
    },
    {
      "epoch": 3112.7272727272725,
      "grad_norm": 0.05371258035302162,
      "learning_rate": 9.993916467239494e-07,
      "loss": 0.0015,
      "step": 17120
    },
    {
      "epoch": 3114.5454545454545,
      "grad_norm": 0.5461000800132751,
      "learning_rate": 9.993858951841722e-07,
      "loss": 0.0019,
      "step": 17130
    },
    {
      "epoch": 3116.3636363636365,
      "grad_norm": 0.334399551153183,
      "learning_rate": 9.993801166005474e-07,
      "loss": 0.0013,
      "step": 17140
    },
    {
      "epoch": 3118.181818181818,
      "grad_norm": 0.04180266335606575,
      "learning_rate": 9.993743109733881e-07,
      "loss": 0.0018,
      "step": 17150
    },
    {
      "epoch": 3120.0,
      "grad_norm": 1.3013726472854614,
      "learning_rate": 9.993684783030088e-07,
      "loss": 0.0018,
      "step": 17160
    },
    {
      "epoch": 3121.818181818182,
      "grad_norm": 1.0464988946914673,
      "learning_rate": 9.99362618589725e-07,
      "loss": 0.0016,
      "step": 17170
    },
    {
      "epoch": 3123.6363636363635,
      "grad_norm": 0.04248503968119621,
      "learning_rate": 9.993567318338544e-07,
      "loss": 0.0017,
      "step": 17180
    },
    {
      "epoch": 3125.4545454545455,
      "grad_norm": 0.026564929634332657,
      "learning_rate": 9.993508180357154e-07,
      "loss": 0.0015,
      "step": 17190
    },
    {
      "epoch": 3127.2727272727275,
      "grad_norm": 0.04675424098968506,
      "learning_rate": 9.993448771956284e-07,
      "loss": 0.0017,
      "step": 17200
    },
    {
      "epoch": 3129.090909090909,
      "grad_norm": 0.5097691416740417,
      "learning_rate": 9.993389093139154e-07,
      "loss": 0.002,
      "step": 17210
    },
    {
      "epoch": 3130.909090909091,
      "grad_norm": 1.0588434934616089,
      "learning_rate": 9.993329143908992e-07,
      "loss": 0.0018,
      "step": 17220
    },
    {
      "epoch": 3132.7272727272725,
      "grad_norm": 0.9529176950454712,
      "learning_rate": 9.993268924269049e-07,
      "loss": 0.0017,
      "step": 17230
    },
    {
      "epoch": 3134.5454545454545,
      "grad_norm": 0.73177170753479,
      "learning_rate": 9.993208434222582e-07,
      "loss": 0.0015,
      "step": 17240
    },
    {
      "epoch": 3136.3636363636365,
      "grad_norm": 0.437383234500885,
      "learning_rate": 9.99314767377287e-07,
      "loss": 0.0018,
      "step": 17250
    },
    {
      "epoch": 3138.181818181818,
      "grad_norm": 0.03805994242429733,
      "learning_rate": 9.9930866429232e-07,
      "loss": 0.0016,
      "step": 17260
    },
    {
      "epoch": 3140.0,
      "grad_norm": 0.5930259227752686,
      "learning_rate": 9.993025341676877e-07,
      "loss": 0.0018,
      "step": 17270
    },
    {
      "epoch": 3141.818181818182,
      "grad_norm": 0.6968690156936646,
      "learning_rate": 9.992963770037227e-07,
      "loss": 0.0016,
      "step": 17280
    },
    {
      "epoch": 3143.6363636363635,
      "grad_norm": 1.027055263519287,
      "learning_rate": 9.992901928007575e-07,
      "loss": 0.0016,
      "step": 17290
    },
    {
      "epoch": 3145.4545454545455,
      "grad_norm": 0.036697644740343094,
      "learning_rate": 9.992839815591279e-07,
      "loss": 0.0021,
      "step": 17300
    },
    {
      "epoch": 3147.2727272727275,
      "grad_norm": 1.058288335800171,
      "learning_rate": 9.992777432791695e-07,
      "loss": 0.0018,
      "step": 17310
    },
    {
      "epoch": 3149.090909090909,
      "grad_norm": 0.03949641436338425,
      "learning_rate": 9.992714779612207e-07,
      "loss": 0.0015,
      "step": 17320
    },
    {
      "epoch": 3150.909090909091,
      "grad_norm": 1.0786129236221313,
      "learning_rate": 9.992651856056206e-07,
      "loss": 0.0018,
      "step": 17330
    },
    {
      "epoch": 3152.7272727272725,
      "grad_norm": 0.038999173790216446,
      "learning_rate": 9.992588662127099e-07,
      "loss": 0.0017,
      "step": 17340
    },
    {
      "epoch": 3154.5454545454545,
      "grad_norm": 0.04567896947264671,
      "learning_rate": 9.992525197828307e-07,
      "loss": 0.0015,
      "step": 17350
    },
    {
      "epoch": 3156.3636363636365,
      "grad_norm": 0.09321118891239166,
      "learning_rate": 9.99246146316327e-07,
      "loss": 0.0015,
      "step": 17360
    },
    {
      "epoch": 3158.181818181818,
      "grad_norm": 0.8945850729942322,
      "learning_rate": 9.992397458135438e-07,
      "loss": 0.0019,
      "step": 17370
    },
    {
      "epoch": 3160.0,
      "grad_norm": 0.7807798981666565,
      "learning_rate": 9.992333182748275e-07,
      "loss": 0.0015,
      "step": 17380
    },
    {
      "epoch": 3161.818181818182,
      "grad_norm": 0.7253323793411255,
      "learning_rate": 9.992268637005266e-07,
      "loss": 0.0014,
      "step": 17390
    },
    {
      "epoch": 3163.6363636363635,
      "grad_norm": 0.04464391618967056,
      "learning_rate": 9.992203820909905e-07,
      "loss": 0.002,
      "step": 17400
    },
    {
      "epoch": 3165.4545454545455,
      "grad_norm": 0.06303578615188599,
      "learning_rate": 9.992138734465699e-07,
      "loss": 0.0014,
      "step": 17410
    },
    {
      "epoch": 3167.2727272727275,
      "grad_norm": 1.0601614713668823,
      "learning_rate": 9.992073377676176e-07,
      "loss": 0.0017,
      "step": 17420
    },
    {
      "epoch": 3169.090909090909,
      "grad_norm": 0.8535864949226379,
      "learning_rate": 9.992007750544876e-07,
      "loss": 0.0015,
      "step": 17430
    },
    {
      "epoch": 3170.909090909091,
      "grad_norm": 0.1234716922044754,
      "learning_rate": 9.991941853075349e-07,
      "loss": 0.0017,
      "step": 17440
    },
    {
      "epoch": 3172.7272727272725,
      "grad_norm": 0.05286489054560661,
      "learning_rate": 9.991875685271166e-07,
      "loss": 0.0012,
      "step": 17450
    },
    {
      "epoch": 3174.5454545454545,
      "grad_norm": 0.883510947227478,
      "learning_rate": 9.991809247135911e-07,
      "loss": 0.0018,
      "step": 17460
    },
    {
      "epoch": 3176.3636363636365,
      "grad_norm": 0.030005818232893944,
      "learning_rate": 9.99174253867318e-07,
      "loss": 0.0018,
      "step": 17470
    },
    {
      "epoch": 3178.181818181818,
      "grad_norm": 0.035696834325790405,
      "learning_rate": 9.991675559886588e-07,
      "loss": 0.0017,
      "step": 17480
    },
    {
      "epoch": 3180.0,
      "grad_norm": 0.15665188431739807,
      "learning_rate": 9.991608310779762e-07,
      "loss": 0.0017,
      "step": 17490
    },
    {
      "epoch": 3181.818181818182,
      "grad_norm": 0.6115556955337524,
      "learning_rate": 9.991540791356342e-07,
      "loss": 0.0016,
      "step": 17500
    },
    {
      "epoch": 3181.818181818182,
      "eval_loss": 4.006494522094727,
      "eval_runtime": 0.9527,
      "eval_samples_per_second": 10.497,
      "eval_steps_per_second": 5.249,
      "step": 17500
    },
    {
      "epoch": 3183.6363636363635,
      "grad_norm": 0.03103802725672722,
      "learning_rate": 9.991473001619984e-07,
      "loss": 0.0019,
      "step": 17510
    },
    {
      "epoch": 3185.4545454545455,
      "grad_norm": 0.039729878306388855,
      "learning_rate": 9.99140494157436e-07,
      "loss": 0.0017,
      "step": 17520
    },
    {
      "epoch": 3187.2727272727275,
      "grad_norm": 0.6076997518539429,
      "learning_rate": 9.991336611223155e-07,
      "loss": 0.0017,
      "step": 17530
    },
    {
      "epoch": 3189.090909090909,
      "grad_norm": 0.056423284113407135,
      "learning_rate": 9.991268010570072e-07,
      "loss": 0.0016,
      "step": 17540
    },
    {
      "epoch": 3190.909090909091,
      "grad_norm": 0.05884562432765961,
      "learning_rate": 9.991199139618826e-07,
      "loss": 0.0014,
      "step": 17550
    },
    {
      "epoch": 3192.7272727272725,
      "grad_norm": 0.6760937571525574,
      "learning_rate": 9.991129998373144e-07,
      "loss": 0.0018,
      "step": 17560
    },
    {
      "epoch": 3194.5454545454545,
      "grad_norm": 1.0105805397033691,
      "learning_rate": 9.99106058683677e-07,
      "loss": 0.0017,
      "step": 17570
    },
    {
      "epoch": 3196.3636363636365,
      "grad_norm": 0.8867075443267822,
      "learning_rate": 9.990990905013465e-07,
      "loss": 0.0016,
      "step": 17580
    },
    {
      "epoch": 3198.181818181818,
      "grad_norm": 0.027285216376185417,
      "learning_rate": 9.990920952907003e-07,
      "loss": 0.0012,
      "step": 17590
    },
    {
      "epoch": 3200.0,
      "grad_norm": 0.045836929231882095,
      "learning_rate": 9.99085073052117e-07,
      "loss": 0.0018,
      "step": 17600
    },
    {
      "epoch": 3201.818181818182,
      "grad_norm": 0.9367605447769165,
      "learning_rate": 9.990780237859768e-07,
      "loss": 0.0014,
      "step": 17610
    },
    {
      "epoch": 3203.6363636363635,
      "grad_norm": 0.0617639385163784,
      "learning_rate": 9.99070947492662e-07,
      "loss": 0.0015,
      "step": 17620
    },
    {
      "epoch": 3205.4545454545455,
      "grad_norm": 0.7027535438537598,
      "learning_rate": 9.990638441725552e-07,
      "loss": 0.0015,
      "step": 17630
    },
    {
      "epoch": 3207.2727272727275,
      "grad_norm": 0.690045177936554,
      "learning_rate": 9.990567138260412e-07,
      "loss": 0.0017,
      "step": 17640
    },
    {
      "epoch": 3209.090909090909,
      "grad_norm": 0.7154015302658081,
      "learning_rate": 9.990495564535064e-07,
      "loss": 0.0018,
      "step": 17650
    },
    {
      "epoch": 3210.909090909091,
      "grad_norm": 0.7797028422355652,
      "learning_rate": 9.990423720553384e-07,
      "loss": 0.0015,
      "step": 17660
    },
    {
      "epoch": 3212.7272727272725,
      "grad_norm": 0.3162379860877991,
      "learning_rate": 9.990351606319261e-07,
      "loss": 0.0017,
      "step": 17670
    },
    {
      "epoch": 3214.5454545454545,
      "grad_norm": 0.9118853211402893,
      "learning_rate": 9.990279221836598e-07,
      "loss": 0.0017,
      "step": 17680
    },
    {
      "epoch": 3216.3636363636365,
      "grad_norm": 0.9986271858215332,
      "learning_rate": 9.99020656710932e-07,
      "loss": 0.0016,
      "step": 17690
    },
    {
      "epoch": 3218.181818181818,
      "grad_norm": 0.8239179253578186,
      "learning_rate": 9.990133642141357e-07,
      "loss": 0.0016,
      "step": 17700
    },
    {
      "epoch": 3220.0,
      "grad_norm": 0.8238248825073242,
      "learning_rate": 9.990060446936662e-07,
      "loss": 0.0016,
      "step": 17710
    },
    {
      "epoch": 3221.818181818182,
      "grad_norm": 0.6746992468833923,
      "learning_rate": 9.989986981499196e-07,
      "loss": 0.0017,
      "step": 17720
    },
    {
      "epoch": 3223.6363636363635,
      "grad_norm": 0.03321792557835579,
      "learning_rate": 9.98991324583294e-07,
      "loss": 0.0011,
      "step": 17730
    },
    {
      "epoch": 3225.4545454545455,
      "grad_norm": 0.12653735280036926,
      "learning_rate": 9.989839239941883e-07,
      "loss": 0.0023,
      "step": 17740
    },
    {
      "epoch": 3227.2727272727275,
      "grad_norm": 0.09448441118001938,
      "learning_rate": 9.989764963830037e-07,
      "loss": 0.0012,
      "step": 17750
    },
    {
      "epoch": 3229.090909090909,
      "grad_norm": 0.046524304896593094,
      "learning_rate": 9.989690417501422e-07,
      "loss": 0.0017,
      "step": 17760
    },
    {
      "epoch": 3230.909090909091,
      "grad_norm": 0.8292115926742554,
      "learning_rate": 9.989615600960076e-07,
      "loss": 0.0015,
      "step": 17770
    },
    {
      "epoch": 3232.7272727272725,
      "grad_norm": 0.6966817378997803,
      "learning_rate": 9.98954051421005e-07,
      "loss": 0.0016,
      "step": 17780
    },
    {
      "epoch": 3234.5454545454545,
      "grad_norm": 0.5094337463378906,
      "learning_rate": 9.98946515725541e-07,
      "loss": 0.0016,
      "step": 17790
    },
    {
      "epoch": 3236.3636363636365,
      "grad_norm": 0.5229318737983704,
      "learning_rate": 9.98938953010024e-07,
      "loss": 0.0019,
      "step": 17800
    },
    {
      "epoch": 3238.181818181818,
      "grad_norm": 0.07056701183319092,
      "learning_rate": 9.98931363274863e-07,
      "loss": 0.0016,
      "step": 17810
    },
    {
      "epoch": 3240.0,
      "grad_norm": 0.7977690100669861,
      "learning_rate": 9.989237465204696e-07,
      "loss": 0.0016,
      "step": 17820
    },
    {
      "epoch": 3241.818181818182,
      "grad_norm": 0.7580976486206055,
      "learning_rate": 9.98916102747256e-07,
      "loss": 0.0017,
      "step": 17830
    },
    {
      "epoch": 3243.6363636363635,
      "grad_norm": 0.9685220122337341,
      "learning_rate": 9.989084319556358e-07,
      "loss": 0.0015,
      "step": 17840
    },
    {
      "epoch": 3245.4545454545455,
      "grad_norm": 1.3597488403320312,
      "learning_rate": 9.989007341460249e-07,
      "loss": 0.0015,
      "step": 17850
    },
    {
      "epoch": 3247.2727272727275,
      "grad_norm": 1.0657135248184204,
      "learning_rate": 9.988930093188402e-07,
      "loss": 0.0016,
      "step": 17860
    },
    {
      "epoch": 3249.090909090909,
      "grad_norm": 0.8380653262138367,
      "learning_rate": 9.988852574744997e-07,
      "loss": 0.0015,
      "step": 17870
    },
    {
      "epoch": 3250.909090909091,
      "grad_norm": 0.09050021320581436,
      "learning_rate": 9.988774786134233e-07,
      "loss": 0.0016,
      "step": 17880
    },
    {
      "epoch": 3252.7272727272725,
      "grad_norm": 1.2764394283294678,
      "learning_rate": 9.988696727360322e-07,
      "loss": 0.0016,
      "step": 17890
    },
    {
      "epoch": 3254.5454545454545,
      "grad_norm": 1.1165515184402466,
      "learning_rate": 9.988618398427493e-07,
      "loss": 0.0015,
      "step": 17900
    },
    {
      "epoch": 3256.3636363636365,
      "grad_norm": 0.1414199322462082,
      "learning_rate": 9.988539799339988e-07,
      "loss": 0.0019,
      "step": 17910
    },
    {
      "epoch": 3258.181818181818,
      "grad_norm": 0.04549501836299896,
      "learning_rate": 9.988460930102062e-07,
      "loss": 0.0013,
      "step": 17920
    },
    {
      "epoch": 3260.0,
      "grad_norm": 0.808117687702179,
      "learning_rate": 9.988381790717987e-07,
      "loss": 0.0017,
      "step": 17930
    },
    {
      "epoch": 3261.818181818182,
      "grad_norm": 0.06688842922449112,
      "learning_rate": 9.98830238119205e-07,
      "loss": 0.0016,
      "step": 17940
    },
    {
      "epoch": 3263.6363636363635,
      "grad_norm": 0.037854161113500595,
      "learning_rate": 9.988222701528547e-07,
      "loss": 0.0015,
      "step": 17950
    },
    {
      "epoch": 3265.4545454545455,
      "grad_norm": 0.037645742297172546,
      "learning_rate": 9.988142751731796e-07,
      "loss": 0.0014,
      "step": 17960
    },
    {
      "epoch": 3267.2727272727275,
      "grad_norm": 0.6781859397888184,
      "learning_rate": 9.988062531806124e-07,
      "loss": 0.0015,
      "step": 17970
    },
    {
      "epoch": 3269.090909090909,
      "grad_norm": 0.7606452703475952,
      "learning_rate": 9.987982041755882e-07,
      "loss": 0.0016,
      "step": 17980
    },
    {
      "epoch": 3270.909090909091,
      "grad_norm": 0.10759688168764114,
      "learning_rate": 9.987901281585422e-07,
      "loss": 0.0014,
      "step": 17990
    },
    {
      "epoch": 3272.7272727272725,
      "grad_norm": 0.8109726309776306,
      "learning_rate": 9.98782025129912e-07,
      "loss": 0.0018,
      "step": 18000
    },
    {
      "epoch": 3272.7272727272725,
      "eval_loss": 4.072878837585449,
      "eval_runtime": 0.9521,
      "eval_samples_per_second": 10.503,
      "eval_steps_per_second": 5.252,
      "step": 18000
    },
    {
      "epoch": 3274.5454545454545,
      "grad_norm": 0.027838950976729393,
      "learning_rate": 9.987738950901365e-07,
      "loss": 0.0012,
      "step": 18010
    },
    {
      "epoch": 3276.3636363636365,
      "grad_norm": 0.486865758895874,
      "learning_rate": 9.987657380396558e-07,
      "loss": 0.0019,
      "step": 18020
    },
    {
      "epoch": 3278.181818181818,
      "grad_norm": 0.7427374720573425,
      "learning_rate": 9.98757553978912e-07,
      "loss": 0.0014,
      "step": 18030
    },
    {
      "epoch": 3280.0,
      "grad_norm": 0.03171990066766739,
      "learning_rate": 9.987493429083475e-07,
      "loss": 0.0015,
      "step": 18040
    },
    {
      "epoch": 3281.818181818182,
      "grad_norm": 0.0321052111685276,
      "learning_rate": 9.987411048284078e-07,
      "loss": 0.0016,
      "step": 18050
    },
    {
      "epoch": 3283.6363636363635,
      "grad_norm": 0.3508734405040741,
      "learning_rate": 9.987328397395386e-07,
      "loss": 0.0012,
      "step": 18060
    },
    {
      "epoch": 3285.4545454545455,
      "grad_norm": 0.03044073097407818,
      "learning_rate": 9.987245476421878e-07,
      "loss": 0.0016,
      "step": 18070
    },
    {
      "epoch": 3287.2727272727275,
      "grad_norm": 0.1021239385008812,
      "learning_rate": 9.987162285368042e-07,
      "loss": 0.0019,
      "step": 18080
    },
    {
      "epoch": 3289.090909090909,
      "grad_norm": 0.03850984573364258,
      "learning_rate": 9.987078824238382e-07,
      "loss": 0.0013,
      "step": 18090
    },
    {
      "epoch": 3290.909090909091,
      "grad_norm": 0.7696910500526428,
      "learning_rate": 9.98699509303742e-07,
      "loss": 0.0016,
      "step": 18100
    },
    {
      "epoch": 3292.7272727272725,
      "grad_norm": 0.08671621233224869,
      "learning_rate": 9.98691109176969e-07,
      "loss": 0.0015,
      "step": 18110
    },
    {
      "epoch": 3294.5454545454545,
      "grad_norm": 0.042882293462753296,
      "learning_rate": 9.986826820439743e-07,
      "loss": 0.0013,
      "step": 18120
    },
    {
      "epoch": 3296.3636363636365,
      "grad_norm": 0.3588750660419464,
      "learning_rate": 9.986742279052138e-07,
      "loss": 0.0017,
      "step": 18130
    },
    {
      "epoch": 3298.181818181818,
      "grad_norm": 0.025808900594711304,
      "learning_rate": 9.986657467611456e-07,
      "loss": 0.0015,
      "step": 18140
    },
    {
      "epoch": 3300.0,
      "grad_norm": 0.9858134388923645,
      "learning_rate": 9.98657238612229e-07,
      "loss": 0.0018,
      "step": 18150
    },
    {
      "epoch": 3301.818181818182,
      "grad_norm": 0.865651547908783,
      "learning_rate": 9.986487034589248e-07,
      "loss": 0.0018,
      "step": 18160
    },
    {
      "epoch": 3303.6363636363635,
      "grad_norm": 0.6345189809799194,
      "learning_rate": 9.98640141301695e-07,
      "loss": 0.0016,
      "step": 18170
    },
    {
      "epoch": 3305.4545454545455,
      "grad_norm": 0.7995860576629639,
      "learning_rate": 9.986315521410034e-07,
      "loss": 0.0016,
      "step": 18180
    },
    {
      "epoch": 3307.2727272727275,
      "grad_norm": 0.205453023314476,
      "learning_rate": 9.986229359773152e-07,
      "loss": 0.0015,
      "step": 18190
    },
    {
      "epoch": 3309.090909090909,
      "grad_norm": 0.03344428911805153,
      "learning_rate": 9.986142928110972e-07,
      "loss": 0.0013,
      "step": 18200
    },
    {
      "epoch": 3310.909090909091,
      "grad_norm": 0.9868326187133789,
      "learning_rate": 9.986056226428168e-07,
      "loss": 0.0017,
      "step": 18210
    },
    {
      "epoch": 3312.7272727272725,
      "grad_norm": 0.167056143283844,
      "learning_rate": 9.985969254729443e-07,
      "loss": 0.0016,
      "step": 18220
    },
    {
      "epoch": 3314.5454545454545,
      "grad_norm": 0.7116246223449707,
      "learning_rate": 9.9858820130195e-07,
      "loss": 0.0013,
      "step": 18230
    },
    {
      "epoch": 3316.3636363636365,
      "grad_norm": 0.5645108222961426,
      "learning_rate": 9.98579450130307e-07,
      "loss": 0.0019,
      "step": 18240
    },
    {
      "epoch": 3318.181818181818,
      "grad_norm": 0.6850662231445312,
      "learning_rate": 9.985706719584886e-07,
      "loss": 0.0013,
      "step": 18250
    },
    {
      "epoch": 3320.0,
      "grad_norm": 1.2093915939331055,
      "learning_rate": 9.985618667869707e-07,
      "loss": 0.0016,
      "step": 18260
    },
    {
      "epoch": 3321.818181818182,
      "grad_norm": 0.05824984237551689,
      "learning_rate": 9.985530346162298e-07,
      "loss": 0.0013,
      "step": 18270
    },
    {
      "epoch": 3323.6363636363635,
      "grad_norm": 0.7155154943466187,
      "learning_rate": 9.985441754467443e-07,
      "loss": 0.0018,
      "step": 18280
    },
    {
      "epoch": 3325.4545454545455,
      "grad_norm": 0.6608107089996338,
      "learning_rate": 9.98535289278994e-07,
      "loss": 0.0015,
      "step": 18290
    },
    {
      "epoch": 3327.2727272727275,
      "grad_norm": 0.8934088349342346,
      "learning_rate": 9.9852637611346e-07,
      "loss": 0.0016,
      "step": 18300
    },
    {
      "epoch": 3329.090909090909,
      "grad_norm": 0.6281945109367371,
      "learning_rate": 9.985174359506252e-07,
      "loss": 0.0016,
      "step": 18310
    },
    {
      "epoch": 3330.909090909091,
      "grad_norm": 1.088651180267334,
      "learning_rate": 9.985084687909736e-07,
      "loss": 0.0015,
      "step": 18320
    },
    {
      "epoch": 3332.7272727272725,
      "grad_norm": 0.665527880191803,
      "learning_rate": 9.984994746349908e-07,
      "loss": 0.0014,
      "step": 18330
    },
    {
      "epoch": 3334.5454545454545,
      "grad_norm": 0.9631626605987549,
      "learning_rate": 9.98490453483164e-07,
      "loss": 0.0018,
      "step": 18340
    },
    {
      "epoch": 3336.3636363636365,
      "grad_norm": 0.17303653061389923,
      "learning_rate": 9.984814053359814e-07,
      "loss": 0.0012,
      "step": 18350
    },
    {
      "epoch": 3338.181818181818,
      "grad_norm": 0.018352998420596123,
      "learning_rate": 9.984723301939335e-07,
      "loss": 0.0013,
      "step": 18360
    },
    {
      "epoch": 3340.0,
      "grad_norm": 0.6191648840904236,
      "learning_rate": 9.984632280575113e-07,
      "loss": 0.0016,
      "step": 18370
    },
    {
      "epoch": 3341.818181818182,
      "grad_norm": 0.031646281480789185,
      "learning_rate": 9.98454098927208e-07,
      "loss": 0.0015,
      "step": 18380
    },
    {
      "epoch": 3343.6363636363635,
      "grad_norm": 0.02722638100385666,
      "learning_rate": 9.98444942803518e-07,
      "loss": 0.0014,
      "step": 18390
    },
    {
      "epoch": 3345.4545454545455,
      "grad_norm": 0.023550620302557945,
      "learning_rate": 9.984357596869368e-07,
      "loss": 0.0016,
      "step": 18400
    },
    {
      "epoch": 3347.2727272727275,
      "grad_norm": 0.8888351321220398,
      "learning_rate": 9.98426549577962e-07,
      "loss": 0.0015,
      "step": 18410
    },
    {
      "epoch": 3349.090909090909,
      "grad_norm": 0.08130642771720886,
      "learning_rate": 9.984173124770923e-07,
      "loss": 0.0016,
      "step": 18420
    },
    {
      "epoch": 3350.909090909091,
      "grad_norm": 1.0180562734603882,
      "learning_rate": 9.98408048384828e-07,
      "loss": 0.0015,
      "step": 18430
    },
    {
      "epoch": 3352.7272727272725,
      "grad_norm": 1.162081003189087,
      "learning_rate": 9.983987573016707e-07,
      "loss": 0.0018,
      "step": 18440
    },
    {
      "epoch": 3354.5454545454545,
      "grad_norm": 1.3157076835632324,
      "learning_rate": 9.983894392281235e-07,
      "loss": 0.0017,
      "step": 18450
    },
    {
      "epoch": 3356.3636363636365,
      "grad_norm": 0.032893478870391846,
      "learning_rate": 9.983800941646912e-07,
      "loss": 0.0013,
      "step": 18460
    },
    {
      "epoch": 3358.181818181818,
      "grad_norm": 0.020752474665641785,
      "learning_rate": 9.983707221118796e-07,
      "loss": 0.0015,
      "step": 18470
    },
    {
      "epoch": 3360.0,
      "grad_norm": 0.19735895097255707,
      "learning_rate": 9.983613230701966e-07,
      "loss": 0.0016,
      "step": 18480
    },
    {
      "epoch": 3361.818181818182,
      "grad_norm": 0.7597088813781738,
      "learning_rate": 9.983518970401508e-07,
      "loss": 0.0014,
      "step": 18490
    },
    {
      "epoch": 3363.6363636363635,
      "grad_norm": 0.7239081263542175,
      "learning_rate": 9.983424440222529e-07,
      "loss": 0.0017,
      "step": 18500
    },
    {
      "epoch": 3363.6363636363635,
      "eval_loss": 4.0669074058532715,
      "eval_runtime": 0.9547,
      "eval_samples_per_second": 10.475,
      "eval_steps_per_second": 5.237,
      "step": 18500
    },
    {
      "epoch": 3365.4545454545455,
      "grad_norm": 0.6231215596199036,
      "learning_rate": 9.983329640170147e-07,
      "loss": 0.0015,
      "step": 18510
    },
    {
      "epoch": 3367.2727272727275,
      "grad_norm": 0.6169325709342957,
      "learning_rate": 9.983234570249499e-07,
      "loss": 0.0015,
      "step": 18520
    },
    {
      "epoch": 3369.090909090909,
      "grad_norm": 0.05128967761993408,
      "learning_rate": 9.98313923046573e-07,
      "loss": 0.0012,
      "step": 18530
    },
    {
      "epoch": 3370.909090909091,
      "grad_norm": 0.021394092589616776,
      "learning_rate": 9.983043620824004e-07,
      "loss": 0.0015,
      "step": 18540
    },
    {
      "epoch": 3372.7272727272725,
      "grad_norm": 0.5118427872657776,
      "learning_rate": 9.982947741329498e-07,
      "loss": 0.0016,
      "step": 18550
    },
    {
      "epoch": 3374.5454545454545,
      "grad_norm": 0.6181852221488953,
      "learning_rate": 9.982851591987405e-07,
      "loss": 0.0015,
      "step": 18560
    },
    {
      "epoch": 3376.3636363636365,
      "grad_norm": 0.014360359869897366,
      "learning_rate": 9.982755172802932e-07,
      "loss": 0.0012,
      "step": 18570
    },
    {
      "epoch": 3378.181818181818,
      "grad_norm": 0.703324556350708,
      "learning_rate": 9.982658483781302e-07,
      "loss": 0.0018,
      "step": 18580
    },
    {
      "epoch": 3380.0,
      "grad_norm": 0.8020220398902893,
      "learning_rate": 9.982561524927748e-07,
      "loss": 0.0014,
      "step": 18590
    },
    {
      "epoch": 3381.818181818182,
      "grad_norm": 0.2161121368408203,
      "learning_rate": 9.982464296247522e-07,
      "loss": 0.0015,
      "step": 18600
    },
    {
      "epoch": 3383.6363636363635,
      "grad_norm": 0.8782758712768555,
      "learning_rate": 9.98236679774589e-07,
      "loss": 0.0015,
      "step": 18610
    },
    {
      "epoch": 3385.4545454545455,
      "grad_norm": 0.09562245011329651,
      "learning_rate": 9.982269029428129e-07,
      "loss": 0.0016,
      "step": 18620
    },
    {
      "epoch": 3387.2727272727275,
      "grad_norm": 0.03781655430793762,
      "learning_rate": 9.982170991299538e-07,
      "loss": 0.0012,
      "step": 18630
    },
    {
      "epoch": 3389.090909090909,
      "grad_norm": 0.02173450030386448,
      "learning_rate": 9.982072683365426e-07,
      "loss": 0.0016,
      "step": 18640
    },
    {
      "epoch": 3390.909090909091,
      "grad_norm": 0.6482617259025574,
      "learning_rate": 9.981974105631115e-07,
      "loss": 0.0016,
      "step": 18650
    },
    {
      "epoch": 3392.7272727272725,
      "grad_norm": 0.0913829430937767,
      "learning_rate": 9.981875258101942e-07,
      "loss": 0.0013,
      "step": 18660
    },
    {
      "epoch": 3394.5454545454545,
      "grad_norm": 0.9170851707458496,
      "learning_rate": 9.98177614078326e-07,
      "loss": 0.0015,
      "step": 18670
    },
    {
      "epoch": 3396.3636363636365,
      "grad_norm": 0.5964849591255188,
      "learning_rate": 9.98167675368044e-07,
      "loss": 0.0017,
      "step": 18680
    },
    {
      "epoch": 3398.181818181818,
      "grad_norm": 0.03342992067337036,
      "learning_rate": 9.981577096798863e-07,
      "loss": 0.0009,
      "step": 18690
    },
    {
      "epoch": 3400.0,
      "grad_norm": 0.2676316499710083,
      "learning_rate": 9.981477170143924e-07,
      "loss": 0.0016,
      "step": 18700
    },
    {
      "epoch": 3401.818181818182,
      "grad_norm": 1.0656200647354126,
      "learning_rate": 9.981376973721033e-07,
      "loss": 0.0017,
      "step": 18710
    },
    {
      "epoch": 3403.6363636363635,
      "grad_norm": 0.019512858241796494,
      "learning_rate": 9.981276507535624e-07,
      "loss": 0.0014,
      "step": 18720
    },
    {
      "epoch": 3405.4545454545455,
      "grad_norm": 0.6900899410247803,
      "learning_rate": 9.981175771593129e-07,
      "loss": 0.0018,
      "step": 18730
    },
    {
      "epoch": 3407.2727272727275,
      "grad_norm": 0.5342694520950317,
      "learning_rate": 9.981074765899007e-07,
      "loss": 0.0012,
      "step": 18740
    },
    {
      "epoch": 3409.090909090909,
      "grad_norm": 0.6186282634735107,
      "learning_rate": 9.980973490458728e-07,
      "loss": 0.0017,
      "step": 18750
    },
    {
      "epoch": 3410.909090909091,
      "grad_norm": 0.6666581630706787,
      "learning_rate": 9.980871945277775e-07,
      "loss": 0.0016,
      "step": 18760
    },
    {
      "epoch": 3412.7272727272725,
      "grad_norm": 0.02198634296655655,
      "learning_rate": 9.980770130361649e-07,
      "loss": 0.0014,
      "step": 18770
    },
    {
      "epoch": 3414.5454545454545,
      "grad_norm": 0.02728545106947422,
      "learning_rate": 9.980668045715862e-07,
      "loss": 0.0017,
      "step": 18780
    },
    {
      "epoch": 3416.3636363636365,
      "grad_norm": 0.05163360387086868,
      "learning_rate": 9.980565691345943e-07,
      "loss": 0.0012,
      "step": 18790
    },
    {
      "epoch": 3418.181818181818,
      "grad_norm": 0.09502919018268585,
      "learning_rate": 9.980463067257436e-07,
      "loss": 0.0014,
      "step": 18800
    },
    {
      "epoch": 3420.0,
      "grad_norm": 0.588010847568512,
      "learning_rate": 9.980360173455898e-07,
      "loss": 0.0015,
      "step": 18810
    },
    {
      "epoch": 3421.818181818182,
      "grad_norm": 0.01778889261186123,
      "learning_rate": 9.9802570099469e-07,
      "loss": 0.0015,
      "step": 18820
    },
    {
      "epoch": 3423.6363636363635,
      "grad_norm": 0.770879328250885,
      "learning_rate": 9.980153576736029e-07,
      "loss": 0.0016,
      "step": 18830
    },
    {
      "epoch": 3425.4545454545455,
      "grad_norm": 0.09096691757440567,
      "learning_rate": 9.980049873828885e-07,
      "loss": 0.0014,
      "step": 18840
    },
    {
      "epoch": 3427.2727272727275,
      "grad_norm": 0.028252962976694107,
      "learning_rate": 9.97994590123109e-07,
      "loss": 0.0012,
      "step": 18850
    },
    {
      "epoch": 3429.090909090909,
      "grad_norm": 0.02452527917921543,
      "learning_rate": 9.979841658948266e-07,
      "loss": 0.0016,
      "step": 18860
    },
    {
      "epoch": 3430.909090909091,
      "grad_norm": 0.6045308113098145,
      "learning_rate": 9.979737146986063e-07,
      "loss": 0.0015,
      "step": 18870
    },
    {
      "epoch": 3432.7272727272725,
      "grad_norm": 0.6794413924217224,
      "learning_rate": 9.979632365350142e-07,
      "loss": 0.0013,
      "step": 18880
    },
    {
      "epoch": 3434.5454545454545,
      "grad_norm": 0.608725905418396,
      "learning_rate": 9.979527314046174e-07,
      "loss": 0.0015,
      "step": 18890
    },
    {
      "epoch": 3436.3636363636365,
      "grad_norm": 0.9333797693252563,
      "learning_rate": 9.97942199307985e-07,
      "loss": 0.0018,
      "step": 18900
    },
    {
      "epoch": 3438.181818181818,
      "grad_norm": 0.5648797750473022,
      "learning_rate": 9.979316402456874e-07,
      "loss": 0.0012,
      "step": 18910
    },
    {
      "epoch": 3440.0,
      "grad_norm": 0.7745028734207153,
      "learning_rate": 9.979210542182962e-07,
      "loss": 0.0015,
      "step": 18920
    },
    {
      "epoch": 3441.818181818182,
      "grad_norm": 0.7144650816917419,
      "learning_rate": 9.97910441226385e-07,
      "loss": 0.0016,
      "step": 18930
    },
    {
      "epoch": 3443.6363636363635,
      "grad_norm": 0.02200382575392723,
      "learning_rate": 9.978998012705282e-07,
      "loss": 0.0015,
      "step": 18940
    },
    {
      "epoch": 3445.4545454545455,
      "grad_norm": 0.5909247994422913,
      "learning_rate": 9.978891343513022e-07,
      "loss": 0.0014,
      "step": 18950
    },
    {
      "epoch": 3447.2727272727275,
      "grad_norm": 0.6500568389892578,
      "learning_rate": 9.978784404692846e-07,
      "loss": 0.0016,
      "step": 18960
    },
    {
      "epoch": 3449.090909090909,
      "grad_norm": 0.5186755061149597,
      "learning_rate": 9.978677196250545e-07,
      "loss": 0.0015,
      "step": 18970
    },
    {
      "epoch": 3450.909090909091,
      "grad_norm": 0.02781355381011963,
      "learning_rate": 9.978569718191923e-07,
      "loss": 0.0012,
      "step": 18980
    },
    {
      "epoch": 3452.7272727272725,
      "grad_norm": 1.018973469734192,
      "learning_rate": 9.978461970522805e-07,
      "loss": 0.0015,
      "step": 18990
    },
    {
      "epoch": 3454.5454545454545,
      "grad_norm": 0.6289427876472473,
      "learning_rate": 9.978353953249021e-07,
      "loss": 0.0018,
      "step": 19000
    },
    {
      "epoch": 3454.5454545454545,
      "eval_loss": 4.105101585388184,
      "eval_runtime": 0.9508,
      "eval_samples_per_second": 10.518,
      "eval_steps_per_second": 5.259,
      "step": 19000
    },
    {
      "epoch": 3456.3636363636365,
      "grad_norm": 0.019797855988144875,
      "learning_rate": 9.978245666376426e-07,
      "loss": 0.0013,
      "step": 19010
    },
    {
      "epoch": 3458.181818181818,
      "grad_norm": 0.46762627363204956,
      "learning_rate": 9.978137109910878e-07,
      "loss": 0.0014,
      "step": 19020
    },
    {
      "epoch": 3460.0,
      "grad_norm": 0.6340343952178955,
      "learning_rate": 9.97802828385826e-07,
      "loss": 0.0014,
      "step": 19030
    },
    {
      "epoch": 3461.818181818182,
      "grad_norm": 0.4676205813884735,
      "learning_rate": 9.977919188224465e-07,
      "loss": 0.0011,
      "step": 19040
    },
    {
      "epoch": 3463.6363636363635,
      "grad_norm": 1.0188665390014648,
      "learning_rate": 9.9778098230154e-07,
      "loss": 0.002,
      "step": 19050
    },
    {
      "epoch": 3465.4545454545455,
      "grad_norm": 0.7932378649711609,
      "learning_rate": 9.977700188236988e-07,
      "loss": 0.0014,
      "step": 19060
    },
    {
      "epoch": 3467.2727272727275,
      "grad_norm": 0.47234025597572327,
      "learning_rate": 9.977590283895166e-07,
      "loss": 0.0014,
      "step": 19070
    },
    {
      "epoch": 3469.090909090909,
      "grad_norm": 0.9124956130981445,
      "learning_rate": 9.977480109995883e-07,
      "loss": 0.0016,
      "step": 19080
    },
    {
      "epoch": 3470.909090909091,
      "grad_norm": 0.5818092823028564,
      "learning_rate": 9.977369666545113e-07,
      "loss": 0.0016,
      "step": 19090
    },
    {
      "epoch": 3472.7272727272725,
      "grad_norm": 0.8529733419418335,
      "learning_rate": 9.977258953548829e-07,
      "loss": 0.0015,
      "step": 19100
    },
    {
      "epoch": 3474.5454545454545,
      "grad_norm": 0.5806490182876587,
      "learning_rate": 9.977147971013032e-07,
      "loss": 0.0015,
      "step": 19110
    },
    {
      "epoch": 3476.3636363636365,
      "grad_norm": 0.02676643803715706,
      "learning_rate": 9.977036718943728e-07,
      "loss": 0.0013,
      "step": 19120
    },
    {
      "epoch": 3478.181818181818,
      "grad_norm": 0.5971592664718628,
      "learning_rate": 9.976925197346944e-07,
      "loss": 0.0014,
      "step": 19130
    },
    {
      "epoch": 3480.0,
      "grad_norm": 1.0321251153945923,
      "learning_rate": 9.976813406228719e-07,
      "loss": 0.0015,
      "step": 19140
    },
    {
      "epoch": 3481.818181818182,
      "grad_norm": 0.6990383863449097,
      "learning_rate": 9.976701345595109e-07,
      "loss": 0.0016,
      "step": 19150
    },
    {
      "epoch": 3483.6363636363635,
      "grad_norm": 0.11746776849031448,
      "learning_rate": 9.976589015452176e-07,
      "loss": 0.0013,
      "step": 19160
    },
    {
      "epoch": 3485.4545454545455,
      "grad_norm": 0.21289609372615814,
      "learning_rate": 9.976476415806012e-07,
      "loss": 0.0018,
      "step": 19170
    },
    {
      "epoch": 3487.2727272727275,
      "grad_norm": 0.020593468099832535,
      "learning_rate": 9.97636354666271e-07,
      "loss": 0.0015,
      "step": 19180
    },
    {
      "epoch": 3489.090909090909,
      "grad_norm": 0.10495951026678085,
      "learning_rate": 9.976250408028381e-07,
      "loss": 0.0012,
      "step": 19190
    },
    {
      "epoch": 3490.909090909091,
      "grad_norm": 0.778415322303772,
      "learning_rate": 9.976136999909155e-07,
      "loss": 0.0015,
      "step": 19200
    },
    {
      "epoch": 3492.7272727272725,
      "grad_norm": 0.025068920105695724,
      "learning_rate": 9.976023322311172e-07,
      "loss": 0.0013,
      "step": 19210
    },
    {
      "epoch": 3494.5454545454545,
      "grad_norm": 0.016204889863729477,
      "learning_rate": 9.97590937524059e-07,
      "loss": 0.0015,
      "step": 19220
    },
    {
      "epoch": 3496.3636363636365,
      "grad_norm": 0.5784905552864075,
      "learning_rate": 9.975795158703576e-07,
      "loss": 0.0015,
      "step": 19230
    },
    {
      "epoch": 3498.181818181818,
      "grad_norm": 0.02575662173330784,
      "learning_rate": 9.975680672706318e-07,
      "loss": 0.0013,
      "step": 19240
    },
    {
      "epoch": 3500.0,
      "grad_norm": 0.758732259273529,
      "learning_rate": 9.975565917255015e-07,
      "loss": 0.0016,
      "step": 19250
    },
    {
      "epoch": 3501.818181818182,
      "grad_norm": 0.9592216610908508,
      "learning_rate": 9.975450892355882e-07,
      "loss": 0.0015,
      "step": 19260
    },
    {
      "epoch": 3503.6363636363635,
      "grad_norm": 0.026835670694708824,
      "learning_rate": 9.975335598015147e-07,
      "loss": 0.0011,
      "step": 19270
    },
    {
      "epoch": 3505.4545454545455,
      "grad_norm": 0.02038384974002838,
      "learning_rate": 9.975220034239055e-07,
      "loss": 0.0014,
      "step": 19280
    },
    {
      "epoch": 3507.2727272727275,
      "grad_norm": 0.6794761419296265,
      "learning_rate": 9.975104201033866e-07,
      "loss": 0.0015,
      "step": 19290
    },
    {
      "epoch": 3509.090909090909,
      "grad_norm": 0.03429122641682625,
      "learning_rate": 9.97498809840585e-07,
      "loss": 0.0015,
      "step": 19300
    },
    {
      "epoch": 3510.909090909091,
      "grad_norm": 0.0236504003405571,
      "learning_rate": 9.974871726361294e-07,
      "loss": 0.0016,
      "step": 19310
    },
    {
      "epoch": 3512.7272727272725,
      "grad_norm": 0.6051403284072876,
      "learning_rate": 9.9747550849065e-07,
      "loss": 0.0015,
      "step": 19320
    },
    {
      "epoch": 3514.5454545454545,
      "grad_norm": 0.561345636844635,
      "learning_rate": 9.974638174047787e-07,
      "loss": 0.0013,
      "step": 19330
    },
    {
      "epoch": 3516.3636363636365,
      "grad_norm": 0.03360164165496826,
      "learning_rate": 9.974520993791486e-07,
      "loss": 0.0011,
      "step": 19340
    },
    {
      "epoch": 3518.181818181818,
      "grad_norm": 0.6676942706108093,
      "learning_rate": 9.974403544143942e-07,
      "loss": 0.0017,
      "step": 19350
    },
    {
      "epoch": 3520.0,
      "grad_norm": 0.7055047154426575,
      "learning_rate": 9.974285825111512e-07,
      "loss": 0.0013,
      "step": 19360
    },
    {
      "epoch": 3521.818181818182,
      "grad_norm": 0.6976327896118164,
      "learning_rate": 9.974167836700577e-07,
      "loss": 0.0014,
      "step": 19370
    },
    {
      "epoch": 3523.6363636363635,
      "grad_norm": 0.6083614230155945,
      "learning_rate": 9.974049578917523e-07,
      "loss": 0.0016,
      "step": 19380
    },
    {
      "epoch": 3525.4545454545455,
      "grad_norm": 0.740580141544342,
      "learning_rate": 9.973931051768756e-07,
      "loss": 0.0013,
      "step": 19390
    },
    {
      "epoch": 3527.2727272727275,
      "grad_norm": 0.021293822675943375,
      "learning_rate": 9.973812255260692e-07,
      "loss": 0.0015,
      "step": 19400
    },
    {
      "epoch": 3529.090909090909,
      "grad_norm": 0.02647755853831768,
      "learning_rate": 9.973693189399767e-07,
      "loss": 0.0013,
      "step": 19410
    },
    {
      "epoch": 3530.909090909091,
      "grad_norm": 0.024084866046905518,
      "learning_rate": 9.973573854192428e-07,
      "loss": 0.0015,
      "step": 19420
    },
    {
      "epoch": 3532.7272727272725,
      "grad_norm": 0.10004471242427826,
      "learning_rate": 9.973454249645137e-07,
      "loss": 0.0013,
      "step": 19430
    },
    {
      "epoch": 3534.5454545454545,
      "grad_norm": 0.6652141213417053,
      "learning_rate": 9.97333437576437e-07,
      "loss": 0.0013,
      "step": 19440
    },
    {
      "epoch": 3536.3636363636365,
      "grad_norm": 0.05570073425769806,
      "learning_rate": 9.973214232556621e-07,
      "loss": 0.0014,
      "step": 19450
    },
    {
      "epoch": 3538.181818181818,
      "grad_norm": 0.7163782119750977,
      "learning_rate": 9.973093820028396e-07,
      "loss": 0.0017,
      "step": 19460
    },
    {
      "epoch": 3540.0,
      "grad_norm": 0.021790290251374245,
      "learning_rate": 9.972973138186216e-07,
      "loss": 0.0014,
      "step": 19470
    },
    {
      "epoch": 3541.818181818182,
      "grad_norm": 0.01565249264240265,
      "learning_rate": 9.972852187036615e-07,
      "loss": 0.0016,
      "step": 19480
    },
    {
      "epoch": 3543.6363636363635,
      "grad_norm": 0.5653706789016724,
      "learning_rate": 9.972730966586143e-07,
      "loss": 0.0011,
      "step": 19490
    },
    {
      "epoch": 3545.4545454545455,
      "grad_norm": 0.843375563621521,
      "learning_rate": 9.972609476841365e-07,
      "loss": 0.0015,
      "step": 19500
    },
    {
      "epoch": 3545.4545454545455,
      "eval_loss": 4.090299129486084,
      "eval_runtime": 0.9542,
      "eval_samples_per_second": 10.48,
      "eval_steps_per_second": 5.24,
      "step": 19500
    },
    {
      "epoch": 3547.2727272727275,
      "grad_norm": 0.48926490545272827,
      "learning_rate": 9.972487717808862e-07,
      "loss": 0.0014,
      "step": 19510
    },
    {
      "epoch": 3549.090909090909,
      "grad_norm": 0.6523905396461487,
      "learning_rate": 9.972365689495225e-07,
      "loss": 0.0015,
      "step": 19520
    },
    {
      "epoch": 3550.909090909091,
      "grad_norm": 0.7374210953712463,
      "learning_rate": 9.972243391907066e-07,
      "loss": 0.0011,
      "step": 19530
    },
    {
      "epoch": 3552.7272727272725,
      "grad_norm": 0.01902635768055916,
      "learning_rate": 9.972120825051003e-07,
      "loss": 0.0014,
      "step": 19540
    },
    {
      "epoch": 3554.5454545454545,
      "grad_norm": 0.9286897778511047,
      "learning_rate": 9.971997988933678e-07,
      "loss": 0.0019,
      "step": 19550
    },
    {
      "epoch": 3556.3636363636365,
      "grad_norm": 0.6279535889625549,
      "learning_rate": 9.971874883561739e-07,
      "loss": 0.0011,
      "step": 19560
    },
    {
      "epoch": 3558.181818181818,
      "grad_norm": 0.6543020606040955,
      "learning_rate": 9.971751508941856e-07,
      "loss": 0.0015,
      "step": 19570
    },
    {
      "epoch": 3560.0,
      "grad_norm": 0.7240537405014038,
      "learning_rate": 9.97162786508071e-07,
      "loss": 0.0013,
      "step": 19580
    },
    {
      "epoch": 3561.818181818182,
      "grad_norm": 0.7279441356658936,
      "learning_rate": 9.971503951984993e-07,
      "loss": 0.0015,
      "step": 19590
    },
    {
      "epoch": 3563.6363636363635,
      "grad_norm": 0.028265444561839104,
      "learning_rate": 9.971379769661421e-07,
      "loss": 0.0014,
      "step": 19600
    },
    {
      "epoch": 3565.4545454545455,
      "grad_norm": 0.015169063583016396,
      "learning_rate": 9.971255318116716e-07,
      "loss": 0.0012,
      "step": 19610
    },
    {
      "epoch": 3567.2727272727275,
      "grad_norm": 0.387345552444458,
      "learning_rate": 9.971130597357618e-07,
      "loss": 0.0017,
      "step": 19620
    },
    {
      "epoch": 3569.090909090909,
      "grad_norm": 0.017283327877521515,
      "learning_rate": 9.971005607390878e-07,
      "loss": 0.0013,
      "step": 19630
    },
    {
      "epoch": 3570.909090909091,
      "grad_norm": 0.5507768392562866,
      "learning_rate": 9.970880348223273e-07,
      "loss": 0.0016,
      "step": 19640
    },
    {
      "epoch": 3572.7272727272725,
      "grad_norm": 0.6458188891410828,
      "learning_rate": 9.970754819861577e-07,
      "loss": 0.0014,
      "step": 19650
    },
    {
      "epoch": 3574.5454545454545,
      "grad_norm": 0.026855165138840675,
      "learning_rate": 9.970629022312593e-07,
      "loss": 0.0013,
      "step": 19660
    },
    {
      "epoch": 3576.3636363636365,
      "grad_norm": 0.627789318561554,
      "learning_rate": 9.970502955583133e-07,
      "loss": 0.0013,
      "step": 19670
    },
    {
      "epoch": 3578.181818181818,
      "grad_norm": 0.04255141690373421,
      "learning_rate": 9.970376619680022e-07,
      "loss": 0.0013,
      "step": 19680
    },
    {
      "epoch": 3580.0,
      "grad_norm": 0.6807352304458618,
      "learning_rate": 9.970250014610105e-07,
      "loss": 0.0015,
      "step": 19690
    },
    {
      "epoch": 3581.818181818182,
      "grad_norm": 0.03364301845431328,
      "learning_rate": 9.970123140380234e-07,
      "loss": 0.001,
      "step": 19700
    },
    {
      "epoch": 3583.6363636363635,
      "grad_norm": 0.5709851384162903,
      "learning_rate": 9.969995996997285e-07,
      "loss": 0.0019,
      "step": 19710
    },
    {
      "epoch": 3585.4545454545455,
      "grad_norm": 0.8521291017532349,
      "learning_rate": 9.969868584468138e-07,
      "loss": 0.0014,
      "step": 19720
    },
    {
      "epoch": 3587.2727272727275,
      "grad_norm": 0.017707079648971558,
      "learning_rate": 9.969740902799698e-07,
      "loss": 0.001,
      "step": 19730
    },
    {
      "epoch": 3589.090909090909,
      "grad_norm": 0.7938735485076904,
      "learning_rate": 9.969612951998873e-07,
      "loss": 0.0017,
      "step": 19740
    },
    {
      "epoch": 3590.909090909091,
      "grad_norm": 0.9226093888282776,
      "learning_rate": 9.9694847320726e-07,
      "loss": 0.0013,
      "step": 19750
    },
    {
      "epoch": 3592.7272727272725,
      "grad_norm": 0.5774974226951599,
      "learning_rate": 9.969356243027817e-07,
      "loss": 0.0014,
      "step": 19760
    },
    {
      "epoch": 3594.5454545454545,
      "grad_norm": 0.10473829507827759,
      "learning_rate": 9.969227484871482e-07,
      "loss": 0.0014,
      "step": 19770
    },
    {
      "epoch": 3596.3636363636365,
      "grad_norm": 0.5607587099075317,
      "learning_rate": 9.969098457610572e-07,
      "loss": 0.0014,
      "step": 19780
    },
    {
      "epoch": 3598.181818181818,
      "grad_norm": 0.538138747215271,
      "learning_rate": 9.96896916125207e-07,
      "loss": 0.0014,
      "step": 19790
    },
    {
      "epoch": 3600.0,
      "grad_norm": 0.7456148862838745,
      "learning_rate": 9.968839595802981e-07,
      "loss": 0.0014,
      "step": 19800
    },
    {
      "epoch": 3601.818181818182,
      "grad_norm": 0.8805556297302246,
      "learning_rate": 9.968709761270322e-07,
      "loss": 0.0016,
      "step": 19810
    },
    {
      "epoch": 3603.6363636363635,
      "grad_norm": 0.011285685002803802,
      "learning_rate": 9.96857965766112e-07,
      "loss": 0.0012,
      "step": 19820
    },
    {
      "epoch": 3605.4545454545455,
      "grad_norm": 0.037013884633779526,
      "learning_rate": 9.968449284982424e-07,
      "loss": 0.0011,
      "step": 19830
    },
    {
      "epoch": 3607.2727272727275,
      "grad_norm": 0.018193891271948814,
      "learning_rate": 9.968318643241293e-07,
      "loss": 0.0016,
      "step": 19840
    },
    {
      "epoch": 3609.090909090909,
      "grad_norm": 0.6418598890304565,
      "learning_rate": 9.968187732444803e-07,
      "loss": 0.0015,
      "step": 19850
    },
    {
      "epoch": 3610.909090909091,
      "grad_norm": 0.42633700370788574,
      "learning_rate": 9.968056552600042e-07,
      "loss": 0.0013,
      "step": 19860
    },
    {
      "epoch": 3612.7272727272725,
      "grad_norm": 0.03344232961535454,
      "learning_rate": 9.967925103714115e-07,
      "loss": 0.0013,
      "step": 19870
    },
    {
      "epoch": 3614.5454545454545,
      "grad_norm": 0.5833625793457031,
      "learning_rate": 9.967793385794139e-07,
      "loss": 0.0016,
      "step": 19880
    },
    {
      "epoch": 3616.3636363636365,
      "grad_norm": 0.44023188948631287,
      "learning_rate": 9.967661398847249e-07,
      "loss": 0.0011,
      "step": 19890
    },
    {
      "epoch": 3618.181818181818,
      "grad_norm": 0.020434612408280373,
      "learning_rate": 9.96752914288059e-07,
      "loss": 0.0015,
      "step": 19900
    },
    {
      "epoch": 3620.0,
      "grad_norm": 0.017110364511609077,
      "learning_rate": 9.96739661790133e-07,
      "loss": 0.0015,
      "step": 19910
    },
    {
      "epoch": 3621.818181818182,
      "grad_norm": 0.5681207180023193,
      "learning_rate": 9.967263823916637e-07,
      "loss": 0.0015,
      "step": 19920
    },
    {
      "epoch": 3623.6363636363635,
      "grad_norm": 0.025538332760334015,
      "learning_rate": 9.96713076093371e-07,
      "loss": 0.0015,
      "step": 19930
    },
    {
      "epoch": 3625.4545454545455,
      "grad_norm": 0.01603453792631626,
      "learning_rate": 9.966997428959752e-07,
      "loss": 0.0013,
      "step": 19940
    },
    {
      "epoch": 3627.2727272727275,
      "grad_norm": 0.017553547397255898,
      "learning_rate": 9.96686382800198e-07,
      "loss": 0.0016,
      "step": 19950
    },
    {
      "epoch": 3629.090909090909,
      "grad_norm": 0.9323655366897583,
      "learning_rate": 9.966729958067636e-07,
      "loss": 0.0013,
      "step": 19960
    },
    {
      "epoch": 3630.909090909091,
      "grad_norm": 0.01810276135802269,
      "learning_rate": 9.966595819163966e-07,
      "loss": 0.0013,
      "step": 19970
    },
    {
      "epoch": 3632.7272727272725,
      "grad_norm": 0.599209189414978,
      "learning_rate": 9.966461411298236e-07,
      "loss": 0.0017,
      "step": 19980
    },
    {
      "epoch": 3634.5454545454545,
      "grad_norm": 0.1292218565940857,
      "learning_rate": 9.966326734477719e-07,
      "loss": 0.0014,
      "step": 19990
    },
    {
      "epoch": 3636.3636363636365,
      "grad_norm": 0.021834218874573708,
      "learning_rate": 9.966191788709714e-07,
      "loss": 0.001,
      "step": 20000
    },
    {
      "epoch": 3636.3636363636365,
      "eval_loss": 4.101962089538574,
      "eval_runtime": 0.9478,
      "eval_samples_per_second": 10.551,
      "eval_steps_per_second": 5.276,
      "step": 20000
    },
    {
      "epoch": 3638.181818181818,
      "grad_norm": 0.7812700867652893,
      "learning_rate": 9.966056574001528e-07,
      "loss": 0.0018,
      "step": 20010
    },
    {
      "epoch": 3640.0,
      "grad_norm": 0.019619936123490334,
      "learning_rate": 9.965921090360482e-07,
      "loss": 0.0012,
      "step": 20020
    },
    {
      "epoch": 3641.818181818182,
      "grad_norm": 0.016207613050937653,
      "learning_rate": 9.965785337793915e-07,
      "loss": 0.0013,
      "step": 20030
    },
    {
      "epoch": 3643.6363636363635,
      "grad_norm": 0.6487663388252258,
      "learning_rate": 9.965649316309176e-07,
      "loss": 0.0014,
      "step": 20040
    },
    {
      "epoch": 3645.4545454545455,
      "grad_norm": 0.0233337190002203,
      "learning_rate": 9.965513025913633e-07,
      "loss": 0.0014,
      "step": 20050
    },
    {
      "epoch": 3647.2727272727275,
      "grad_norm": 0.8578993678092957,
      "learning_rate": 9.965376466614665e-07,
      "loss": 0.0017,
      "step": 20060
    },
    {
      "epoch": 3649.090909090909,
      "grad_norm": 0.4892120361328125,
      "learning_rate": 9.965239638419672e-07,
      "loss": 0.0013,
      "step": 20070
    },
    {
      "epoch": 3650.909090909091,
      "grad_norm": 0.5002711415290833,
      "learning_rate": 9.965102541336057e-07,
      "loss": 0.0014,
      "step": 20080
    },
    {
      "epoch": 3652.7272727272725,
      "grad_norm": 0.03654604032635689,
      "learning_rate": 9.964965175371248e-07,
      "loss": 0.0013,
      "step": 20090
    },
    {
      "epoch": 3654.5454545454545,
      "grad_norm": 0.01968849077820778,
      "learning_rate": 9.964827540532684e-07,
      "loss": 0.0018,
      "step": 20100
    },
    {
      "epoch": 3656.3636363636365,
      "grad_norm": 0.7137338519096375,
      "learning_rate": 9.96468963682782e-07,
      "loss": 0.001,
      "step": 20110
    },
    {
      "epoch": 3658.181818181818,
      "grad_norm": 0.4529176354408264,
      "learning_rate": 9.964551464264119e-07,
      "loss": 0.0015,
      "step": 20120
    },
    {
      "epoch": 3660.0,
      "grad_norm": 0.03860205411911011,
      "learning_rate": 9.964413022849067e-07,
      "loss": 0.0014,
      "step": 20130
    },
    {
      "epoch": 3661.818181818182,
      "grad_norm": 0.6774091720581055,
      "learning_rate": 9.964274312590163e-07,
      "loss": 0.0015,
      "step": 20140
    },
    {
      "epoch": 3663.6363636363635,
      "grad_norm": 0.02630767598748207,
      "learning_rate": 9.964135333494917e-07,
      "loss": 0.0014,
      "step": 20150
    },
    {
      "epoch": 3665.4545454545455,
      "grad_norm": 0.8262094855308533,
      "learning_rate": 9.963996085570853e-07,
      "loss": 0.0013,
      "step": 20160
    },
    {
      "epoch": 3667.2727272727275,
      "grad_norm": 0.06496064364910126,
      "learning_rate": 9.963856568825513e-07,
      "loss": 0.0013,
      "step": 20170
    },
    {
      "epoch": 3669.090909090909,
      "grad_norm": 0.5368854403495789,
      "learning_rate": 9.963716783266456e-07,
      "loss": 0.0017,
      "step": 20180
    },
    {
      "epoch": 3670.909090909091,
      "grad_norm": 0.03870866820216179,
      "learning_rate": 9.963576728901248e-07,
      "loss": 0.0009,
      "step": 20190
    },
    {
      "epoch": 3672.7272727272725,
      "grad_norm": 0.6703085899353027,
      "learning_rate": 9.963436405737475e-07,
      "loss": 0.0018,
      "step": 20200
    },
    {
      "epoch": 3674.5454545454545,
      "grad_norm": 0.016285588964819908,
      "learning_rate": 9.963295813782736e-07,
      "loss": 0.0009,
      "step": 20210
    },
    {
      "epoch": 3676.3636363636365,
      "grad_norm": 0.018245454877614975,
      "learning_rate": 9.963154953044644e-07,
      "loss": 0.0016,
      "step": 20220
    },
    {
      "epoch": 3678.181818181818,
      "grad_norm": 0.5595293045043945,
      "learning_rate": 9.963013823530828e-07,
      "loss": 0.0015,
      "step": 20230
    },
    {
      "epoch": 3680.0,
      "grad_norm": 0.5664457678794861,
      "learning_rate": 9.962872425248932e-07,
      "loss": 0.0013,
      "step": 20240
    },
    {
      "epoch": 3681.818181818182,
      "grad_norm": 0.17235524952411652,
      "learning_rate": 9.96273075820661e-07,
      "loss": 0.0015,
      "step": 20250
    },
    {
      "epoch": 3683.6363636363635,
      "grad_norm": 0.01904538832604885,
      "learning_rate": 9.962588822411535e-07,
      "loss": 0.0013,
      "step": 20260
    },
    {
      "epoch": 3685.4545454545455,
      "grad_norm": 0.4546191394329071,
      "learning_rate": 9.962446617871397e-07,
      "loss": 0.0012,
      "step": 20270
    },
    {
      "epoch": 3687.2727272727275,
      "grad_norm": 0.041743360459804535,
      "learning_rate": 9.962304144593893e-07,
      "loss": 0.0014,
      "step": 20280
    },
    {
      "epoch": 3689.090909090909,
      "grad_norm": 0.021040000021457672,
      "learning_rate": 9.962161402586737e-07,
      "loss": 0.0014,
      "step": 20290
    },
    {
      "epoch": 3690.909090909091,
      "grad_norm": 0.542350709438324,
      "learning_rate": 9.962018391857665e-07,
      "loss": 0.0014,
      "step": 20300
    },
    {
      "epoch": 3692.7272727272725,
      "grad_norm": 0.4950917065143585,
      "learning_rate": 9.961875112414414e-07,
      "loss": 0.0013,
      "step": 20310
    },
    {
      "epoch": 3694.5454545454545,
      "grad_norm": 0.5203070640563965,
      "learning_rate": 9.961731564264753e-07,
      "loss": 0.0016,
      "step": 20320
    },
    {
      "epoch": 3696.3636363636365,
      "grad_norm": 0.023955056443810463,
      "learning_rate": 9.961587747416447e-07,
      "loss": 0.0009,
      "step": 20330
    },
    {
      "epoch": 3698.181818181818,
      "grad_norm": 0.017819184809923172,
      "learning_rate": 9.961443661877288e-07,
      "loss": 0.0015,
      "step": 20340
    },
    {
      "epoch": 3700.0,
      "grad_norm": 0.017225829884409904,
      "learning_rate": 9.961299307655077e-07,
      "loss": 0.0014,
      "step": 20350
    },
    {
      "epoch": 3701.818181818182,
      "grad_norm": 0.8332064747810364,
      "learning_rate": 9.961154684757635e-07,
      "loss": 0.0014,
      "step": 20360
    },
    {
      "epoch": 3703.6363636363635,
      "grad_norm": 0.7724996209144592,
      "learning_rate": 9.961009793192793e-07,
      "loss": 0.0012,
      "step": 20370
    },
    {
      "epoch": 3705.4545454545455,
      "grad_norm": 0.018235519528388977,
      "learning_rate": 9.960864632968393e-07,
      "loss": 0.0012,
      "step": 20380
    },
    {
      "epoch": 3707.2727272727275,
      "grad_norm": 0.8448548316955566,
      "learning_rate": 9.960719204092299e-07,
      "loss": 0.0017,
      "step": 20390
    },
    {
      "epoch": 3709.090909090909,
      "grad_norm": 0.03771527111530304,
      "learning_rate": 9.960573506572389e-07,
      "loss": 0.0012,
      "step": 20400
    },
    {
      "epoch": 3710.909090909091,
      "grad_norm": 0.5535852909088135,
      "learning_rate": 9.96042754041655e-07,
      "loss": 0.0014,
      "step": 20410
    },
    {
      "epoch": 3712.7272727272725,
      "grad_norm": 0.18831385672092438,
      "learning_rate": 9.960281305632687e-07,
      "loss": 0.0014,
      "step": 20420
    },
    {
      "epoch": 3714.5454545454545,
      "grad_norm": 0.5523515939712524,
      "learning_rate": 9.960134802228722e-07,
      "loss": 0.0012,
      "step": 20430
    },
    {
      "epoch": 3716.3636363636365,
      "grad_norm": 0.023192955181002617,
      "learning_rate": 9.959988030212585e-07,
      "loss": 0.0013,
      "step": 20440
    },
    {
      "epoch": 3718.181818181818,
      "grad_norm": 0.553835391998291,
      "learning_rate": 9.959840989592225e-07,
      "loss": 0.0015,
      "step": 20450
    },
    {
      "epoch": 3720.0,
      "grad_norm": 0.7136830687522888,
      "learning_rate": 9.959693680375608e-07,
      "loss": 0.0014,
      "step": 20460
    },
    {
      "epoch": 3721.818181818182,
      "grad_norm": 0.4855654835700989,
      "learning_rate": 9.959546102570706e-07,
      "loss": 0.0013,
      "step": 20470
    },
    {
      "epoch": 3723.6363636363635,
      "grad_norm": 0.6714903712272644,
      "learning_rate": 9.959398256185515e-07,
      "loss": 0.0015,
      "step": 20480
    },
    {
      "epoch": 3725.4545454545455,
      "grad_norm": 0.022023728117346764,
      "learning_rate": 9.959250141228043e-07,
      "loss": 0.0014,
      "step": 20490
    },
    {
      "epoch": 3727.2727272727275,
      "grad_norm": 0.06749521195888519,
      "learning_rate": 9.959101757706308e-07,
      "loss": 0.0014,
      "step": 20500
    },
    {
      "epoch": 3727.2727272727275,
      "eval_loss": 4.091845512390137,
      "eval_runtime": 0.9516,
      "eval_samples_per_second": 10.509,
      "eval_steps_per_second": 5.255,
      "step": 20500
    },
    {
      "epoch": 3729.090909090909,
      "grad_norm": 0.021082032471895218,
      "learning_rate": 9.958953105628345e-07,
      "loss": 0.0012,
      "step": 20510
    },
    {
      "epoch": 3730.909090909091,
      "grad_norm": 0.01449020765721798,
      "learning_rate": 9.958804185002208e-07,
      "loss": 0.0015,
      "step": 20520
    },
    {
      "epoch": 3732.7272727272725,
      "grad_norm": 0.05798555910587311,
      "learning_rate": 9.958654995835957e-07,
      "loss": 0.0012,
      "step": 20530
    },
    {
      "epoch": 3734.5454545454545,
      "grad_norm": 0.02440594509243965,
      "learning_rate": 9.958505538137674e-07,
      "loss": 0.0014,
      "step": 20540
    },
    {
      "epoch": 3736.3636363636365,
      "grad_norm": 0.6071258783340454,
      "learning_rate": 9.958355811915451e-07,
      "loss": 0.0014,
      "step": 20550
    },
    {
      "epoch": 3738.181818181818,
      "grad_norm": 0.0168434027582407,
      "learning_rate": 9.958205817177398e-07,
      "loss": 0.0013,
      "step": 20560
    },
    {
      "epoch": 3740.0,
      "grad_norm": 0.018058666959404945,
      "learning_rate": 9.958055553931639e-07,
      "loss": 0.0014,
      "step": 20570
    },
    {
      "epoch": 3741.818181818182,
      "grad_norm": 0.0505049042403698,
      "learning_rate": 9.957905022186308e-07,
      "loss": 0.0014,
      "step": 20580
    },
    {
      "epoch": 3743.6363636363635,
      "grad_norm": 0.04341483488678932,
      "learning_rate": 9.95775422194956e-07,
      "loss": 0.0012,
      "step": 20590
    },
    {
      "epoch": 3745.4545454545455,
      "grad_norm": 0.01562601700425148,
      "learning_rate": 9.957603153229559e-07,
      "loss": 0.0014,
      "step": 20600
    },
    {
      "epoch": 3747.2727272727275,
      "grad_norm": 0.019222749397158623,
      "learning_rate": 9.957451816034487e-07,
      "loss": 0.0013,
      "step": 20610
    },
    {
      "epoch": 3749.090909090909,
      "grad_norm": 0.9084425568580627,
      "learning_rate": 9.95730021037254e-07,
      "loss": 0.0016,
      "step": 20620
    },
    {
      "epoch": 3750.909090909091,
      "grad_norm": 0.659515380859375,
      "learning_rate": 9.957148336251927e-07,
      "loss": 0.0013,
      "step": 20630
    },
    {
      "epoch": 3752.7272727272725,
      "grad_norm": 0.6493903398513794,
      "learning_rate": 9.956996193680873e-07,
      "loss": 0.0013,
      "step": 20640
    },
    {
      "epoch": 3754.5454545454545,
      "grad_norm": 0.5665594935417175,
      "learning_rate": 9.956843782667617e-07,
      "loss": 0.0015,
      "step": 20650
    },
    {
      "epoch": 3756.3636363636365,
      "grad_norm": 0.4679787755012512,
      "learning_rate": 9.956691103220415e-07,
      "loss": 0.0015,
      "step": 20660
    },
    {
      "epoch": 3758.181818181818,
      "grad_norm": 0.020110882818698883,
      "learning_rate": 9.956538155347532e-07,
      "loss": 0.0009,
      "step": 20670
    },
    {
      "epoch": 3760.0,
      "grad_norm": 0.020856063812971115,
      "learning_rate": 9.956384939057254e-07,
      "loss": 0.0015,
      "step": 20680
    },
    {
      "epoch": 3761.818181818182,
      "grad_norm": 0.01754593476653099,
      "learning_rate": 9.956231454357875e-07,
      "loss": 0.0015,
      "step": 20690
    },
    {
      "epoch": 3763.6363636363635,
      "grad_norm": 0.02353501133620739,
      "learning_rate": 9.956077701257707e-07,
      "loss": 0.0011,
      "step": 20700
    },
    {
      "epoch": 3765.4545454545455,
      "grad_norm": 0.04804538935422897,
      "learning_rate": 9.95592367976508e-07,
      "loss": 0.0013,
      "step": 20710
    },
    {
      "epoch": 3767.2727272727275,
      "grad_norm": 0.49278855323791504,
      "learning_rate": 9.955769389888333e-07,
      "loss": 0.0015,
      "step": 20720
    },
    {
      "epoch": 3769.090909090909,
      "grad_norm": 0.09038636833429337,
      "learning_rate": 9.95561483163582e-07,
      "loss": 0.0014,
      "step": 20730
    },
    {
      "epoch": 3770.909090909091,
      "grad_norm": 0.021187391132116318,
      "learning_rate": 9.955460005015911e-07,
      "loss": 0.0013,
      "step": 20740
    },
    {
      "epoch": 3772.7272727272725,
      "grad_norm": 0.5589606165885925,
      "learning_rate": 9.955304910036992e-07,
      "loss": 0.0016,
      "step": 20750
    },
    {
      "epoch": 3774.5454545454545,
      "grad_norm": 0.6281201243400574,
      "learning_rate": 9.955149546707464e-07,
      "loss": 0.0012,
      "step": 20760
    },
    {
      "epoch": 3776.3636363636365,
      "grad_norm": 0.019378721714019775,
      "learning_rate": 9.954993915035736e-07,
      "loss": 0.0012,
      "step": 20770
    },
    {
      "epoch": 3778.181818181818,
      "grad_norm": 0.08711439371109009,
      "learning_rate": 9.95483801503024e-07,
      "loss": 0.0014,
      "step": 20780
    },
    {
      "epoch": 3780.0,
      "grad_norm": 0.6197519302368164,
      "learning_rate": 9.954681846699413e-07,
      "loss": 0.0014,
      "step": 20790
    },
    {
      "epoch": 3781.818181818182,
      "grad_norm": 0.029743732884526253,
      "learning_rate": 9.95452541005172e-07,
      "loss": 0.001,
      "step": 20800
    },
    {
      "epoch": 3783.6363636363635,
      "grad_norm": 0.019568489864468575,
      "learning_rate": 9.954368705095627e-07,
      "loss": 0.0013,
      "step": 20810
    },
    {
      "epoch": 3785.4545454545455,
      "grad_norm": 0.014147737063467503,
      "learning_rate": 9.954211731839623e-07,
      "loss": 0.0017,
      "step": 20820
    },
    {
      "epoch": 3787.2727272727275,
      "grad_norm": 0.08595271408557892,
      "learning_rate": 9.954054490292207e-07,
      "loss": 0.0011,
      "step": 20830
    },
    {
      "epoch": 3789.090909090909,
      "grad_norm": 0.6095008254051208,
      "learning_rate": 9.953896980461895e-07,
      "loss": 0.0016,
      "step": 20840
    },
    {
      "epoch": 3790.909090909091,
      "grad_norm": 0.5559301972389221,
      "learning_rate": 9.953739202357217e-07,
      "loss": 0.0013,
      "step": 20850
    },
    {
      "epoch": 3792.7272727272725,
      "grad_norm": 0.4859742820262909,
      "learning_rate": 9.95358115598672e-07,
      "loss": 0.0011,
      "step": 20860
    },
    {
      "epoch": 3794.5454545454545,
      "grad_norm": 0.015757637098431587,
      "learning_rate": 9.953422841358955e-07,
      "loss": 0.0013,
      "step": 20870
    },
    {
      "epoch": 3796.3636363636365,
      "grad_norm": 0.6830332279205322,
      "learning_rate": 9.953264258482504e-07,
      "loss": 0.0016,
      "step": 20880
    },
    {
      "epoch": 3798.181818181818,
      "grad_norm": 1.0690693855285645,
      "learning_rate": 9.953105407365952e-07,
      "loss": 0.0011,
      "step": 20890
    },
    {
      "epoch": 3800.0,
      "grad_norm": 0.030771585181355476,
      "learning_rate": 9.952946288017898e-07,
      "loss": 0.0013,
      "step": 20900
    },
    {
      "epoch": 3801.818181818182,
      "grad_norm": 0.0267794206738472,
      "learning_rate": 9.952786900446962e-07,
      "loss": 0.0016,
      "step": 20910
    },
    {
      "epoch": 3803.6363636363635,
      "grad_norm": 0.02817363664507866,
      "learning_rate": 9.952627244661778e-07,
      "loss": 0.0008,
      "step": 20920
    },
    {
      "epoch": 3805.4545454545455,
      "grad_norm": 0.11092284321784973,
      "learning_rate": 9.95246732067099e-07,
      "loss": 0.0017,
      "step": 20930
    },
    {
      "epoch": 3807.2727272727275,
      "grad_norm": 0.025865018367767334,
      "learning_rate": 9.952307128483256e-07,
      "loss": 0.0011,
      "step": 20940
    },
    {
      "epoch": 3809.090909090909,
      "grad_norm": 0.01852724887430668,
      "learning_rate": 9.952146668107254e-07,
      "loss": 0.0015,
      "step": 20950
    },
    {
      "epoch": 3810.909090909091,
      "grad_norm": 0.6584116220474243,
      "learning_rate": 9.951985939551672e-07,
      "loss": 0.0012,
      "step": 20960
    },
    {
      "epoch": 3812.7272727272725,
      "grad_norm": 0.43851643800735474,
      "learning_rate": 9.951824942825215e-07,
      "loss": 0.0016,
      "step": 20970
    },
    {
      "epoch": 3814.5454545454545,
      "grad_norm": 0.6179155111312866,
      "learning_rate": 9.951663677936602e-07,
      "loss": 0.0016,
      "step": 20980
    },
    {
      "epoch": 3816.3636363636365,
      "grad_norm": 0.6145653128623962,
      "learning_rate": 9.951502144894563e-07,
      "loss": 0.001,
      "step": 20990
    },
    {
      "epoch": 3818.181818181818,
      "grad_norm": 0.020125143229961395,
      "learning_rate": 9.95134034370785e-07,
      "loss": 0.0012,
      "step": 21000
    },
    {
      "epoch": 3818.181818181818,
      "eval_loss": 4.112173557281494,
      "eval_runtime": 0.9521,
      "eval_samples_per_second": 10.504,
      "eval_steps_per_second": 5.252,
      "step": 21000
    },
    {
      "epoch": 3820.0,
      "grad_norm": 0.5634179711341858,
      "learning_rate": 9.951178274385225e-07,
      "loss": 0.0014,
      "step": 21010
    },
    {
      "epoch": 3821.818181818182,
      "grad_norm": 0.04411272704601288,
      "learning_rate": 9.951015936935463e-07,
      "loss": 0.0013,
      "step": 21020
    },
    {
      "epoch": 3823.6363636363635,
      "grad_norm": 0.4415774643421173,
      "learning_rate": 9.950853331367355e-07,
      "loss": 0.0013,
      "step": 21030
    },
    {
      "epoch": 3825.4545454545455,
      "grad_norm": 0.021471716463565826,
      "learning_rate": 9.950690457689704e-07,
      "loss": 0.0012,
      "step": 21040
    },
    {
      "epoch": 3827.2727272727275,
      "grad_norm": 0.010964271612465382,
      "learning_rate": 9.950527315911339e-07,
      "loss": 0.0014,
      "step": 21050
    },
    {
      "epoch": 3829.090909090909,
      "grad_norm": 0.23320071399211884,
      "learning_rate": 9.950363906041086e-07,
      "loss": 0.0014,
      "step": 21060
    },
    {
      "epoch": 3830.909090909091,
      "grad_norm": 0.5647223591804504,
      "learning_rate": 9.950200228087799e-07,
      "loss": 0.0013,
      "step": 21070
    },
    {
      "epoch": 3832.7272727272725,
      "grad_norm": 0.014596603810787201,
      "learning_rate": 9.95003628206034e-07,
      "loss": 0.001,
      "step": 21080
    },
    {
      "epoch": 3834.5454545454545,
      "grad_norm": 0.018185533583164215,
      "learning_rate": 9.94987206796759e-07,
      "loss": 0.0016,
      "step": 21090
    },
    {
      "epoch": 3836.3636363636365,
      "grad_norm": 0.7505365610122681,
      "learning_rate": 9.949707585818439e-07,
      "loss": 0.0014,
      "step": 21100
    },
    {
      "epoch": 3838.181818181818,
      "grad_norm": 0.014435223303735256,
      "learning_rate": 9.949542835621794e-07,
      "loss": 0.0011,
      "step": 21110
    },
    {
      "epoch": 3840.0,
      "grad_norm": 0.7226189970970154,
      "learning_rate": 9.949377817386578e-07,
      "loss": 0.0015,
      "step": 21120
    },
    {
      "epoch": 3841.818181818182,
      "grad_norm": 0.828079104423523,
      "learning_rate": 9.94921253112173e-07,
      "loss": 0.0015,
      "step": 21130
    },
    {
      "epoch": 3843.6363636363635,
      "grad_norm": 0.014948960393667221,
      "learning_rate": 9.949046976836195e-07,
      "loss": 0.001,
      "step": 21140
    },
    {
      "epoch": 3845.4545454545455,
      "grad_norm": 0.6343014240264893,
      "learning_rate": 9.948881154538944e-07,
      "loss": 0.0016,
      "step": 21150
    },
    {
      "epoch": 3847.2727272727275,
      "grad_norm": 0.19994744658470154,
      "learning_rate": 9.948715064238955e-07,
      "loss": 0.0012,
      "step": 21160
    },
    {
      "epoch": 3849.090909090909,
      "grad_norm": 0.02114376425743103,
      "learning_rate": 9.948548705945222e-07,
      "loss": 0.0013,
      "step": 21170
    },
    {
      "epoch": 3850.909090909091,
      "grad_norm": 0.5014857053756714,
      "learning_rate": 9.948382079666755e-07,
      "loss": 0.0014,
      "step": 21180
    },
    {
      "epoch": 3852.7272727272725,
      "grad_norm": 0.1683262288570404,
      "learning_rate": 9.948215185412578e-07,
      "loss": 0.0012,
      "step": 21190
    },
    {
      "epoch": 3854.5454545454545,
      "grad_norm": 0.049814220517873764,
      "learning_rate": 9.948048023191726e-07,
      "loss": 0.0013,
      "step": 21200
    },
    {
      "epoch": 3856.3636363636365,
      "grad_norm": 0.6057694554328918,
      "learning_rate": 9.947880593013254e-07,
      "loss": 0.0014,
      "step": 21210
    },
    {
      "epoch": 3858.181818181818,
      "grad_norm": 0.4507443606853485,
      "learning_rate": 9.94771289488623e-07,
      "loss": 0.0012,
      "step": 21220
    },
    {
      "epoch": 3860.0,
      "grad_norm": 0.0220085009932518,
      "learning_rate": 9.947544928819733e-07,
      "loss": 0.0012,
      "step": 21230
    },
    {
      "epoch": 3861.818181818182,
      "grad_norm": 0.03625625744462013,
      "learning_rate": 9.94737669482286e-07,
      "loss": 0.0011,
      "step": 21240
    },
    {
      "epoch": 3863.6363636363635,
      "grad_norm": 0.01698887161910534,
      "learning_rate": 9.947208192904722e-07,
      "loss": 0.0018,
      "step": 21250
    },
    {
      "epoch": 3865.4545454545455,
      "grad_norm": 0.5634298324584961,
      "learning_rate": 9.947039423074445e-07,
      "loss": 0.0011,
      "step": 21260
    },
    {
      "epoch": 3867.2727272727275,
      "grad_norm": 0.692665159702301,
      "learning_rate": 9.946870385341167e-07,
      "loss": 0.0014,
      "step": 21270
    },
    {
      "epoch": 3869.090909090909,
      "grad_norm": 0.5465291738510132,
      "learning_rate": 9.946701079714041e-07,
      "loss": 0.0012,
      "step": 21280
    },
    {
      "epoch": 3870.909090909091,
      "grad_norm": 0.047195591032505035,
      "learning_rate": 9.946531506202239e-07,
      "loss": 0.0013,
      "step": 21290
    },
    {
      "epoch": 3872.7272727272725,
      "grad_norm": 0.02007438614964485,
      "learning_rate": 9.946361664814943e-07,
      "loss": 0.0015,
      "step": 21300
    },
    {
      "epoch": 3874.5454545454545,
      "grad_norm": 0.4776567220687866,
      "learning_rate": 9.946191555561348e-07,
      "loss": 0.0016,
      "step": 21310
    },
    {
      "epoch": 3876.3636363636365,
      "grad_norm": 0.4521059989929199,
      "learning_rate": 9.946021178450667e-07,
      "loss": 0.0011,
      "step": 21320
    },
    {
      "epoch": 3878.181818181818,
      "grad_norm": 0.026853062212467194,
      "learning_rate": 9.945850533492131e-07,
      "loss": 0.0013,
      "step": 21330
    },
    {
      "epoch": 3880.0,
      "grad_norm": 0.5606249570846558,
      "learning_rate": 9.945679620694976e-07,
      "loss": 0.0014,
      "step": 21340
    },
    {
      "epoch": 3881.818181818182,
      "grad_norm": 0.4476928114891052,
      "learning_rate": 9.945508440068459e-07,
      "loss": 0.0014,
      "step": 21350
    },
    {
      "epoch": 3883.6363636363635,
      "grad_norm": 0.011919321492314339,
      "learning_rate": 9.945336991621851e-07,
      "loss": 0.0012,
      "step": 21360
    },
    {
      "epoch": 3885.4545454545455,
      "grad_norm": 0.014033720828592777,
      "learning_rate": 9.945165275364438e-07,
      "loss": 0.0015,
      "step": 21370
    },
    {
      "epoch": 3887.2727272727275,
      "grad_norm": 0.010826475918293,
      "learning_rate": 9.944993291305517e-07,
      "loss": 0.0013,
      "step": 21380
    },
    {
      "epoch": 3889.090909090909,
      "grad_norm": 0.7324661612510681,
      "learning_rate": 9.944821039454401e-07,
      "loss": 0.0016,
      "step": 21390
    },
    {
      "epoch": 3890.909090909091,
      "grad_norm": 0.8898080587387085,
      "learning_rate": 9.94464851982042e-07,
      "loss": 0.0015,
      "step": 21400
    },
    {
      "epoch": 3892.7272727272725,
      "grad_norm": 0.845349133014679,
      "learning_rate": 9.944475732412916e-07,
      "loss": 0.0013,
      "step": 21410
    },
    {
      "epoch": 3894.5454545454545,
      "grad_norm": 0.013965025544166565,
      "learning_rate": 9.944302677241246e-07,
      "loss": 0.0013,
      "step": 21420
    },
    {
      "epoch": 3896.3636363636365,
      "grad_norm": 0.6162915229797363,
      "learning_rate": 9.944129354314781e-07,
      "loss": 0.0014,
      "step": 21430
    },
    {
      "epoch": 3898.181818181818,
      "grad_norm": 0.023052578791975975,
      "learning_rate": 9.943955763642909e-07,
      "loss": 0.0012,
      "step": 21440
    },
    {
      "epoch": 3900.0,
      "grad_norm": 0.7403808236122131,
      "learning_rate": 9.943781905235028e-07,
      "loss": 0.0015,
      "step": 21450
    },
    {
      "epoch": 3901.818181818182,
      "grad_norm": 0.019262302666902542,
      "learning_rate": 9.943607779100557e-07,
      "loss": 0.0013,
      "step": 21460
    },
    {
      "epoch": 3903.6363636363635,
      "grad_norm": 0.011084492318332195,
      "learning_rate": 9.943433385248924e-07,
      "loss": 0.0015,
      "step": 21470
    },
    {
      "epoch": 3905.4545454545455,
      "grad_norm": 0.018984466791152954,
      "learning_rate": 9.94325872368957e-07,
      "loss": 0.0012,
      "step": 21480
    },
    {
      "epoch": 3907.2727272727275,
      "grad_norm": 0.01124625839293003,
      "learning_rate": 9.943083794431958e-07,
      "loss": 0.0013,
      "step": 21490
    },
    {
      "epoch": 3909.090909090909,
      "grad_norm": 0.012441885657608509,
      "learning_rate": 9.942908597485558e-07,
      "loss": 0.0016,
      "step": 21500
    },
    {
      "epoch": 3909.090909090909,
      "eval_loss": 4.178633689880371,
      "eval_runtime": 0.9549,
      "eval_samples_per_second": 10.472,
      "eval_steps_per_second": 5.236,
      "step": 21500
    },
    {
      "epoch": 3910.909090909091,
      "grad_norm": 0.036413032561540604,
      "learning_rate": 9.942733132859859e-07,
      "loss": 0.0014,
      "step": 21510
    },
    {
      "epoch": 3912.7272727272725,
      "grad_norm": 0.7128260135650635,
      "learning_rate": 9.942557400564364e-07,
      "loss": 0.0012,
      "step": 21520
    },
    {
      "epoch": 3914.5454545454545,
      "grad_norm": 0.016060741618275642,
      "learning_rate": 9.942381400608591e-07,
      "loss": 0.0011,
      "step": 21530
    },
    {
      "epoch": 3916.3636363636365,
      "grad_norm": 0.0072855520993471146,
      "learning_rate": 9.942205133002066e-07,
      "loss": 0.0014,
      "step": 21540
    },
    {
      "epoch": 3918.181818181818,
      "grad_norm": 0.37936270236968994,
      "learning_rate": 9.94202859775434e-07,
      "loss": 0.0013,
      "step": 21550
    },
    {
      "epoch": 3920.0,
      "grad_norm": 0.45005613565444946,
      "learning_rate": 9.941851794874968e-07,
      "loss": 0.0013,
      "step": 21560
    },
    {
      "epoch": 3921.818181818182,
      "grad_norm": 0.5559307932853699,
      "learning_rate": 9.94167472437353e-07,
      "loss": 0.0014,
      "step": 21570
    },
    {
      "epoch": 3923.6363636363635,
      "grad_norm": 0.020174117758870125,
      "learning_rate": 9.94149738625961e-07,
      "loss": 0.0012,
      "step": 21580
    },
    {
      "epoch": 3925.4545454545455,
      "grad_norm": 0.01772186905145645,
      "learning_rate": 9.941319780542816e-07,
      "loss": 0.0013,
      "step": 21590
    },
    {
      "epoch": 3927.2727272727275,
      "grad_norm": 0.5891669392585754,
      "learning_rate": 9.941141907232763e-07,
      "loss": 0.0013,
      "step": 21600
    },
    {
      "epoch": 3929.090909090909,
      "grad_norm": 0.011486345902085304,
      "learning_rate": 9.940963766339086e-07,
      "loss": 0.0013,
      "step": 21610
    },
    {
      "epoch": 3930.909090909091,
      "grad_norm": 0.018402330577373505,
      "learning_rate": 9.940785357871432e-07,
      "loss": 0.0014,
      "step": 21620
    },
    {
      "epoch": 3932.7272727272725,
      "grad_norm": 0.015746882185339928,
      "learning_rate": 9.940606681839459e-07,
      "loss": 0.0014,
      "step": 21630
    },
    {
      "epoch": 3934.5454545454545,
      "grad_norm": 0.5725876092910767,
      "learning_rate": 9.940427738252845e-07,
      "loss": 0.0015,
      "step": 21640
    },
    {
      "epoch": 3936.3636363636365,
      "grad_norm": 0.497978538274765,
      "learning_rate": 9.940248527121282e-07,
      "loss": 0.0014,
      "step": 21650
    },
    {
      "epoch": 3938.181818181818,
      "grad_norm": 0.025003599002957344,
      "learning_rate": 9.940069048454475e-07,
      "loss": 0.0007,
      "step": 21660
    },
    {
      "epoch": 3940.0,
      "grad_norm": 0.05686551705002785,
      "learning_rate": 9.939889302262141e-07,
      "loss": 0.0014,
      "step": 21670
    },
    {
      "epoch": 3941.818181818182,
      "grad_norm": 0.47131383419036865,
      "learning_rate": 9.939709288554018e-07,
      "loss": 0.0014,
      "step": 21680
    },
    {
      "epoch": 3943.6363636363635,
      "grad_norm": 0.7113358974456787,
      "learning_rate": 9.939529007339851e-07,
      "loss": 0.0011,
      "step": 21690
    },
    {
      "epoch": 3945.4545454545455,
      "grad_norm": 0.03447338193655014,
      "learning_rate": 9.939348458629404e-07,
      "loss": 0.0011,
      "step": 21700
    },
    {
      "epoch": 3947.2727272727275,
      "grad_norm": 0.012954948469996452,
      "learning_rate": 9.939167642432454e-07,
      "loss": 0.0015,
      "step": 21710
    },
    {
      "epoch": 3949.090909090909,
      "grad_norm": 0.025960588827729225,
      "learning_rate": 9.938986558758793e-07,
      "loss": 0.0013,
      "step": 21720
    },
    {
      "epoch": 3950.909090909091,
      "grad_norm": 0.6086409091949463,
      "learning_rate": 9.938805207618231e-07,
      "loss": 0.0014,
      "step": 21730
    },
    {
      "epoch": 3952.7272727272725,
      "grad_norm": 0.4198457598686218,
      "learning_rate": 9.938623589020583e-07,
      "loss": 0.0012,
      "step": 21740
    },
    {
      "epoch": 3954.5454545454545,
      "grad_norm": 0.49885624647140503,
      "learning_rate": 9.938441702975689e-07,
      "loss": 0.0014,
      "step": 21750
    },
    {
      "epoch": 3956.3636363636365,
      "grad_norm": 0.46007516980171204,
      "learning_rate": 9.938259549493395e-07,
      "loss": 0.0015,
      "step": 21760
    },
    {
      "epoch": 3958.181818181818,
      "grad_norm": 0.010909448377788067,
      "learning_rate": 9.938077128583569e-07,
      "loss": 0.0012,
      "step": 21770
    },
    {
      "epoch": 3960.0,
      "grad_norm": 0.035345323383808136,
      "learning_rate": 9.93789444025609e-07,
      "loss": 0.0014,
      "step": 21780
    },
    {
      "epoch": 3961.818181818182,
      "grad_norm": 0.48920491337776184,
      "learning_rate": 9.937711484520848e-07,
      "loss": 0.0014,
      "step": 21790
    },
    {
      "epoch": 3963.6363636363635,
      "grad_norm": 0.6422502398490906,
      "learning_rate": 9.937528261387752e-07,
      "loss": 0.0013,
      "step": 21800
    },
    {
      "epoch": 3965.4545454545455,
      "grad_norm": 0.5779159665107727,
      "learning_rate": 9.937344770866728e-07,
      "loss": 0.0012,
      "step": 21810
    },
    {
      "epoch": 3967.2727272727275,
      "grad_norm": 0.6881405711174011,
      "learning_rate": 9.937161012967705e-07,
      "loss": 0.0013,
      "step": 21820
    },
    {
      "epoch": 3969.090909090909,
      "grad_norm": 0.4405340552330017,
      "learning_rate": 9.936976987700644e-07,
      "loss": 0.0013,
      "step": 21830
    },
    {
      "epoch": 3970.909090909091,
      "grad_norm": 0.01298233587294817,
      "learning_rate": 9.936792695075502e-07,
      "loss": 0.0013,
      "step": 21840
    },
    {
      "epoch": 3972.7272727272725,
      "grad_norm": 0.01616799458861351,
      "learning_rate": 9.936608135102263e-07,
      "loss": 0.0014,
      "step": 21850
    },
    {
      "epoch": 3974.5454545454545,
      "grad_norm": 0.018040262162685394,
      "learning_rate": 9.936423307790924e-07,
      "loss": 0.0011,
      "step": 21860
    },
    {
      "epoch": 3976.3636363636365,
      "grad_norm": 0.0615210197865963,
      "learning_rate": 9.93623821315149e-07,
      "loss": 0.0013,
      "step": 21870
    },
    {
      "epoch": 3978.181818181818,
      "grad_norm": 0.7009211182594299,
      "learning_rate": 9.936052851193987e-07,
      "loss": 0.0019,
      "step": 21880
    },
    {
      "epoch": 3980.0,
      "grad_norm": 0.7358247637748718,
      "learning_rate": 9.935867221928453e-07,
      "loss": 0.0012,
      "step": 21890
    },
    {
      "epoch": 3981.818181818182,
      "grad_norm": 0.5959166884422302,
      "learning_rate": 9.93568132536494e-07,
      "loss": 0.0014,
      "step": 21900
    },
    {
      "epoch": 3983.6363636363635,
      "grad_norm": 0.018994320183992386,
      "learning_rate": 9.935495161513514e-07,
      "loss": 0.0013,
      "step": 21910
    },
    {
      "epoch": 3985.4545454545455,
      "grad_norm": 0.5279090404510498,
      "learning_rate": 9.93530873038426e-07,
      "loss": 0.0011,
      "step": 21920
    },
    {
      "epoch": 3987.2727272727275,
      "grad_norm": 0.017907394096255302,
      "learning_rate": 9.93512203198727e-07,
      "loss": 0.0012,
      "step": 21930
    },
    {
      "epoch": 3989.090909090909,
      "grad_norm": 0.043617114424705505,
      "learning_rate": 9.934935066332656e-07,
      "loss": 0.0014,
      "step": 21940
    },
    {
      "epoch": 3990.909090909091,
      "grad_norm": 0.5683532357215881,
      "learning_rate": 9.934747833430545e-07,
      "loss": 0.0014,
      "step": 21950
    },
    {
      "epoch": 3992.7272727272725,
      "grad_norm": 0.4597336947917938,
      "learning_rate": 9.934560333291076e-07,
      "loss": 0.0014,
      "step": 21960
    },
    {
      "epoch": 3994.5454545454545,
      "grad_norm": 0.476396769285202,
      "learning_rate": 9.934372565924398e-07,
      "loss": 0.0012,
      "step": 21970
    },
    {
      "epoch": 3996.3636363636365,
      "grad_norm": 0.022080177441239357,
      "learning_rate": 9.934184531340686e-07,
      "loss": 0.0013,
      "step": 21980
    },
    {
      "epoch": 3998.181818181818,
      "grad_norm": 0.6353350877761841,
      "learning_rate": 9.933996229550119e-07,
      "loss": 0.0014,
      "step": 21990
    },
    {
      "epoch": 4000.0,
      "grad_norm": 0.53999263048172,
      "learning_rate": 9.933807660562896e-07,
      "loss": 0.0013,
      "step": 22000
    },
    {
      "epoch": 4000.0,
      "eval_loss": 4.224537372589111,
      "eval_runtime": 0.9528,
      "eval_samples_per_second": 10.495,
      "eval_steps_per_second": 5.247,
      "step": 22000
    },
    {
      "epoch": 4001.818181818182,
      "grad_norm": 0.012352549470961094,
      "learning_rate": 9.933618824389226e-07,
      "loss": 0.0012,
      "step": 22010
    },
    {
      "epoch": 4003.6363636363635,
      "grad_norm": 0.02145465649664402,
      "learning_rate": 9.93342972103934e-07,
      "loss": 0.0013,
      "step": 22020
    },
    {
      "epoch": 4005.4545454545455,
      "grad_norm": 0.45165106654167175,
      "learning_rate": 9.933240350523476e-07,
      "loss": 0.0014,
      "step": 22030
    },
    {
      "epoch": 4007.2727272727275,
      "grad_norm": 0.017651794478297234,
      "learning_rate": 9.933050712851887e-07,
      "loss": 0.0013,
      "step": 22040
    },
    {
      "epoch": 4009.090909090909,
      "grad_norm": 0.5991668701171875,
      "learning_rate": 9.932860808034846e-07,
      "loss": 0.0015,
      "step": 22050
    },
    {
      "epoch": 4010.909090909091,
      "grad_norm": 0.015836263075470924,
      "learning_rate": 9.932670636082638e-07,
      "loss": 0.0011,
      "step": 22060
    },
    {
      "epoch": 4012.7272727272725,
      "grad_norm": 0.7880997657775879,
      "learning_rate": 9.932480197005556e-07,
      "loss": 0.0014,
      "step": 22070
    },
    {
      "epoch": 4014.5454545454545,
      "grad_norm": 0.029077041894197464,
      "learning_rate": 9.932289490813922e-07,
      "loss": 0.001,
      "step": 22080
    },
    {
      "epoch": 4016.3636363636365,
      "grad_norm": 0.8504512906074524,
      "learning_rate": 9.932098517518054e-07,
      "loss": 0.0017,
      "step": 22090
    },
    {
      "epoch": 4018.181818181818,
      "grad_norm": 0.05153545364737511,
      "learning_rate": 9.9319072771283e-07,
      "loss": 0.0011,
      "step": 22100
    },
    {
      "epoch": 4020.0,
      "grad_norm": 0.015202054753899574,
      "learning_rate": 9.931715769655014e-07,
      "loss": 0.0014,
      "step": 22110
    },
    {
      "epoch": 4021.818181818182,
      "grad_norm": 0.6228069067001343,
      "learning_rate": 9.931523995108569e-07,
      "loss": 0.0013,
      "step": 22120
    },
    {
      "epoch": 4023.6363636363635,
      "grad_norm": 0.0229666605591774,
      "learning_rate": 9.931331953499348e-07,
      "loss": 0.0013,
      "step": 22130
    },
    {
      "epoch": 4025.4545454545455,
      "grad_norm": 0.023364022374153137,
      "learning_rate": 9.931139644837754e-07,
      "loss": 0.0012,
      "step": 22140
    },
    {
      "epoch": 4027.2727272727275,
      "grad_norm": 0.02915465645492077,
      "learning_rate": 9.9309470691342e-07,
      "loss": 0.0014,
      "step": 22150
    },
    {
      "epoch": 4029.090909090909,
      "grad_norm": 0.6961771249771118,
      "learning_rate": 9.93075422639911e-07,
      "loss": 0.0013,
      "step": 22160
    },
    {
      "epoch": 4030.909090909091,
      "grad_norm": 0.6773921251296997,
      "learning_rate": 9.930561116642934e-07,
      "loss": 0.0015,
      "step": 22170
    },
    {
      "epoch": 4032.7272727272725,
      "grad_norm": 0.059593673795461655,
      "learning_rate": 9.930367739876128e-07,
      "loss": 0.0014,
      "step": 22180
    },
    {
      "epoch": 4034.5454545454545,
      "grad_norm": 0.014130538329482079,
      "learning_rate": 9.930174096109162e-07,
      "loss": 0.0009,
      "step": 22190
    },
    {
      "epoch": 4036.3636363636365,
      "grad_norm": 0.7550578713417053,
      "learning_rate": 9.929980185352525e-07,
      "loss": 0.002,
      "step": 22200
    },
    {
      "epoch": 4038.181818181818,
      "grad_norm": 0.5768067836761475,
      "learning_rate": 9.929786007616717e-07,
      "loss": 0.0013,
      "step": 22210
    },
    {
      "epoch": 4040.0,
      "grad_norm": 0.03457946330308914,
      "learning_rate": 9.92959156291225e-07,
      "loss": 0.0013,
      "step": 22220
    },
    {
      "epoch": 4041.818181818182,
      "grad_norm": 0.49313777685165405,
      "learning_rate": 9.92939685124966e-07,
      "loss": 0.0012,
      "step": 22230
    },
    {
      "epoch": 4043.6363636363635,
      "grad_norm": 0.8608730435371399,
      "learning_rate": 9.92920187263949e-07,
      "loss": 0.0016,
      "step": 22240
    },
    {
      "epoch": 4045.4545454545455,
      "grad_norm": 0.019693341106176376,
      "learning_rate": 9.929006627092297e-07,
      "loss": 0.0009,
      "step": 22250
    },
    {
      "epoch": 4047.2727272727275,
      "grad_norm": 0.42182567715644836,
      "learning_rate": 9.928811114618655e-07,
      "loss": 0.0014,
      "step": 22260
    },
    {
      "epoch": 4049.090909090909,
      "grad_norm": 0.6669459939002991,
      "learning_rate": 9.928615335229153e-07,
      "loss": 0.0015,
      "step": 22270
    },
    {
      "epoch": 4050.909090909091,
      "grad_norm": 0.025816289708018303,
      "learning_rate": 9.928419288934392e-07,
      "loss": 0.0009,
      "step": 22280
    },
    {
      "epoch": 4052.7272727272725,
      "grad_norm": 0.42589452862739563,
      "learning_rate": 9.92822297574499e-07,
      "loss": 0.0016,
      "step": 22290
    },
    {
      "epoch": 4054.5454545454545,
      "grad_norm": 0.008211233653128147,
      "learning_rate": 9.928026395671576e-07,
      "loss": 0.001,
      "step": 22300
    },
    {
      "epoch": 4056.3636363636365,
      "grad_norm": 0.027587901800870895,
      "learning_rate": 9.927829548724798e-07,
      "loss": 0.0013,
      "step": 22310
    },
    {
      "epoch": 4058.181818181818,
      "grad_norm": 0.3864474892616272,
      "learning_rate": 9.927632434915313e-07,
      "loss": 0.0015,
      "step": 22320
    },
    {
      "epoch": 4060.0,
      "grad_norm": 0.40702056884765625,
      "learning_rate": 9.9274350542538e-07,
      "loss": 0.0014,
      "step": 22330
    },
    {
      "epoch": 4061.818181818182,
      "grad_norm": 0.038371842354536057,
      "learning_rate": 9.927237406750943e-07,
      "loss": 0.0012,
      "step": 22340
    },
    {
      "epoch": 4063.6363636363635,
      "grad_norm": 0.4311145544052124,
      "learning_rate": 9.92703949241745e-07,
      "loss": 0.0012,
      "step": 22350
    },
    {
      "epoch": 4065.4545454545455,
      "grad_norm": 0.03972861170768738,
      "learning_rate": 9.926841311264035e-07,
      "loss": 0.0013,
      "step": 22360
    },
    {
      "epoch": 4067.2727272727275,
      "grad_norm": 0.5186442136764526,
      "learning_rate": 9.926642863301435e-07,
      "loss": 0.0015,
      "step": 22370
    },
    {
      "epoch": 4069.090909090909,
      "grad_norm": 0.014546609483659267,
      "learning_rate": 9.926444148540392e-07,
      "loss": 0.0011,
      "step": 22380
    },
    {
      "epoch": 4070.909090909091,
      "grad_norm": 0.03159576281905174,
      "learning_rate": 9.92624516699167e-07,
      "loss": 0.0014,
      "step": 22390
    },
    {
      "epoch": 4072.7272727272725,
      "grad_norm": 0.05884723365306854,
      "learning_rate": 9.926045918666043e-07,
      "loss": 0.0014,
      "step": 22400
    },
    {
      "epoch": 4074.5454545454545,
      "grad_norm": 0.044115886092185974,
      "learning_rate": 9.925846403574303e-07,
      "loss": 0.001,
      "step": 22410
    },
    {
      "epoch": 4076.3636363636365,
      "grad_norm": 0.012266484089195728,
      "learning_rate": 9.925646621727254e-07,
      "loss": 0.0011,
      "step": 22420
    },
    {
      "epoch": 4078.181818181818,
      "grad_norm": 0.7331945300102234,
      "learning_rate": 9.925446573135713e-07,
      "loss": 0.0016,
      "step": 22430
    },
    {
      "epoch": 4080.0,
      "grad_norm": 0.4751277565956116,
      "learning_rate": 9.925246257810517e-07,
      "loss": 0.0011,
      "step": 22440
    },
    {
      "epoch": 4081.818181818182,
      "grad_norm": 0.02784636802971363,
      "learning_rate": 9.925045675762513e-07,
      "loss": 0.0014,
      "step": 22450
    },
    {
      "epoch": 4083.6363636363635,
      "grad_norm": 0.5786316990852356,
      "learning_rate": 9.924844827002562e-07,
      "loss": 0.0012,
      "step": 22460
    },
    {
      "epoch": 4085.4545454545455,
      "grad_norm": 0.01616414450109005,
      "learning_rate": 9.92464371154154e-07,
      "loss": 0.0013,
      "step": 22470
    },
    {
      "epoch": 4087.2727272727275,
      "grad_norm": 0.015451430343091488,
      "learning_rate": 9.924442329390338e-07,
      "loss": 0.0013,
      "step": 22480
    },
    {
      "epoch": 4089.090909090909,
      "grad_norm": 0.6075975298881531,
      "learning_rate": 9.924240680559866e-07,
      "loss": 0.0016,
      "step": 22490
    },
    {
      "epoch": 4090.909090909091,
      "grad_norm": 0.018848154693841934,
      "learning_rate": 9.92403876506104e-07,
      "loss": 0.0012,
      "step": 22500
    },
    {
      "epoch": 4090.909090909091,
      "eval_loss": 4.1205267906188965,
      "eval_runtime": 0.951,
      "eval_samples_per_second": 10.515,
      "eval_steps_per_second": 5.258,
      "step": 22500
    },
    {
      "epoch": 4092.7272727272725,
      "grad_norm": 0.013637203723192215,
      "learning_rate": 9.923836582904796e-07,
      "loss": 0.0015,
      "step": 22510
    },
    {
      "epoch": 4094.5454545454545,
      "grad_norm": 0.5946133136749268,
      "learning_rate": 9.923634134102083e-07,
      "loss": 0.0012,
      "step": 22520
    },
    {
      "epoch": 4096.363636363636,
      "grad_norm": 0.3984019458293915,
      "learning_rate": 9.923431418663865e-07,
      "loss": 0.0011,
      "step": 22530
    },
    {
      "epoch": 4098.181818181818,
      "grad_norm": 0.5744966864585876,
      "learning_rate": 9.923228436601118e-07,
      "loss": 0.0015,
      "step": 22540
    },
    {
      "epoch": 4100.0,
      "grad_norm": 0.7559581995010376,
      "learning_rate": 9.923025187924836e-07,
      "loss": 0.0013,
      "step": 22550
    },
    {
      "epoch": 4101.818181818182,
      "grad_norm": 0.019104180857539177,
      "learning_rate": 9.922821672646027e-07,
      "loss": 0.0014,
      "step": 22560
    },
    {
      "epoch": 4103.636363636364,
      "grad_norm": 0.4050484895706177,
      "learning_rate": 9.922617890775709e-07,
      "loss": 0.0012,
      "step": 22570
    },
    {
      "epoch": 4105.454545454545,
      "grad_norm": 0.015914766117930412,
      "learning_rate": 9.92241384232492e-07,
      "loss": 0.0012,
      "step": 22580
    },
    {
      "epoch": 4107.272727272727,
      "grad_norm": 0.3685648739337921,
      "learning_rate": 9.922209527304708e-07,
      "loss": 0.0015,
      "step": 22590
    },
    {
      "epoch": 4109.090909090909,
      "grad_norm": 0.3986196517944336,
      "learning_rate": 9.922004945726139e-07,
      "loss": 0.001,
      "step": 22600
    },
    {
      "epoch": 4110.909090909091,
      "grad_norm": 0.01433828379958868,
      "learning_rate": 9.921800097600293e-07,
      "loss": 0.0014,
      "step": 22610
    },
    {
      "epoch": 4112.727272727273,
      "grad_norm": 0.5690823793411255,
      "learning_rate": 9.921594982938262e-07,
      "loss": 0.0015,
      "step": 22620
    },
    {
      "epoch": 4114.545454545455,
      "grad_norm": 0.6653439402580261,
      "learning_rate": 9.921389601751152e-07,
      "loss": 0.0011,
      "step": 22630
    },
    {
      "epoch": 4116.363636363636,
      "grad_norm": 0.010022670030593872,
      "learning_rate": 9.92118395405009e-07,
      "loss": 0.0011,
      "step": 22640
    },
    {
      "epoch": 4118.181818181818,
      "grad_norm": 0.010759842582046986,
      "learning_rate": 9.92097803984621e-07,
      "loss": 0.0013,
      "step": 22650
    },
    {
      "epoch": 4120.0,
      "grad_norm": 0.009671639651060104,
      "learning_rate": 9.920771859150661e-07,
      "loss": 0.0013,
      "step": 22660
    },
    {
      "epoch": 4121.818181818182,
      "grad_norm": 0.4142839014530182,
      "learning_rate": 9.920565411974612e-07,
      "loss": 0.0013,
      "step": 22670
    },
    {
      "epoch": 4123.636363636364,
      "grad_norm": 0.5644431114196777,
      "learning_rate": 9.920358698329241e-07,
      "loss": 0.0011,
      "step": 22680
    },
    {
      "epoch": 4125.454545454545,
      "grad_norm": 0.00788380578160286,
      "learning_rate": 9.920151718225743e-07,
      "loss": 0.0013,
      "step": 22690
    },
    {
      "epoch": 4127.272727272727,
      "grad_norm": 0.6116349101066589,
      "learning_rate": 9.919944471675326e-07,
      "loss": 0.0013,
      "step": 22700
    },
    {
      "epoch": 4129.090909090909,
      "grad_norm": 0.017448287457227707,
      "learning_rate": 9.919736958689215e-07,
      "loss": 0.0011,
      "step": 22710
    },
    {
      "epoch": 4130.909090909091,
      "grad_norm": 0.4682861864566803,
      "learning_rate": 9.919529179278646e-07,
      "loss": 0.0013,
      "step": 22720
    },
    {
      "epoch": 4132.727272727273,
      "grad_norm": 0.013008750975131989,
      "learning_rate": 9.919321133454875e-07,
      "loss": 0.0011,
      "step": 22730
    },
    {
      "epoch": 4134.545454545455,
      "grad_norm": 0.2118387371301651,
      "learning_rate": 9.919112821229164e-07,
      "loss": 0.0017,
      "step": 22740
    },
    {
      "epoch": 4136.363636363636,
      "grad_norm": 0.011143281124532223,
      "learning_rate": 9.918904242612794e-07,
      "loss": 0.0009,
      "step": 22750
    },
    {
      "epoch": 4138.181818181818,
      "grad_norm": 0.4169922471046448,
      "learning_rate": 9.918695397617064e-07,
      "loss": 0.0013,
      "step": 22760
    },
    {
      "epoch": 4140.0,
      "grad_norm": 0.6007094979286194,
      "learning_rate": 9.918486286253278e-07,
      "loss": 0.0012,
      "step": 22770
    },
    {
      "epoch": 4141.818181818182,
      "grad_norm": 0.5118424296379089,
      "learning_rate": 9.918276908532767e-07,
      "loss": 0.0013,
      "step": 22780
    },
    {
      "epoch": 4143.636363636364,
      "grad_norm": 0.0162825807929039,
      "learning_rate": 9.918067264466867e-07,
      "loss": 0.0012,
      "step": 22790
    },
    {
      "epoch": 4145.454545454545,
      "grad_norm": 0.012757759541273117,
      "learning_rate": 9.91785735406693e-07,
      "loss": 0.0011,
      "step": 22800
    },
    {
      "epoch": 4147.272727272727,
      "grad_norm": 0.4868265688419342,
      "learning_rate": 9.917647177344324e-07,
      "loss": 0.0019,
      "step": 22810
    },
    {
      "epoch": 4149.090909090909,
      "grad_norm": 0.5027093887329102,
      "learning_rate": 9.91743673431043e-07,
      "loss": 0.0013,
      "step": 22820
    },
    {
      "epoch": 4150.909090909091,
      "grad_norm": 0.4954128861427307,
      "learning_rate": 9.91722602497665e-07,
      "loss": 0.0011,
      "step": 22830
    },
    {
      "epoch": 4152.727272727273,
      "grad_norm": 0.5370261073112488,
      "learning_rate": 9.917015049354385e-07,
      "loss": 0.0016,
      "step": 22840
    },
    {
      "epoch": 4154.545454545455,
      "grad_norm": 0.48924288153648376,
      "learning_rate": 9.916803807455069e-07,
      "loss": 0.001,
      "step": 22850
    },
    {
      "epoch": 4156.363636363636,
      "grad_norm": 0.010862933471798897,
      "learning_rate": 9.916592299290136e-07,
      "loss": 0.0011,
      "step": 22860
    },
    {
      "epoch": 4158.181818181818,
      "grad_norm": 0.02496018260717392,
      "learning_rate": 9.916380524871044e-07,
      "loss": 0.0013,
      "step": 22870
    },
    {
      "epoch": 4160.0,
      "grad_norm": 0.7101922631263733,
      "learning_rate": 9.916168484209261e-07,
      "loss": 0.0013,
      "step": 22880
    },
    {
      "epoch": 4161.818181818182,
      "grad_norm": 0.01668481156229973,
      "learning_rate": 9.915956177316267e-07,
      "loss": 0.0014,
      "step": 22890
    },
    {
      "epoch": 4163.636363636364,
      "grad_norm": 0.022407663986086845,
      "learning_rate": 9.915743604203563e-07,
      "loss": 0.0011,
      "step": 22900
    },
    {
      "epoch": 4165.454545454545,
      "grad_norm": 0.013114161789417267,
      "learning_rate": 9.915530764882656e-07,
      "loss": 0.0011,
      "step": 22910
    },
    {
      "epoch": 4167.272727272727,
      "grad_norm": 0.03347112610936165,
      "learning_rate": 9.915317659365077e-07,
      "loss": 0.0014,
      "step": 22920
    },
    {
      "epoch": 4169.090909090909,
      "grad_norm": 0.009145001880824566,
      "learning_rate": 9.915104287662365e-07,
      "loss": 0.0014,
      "step": 22930
    },
    {
      "epoch": 4170.909090909091,
      "grad_norm": 0.44904962182044983,
      "learning_rate": 9.914890649786071e-07,
      "loss": 0.0023,
      "step": 22940
    },
    {
      "epoch": 4172.727272727273,
      "grad_norm": 0.026061424985527992,
      "learning_rate": 9.91467674574777e-07,
      "loss": 0.0012,
      "step": 22950
    },
    {
      "epoch": 4174.545454545455,
      "grad_norm": 0.7433971166610718,
      "learning_rate": 9.914462575559045e-07,
      "loss": 0.0014,
      "step": 22960
    },
    {
      "epoch": 4176.363636363636,
      "grad_norm": 0.059769678860902786,
      "learning_rate": 9.91424813923149e-07,
      "loss": 0.001,
      "step": 22970
    },
    {
      "epoch": 4178.181818181818,
      "grad_norm": 0.44324904680252075,
      "learning_rate": 9.914033436776724e-07,
      "loss": 0.0014,
      "step": 22980
    },
    {
      "epoch": 4180.0,
      "grad_norm": 0.04413505643606186,
      "learning_rate": 9.913818468206368e-07,
      "loss": 0.0012,
      "step": 22990
    },
    {
      "epoch": 4181.818181818182,
      "grad_norm": 0.5129106044769287,
      "learning_rate": 9.913603233532067e-07,
      "loss": 0.0013,
      "step": 23000
    },
    {
      "epoch": 4181.818181818182,
      "eval_loss": 4.262804985046387,
      "eval_runtime": 0.9554,
      "eval_samples_per_second": 10.467,
      "eval_steps_per_second": 5.234,
      "step": 23000
    },
    {
      "epoch": 4183.636363636364,
      "grad_norm": 0.5516257286071777,
      "learning_rate": 9.913387732765474e-07,
      "loss": 0.0011,
      "step": 23010
    },
    {
      "epoch": 4185.454545454545,
      "grad_norm": 0.009428020566701889,
      "learning_rate": 9.913171965918264e-07,
      "loss": 0.0013,
      "step": 23020
    },
    {
      "epoch": 4187.272727272727,
      "grad_norm": 0.023650215938687325,
      "learning_rate": 9.912955933002117e-07,
      "loss": 0.0011,
      "step": 23030
    },
    {
      "epoch": 4189.090909090909,
      "grad_norm": 0.026065176352858543,
      "learning_rate": 9.912739634028733e-07,
      "loss": 0.0013,
      "step": 23040
    },
    {
      "epoch": 4190.909090909091,
      "grad_norm": 0.057460419833660126,
      "learning_rate": 9.912523069009827e-07,
      "loss": 0.0014,
      "step": 23050
    },
    {
      "epoch": 4192.727272727273,
      "grad_norm": 0.4550483822822571,
      "learning_rate": 9.912306237957129e-07,
      "loss": 0.001,
      "step": 23060
    },
    {
      "epoch": 4194.545454545455,
      "grad_norm": 0.014754267409443855,
      "learning_rate": 9.912089140882376e-07,
      "loss": 0.0013,
      "step": 23070
    },
    {
      "epoch": 4196.363636363636,
      "grad_norm": 0.013118025846779346,
      "learning_rate": 9.911871777797327e-07,
      "loss": 0.0013,
      "step": 23080
    },
    {
      "epoch": 4198.181818181818,
      "grad_norm": 0.021536480635404587,
      "learning_rate": 9.911654148713754e-07,
      "loss": 0.0021,
      "step": 23090
    },
    {
      "epoch": 4200.0,
      "grad_norm": 0.18003934621810913,
      "learning_rate": 9.911436253643443e-07,
      "loss": 0.0014,
      "step": 23100
    },
    {
      "epoch": 4201.818181818182,
      "grad_norm": 0.018684417009353638,
      "learning_rate": 9.911218092598193e-07,
      "loss": 0.0013,
      "step": 23110
    },
    {
      "epoch": 4203.636363636364,
      "grad_norm": 0.014035957865417004,
      "learning_rate": 9.910999665589816e-07,
      "loss": 0.0011,
      "step": 23120
    },
    {
      "epoch": 4205.454545454545,
      "grad_norm": 0.7066789865493774,
      "learning_rate": 9.910780972630145e-07,
      "loss": 0.0015,
      "step": 23130
    },
    {
      "epoch": 4207.272727272727,
      "grad_norm": 0.4845482409000397,
      "learning_rate": 9.91056201373102e-07,
      "loss": 0.0011,
      "step": 23140
    },
    {
      "epoch": 4209.090909090909,
      "grad_norm": 0.06848593801259995,
      "learning_rate": 9.910342788904302e-07,
      "loss": 0.0011,
      "step": 23150
    },
    {
      "epoch": 4210.909090909091,
      "grad_norm": 0.08542035520076752,
      "learning_rate": 9.91012329816186e-07,
      "loss": 0.0013,
      "step": 23160
    },
    {
      "epoch": 4212.727272727273,
      "grad_norm": 0.6476743817329407,
      "learning_rate": 9.909903541515578e-07,
      "loss": 0.0012,
      "step": 23170
    },
    {
      "epoch": 4214.545454545455,
      "grad_norm": 0.4485290050506592,
      "learning_rate": 9.909683518977361e-07,
      "loss": 0.001,
      "step": 23180
    },
    {
      "epoch": 4216.363636363636,
      "grad_norm": 0.011780545115470886,
      "learning_rate": 9.909463230559126e-07,
      "loss": 0.0013,
      "step": 23190
    },
    {
      "epoch": 4218.181818181818,
      "grad_norm": 0.012187883257865906,
      "learning_rate": 9.909242676272795e-07,
      "loss": 0.0012,
      "step": 23200
    },
    {
      "epoch": 4220.0,
      "grad_norm": 0.4703103303909302,
      "learning_rate": 9.90902185613032e-07,
      "loss": 0.0014,
      "step": 23210
    },
    {
      "epoch": 4221.818181818182,
      "grad_norm": 0.011264663189649582,
      "learning_rate": 9.908800770143652e-07,
      "loss": 0.0014,
      "step": 23220
    },
    {
      "epoch": 4223.636363636364,
      "grad_norm": 0.5025312900543213,
      "learning_rate": 9.90857941832477e-07,
      "loss": 0.001,
      "step": 23230
    },
    {
      "epoch": 4225.454545454545,
      "grad_norm": 0.49446508288383484,
      "learning_rate": 9.90835780068566e-07,
      "loss": 0.0015,
      "step": 23240
    },
    {
      "epoch": 4227.272727272727,
      "grad_norm": 0.05946885049343109,
      "learning_rate": 9.90813591723832e-07,
      "loss": 0.0012,
      "step": 23250
    },
    {
      "epoch": 4229.090909090909,
      "grad_norm": 0.012614564970135689,
      "learning_rate": 9.907913767994769e-07,
      "loss": 0.0012,
      "step": 23260
    },
    {
      "epoch": 4230.909090909091,
      "grad_norm": 0.5268365144729614,
      "learning_rate": 9.907691352967034e-07,
      "loss": 0.0013,
      "step": 23270
    },
    {
      "epoch": 4232.727272727273,
      "grad_norm": 0.642951488494873,
      "learning_rate": 9.907468672167163e-07,
      "loss": 0.0013,
      "step": 23280
    },
    {
      "epoch": 4234.545454545455,
      "grad_norm": 0.010850219056010246,
      "learning_rate": 9.907245725607217e-07,
      "loss": 0.0015,
      "step": 23290
    },
    {
      "epoch": 4236.363636363636,
      "grad_norm": 0.010561744682490826,
      "learning_rate": 9.907022513299264e-07,
      "loss": 0.001,
      "step": 23300
    },
    {
      "epoch": 4238.181818181818,
      "grad_norm": 0.02495330199599266,
      "learning_rate": 9.906799035255394e-07,
      "loss": 0.0015,
      "step": 23310
    },
    {
      "epoch": 4240.0,
      "grad_norm": 0.05749022588133812,
      "learning_rate": 9.90657529148771e-07,
      "loss": 0.001,
      "step": 23320
    },
    {
      "epoch": 4241.818181818182,
      "grad_norm": 0.4989840090274811,
      "learning_rate": 9.90635128200833e-07,
      "loss": 0.0013,
      "step": 23330
    },
    {
      "epoch": 4243.636363636364,
      "grad_norm": 0.4205361306667328,
      "learning_rate": 9.906127006829383e-07,
      "loss": 0.0012,
      "step": 23340
    },
    {
      "epoch": 4245.454545454545,
      "grad_norm": 0.014225094579160213,
      "learning_rate": 9.905902465963015e-07,
      "loss": 0.0009,
      "step": 23350
    },
    {
      "epoch": 4247.272727272727,
      "grad_norm": 0.007136186119168997,
      "learning_rate": 9.905677659421386e-07,
      "loss": 0.0014,
      "step": 23360
    },
    {
      "epoch": 4249.090909090909,
      "grad_norm": 0.013111562468111515,
      "learning_rate": 9.90545258721667e-07,
      "loss": 0.0014,
      "step": 23370
    },
    {
      "epoch": 4250.909090909091,
      "grad_norm": 0.41359856724739075,
      "learning_rate": 9.905227249361056e-07,
      "loss": 0.0013,
      "step": 23380
    },
    {
      "epoch": 4252.727272727273,
      "grad_norm": 0.5380956530570984,
      "learning_rate": 9.905001645866745e-07,
      "loss": 0.0012,
      "step": 23390
    },
    {
      "epoch": 4254.545454545455,
      "grad_norm": 0.013262752443552017,
      "learning_rate": 9.904775776745956e-07,
      "loss": 0.001,
      "step": 23400
    },
    {
      "epoch": 4256.363636363636,
      "grad_norm": 0.009097928181290627,
      "learning_rate": 9.904549642010924e-07,
      "loss": 0.0014,
      "step": 23410
    },
    {
      "epoch": 4258.181818181818,
      "grad_norm": 0.6352813243865967,
      "learning_rate": 9.904323241673888e-07,
      "loss": 0.0017,
      "step": 23420
    },
    {
      "epoch": 4260.0,
      "grad_norm": 0.052181266248226166,
      "learning_rate": 9.904096575747115e-07,
      "loss": 0.0011,
      "step": 23430
    },
    {
      "epoch": 4261.818181818182,
      "grad_norm": 0.605033814907074,
      "learning_rate": 9.903869644242877e-07,
      "loss": 0.0014,
      "step": 23440
    },
    {
      "epoch": 4263.636363636364,
      "grad_norm": 0.7302868366241455,
      "learning_rate": 9.903642447173465e-07,
      "loss": 0.0015,
      "step": 23450
    },
    {
      "epoch": 4265.454545454545,
      "grad_norm": 0.013796421699225903,
      "learning_rate": 9.903414984551176e-07,
      "loss": 0.0007,
      "step": 23460
    },
    {
      "epoch": 4267.272727272727,
      "grad_norm": 0.3935078978538513,
      "learning_rate": 9.903187256388339e-07,
      "loss": 0.0015,
      "step": 23470
    },
    {
      "epoch": 4269.090909090909,
      "grad_norm": 0.3975484073162079,
      "learning_rate": 9.902959262697278e-07,
      "loss": 0.0014,
      "step": 23480
    },
    {
      "epoch": 4270.909090909091,
      "grad_norm": 0.011338034644722939,
      "learning_rate": 9.902731003490342e-07,
      "loss": 0.0012,
      "step": 23490
    },
    {
      "epoch": 4272.727272727273,
      "grad_norm": 0.442024290561676,
      "learning_rate": 9.902502478779896e-07,
      "loss": 0.001,
      "step": 23500
    },
    {
      "epoch": 4272.727272727273,
      "eval_loss": 4.306507110595703,
      "eval_runtime": 0.9514,
      "eval_samples_per_second": 10.511,
      "eval_steps_per_second": 5.256,
      "step": 23500
    },
    {
      "epoch": 4274.545454545455,
      "grad_norm": 0.02782500721514225,
      "learning_rate": 9.902273688578312e-07,
      "loss": 0.0012,
      "step": 23510
    },
    {
      "epoch": 4276.363636363636,
      "grad_norm": 0.019566642120480537,
      "learning_rate": 9.902044632897978e-07,
      "loss": 0.0014,
      "step": 23520
    },
    {
      "epoch": 4278.181818181818,
      "grad_norm": 0.05388651788234711,
      "learning_rate": 9.9018153117513e-07,
      "loss": 0.0015,
      "step": 23530
    },
    {
      "epoch": 4280.0,
      "grad_norm": 0.023933451622724533,
      "learning_rate": 9.9015857251507e-07,
      "loss": 0.001,
      "step": 23540
    },
    {
      "epoch": 4281.818181818182,
      "grad_norm": 0.009397970512509346,
      "learning_rate": 9.90135587310861e-07,
      "loss": 0.0013,
      "step": 23550
    },
    {
      "epoch": 4283.636363636364,
      "grad_norm": 0.6768114566802979,
      "learning_rate": 9.901125755637473e-07,
      "loss": 0.0013,
      "step": 23560
    },
    {
      "epoch": 4285.454545454545,
      "grad_norm": 0.4707019031047821,
      "learning_rate": 9.900895372749754e-07,
      "loss": 0.0014,
      "step": 23570
    },
    {
      "epoch": 4287.272727272727,
      "grad_norm": 0.00817807950079441,
      "learning_rate": 9.90066472445793e-07,
      "loss": 0.001,
      "step": 23580
    },
    {
      "epoch": 4289.090909090909,
      "grad_norm": 0.08944187313318253,
      "learning_rate": 9.900433810774489e-07,
      "loss": 0.0016,
      "step": 23590
    },
    {
      "epoch": 4290.909090909091,
      "grad_norm": 0.5206738114356995,
      "learning_rate": 9.900202631711938e-07,
      "loss": 0.0013,
      "step": 23600
    },
    {
      "epoch": 4292.727272727273,
      "grad_norm": 0.013676374219357967,
      "learning_rate": 9.899971187282797e-07,
      "loss": 0.0013,
      "step": 23610
    },
    {
      "epoch": 4294.545454545455,
      "grad_norm": 0.6065059900283813,
      "learning_rate": 9.899739477499599e-07,
      "loss": 0.0011,
      "step": 23620
    },
    {
      "epoch": 4296.363636363636,
      "grad_norm": 0.40743401646614075,
      "learning_rate": 9.899507502374889e-07,
      "loss": 0.0011,
      "step": 23630
    },
    {
      "epoch": 4298.181818181818,
      "grad_norm": 0.6082785129547119,
      "learning_rate": 9.899275261921233e-07,
      "loss": 0.0015,
      "step": 23640
    },
    {
      "epoch": 4300.0,
      "grad_norm": 0.4253740608692169,
      "learning_rate": 9.899042756151208e-07,
      "loss": 0.0011,
      "step": 23650
    },
    {
      "epoch": 4301.818181818182,
      "grad_norm": 0.04147147014737129,
      "learning_rate": 9.898809985077403e-07,
      "loss": 0.0012,
      "step": 23660
    },
    {
      "epoch": 4303.636363636364,
      "grad_norm": 0.01140536554157734,
      "learning_rate": 9.898576948712425e-07,
      "loss": 0.0012,
      "step": 23670
    },
    {
      "epoch": 4305.454545454545,
      "grad_norm": 0.010129407048225403,
      "learning_rate": 9.898343647068894e-07,
      "loss": 0.0013,
      "step": 23680
    },
    {
      "epoch": 4307.272727272727,
      "grad_norm": 0.38551533222198486,
      "learning_rate": 9.898110080159443e-07,
      "loss": 0.0013,
      "step": 23690
    },
    {
      "epoch": 4309.090909090909,
      "grad_norm": 0.48339805006980896,
      "learning_rate": 9.89787624799672e-07,
      "loss": 0.0014,
      "step": 23700
    },
    {
      "epoch": 4310.909090909091,
      "grad_norm": 0.011511724442243576,
      "learning_rate": 9.89764215059339e-07,
      "loss": 0.0012,
      "step": 23710
    },
    {
      "epoch": 4312.727272727273,
      "grad_norm": 0.29566243290901184,
      "learning_rate": 9.89740778796213e-07,
      "loss": 0.001,
      "step": 23720
    },
    {
      "epoch": 4314.545454545455,
      "grad_norm": 0.38362425565719604,
      "learning_rate": 9.897173160115632e-07,
      "loss": 0.0013,
      "step": 23730
    },
    {
      "epoch": 4316.363636363636,
      "grad_norm": 0.027201006188988686,
      "learning_rate": 9.8969382670666e-07,
      "loss": 0.0017,
      "step": 23740
    },
    {
      "epoch": 4318.181818181818,
      "grad_norm": 0.014220966026186943,
      "learning_rate": 9.896703108827757e-07,
      "loss": 0.0009,
      "step": 23750
    },
    {
      "epoch": 4320.0,
      "grad_norm": 0.4232959449291229,
      "learning_rate": 9.896467685411836e-07,
      "loss": 0.0014,
      "step": 23760
    },
    {
      "epoch": 4321.818181818182,
      "grad_norm": 0.04043184220790863,
      "learning_rate": 9.896231996831588e-07,
      "loss": 0.0014,
      "step": 23770
    },
    {
      "epoch": 4323.636363636364,
      "grad_norm": 0.013170485384762287,
      "learning_rate": 9.895996043099775e-07,
      "loss": 0.0015,
      "step": 23780
    },
    {
      "epoch": 4325.454545454545,
      "grad_norm": 0.6529780626296997,
      "learning_rate": 9.895759824229175e-07,
      "loss": 0.0012,
      "step": 23790
    },
    {
      "epoch": 4327.272727272727,
      "grad_norm": 0.00892612710595131,
      "learning_rate": 9.89552334023258e-07,
      "loss": 0.0011,
      "step": 23800
    },
    {
      "epoch": 4329.090909090909,
      "grad_norm": 0.40458914637565613,
      "learning_rate": 9.895286591122798e-07,
      "loss": 0.0013,
      "step": 23810
    },
    {
      "epoch": 4330.909090909091,
      "grad_norm": 0.022202065214514732,
      "learning_rate": 9.895049576912647e-07,
      "loss": 0.0012,
      "step": 23820
    },
    {
      "epoch": 4332.727272727273,
      "grad_norm": 0.45749324560165405,
      "learning_rate": 9.894812297614968e-07,
      "loss": 0.0012,
      "step": 23830
    },
    {
      "epoch": 4334.545454545455,
      "grad_norm": 0.06192656606435776,
      "learning_rate": 9.894574753242605e-07,
      "loss": 0.001,
      "step": 23840
    },
    {
      "epoch": 4336.363636363636,
      "grad_norm": 0.42217695713043213,
      "learning_rate": 9.894336943808424e-07,
      "loss": 0.0015,
      "step": 23850
    },
    {
      "epoch": 4338.181818181818,
      "grad_norm": 0.45311078429222107,
      "learning_rate": 9.894098869325304e-07,
      "loss": 0.0013,
      "step": 23860
    },
    {
      "epoch": 4340.0,
      "grad_norm": 0.583134114742279,
      "learning_rate": 9.893860529806137e-07,
      "loss": 0.0011,
      "step": 23870
    },
    {
      "epoch": 4341.818181818182,
      "grad_norm": 0.012708096764981747,
      "learning_rate": 9.89362192526383e-07,
      "loss": 0.001,
      "step": 23880
    },
    {
      "epoch": 4343.636363636364,
      "grad_norm": 0.4729774594306946,
      "learning_rate": 9.893383055711305e-07,
      "loss": 0.0015,
      "step": 23890
    },
    {
      "epoch": 4345.454545454545,
      "grad_norm": 0.013449113816022873,
      "learning_rate": 9.8931439211615e-07,
      "loss": 0.001,
      "step": 23900
    },
    {
      "epoch": 4347.272727272727,
      "grad_norm": 0.6443615555763245,
      "learning_rate": 9.89290452162736e-07,
      "loss": 0.0018,
      "step": 23910
    },
    {
      "epoch": 4349.090909090909,
      "grad_norm": 0.014704097993671894,
      "learning_rate": 9.892664857121854e-07,
      "loss": 0.0008,
      "step": 23920
    },
    {
      "epoch": 4350.909090909091,
      "grad_norm": 0.019577164202928543,
      "learning_rate": 9.892424927657957e-07,
      "loss": 0.0014,
      "step": 23930
    },
    {
      "epoch": 4352.727272727273,
      "grad_norm": 0.009915576316416264,
      "learning_rate": 9.892184733248665e-07,
      "loss": 0.0013,
      "step": 23940
    },
    {
      "epoch": 4354.545454545455,
      "grad_norm": 0.013107151724398136,
      "learning_rate": 9.891944273906984e-07,
      "loss": 0.0012,
      "step": 23950
    },
    {
      "epoch": 4356.363636363636,
      "grad_norm": 0.35797491669654846,
      "learning_rate": 9.891703549645937e-07,
      "loss": 0.0013,
      "step": 23960
    },
    {
      "epoch": 4358.181818181818,
      "grad_norm": 0.3341551721096039,
      "learning_rate": 9.891462560478562e-07,
      "loss": 0.0013,
      "step": 23970
    },
    {
      "epoch": 4360.0,
      "grad_norm": 0.010636608116328716,
      "learning_rate": 9.891221306417905e-07,
      "loss": 0.0012,
      "step": 23980
    },
    {
      "epoch": 4361.818181818182,
      "grad_norm": 0.43479207158088684,
      "learning_rate": 9.890979787477034e-07,
      "loss": 0.0013,
      "step": 23990
    },
    {
      "epoch": 4363.636363636364,
      "grad_norm": 0.5323617458343506,
      "learning_rate": 9.890738003669027e-07,
      "loss": 0.0015,
      "step": 24000
    },
    {
      "epoch": 4363.636363636364,
      "eval_loss": 4.235503196716309,
      "eval_runtime": 0.9513,
      "eval_samples_per_second": 10.511,
      "eval_steps_per_second": 5.256,
      "step": 24000
    },
    {
      "epoch": 4365.454545454545,
      "grad_norm": 0.6271572709083557,
      "learning_rate": 9.89049595500698e-07,
      "loss": 0.0011,
      "step": 24010
    },
    {
      "epoch": 4367.272727272727,
      "grad_norm": 0.4619000256061554,
      "learning_rate": 9.890253641503999e-07,
      "loss": 0.0012,
      "step": 24020
    },
    {
      "epoch": 4369.090909090909,
      "grad_norm": 0.43969976902008057,
      "learning_rate": 9.890011063173205e-07,
      "loss": 0.0012,
      "step": 24030
    },
    {
      "epoch": 4370.909090909091,
      "grad_norm": 0.33892932534217834,
      "learning_rate": 9.889768220027736e-07,
      "loss": 0.0013,
      "step": 24040
    },
    {
      "epoch": 4372.727272727273,
      "grad_norm": 0.00856256578117609,
      "learning_rate": 9.889525112080745e-07,
      "loss": 0.0012,
      "step": 24050
    },
    {
      "epoch": 4374.545454545455,
      "grad_norm": 0.5125009417533875,
      "learning_rate": 9.889281739345395e-07,
      "loss": 0.0012,
      "step": 24060
    },
    {
      "epoch": 4376.363636363636,
      "grad_norm": 0.43207821249961853,
      "learning_rate": 9.889038101834864e-07,
      "loss": 0.0013,
      "step": 24070
    },
    {
      "epoch": 4378.181818181818,
      "grad_norm": 0.0118974344804883,
      "learning_rate": 9.88879419956235e-07,
      "loss": 0.0011,
      "step": 24080
    },
    {
      "epoch": 4380.0,
      "grad_norm": 0.01934727653861046,
      "learning_rate": 9.88855003254106e-07,
      "loss": 0.0013,
      "step": 24090
    },
    {
      "epoch": 4381.818181818182,
      "grad_norm": 0.018918871879577637,
      "learning_rate": 9.888305600784216e-07,
      "loss": 0.0011,
      "step": 24100
    },
    {
      "epoch": 4383.636363636364,
      "grad_norm": 0.009467833675444126,
      "learning_rate": 9.888060904305054e-07,
      "loss": 0.0013,
      "step": 24110
    },
    {
      "epoch": 4385.454545454545,
      "grad_norm": 0.44285041093826294,
      "learning_rate": 9.887815943116826e-07,
      "loss": 0.0013,
      "step": 24120
    },
    {
      "epoch": 4387.272727272727,
      "grad_norm": 0.3994672894477844,
      "learning_rate": 9.887570717232797e-07,
      "loss": 0.0013,
      "step": 24130
    },
    {
      "epoch": 4389.090909090909,
      "grad_norm": 0.008116194978356361,
      "learning_rate": 9.887325226666253e-07,
      "loss": 0.0011,
      "step": 24140
    },
    {
      "epoch": 4390.909090909091,
      "grad_norm": 0.06927306205034256,
      "learning_rate": 9.887079471430479e-07,
      "loss": 0.0014,
      "step": 24150
    },
    {
      "epoch": 4392.727272727273,
      "grad_norm": 0.40188097953796387,
      "learning_rate": 9.88683345153879e-07,
      "loss": 0.001,
      "step": 24160
    },
    {
      "epoch": 4394.545454545455,
      "grad_norm": 0.017351040616631508,
      "learning_rate": 9.886587167004506e-07,
      "loss": 0.0012,
      "step": 24170
    },
    {
      "epoch": 4396.363636363636,
      "grad_norm": 0.006787010468542576,
      "learning_rate": 9.886340617840966e-07,
      "loss": 0.001,
      "step": 24180
    },
    {
      "epoch": 4398.181818181818,
      "grad_norm": 0.008192338980734348,
      "learning_rate": 9.886093804061522e-07,
      "loss": 0.0013,
      "step": 24190
    },
    {
      "epoch": 4400.0,
      "grad_norm": 0.009066754020750523,
      "learning_rate": 9.885846725679538e-07,
      "loss": 0.0013,
      "step": 24200
    },
    {
      "epoch": 4401.818181818182,
      "grad_norm": 0.011437841691076756,
      "learning_rate": 9.885599382708397e-07,
      "loss": 0.0014,
      "step": 24210
    },
    {
      "epoch": 4403.636363636364,
      "grad_norm": 0.38974910974502563,
      "learning_rate": 9.885351775161492e-07,
      "loss": 0.0013,
      "step": 24220
    },
    {
      "epoch": 4405.454545454545,
      "grad_norm": 0.007988804951310158,
      "learning_rate": 9.885103903052232e-07,
      "loss": 0.0011,
      "step": 24230
    },
    {
      "epoch": 4407.272727272727,
      "grad_norm": 0.3414747714996338,
      "learning_rate": 9.88485576639404e-07,
      "loss": 0.0012,
      "step": 24240
    },
    {
      "epoch": 4409.090909090909,
      "grad_norm": 0.013527542352676392,
      "learning_rate": 9.884607365200355e-07,
      "loss": 0.0013,
      "step": 24250
    },
    {
      "epoch": 4410.909090909091,
      "grad_norm": 0.40557438135147095,
      "learning_rate": 9.884358699484627e-07,
      "loss": 0.0012,
      "step": 24260
    },
    {
      "epoch": 4412.727272727273,
      "grad_norm": 0.007658065762370825,
      "learning_rate": 9.884109769260325e-07,
      "loss": 0.0008,
      "step": 24270
    },
    {
      "epoch": 4414.545454545455,
      "grad_norm": 0.025270920246839523,
      "learning_rate": 9.883860574540925e-07,
      "loss": 0.0015,
      "step": 24280
    },
    {
      "epoch": 4416.363636363636,
      "grad_norm": 0.45209795236587524,
      "learning_rate": 9.883611115339926e-07,
      "loss": 0.0015,
      "step": 24290
    },
    {
      "epoch": 4418.181818181818,
      "grad_norm": 0.4937788248062134,
      "learning_rate": 9.883361391670839e-07,
      "loss": 0.0012,
      "step": 24300
    },
    {
      "epoch": 4420.0,
      "grad_norm": 0.014082268811762333,
      "learning_rate": 9.883111403547182e-07,
      "loss": 0.0012,
      "step": 24310
    },
    {
      "epoch": 4421.818181818182,
      "grad_norm": 0.47932168841362,
      "learning_rate": 9.882861150982497e-07,
      "loss": 0.0013,
      "step": 24320
    },
    {
      "epoch": 4423.636363636364,
      "grad_norm": 0.034698486328125,
      "learning_rate": 9.882610633990336e-07,
      "loss": 0.0014,
      "step": 24330
    },
    {
      "epoch": 4425.454545454545,
      "grad_norm": 0.01436789333820343,
      "learning_rate": 9.882359852584263e-07,
      "loss": 0.0009,
      "step": 24340
    },
    {
      "epoch": 4427.272727272727,
      "grad_norm": 0.4701365530490875,
      "learning_rate": 9.88210880677786e-07,
      "loss": 0.0016,
      "step": 24350
    },
    {
      "epoch": 4429.090909090909,
      "grad_norm": 0.6391541957855225,
      "learning_rate": 9.881857496584724e-07,
      "loss": 0.0011,
      "step": 24360
    },
    {
      "epoch": 4430.909090909091,
      "grad_norm": 0.5382658839225769,
      "learning_rate": 9.881605922018462e-07,
      "loss": 0.0014,
      "step": 24370
    },
    {
      "epoch": 4432.727272727273,
      "grad_norm": 0.4256967604160309,
      "learning_rate": 9.8813540830927e-07,
      "loss": 0.0009,
      "step": 24380
    },
    {
      "epoch": 4434.545454545455,
      "grad_norm": 0.5030732750892639,
      "learning_rate": 9.881101979821073e-07,
      "loss": 0.0016,
      "step": 24390
    },
    {
      "epoch": 4436.363636363636,
      "grad_norm": 0.04018830135464668,
      "learning_rate": 9.880849612217237e-07,
      "loss": 0.0012,
      "step": 24400
    },
    {
      "epoch": 4438.181818181818,
      "grad_norm": 0.37473002076148987,
      "learning_rate": 9.880596980294857e-07,
      "loss": 0.0013,
      "step": 24410
    },
    {
      "epoch": 4440.0,
      "grad_norm": 0.01185510866343975,
      "learning_rate": 9.880344084067615e-07,
      "loss": 0.0012,
      "step": 24420
    },
    {
      "epoch": 4441.818181818182,
      "grad_norm": 0.00898642186075449,
      "learning_rate": 9.880090923549204e-07,
      "loss": 0.0012,
      "step": 24430
    },
    {
      "epoch": 4443.636363636364,
      "grad_norm": 0.016202736645936966,
      "learning_rate": 9.87983749875334e-07,
      "loss": 0.0012,
      "step": 24440
    },
    {
      "epoch": 4445.454545454545,
      "grad_norm": 0.012423885986208916,
      "learning_rate": 9.879583809693736e-07,
      "loss": 0.0015,
      "step": 24450
    },
    {
      "epoch": 4447.272727272727,
      "grad_norm": 0.017680276185274124,
      "learning_rate": 9.87932985638414e-07,
      "loss": 0.0012,
      "step": 24460
    },
    {
      "epoch": 4449.090909090909,
      "grad_norm": 0.5097087025642395,
      "learning_rate": 9.8790756388383e-07,
      "loss": 0.0013,
      "step": 24470
    },
    {
      "epoch": 4450.909090909091,
      "grad_norm": 0.5282537937164307,
      "learning_rate": 9.878821157069988e-07,
      "loss": 0.0013,
      "step": 24480
    },
    {
      "epoch": 4452.727272727273,
      "grad_norm": 0.038015227764844894,
      "learning_rate": 9.878566411092978e-07,
      "loss": 0.001,
      "step": 24490
    },
    {
      "epoch": 4454.545454545455,
      "grad_norm": 0.5077654123306274,
      "learning_rate": 9.878311400921072e-07,
      "loss": 0.0012,
      "step": 24500
    },
    {
      "epoch": 4454.545454545455,
      "eval_loss": 4.2946295738220215,
      "eval_runtime": 0.9491,
      "eval_samples_per_second": 10.536,
      "eval_steps_per_second": 5.268,
      "step": 24500
    },
    {
      "epoch": 4456.363636363636,
      "grad_norm": 0.015460244379937649,
      "learning_rate": 9.878056126568075e-07,
      "loss": 0.0011,
      "step": 24510
    },
    {
      "epoch": 4458.181818181818,
      "grad_norm": 0.017081407830119133,
      "learning_rate": 9.877800588047814e-07,
      "loss": 0.0013,
      "step": 24520
    },
    {
      "epoch": 4460.0,
      "grad_norm": 0.03945731744170189,
      "learning_rate": 9.877544785374125e-07,
      "loss": 0.0014,
      "step": 24530
    },
    {
      "epoch": 4461.818181818182,
      "grad_norm": 0.022207066416740417,
      "learning_rate": 9.877288718560866e-07,
      "loss": 0.0014,
      "step": 24540
    },
    {
      "epoch": 4463.636363636364,
      "grad_norm": 0.35417619347572327,
      "learning_rate": 9.877032387621897e-07,
      "loss": 0.0012,
      "step": 24550
    },
    {
      "epoch": 4465.454545454545,
      "grad_norm": 0.021080387756228447,
      "learning_rate": 9.876775792571107e-07,
      "loss": 0.001,
      "step": 24560
    },
    {
      "epoch": 4467.272727272727,
      "grad_norm": 0.03340291976928711,
      "learning_rate": 9.876518933422383e-07,
      "loss": 0.0013,
      "step": 24570
    },
    {
      "epoch": 4469.090909090909,
      "grad_norm": 0.4923797845840454,
      "learning_rate": 9.876261810189643e-07,
      "loss": 0.0013,
      "step": 24580
    },
    {
      "epoch": 4470.909090909091,
      "grad_norm": 0.545200765132904,
      "learning_rate": 9.876004422886807e-07,
      "loss": 0.0012,
      "step": 24590
    },
    {
      "epoch": 4472.727272727273,
      "grad_norm": 0.4331454038619995,
      "learning_rate": 9.875746771527815e-07,
      "loss": 0.0011,
      "step": 24600
    },
    {
      "epoch": 4474.545454545455,
      "grad_norm": 0.12945500016212463,
      "learning_rate": 9.87548885612662e-07,
      "loss": 0.0015,
      "step": 24610
    },
    {
      "epoch": 4476.363636363636,
      "grad_norm": 0.007577084470540285,
      "learning_rate": 9.875230676697189e-07,
      "loss": 0.001,
      "step": 24620
    },
    {
      "epoch": 4478.181818181818,
      "grad_norm": 0.3433011770248413,
      "learning_rate": 9.874972233253503e-07,
      "loss": 0.0012,
      "step": 24630
    },
    {
      "epoch": 4480.0,
      "grad_norm": 0.009052300825715065,
      "learning_rate": 9.874713525809557e-07,
      "loss": 0.0012,
      "step": 24640
    },
    {
      "epoch": 4481.818181818182,
      "grad_norm": 0.009471981786191463,
      "learning_rate": 9.874454554379363e-07,
      "loss": 0.0013,
      "step": 24650
    },
    {
      "epoch": 4483.636363636364,
      "grad_norm": 0.4641968309879303,
      "learning_rate": 9.874195318976943e-07,
      "loss": 0.0012,
      "step": 24660
    },
    {
      "epoch": 4485.454545454545,
      "grad_norm": 0.3451915979385376,
      "learning_rate": 9.87393581961634e-07,
      "loss": 0.0014,
      "step": 24670
    },
    {
      "epoch": 4487.272727272727,
      "grad_norm": 0.44101130962371826,
      "learning_rate": 9.873676056311602e-07,
      "loss": 0.001,
      "step": 24680
    },
    {
      "epoch": 4489.090909090909,
      "grad_norm": 0.36784112453460693,
      "learning_rate": 9.8734160290768e-07,
      "loss": 0.0013,
      "step": 24690
    },
    {
      "epoch": 4490.909090909091,
      "grad_norm": 0.007739899214357138,
      "learning_rate": 9.873155737926012e-07,
      "loss": 0.0014,
      "step": 24700
    },
    {
      "epoch": 4492.727272727273,
      "grad_norm": 0.3708173930644989,
      "learning_rate": 9.872895182873338e-07,
      "loss": 0.0011,
      "step": 24710
    },
    {
      "epoch": 4494.545454545455,
      "grad_norm": 0.015266293659806252,
      "learning_rate": 9.872634363932886e-07,
      "loss": 0.0011,
      "step": 24720
    },
    {
      "epoch": 4496.363636363636,
      "grad_norm": 0.35259124636650085,
      "learning_rate": 9.87237328111878e-07,
      "loss": 0.0013,
      "step": 24730
    },
    {
      "epoch": 4498.181818181818,
      "grad_norm": 0.025869518518447876,
      "learning_rate": 9.872111934445158e-07,
      "loss": 0.0012,
      "step": 24740
    },
    {
      "epoch": 4500.0,
      "grad_norm": 0.7758011817932129,
      "learning_rate": 9.871850323926177e-07,
      "loss": 0.0014,
      "step": 24750
    },
    {
      "epoch": 4501.818181818182,
      "grad_norm": 0.40086299180984497,
      "learning_rate": 9.871588449575998e-07,
      "loss": 0.0012,
      "step": 24760
    },
    {
      "epoch": 4503.636363636364,
      "grad_norm": 0.08689278364181519,
      "learning_rate": 9.87132631140881e-07,
      "loss": 0.0013,
      "step": 24770
    },
    {
      "epoch": 4505.454545454545,
      "grad_norm": 0.01245922688394785,
      "learning_rate": 9.8710639094388e-07,
      "loss": 0.0014,
      "step": 24780
    },
    {
      "epoch": 4507.272727272727,
      "grad_norm": 0.007862773723900318,
      "learning_rate": 9.870801243680188e-07,
      "loss": 0.001,
      "step": 24790
    },
    {
      "epoch": 4509.090909090909,
      "grad_norm": 0.008794373832643032,
      "learning_rate": 9.870538314147192e-07,
      "loss": 0.0014,
      "step": 24800
    },
    {
      "epoch": 4510.909090909091,
      "grad_norm": 0.010309631936252117,
      "learning_rate": 9.870275120854054e-07,
      "loss": 0.0013,
      "step": 24810
    },
    {
      "epoch": 4512.727272727273,
      "grad_norm": 0.04532845318317413,
      "learning_rate": 9.870011663815024e-07,
      "loss": 0.0008,
      "step": 24820
    },
    {
      "epoch": 4514.545454545455,
      "grad_norm": 0.37324026226997375,
      "learning_rate": 9.86974794304437e-07,
      "loss": 0.0015,
      "step": 24830
    },
    {
      "epoch": 4516.363636363636,
      "grad_norm": 0.025003578513860703,
      "learning_rate": 9.869483958556375e-07,
      "loss": 0.0011,
      "step": 24840
    },
    {
      "epoch": 4518.181818181818,
      "grad_norm": 0.39202678203582764,
      "learning_rate": 9.869219710365336e-07,
      "loss": 0.0012,
      "step": 24850
    },
    {
      "epoch": 4520.0,
      "grad_norm": 0.36689674854278564,
      "learning_rate": 9.868955198485559e-07,
      "loss": 0.0012,
      "step": 24860
    },
    {
      "epoch": 4521.818181818182,
      "grad_norm": 0.46160954236984253,
      "learning_rate": 9.86869042293137e-07,
      "loss": 0.0012,
      "step": 24870
    },
    {
      "epoch": 4523.636363636364,
      "grad_norm": 0.0766802579164505,
      "learning_rate": 9.868425383717113e-07,
      "loss": 0.0013,
      "step": 24880
    },
    {
      "epoch": 4525.454545454545,
      "grad_norm": 0.010088215582072735,
      "learning_rate": 9.868160080857132e-07,
      "loss": 0.0011,
      "step": 24890
    },
    {
      "epoch": 4527.272727272727,
      "grad_norm": 0.006495940499007702,
      "learning_rate": 9.8678945143658e-07,
      "loss": 0.001,
      "step": 24900
    },
    {
      "epoch": 4529.090909090909,
      "grad_norm": 0.010818373411893845,
      "learning_rate": 9.867628684257499e-07,
      "loss": 0.0014,
      "step": 24910
    },
    {
      "epoch": 4530.909090909091,
      "grad_norm": 0.4344640374183655,
      "learning_rate": 9.867362590546621e-07,
      "loss": 0.0011,
      "step": 24920
    },
    {
      "epoch": 4532.727272727273,
      "grad_norm": 0.48675352334976196,
      "learning_rate": 9.86709623324758e-07,
      "loss": 0.0013,
      "step": 24930
    },
    {
      "epoch": 4534.545454545455,
      "grad_norm": 0.5848901271820068,
      "learning_rate": 9.866829612374798e-07,
      "loss": 0.0013,
      "step": 24940
    },
    {
      "epoch": 4536.363636363636,
      "grad_norm": 0.5848873853683472,
      "learning_rate": 9.866562727942713e-07,
      "loss": 0.0011,
      "step": 24950
    },
    {
      "epoch": 4538.181818181818,
      "grad_norm": 0.009276514872908592,
      "learning_rate": 9.86629557996578e-07,
      "loss": 0.001,
      "step": 24960
    },
    {
      "epoch": 4540.0,
      "grad_norm": 0.4671507179737091,
      "learning_rate": 9.866028168458467e-07,
      "loss": 0.0014,
      "step": 24970
    },
    {
      "epoch": 4541.818181818182,
      "grad_norm": 0.03821085765957832,
      "learning_rate": 9.86576049343525e-07,
      "loss": 0.0012,
      "step": 24980
    },
    {
      "epoch": 4543.636363636364,
      "grad_norm": 0.45934975147247314,
      "learning_rate": 9.865492554910632e-07,
      "loss": 0.001,
      "step": 24990
    },
    {
      "epoch": 4545.454545454545,
      "grad_norm": 0.01681051030755043,
      "learning_rate": 9.865224352899118e-07,
      "loss": 0.0016,
      "step": 25000
    },
    {
      "epoch": 4545.454545454545,
      "eval_loss": 4.289608955383301,
      "eval_runtime": 0.9518,
      "eval_samples_per_second": 10.507,
      "eval_steps_per_second": 5.253,
      "step": 25000
    },
    {
      "epoch": 4547.272727272727,
      "grad_norm": 0.3309325575828552,
      "learning_rate": 9.864955887415234e-07,
      "loss": 0.0012,
      "step": 25010
    },
    {
      "epoch": 4549.090909090909,
      "grad_norm": 0.5078727006912231,
      "learning_rate": 9.86468715847352e-07,
      "loss": 0.0011,
      "step": 25020
    },
    {
      "epoch": 4550.909090909091,
      "grad_norm": 0.39452260732650757,
      "learning_rate": 9.864418166088524e-07,
      "loss": 0.0013,
      "step": 25030
    },
    {
      "epoch": 4552.727272727273,
      "grad_norm": 0.013377969153225422,
      "learning_rate": 9.864148910274819e-07,
      "loss": 0.001,
      "step": 25040
    },
    {
      "epoch": 4554.545454545455,
      "grad_norm": 0.5167462229728699,
      "learning_rate": 9.863879391046983e-07,
      "loss": 0.0017,
      "step": 25050
    },
    {
      "epoch": 4556.363636363636,
      "grad_norm": 0.025990810245275497,
      "learning_rate": 9.863609608419612e-07,
      "loss": 0.0009,
      "step": 25060
    },
    {
      "epoch": 4558.181818181818,
      "grad_norm": 0.0068397303111851215,
      "learning_rate": 9.863339562407316e-07,
      "loss": 0.0013,
      "step": 25070
    },
    {
      "epoch": 4560.0,
      "grad_norm": 0.44959041476249695,
      "learning_rate": 9.863069253024717e-07,
      "loss": 0.0013,
      "step": 25080
    },
    {
      "epoch": 4561.818181818182,
      "grad_norm": 0.04131462797522545,
      "learning_rate": 9.862798680286456e-07,
      "loss": 0.0013,
      "step": 25090
    },
    {
      "epoch": 4563.636363636364,
      "grad_norm": 0.5756855607032776,
      "learning_rate": 9.862527844207187e-07,
      "loss": 0.0013,
      "step": 25100
    },
    {
      "epoch": 4565.454545454545,
      "grad_norm": 0.01060426328331232,
      "learning_rate": 9.862256744801575e-07,
      "loss": 0.0008,
      "step": 25110
    },
    {
      "epoch": 4567.272727272727,
      "grad_norm": 0.5279558897018433,
      "learning_rate": 9.861985382084302e-07,
      "loss": 0.0015,
      "step": 25120
    },
    {
      "epoch": 4569.090909090909,
      "grad_norm": 0.019798122346401215,
      "learning_rate": 9.86171375607006e-07,
      "loss": 0.0011,
      "step": 25130
    },
    {
      "epoch": 4570.909090909091,
      "grad_norm": 0.7560363411903381,
      "learning_rate": 9.861441866773563e-07,
      "loss": 0.0014,
      "step": 25140
    },
    {
      "epoch": 4572.727272727273,
      "grad_norm": 0.008484612219035625,
      "learning_rate": 9.861169714209534e-07,
      "loss": 0.001,
      "step": 25150
    },
    {
      "epoch": 4574.545454545455,
      "grad_norm": 0.0075231557711958885,
      "learning_rate": 9.86089729839271e-07,
      "loss": 0.001,
      "step": 25160
    },
    {
      "epoch": 4576.363636363636,
      "grad_norm": 0.014993718825280666,
      "learning_rate": 9.860624619337842e-07,
      "loss": 0.0014,
      "step": 25170
    },
    {
      "epoch": 4578.181818181818,
      "grad_norm": 0.043514106422662735,
      "learning_rate": 9.860351677059701e-07,
      "loss": 0.0013,
      "step": 25180
    },
    {
      "epoch": 4580.0,
      "grad_norm": 0.41646912693977356,
      "learning_rate": 9.860078471573065e-07,
      "loss": 0.0013,
      "step": 25190
    },
    {
      "epoch": 4581.818181818182,
      "grad_norm": 0.40392783284187317,
      "learning_rate": 9.859805002892731e-07,
      "loss": 0.0014,
      "step": 25200
    },
    {
      "epoch": 4583.636363636364,
      "grad_norm": 0.03915419057011604,
      "learning_rate": 9.859531271033506e-07,
      "loss": 0.0013,
      "step": 25210
    },
    {
      "epoch": 4585.454545454545,
      "grad_norm": 0.4537223279476166,
      "learning_rate": 9.859257276010215e-07,
      "loss": 0.0011,
      "step": 25220
    },
    {
      "epoch": 4587.272727272727,
      "grad_norm": 0.5578285455703735,
      "learning_rate": 9.858983017837698e-07,
      "loss": 0.0013,
      "step": 25230
    },
    {
      "epoch": 4589.090909090909,
      "grad_norm": 0.02489446848630905,
      "learning_rate": 9.858708496530804e-07,
      "loss": 0.001,
      "step": 25240
    },
    {
      "epoch": 4590.909090909091,
      "grad_norm": 0.021170467138290405,
      "learning_rate": 9.8584337121044e-07,
      "loss": 0.0013,
      "step": 25250
    },
    {
      "epoch": 4592.727272727273,
      "grad_norm": 0.012333501130342484,
      "learning_rate": 9.85815866457337e-07,
      "loss": 0.0013,
      "step": 25260
    },
    {
      "epoch": 4594.545454545455,
      "grad_norm": 0.008486617356538773,
      "learning_rate": 9.857883353952604e-07,
      "loss": 0.001,
      "step": 25270
    },
    {
      "epoch": 4596.363636363636,
      "grad_norm": 0.420566201210022,
      "learning_rate": 9.857607780257015e-07,
      "loss": 0.0012,
      "step": 25280
    },
    {
      "epoch": 4598.181818181818,
      "grad_norm": 0.00898807030171156,
      "learning_rate": 9.857331943501525e-07,
      "loss": 0.0011,
      "step": 25290
    },
    {
      "epoch": 4600.0,
      "grad_norm": 0.010807180777192116,
      "learning_rate": 9.85705584370107e-07,
      "loss": 0.0013,
      "step": 25300
    },
    {
      "epoch": 4601.818181818182,
      "grad_norm": 0.010944095440208912,
      "learning_rate": 9.856779480870608e-07,
      "loss": 0.0013,
      "step": 25310
    },
    {
      "epoch": 4603.636363636364,
      "grad_norm": 0.1401170790195465,
      "learning_rate": 9.8565028550251e-07,
      "loss": 0.0012,
      "step": 25320
    },
    {
      "epoch": 4605.454545454545,
      "grad_norm": 0.021509872749447823,
      "learning_rate": 9.856225966179526e-07,
      "loss": 0.0011,
      "step": 25330
    },
    {
      "epoch": 4607.272727272727,
      "grad_norm": 0.35331323742866516,
      "learning_rate": 9.855948814348884e-07,
      "loss": 0.0016,
      "step": 25340
    },
    {
      "epoch": 4609.090909090909,
      "grad_norm": 0.010942945256829262,
      "learning_rate": 9.85567139954818e-07,
      "loss": 0.001,
      "step": 25350
    },
    {
      "epoch": 4610.909090909091,
      "grad_norm": 0.3759315311908722,
      "learning_rate": 9.855393721792438e-07,
      "loss": 0.0013,
      "step": 25360
    },
    {
      "epoch": 4612.727272727273,
      "grad_norm": 0.01048873271793127,
      "learning_rate": 9.855115781096696e-07,
      "loss": 0.0012,
      "step": 25370
    },
    {
      "epoch": 4614.545454545455,
      "grad_norm": 0.4180980622768402,
      "learning_rate": 9.854837577476008e-07,
      "loss": 0.001,
      "step": 25380
    },
    {
      "epoch": 4616.363636363636,
      "grad_norm": 0.4497555196285248,
      "learning_rate": 9.854559110945435e-07,
      "loss": 0.0013,
      "step": 25390
    },
    {
      "epoch": 4618.181818181818,
      "grad_norm": 0.009570630267262459,
      "learning_rate": 9.85428038152006e-07,
      "loss": 0.0012,
      "step": 25400
    },
    {
      "epoch": 4620.0,
      "grad_norm": 0.09306702762842178,
      "learning_rate": 9.854001389214977e-07,
      "loss": 0.0013,
      "step": 25410
    },
    {
      "epoch": 4621.818181818182,
      "grad_norm": 0.5420339703559875,
      "learning_rate": 9.853722134045294e-07,
      "loss": 0.0011,
      "step": 25420
    },
    {
      "epoch": 4623.636363636364,
      "grad_norm": 0.009619803167879581,
      "learning_rate": 9.853442616026136e-07,
      "loss": 0.001,
      "step": 25430
    },
    {
      "epoch": 4625.454545454545,
      "grad_norm": 0.025452790781855583,
      "learning_rate": 9.853162835172635e-07,
      "loss": 0.0016,
      "step": 25440
    },
    {
      "epoch": 4627.272727272727,
      "grad_norm": 0.44482865929603577,
      "learning_rate": 9.85288279149995e-07,
      "loss": 0.0012,
      "step": 25450
    },
    {
      "epoch": 4629.090909090909,
      "grad_norm": 0.5135959982872009,
      "learning_rate": 9.85260248502324e-07,
      "loss": 0.0013,
      "step": 25460
    },
    {
      "epoch": 4630.909090909091,
      "grad_norm": 0.006745596881955862,
      "learning_rate": 9.852321915757686e-07,
      "loss": 0.0012,
      "step": 25470
    },
    {
      "epoch": 4632.727272727273,
      "grad_norm": 0.005887791980057955,
      "learning_rate": 9.852041083718485e-07,
      "loss": 0.0012,
      "step": 25480
    },
    {
      "epoch": 4634.545454545455,
      "grad_norm": 0.39642712473869324,
      "learning_rate": 9.851759988920842e-07,
      "loss": 0.0013,
      "step": 25490
    },
    {
      "epoch": 4636.363636363636,
      "grad_norm": 0.3452109396457672,
      "learning_rate": 9.851478631379982e-07,
      "loss": 0.0012,
      "step": 25500
    },
    {
      "epoch": 4636.363636363636,
      "eval_loss": 4.278331279754639,
      "eval_runtime": 0.9558,
      "eval_samples_per_second": 10.462,
      "eval_steps_per_second": 5.231,
      "step": 25500
    },
    {
      "epoch": 4638.181818181818,
      "grad_norm": 0.012277442961931229,
      "learning_rate": 9.851197011111139e-07,
      "loss": 0.001,
      "step": 25510
    },
    {
      "epoch": 4640.0,
      "grad_norm": 0.017845112830400467,
      "learning_rate": 9.850915128129566e-07,
      "loss": 0.0013,
      "step": 25520
    },
    {
      "epoch": 4641.818181818182,
      "grad_norm": 0.4653942584991455,
      "learning_rate": 9.85063298245053e-07,
      "loss": 0.001,
      "step": 25530
    },
    {
      "epoch": 4643.636363636364,
      "grad_norm": 0.3880830705165863,
      "learning_rate": 9.850350574089305e-07,
      "loss": 0.0013,
      "step": 25540
    },
    {
      "epoch": 4645.454545454545,
      "grad_norm": 0.38067948818206787,
      "learning_rate": 9.850067903061187e-07,
      "loss": 0.0015,
      "step": 25550
    },
    {
      "epoch": 4647.272727272727,
      "grad_norm": 0.011922188103199005,
      "learning_rate": 9.849784969381485e-07,
      "loss": 0.0009,
      "step": 25560
    },
    {
      "epoch": 4649.090909090909,
      "grad_norm": 0.3075202703475952,
      "learning_rate": 9.849501773065523e-07,
      "loss": 0.0014,
      "step": 25570
    },
    {
      "epoch": 4650.909090909091,
      "grad_norm": 0.046192098408937454,
      "learning_rate": 9.849218314128631e-07,
      "loss": 0.0013,
      "step": 25580
    },
    {
      "epoch": 4652.727272727273,
      "grad_norm": 0.020908566191792488,
      "learning_rate": 9.848934592586165e-07,
      "loss": 0.0012,
      "step": 25590
    },
    {
      "epoch": 4654.545454545455,
      "grad_norm": 0.006211257539689541,
      "learning_rate": 9.848650608453488e-07,
      "loss": 0.0013,
      "step": 25600
    },
    {
      "epoch": 4656.363636363636,
      "grad_norm": 0.011453217826783657,
      "learning_rate": 9.848366361745978e-07,
      "loss": 0.0012,
      "step": 25610
    },
    {
      "epoch": 4658.181818181818,
      "grad_norm": 0.005937657319009304,
      "learning_rate": 9.848081852479028e-07,
      "loss": 0.0013,
      "step": 25620
    },
    {
      "epoch": 4660.0,
      "grad_norm": 0.010889007709920406,
      "learning_rate": 9.84779708066805e-07,
      "loss": 0.0014,
      "step": 25630
    },
    {
      "epoch": 4661.818181818182,
      "grad_norm": 0.34317004680633545,
      "learning_rate": 9.84751204632846e-07,
      "loss": 0.0012,
      "step": 25640
    },
    {
      "epoch": 4663.636363636364,
      "grad_norm": 0.00820431113243103,
      "learning_rate": 9.847226749475694e-07,
      "loss": 0.0011,
      "step": 25650
    },
    {
      "epoch": 4665.454545454545,
      "grad_norm": 0.5080210566520691,
      "learning_rate": 9.846941190125204e-07,
      "loss": 0.0016,
      "step": 25660
    },
    {
      "epoch": 4667.272727272727,
      "grad_norm": 0.014853709377348423,
      "learning_rate": 9.846655368292457e-07,
      "loss": 0.0012,
      "step": 25670
    },
    {
      "epoch": 4669.090909090909,
      "grad_norm": 0.33482420444488525,
      "learning_rate": 9.846369283992926e-07,
      "loss": 0.0014,
      "step": 25680
    },
    {
      "epoch": 4670.909090909091,
      "grad_norm": 0.01633179932832718,
      "learning_rate": 9.846082937242106e-07,
      "loss": 0.0012,
      "step": 25690
    },
    {
      "epoch": 4672.727272727273,
      "grad_norm": 0.007945435121655464,
      "learning_rate": 9.845796328055504e-07,
      "loss": 0.001,
      "step": 25700
    },
    {
      "epoch": 4674.545454545455,
      "grad_norm": 0.012145662680268288,
      "learning_rate": 9.845509456448643e-07,
      "loss": 0.0014,
      "step": 25710
    },
    {
      "epoch": 4676.363636363636,
      "grad_norm": 0.36037132143974304,
      "learning_rate": 9.845222322437054e-07,
      "loss": 0.0012,
      "step": 25720
    },
    {
      "epoch": 4678.181818181818,
      "grad_norm": 0.5737608075141907,
      "learning_rate": 9.84493492603629e-07,
      "loss": 0.0015,
      "step": 25730
    },
    {
      "epoch": 4680.0,
      "grad_norm": 0.3340491056442261,
      "learning_rate": 9.844647267261914e-07,
      "loss": 0.0011,
      "step": 25740
    },
    {
      "epoch": 4681.818181818182,
      "grad_norm": 0.35202378034591675,
      "learning_rate": 9.844359346129503e-07,
      "loss": 0.0013,
      "step": 25750
    },
    {
      "epoch": 4683.636363636364,
      "grad_norm": 0.31069961190223694,
      "learning_rate": 9.84407116265465e-07,
      "loss": 0.0008,
      "step": 25760
    },
    {
      "epoch": 4685.454545454545,
      "grad_norm": 0.00803456548601389,
      "learning_rate": 9.843782716852961e-07,
      "loss": 0.0013,
      "step": 25770
    },
    {
      "epoch": 4687.272727272727,
      "grad_norm": 0.4635373651981354,
      "learning_rate": 9.843494008740055e-07,
      "loss": 0.0013,
      "step": 25780
    },
    {
      "epoch": 4689.090909090909,
      "grad_norm": 0.00685475766658783,
      "learning_rate": 9.843205038331573e-07,
      "loss": 0.0011,
      "step": 25790
    },
    {
      "epoch": 4690.909090909091,
      "grad_norm": 0.4559224247932434,
      "learning_rate": 9.842915805643156e-07,
      "loss": 0.0013,
      "step": 25800
    },
    {
      "epoch": 4692.727272727273,
      "grad_norm": 0.007367082871496677,
      "learning_rate": 9.842626310690469e-07,
      "loss": 0.0009,
      "step": 25810
    },
    {
      "epoch": 4694.545454545455,
      "grad_norm": 0.3311075270175934,
      "learning_rate": 9.842336553489194e-07,
      "loss": 0.0013,
      "step": 25820
    },
    {
      "epoch": 4696.363636363636,
      "grad_norm": 0.5130261182785034,
      "learning_rate": 9.842046534055016e-07,
      "loss": 0.0016,
      "step": 25830
    },
    {
      "epoch": 4698.181818181818,
      "grad_norm": 0.43820276856422424,
      "learning_rate": 9.84175625240365e-07,
      "loss": 0.001,
      "step": 25840
    },
    {
      "epoch": 4700.0,
      "grad_norm": 0.07351189851760864,
      "learning_rate": 9.841465708550804e-07,
      "loss": 0.0012,
      "step": 25850
    },
    {
      "epoch": 4701.818181818182,
      "grad_norm": 0.028796304017305374,
      "learning_rate": 9.84117490251222e-07,
      "loss": 0.0013,
      "step": 25860
    },
    {
      "epoch": 4703.636363636364,
      "grad_norm": 0.4310932159423828,
      "learning_rate": 9.840883834303647e-07,
      "loss": 0.0014,
      "step": 25870
    },
    {
      "epoch": 4705.454545454545,
      "grad_norm": 0.00758338812738657,
      "learning_rate": 9.840592503940844e-07,
      "loss": 0.0007,
      "step": 25880
    },
    {
      "epoch": 4707.272727272727,
      "grad_norm": 0.07972665131092072,
      "learning_rate": 9.84030091143959e-07,
      "loss": 0.0013,
      "step": 25890
    },
    {
      "epoch": 4709.090909090909,
      "grad_norm": 0.3433420658111572,
      "learning_rate": 9.840009056815672e-07,
      "loss": 0.0012,
      "step": 25900
    },
    {
      "epoch": 4710.909090909091,
      "grad_norm": 0.4499642550945282,
      "learning_rate": 9.8397169400849e-07,
      "loss": 0.0013,
      "step": 25910
    },
    {
      "epoch": 4712.727272727273,
      "grad_norm": 0.4812331199645996,
      "learning_rate": 9.839424561263092e-07,
      "loss": 0.0009,
      "step": 25920
    },
    {
      "epoch": 4714.545454545455,
      "grad_norm": 0.29507771134376526,
      "learning_rate": 9.839131920366081e-07,
      "loss": 0.0016,
      "step": 25930
    },
    {
      "epoch": 4716.363636363636,
      "grad_norm": 0.47484734654426575,
      "learning_rate": 9.838839017409715e-07,
      "loss": 0.001,
      "step": 25940
    },
    {
      "epoch": 4718.181818181818,
      "grad_norm": 0.5771355032920837,
      "learning_rate": 9.838545852409857e-07,
      "loss": 0.0014,
      "step": 25950
    },
    {
      "epoch": 4720.0,
      "grad_norm": 36.361812591552734,
      "learning_rate": 9.838252425382379e-07,
      "loss": 0.0013,
      "step": 25960
    },
    {
      "epoch": 4721.818181818182,
      "grad_norm": 0.6652432084083557,
      "learning_rate": 9.837958736343175e-07,
      "loss": 0.0013,
      "step": 25970
    },
    {
      "epoch": 4723.636363636364,
      "grad_norm": 0.031168906018137932,
      "learning_rate": 9.837664785308148e-07,
      "loss": 0.0018,
      "step": 25980
    },
    {
      "epoch": 4725.454545454545,
      "grad_norm": 0.011714969761669636,
      "learning_rate": 9.83737057229322e-07,
      "loss": 0.0015,
      "step": 25990
    },
    {
      "epoch": 4727.272727272727,
      "grad_norm": 0.4778072237968445,
      "learning_rate": 9.837076097314318e-07,
      "loss": 0.0013,
      "step": 26000
    },
    {
      "epoch": 4727.272727272727,
      "eval_loss": 4.280333042144775,
      "eval_runtime": 0.9502,
      "eval_samples_per_second": 10.524,
      "eval_steps_per_second": 5.262,
      "step": 26000
    },
    {
      "epoch": 4729.090909090909,
      "grad_norm": 0.012472703121602535,
      "learning_rate": 9.836781360387396e-07,
      "loss": 0.0011,
      "step": 26010
    },
    {
      "epoch": 4730.909090909091,
      "grad_norm": 0.5981910228729248,
      "learning_rate": 9.83648636152841e-07,
      "loss": 0.0012,
      "step": 26020
    },
    {
      "epoch": 4732.727272727273,
      "grad_norm": 0.011435819789767265,
      "learning_rate": 9.836191100753335e-07,
      "loss": 0.0013,
      "step": 26030
    },
    {
      "epoch": 4734.545454545455,
      "grad_norm": 0.028899746015667915,
      "learning_rate": 9.835895578078163e-07,
      "loss": 0.0011,
      "step": 26040
    },
    {
      "epoch": 4736.363636363636,
      "grad_norm": 0.3899253010749817,
      "learning_rate": 9.835599793518899e-07,
      "loss": 0.0013,
      "step": 26050
    },
    {
      "epoch": 4738.181818181818,
      "grad_norm": 0.009576249867677689,
      "learning_rate": 9.83530374709156e-07,
      "loss": 0.001,
      "step": 26060
    },
    {
      "epoch": 4740.0,
      "grad_norm": 0.013133537955582142,
      "learning_rate": 9.835007438812175e-07,
      "loss": 0.0013,
      "step": 26070
    },
    {
      "epoch": 4741.818181818182,
      "grad_norm": 0.029443049803376198,
      "learning_rate": 9.834710868696794e-07,
      "loss": 0.0011,
      "step": 26080
    },
    {
      "epoch": 4743.636363636364,
      "grad_norm": 0.01532876119017601,
      "learning_rate": 9.834414036761476e-07,
      "loss": 0.0012,
      "step": 26090
    },
    {
      "epoch": 4745.454545454545,
      "grad_norm": 0.008588853292167187,
      "learning_rate": 9.834116943022297e-07,
      "loss": 0.0014,
      "step": 26100
    },
    {
      "epoch": 4747.272727272727,
      "grad_norm": 0.008694288320839405,
      "learning_rate": 9.833819587495345e-07,
      "loss": 0.0011,
      "step": 26110
    },
    {
      "epoch": 4749.090909090909,
      "grad_norm": 0.4257171154022217,
      "learning_rate": 9.83352197019672e-07,
      "loss": 0.0012,
      "step": 26120
    },
    {
      "epoch": 4750.909090909091,
      "grad_norm": 0.023542912676930428,
      "learning_rate": 9.833224091142546e-07,
      "loss": 0.0013,
      "step": 26130
    },
    {
      "epoch": 4752.727272727273,
      "grad_norm": 0.02591465227305889,
      "learning_rate": 9.83292595034895e-07,
      "loss": 0.0011,
      "step": 26140
    },
    {
      "epoch": 4754.545454545455,
      "grad_norm": 0.4740442633628845,
      "learning_rate": 9.832627547832077e-07,
      "loss": 0.0011,
      "step": 26150
    },
    {
      "epoch": 4756.363636363636,
      "grad_norm": 0.3796893060207367,
      "learning_rate": 9.832328883608088e-07,
      "loss": 0.0013,
      "step": 26160
    },
    {
      "epoch": 4758.181818181818,
      "grad_norm": 0.4154655635356903,
      "learning_rate": 9.832029957693157e-07,
      "loss": 0.0013,
      "step": 26170
    },
    {
      "epoch": 4760.0,
      "grad_norm": 0.008075930178165436,
      "learning_rate": 9.831730770103472e-07,
      "loss": 0.0012,
      "step": 26180
    },
    {
      "epoch": 4761.818181818182,
      "grad_norm": 0.5412949919700623,
      "learning_rate": 9.831431320855234e-07,
      "loss": 0.0013,
      "step": 26190
    },
    {
      "epoch": 4763.636363636364,
      "grad_norm": 0.5833303332328796,
      "learning_rate": 9.831131609964663e-07,
      "loss": 0.0012,
      "step": 26200
    },
    {
      "epoch": 4765.454545454545,
      "grad_norm": 0.37775948643684387,
      "learning_rate": 9.830831637447988e-07,
      "loss": 0.0013,
      "step": 26210
    },
    {
      "epoch": 4767.272727272727,
      "grad_norm": 0.01619267463684082,
      "learning_rate": 9.830531403321451e-07,
      "loss": 0.0008,
      "step": 26220
    },
    {
      "epoch": 4769.090909090909,
      "grad_norm": 0.5203477740287781,
      "learning_rate": 9.830230907601312e-07,
      "loss": 0.0013,
      "step": 26230
    },
    {
      "epoch": 4770.909090909091,
      "grad_norm": 0.6046490669250488,
      "learning_rate": 9.829930150303847e-07,
      "loss": 0.0013,
      "step": 26240
    },
    {
      "epoch": 4772.727272727273,
      "grad_norm": 0.3515491783618927,
      "learning_rate": 9.82962913144534e-07,
      "loss": 0.0012,
      "step": 26250
    },
    {
      "epoch": 4774.545454545455,
      "grad_norm": 0.27756455540657043,
      "learning_rate": 9.829327851042096e-07,
      "loss": 0.0012,
      "step": 26260
    },
    {
      "epoch": 4776.363636363636,
      "grad_norm": 0.016896327957510948,
      "learning_rate": 9.829026309110425e-07,
      "loss": 0.0012,
      "step": 26270
    },
    {
      "epoch": 4778.181818181818,
      "grad_norm": 0.007563983555883169,
      "learning_rate": 9.828724505666663e-07,
      "loss": 0.0012,
      "step": 26280
    },
    {
      "epoch": 4780.0,
      "grad_norm": 0.5175216794013977,
      "learning_rate": 9.82842244072715e-07,
      "loss": 0.0013,
      "step": 26290
    },
    {
      "epoch": 4781.818181818182,
      "grad_norm": 0.4349822402000427,
      "learning_rate": 9.828120114308247e-07,
      "loss": 0.0013,
      "step": 26300
    },
    {
      "epoch": 4783.636363636364,
      "grad_norm": 0.006864683702588081,
      "learning_rate": 9.827817526426323e-07,
      "loss": 0.0012,
      "step": 26310
    },
    {
      "epoch": 4785.454545454545,
      "grad_norm": 0.00646984251216054,
      "learning_rate": 9.827514677097765e-07,
      "loss": 0.0011,
      "step": 26320
    },
    {
      "epoch": 4787.272727272727,
      "grad_norm": 0.3163893222808838,
      "learning_rate": 9.827211566338977e-07,
      "loss": 0.0013,
      "step": 26330
    },
    {
      "epoch": 4789.090909090909,
      "grad_norm": 0.0056945704855024815,
      "learning_rate": 9.826908194166368e-07,
      "loss": 0.0011,
      "step": 26340
    },
    {
      "epoch": 4790.909090909091,
      "grad_norm": 0.037970565259456635,
      "learning_rate": 9.826604560596373e-07,
      "loss": 0.0012,
      "step": 26350
    },
    {
      "epoch": 4792.727272727273,
      "grad_norm": 0.38750922679901123,
      "learning_rate": 9.826300665645432e-07,
      "loss": 0.0013,
      "step": 26360
    },
    {
      "epoch": 4794.545454545455,
      "grad_norm": 0.5345938801765442,
      "learning_rate": 9.82599650933e-07,
      "loss": 0.0014,
      "step": 26370
    },
    {
      "epoch": 4796.363636363636,
      "grad_norm": 0.009878494776785374,
      "learning_rate": 9.825692091666551e-07,
      "loss": 0.0011,
      "step": 26380
    },
    {
      "epoch": 4798.181818181818,
      "grad_norm": 0.2946021258831024,
      "learning_rate": 9.825387412671569e-07,
      "loss": 0.0013,
      "step": 26390
    },
    {
      "epoch": 4800.0,
      "grad_norm": 0.3351639211177826,
      "learning_rate": 9.825082472361556e-07,
      "loss": 0.0012,
      "step": 26400
    },
    {
      "epoch": 4801.818181818182,
      "grad_norm": 0.5390931963920593,
      "learning_rate": 9.824777270753026e-07,
      "loss": 0.001,
      "step": 26410
    },
    {
      "epoch": 4803.636363636364,
      "grad_norm": 0.004924736451357603,
      "learning_rate": 9.824471807862504e-07,
      "loss": 0.0016,
      "step": 26420
    },
    {
      "epoch": 4805.454545454545,
      "grad_norm": 0.37223705649375916,
      "learning_rate": 9.824166083706534e-07,
      "loss": 0.001,
      "step": 26430
    },
    {
      "epoch": 4807.272727272727,
      "grad_norm": 0.37139350175857544,
      "learning_rate": 9.823860098301671e-07,
      "loss": 0.0014,
      "step": 26440
    },
    {
      "epoch": 4809.090909090909,
      "grad_norm": 0.35331591963768005,
      "learning_rate": 9.823553851664487e-07,
      "loss": 0.0012,
      "step": 26450
    },
    {
      "epoch": 4810.909090909091,
      "grad_norm": 0.3298826217651367,
      "learning_rate": 9.823247343811566e-07,
      "loss": 0.0012,
      "step": 26460
    },
    {
      "epoch": 4812.727272727273,
      "grad_norm": 0.005152099300175905,
      "learning_rate": 9.822940574759506e-07,
      "loss": 0.0013,
      "step": 26470
    },
    {
      "epoch": 4814.545454545455,
      "grad_norm": 0.5820537805557251,
      "learning_rate": 9.822633544524921e-07,
      "loss": 0.0013,
      "step": 26480
    },
    {
      "epoch": 4816.363636363636,
      "grad_norm": 0.28139811754226685,
      "learning_rate": 9.822326253124435e-07,
      "loss": 0.0008,
      "step": 26490
    },
    {
      "epoch": 4818.181818181818,
      "grad_norm": 0.299062043428421,
      "learning_rate": 9.822018700574694e-07,
      "loss": 0.0013,
      "step": 26500
    },
    {
      "epoch": 4818.181818181818,
      "eval_loss": 4.461052894592285,
      "eval_runtime": 0.9528,
      "eval_samples_per_second": 10.495,
      "eval_steps_per_second": 5.248,
      "step": 26500
    },
    {
      "epoch": 4820.0,
      "grad_norm": 0.4059664309024811,
      "learning_rate": 9.82171088689235e-07,
      "loss": 0.0012,
      "step": 26510
    },
    {
      "epoch": 4821.818181818182,
      "grad_norm": 0.007443238515406847,
      "learning_rate": 9.821402812094073e-07,
      "loss": 0.0011,
      "step": 26520
    },
    {
      "epoch": 4823.636363636364,
      "grad_norm": 0.24885296821594238,
      "learning_rate": 9.821094476196547e-07,
      "loss": 0.0014,
      "step": 26530
    },
    {
      "epoch": 4825.454545454545,
      "grad_norm": 0.33434388041496277,
      "learning_rate": 9.820785879216468e-07,
      "loss": 0.0009,
      "step": 26540
    },
    {
      "epoch": 4827.272727272727,
      "grad_norm": 0.6318000555038452,
      "learning_rate": 9.82047702117055e-07,
      "loss": 0.0015,
      "step": 26550
    },
    {
      "epoch": 4829.090909090909,
      "grad_norm": 0.40244442224502563,
      "learning_rate": 9.820167902075519e-07,
      "loss": 0.0011,
      "step": 26560
    },
    {
      "epoch": 4830.909090909091,
      "grad_norm": 0.4629833698272705,
      "learning_rate": 9.819858521948112e-07,
      "loss": 0.0012,
      "step": 26570
    },
    {
      "epoch": 4832.727272727273,
      "grad_norm": 0.5434861779212952,
      "learning_rate": 9.819548880805087e-07,
      "loss": 0.0013,
      "step": 26580
    },
    {
      "epoch": 4834.545454545455,
      "grad_norm": 0.007156160660088062,
      "learning_rate": 9.819238978663212e-07,
      "loss": 0.0007,
      "step": 26590
    },
    {
      "epoch": 4836.363636363636,
      "grad_norm": 0.007780243176966906,
      "learning_rate": 9.818928815539265e-07,
      "loss": 0.0015,
      "step": 26600
    },
    {
      "epoch": 4838.181818181818,
      "grad_norm": 0.4957481026649475,
      "learning_rate": 9.818618391450049e-07,
      "loss": 0.0016,
      "step": 26610
    },
    {
      "epoch": 4840.0,
      "grad_norm": 0.011924011632800102,
      "learning_rate": 9.81830770641237e-07,
      "loss": 0.001,
      "step": 26620
    },
    {
      "epoch": 4841.818181818182,
      "grad_norm": 0.3758350908756256,
      "learning_rate": 9.817996760443055e-07,
      "loss": 0.0011,
      "step": 26630
    },
    {
      "epoch": 4843.636363636364,
      "grad_norm": 0.017677249386906624,
      "learning_rate": 9.817685553558943e-07,
      "loss": 0.0011,
      "step": 26640
    },
    {
      "epoch": 4845.454545454545,
      "grad_norm": 0.502585232257843,
      "learning_rate": 9.817374085776886e-07,
      "loss": 0.0014,
      "step": 26650
    },
    {
      "epoch": 4847.272727272727,
      "grad_norm": 0.014106040820479393,
      "learning_rate": 9.817062357113752e-07,
      "loss": 0.0012,
      "step": 26660
    },
    {
      "epoch": 4849.090909090909,
      "grad_norm": 0.010748750530183315,
      "learning_rate": 9.816750367586424e-07,
      "loss": 0.0011,
      "step": 26670
    },
    {
      "epoch": 4850.909090909091,
      "grad_norm": 0.3930909037590027,
      "learning_rate": 9.816438117211796e-07,
      "loss": 0.0011,
      "step": 26680
    },
    {
      "epoch": 4852.727272727273,
      "grad_norm": 0.0057161725126206875,
      "learning_rate": 9.816125606006775e-07,
      "loss": 0.0014,
      "step": 26690
    },
    {
      "epoch": 4854.545454545455,
      "grad_norm": 0.3230098485946655,
      "learning_rate": 9.81581283398829e-07,
      "loss": 0.0014,
      "step": 26700
    },
    {
      "epoch": 4856.363636363636,
      "grad_norm": 0.01480866875499487,
      "learning_rate": 9.815499801173277e-07,
      "loss": 0.001,
      "step": 26710
    },
    {
      "epoch": 4858.181818181818,
      "grad_norm": 0.40016674995422363,
      "learning_rate": 9.815186507578685e-07,
      "loss": 0.0013,
      "step": 26720
    },
    {
      "epoch": 4860.0,
      "grad_norm": 0.007902732118964195,
      "learning_rate": 9.814872953221485e-07,
      "loss": 0.0012,
      "step": 26730
    },
    {
      "epoch": 4861.818181818182,
      "grad_norm": 0.012415993958711624,
      "learning_rate": 9.814559138118655e-07,
      "loss": 0.0011,
      "step": 26740
    },
    {
      "epoch": 4863.636363636364,
      "grad_norm": 0.3410586416721344,
      "learning_rate": 9.814245062287188e-07,
      "loss": 0.0013,
      "step": 26750
    },
    {
      "epoch": 4865.454545454545,
      "grad_norm": 0.42638397216796875,
      "learning_rate": 9.813930725744094e-07,
      "loss": 0.0013,
      "step": 26760
    },
    {
      "epoch": 4867.272727272727,
      "grad_norm": 0.31658026576042175,
      "learning_rate": 9.813616128506395e-07,
      "loss": 0.0011,
      "step": 26770
    },
    {
      "epoch": 4869.090909090909,
      "grad_norm": 0.012109394185245037,
      "learning_rate": 9.81330127059113e-07,
      "loss": 0.0012,
      "step": 26780
    },
    {
      "epoch": 4870.909090909091,
      "grad_norm": 0.4768311679363251,
      "learning_rate": 9.812986152015347e-07,
      "loss": 0.0013,
      "step": 26790
    },
    {
      "epoch": 4872.727272727273,
      "grad_norm": 0.3195146918296814,
      "learning_rate": 9.812670772796112e-07,
      "loss": 0.0012,
      "step": 26800
    },
    {
      "epoch": 4874.545454545455,
      "grad_norm": 0.02498691715300083,
      "learning_rate": 9.812355132950506e-07,
      "loss": 0.0011,
      "step": 26810
    },
    {
      "epoch": 4876.363636363636,
      "grad_norm": 0.5747798681259155,
      "learning_rate": 9.81203923249562e-07,
      "loss": 0.0013,
      "step": 26820
    },
    {
      "epoch": 4878.181818181818,
      "grad_norm": 0.30585408210754395,
      "learning_rate": 9.81172307144856e-07,
      "loss": 0.0011,
      "step": 26830
    },
    {
      "epoch": 4880.0,
      "grad_norm": 0.44059959053993225,
      "learning_rate": 9.81140664982645e-07,
      "loss": 0.0012,
      "step": 26840
    },
    {
      "epoch": 4881.818181818182,
      "grad_norm": 0.011030429042875767,
      "learning_rate": 9.811089967646427e-07,
      "loss": 0.0013,
      "step": 26850
    },
    {
      "epoch": 4883.636363636364,
      "grad_norm": 0.44514229893684387,
      "learning_rate": 9.810773024925637e-07,
      "loss": 0.0009,
      "step": 26860
    },
    {
      "epoch": 4885.454545454545,
      "grad_norm": 0.007127151824533939,
      "learning_rate": 9.810455821681246e-07,
      "loss": 0.0011,
      "step": 26870
    },
    {
      "epoch": 4887.272727272727,
      "grad_norm": 0.33143049478530884,
      "learning_rate": 9.81013835793043e-07,
      "loss": 0.0015,
      "step": 26880
    },
    {
      "epoch": 4889.090909090909,
      "grad_norm": 0.4054858684539795,
      "learning_rate": 9.809820633690381e-07,
      "loss": 0.0012,
      "step": 26890
    },
    {
      "epoch": 4890.909090909091,
      "grad_norm": 0.5241934061050415,
      "learning_rate": 9.80950264897831e-07,
      "loss": 0.0013,
      "step": 26900
    },
    {
      "epoch": 4892.727272727273,
      "grad_norm": 0.4319535791873932,
      "learning_rate": 9.80918440381143e-07,
      "loss": 0.0012,
      "step": 26910
    },
    {
      "epoch": 4894.545454545455,
      "grad_norm": 0.39027830958366394,
      "learning_rate": 9.80886589820698e-07,
      "loss": 0.0011,
      "step": 26920
    },
    {
      "epoch": 4896.363636363636,
      "grad_norm": 0.010687543079257011,
      "learning_rate": 9.808547132182208e-07,
      "loss": 0.0014,
      "step": 26930
    },
    {
      "epoch": 4898.181818181818,
      "grad_norm": 0.45012927055358887,
      "learning_rate": 9.808228105754375e-07,
      "loss": 0.0012,
      "step": 26940
    },
    {
      "epoch": 4900.0,
      "grad_norm": 0.5734452605247498,
      "learning_rate": 9.80790881894076e-07,
      "loss": 0.0013,
      "step": 26950
    },
    {
      "epoch": 4901.818181818182,
      "grad_norm": 0.006508914288133383,
      "learning_rate": 9.80758927175865e-07,
      "loss": 0.0011,
      "step": 26960
    },
    {
      "epoch": 4903.636363636364,
      "grad_norm": 0.3908453583717346,
      "learning_rate": 9.807269464225353e-07,
      "loss": 0.0013,
      "step": 26970
    },
    {
      "epoch": 4905.454545454545,
      "grad_norm": 0.2950055003166199,
      "learning_rate": 9.806949396358187e-07,
      "loss": 0.0016,
      "step": 26980
    },
    {
      "epoch": 4907.272727272727,
      "grad_norm": 0.3955897390842438,
      "learning_rate": 9.806629068174485e-07,
      "loss": 0.0011,
      "step": 26990
    },
    {
      "epoch": 4909.090909090909,
      "grad_norm": 0.3191215693950653,
      "learning_rate": 9.806308479691594e-07,
      "loss": 0.0013,
      "step": 27000
    },
    {
      "epoch": 4909.090909090909,
      "eval_loss": 4.399933338165283,
      "eval_runtime": 0.9543,
      "eval_samples_per_second": 10.479,
      "eval_steps_per_second": 5.239,
      "step": 27000
    },
    {
      "epoch": 4910.909090909091,
      "grad_norm": 0.006792427506297827,
      "learning_rate": 9.805987630926875e-07,
      "loss": 0.0011,
      "step": 27010
    },
    {
      "epoch": 4912.727272727273,
      "grad_norm": 0.008799386210739613,
      "learning_rate": 9.805666521897705e-07,
      "loss": 0.0011,
      "step": 27020
    },
    {
      "epoch": 4914.545454545455,
      "grad_norm": 0.006780198775231838,
      "learning_rate": 9.805345152621469e-07,
      "loss": 0.001,
      "step": 27030
    },
    {
      "epoch": 4916.363636363636,
      "grad_norm": 0.2787328362464905,
      "learning_rate": 9.805023523115574e-07,
      "loss": 0.0014,
      "step": 27040
    },
    {
      "epoch": 4918.181818181818,
      "grad_norm": 0.3184119164943695,
      "learning_rate": 9.80470163339744e-07,
      "loss": 0.0012,
      "step": 27050
    },
    {
      "epoch": 4920.0,
      "grad_norm": 0.44536641240119934,
      "learning_rate": 9.804379483484492e-07,
      "loss": 0.0012,
      "step": 27060
    },
    {
      "epoch": 4921.818181818182,
      "grad_norm": 0.01377890445291996,
      "learning_rate": 9.804057073394182e-07,
      "loss": 0.0012,
      "step": 27070
    },
    {
      "epoch": 4923.636363636364,
      "grad_norm": 0.4970717430114746,
      "learning_rate": 9.803734403143966e-07,
      "loss": 0.0011,
      "step": 27080
    },
    {
      "epoch": 4925.454545454545,
      "grad_norm": 0.3690048158168793,
      "learning_rate": 9.80341147275132e-07,
      "loss": 0.0012,
      "step": 27090
    },
    {
      "epoch": 4927.272727272727,
      "grad_norm": 0.27580195665359497,
      "learning_rate": 9.803088282233732e-07,
      "loss": 0.0014,
      "step": 27100
    },
    {
      "epoch": 4929.090909090909,
      "grad_norm": 0.4428585171699524,
      "learning_rate": 9.802764831608703e-07,
      "loss": 0.0013,
      "step": 27110
    },
    {
      "epoch": 4930.909090909091,
      "grad_norm": 0.34489110112190247,
      "learning_rate": 9.802441120893748e-07,
      "loss": 0.0011,
      "step": 27120
    },
    {
      "epoch": 4932.727272727273,
      "grad_norm": 0.4871019721031189,
      "learning_rate": 9.8021171501064e-07,
      "loss": 0.0011,
      "step": 27130
    },
    {
      "epoch": 4934.545454545455,
      "grad_norm": 0.4951339662075043,
      "learning_rate": 9.801792919264204e-07,
      "loss": 0.0012,
      "step": 27140
    },
    {
      "epoch": 4936.363636363636,
      "grad_norm": 0.3127773106098175,
      "learning_rate": 9.801468428384716e-07,
      "loss": 0.0013,
      "step": 27150
    },
    {
      "epoch": 4938.181818181818,
      "grad_norm": 0.004916610196232796,
      "learning_rate": 9.801143677485508e-07,
      "loss": 0.001,
      "step": 27160
    },
    {
      "epoch": 4940.0,
      "grad_norm": 0.35085833072662354,
      "learning_rate": 9.80081866658417e-07,
      "loss": 0.0013,
      "step": 27170
    },
    {
      "epoch": 4941.818181818182,
      "grad_norm": 0.3251734972000122,
      "learning_rate": 9.800493395698299e-07,
      "loss": 0.0014,
      "step": 27180
    },
    {
      "epoch": 4943.636363636364,
      "grad_norm": 0.012810426764190197,
      "learning_rate": 9.800167864845512e-07,
      "loss": 0.0013,
      "step": 27190
    },
    {
      "epoch": 4945.454545454545,
      "grad_norm": 0.2850385308265686,
      "learning_rate": 9.799842074043438e-07,
      "loss": 0.0008,
      "step": 27200
    },
    {
      "epoch": 4947.272727272727,
      "grad_norm": 0.01936054229736328,
      "learning_rate": 9.799516023309717e-07,
      "loss": 0.0012,
      "step": 27210
    },
    {
      "epoch": 4949.090909090909,
      "grad_norm": 0.007867589592933655,
      "learning_rate": 9.79918971266201e-07,
      "loss": 0.0013,
      "step": 27220
    },
    {
      "epoch": 4950.909090909091,
      "grad_norm": 0.5179536938667297,
      "learning_rate": 9.798863142117987e-07,
      "loss": 0.0012,
      "step": 27230
    },
    {
      "epoch": 4952.727272727273,
      "grad_norm": 0.02725145034492016,
      "learning_rate": 9.798536311695334e-07,
      "loss": 0.0014,
      "step": 27240
    },
    {
      "epoch": 4954.545454545455,
      "grad_norm": 0.009533396922051907,
      "learning_rate": 9.798209221411746e-07,
      "loss": 0.001,
      "step": 27250
    },
    {
      "epoch": 4956.363636363636,
      "grad_norm": 0.36383575201034546,
      "learning_rate": 9.79788187128494e-07,
      "loss": 0.0015,
      "step": 27260
    },
    {
      "epoch": 4958.181818181818,
      "grad_norm": 0.0061475918628275394,
      "learning_rate": 9.797554261332643e-07,
      "loss": 0.001,
      "step": 27270
    },
    {
      "epoch": 4960.0,
      "grad_norm": 0.5290755033493042,
      "learning_rate": 9.797226391572595e-07,
      "loss": 0.0014,
      "step": 27280
    },
    {
      "epoch": 4961.818181818182,
      "grad_norm": 0.034666795283555984,
      "learning_rate": 9.796898262022554e-07,
      "loss": 0.0013,
      "step": 27290
    },
    {
      "epoch": 4963.636363636364,
      "grad_norm": 0.47560906410217285,
      "learning_rate": 9.796569872700287e-07,
      "loss": 0.0012,
      "step": 27300
    },
    {
      "epoch": 4965.454545454545,
      "grad_norm": 0.2651514708995819,
      "learning_rate": 9.796241223623579e-07,
      "loss": 0.0008,
      "step": 27310
    },
    {
      "epoch": 4967.272727272727,
      "grad_norm": 0.5344234704971313,
      "learning_rate": 9.795912314810228e-07,
      "loss": 0.0017,
      "step": 27320
    },
    {
      "epoch": 4969.090909090909,
      "grad_norm": 0.00873031560331583,
      "learning_rate": 9.795583146278044e-07,
      "loss": 0.0008,
      "step": 27330
    },
    {
      "epoch": 4970.909090909091,
      "grad_norm": 0.40774449706077576,
      "learning_rate": 9.795253718044856e-07,
      "loss": 0.0012,
      "step": 27340
    },
    {
      "epoch": 4972.727272727273,
      "grad_norm": 0.29799821972846985,
      "learning_rate": 9.7949240301285e-07,
      "loss": 0.0011,
      "step": 27350
    },
    {
      "epoch": 4974.545454545455,
      "grad_norm": 0.3275925815105438,
      "learning_rate": 9.794594082546834e-07,
      "loss": 0.0014,
      "step": 27360
    },
    {
      "epoch": 4976.363636363636,
      "grad_norm": 0.04107749089598656,
      "learning_rate": 9.794263875317724e-07,
      "loss": 0.0011,
      "step": 27370
    },
    {
      "epoch": 4978.181818181818,
      "grad_norm": 0.008039221167564392,
      "learning_rate": 9.793933408459052e-07,
      "loss": 0.001,
      "step": 27380
    },
    {
      "epoch": 4980.0,
      "grad_norm": 0.005997644737362862,
      "learning_rate": 9.793602681988712e-07,
      "loss": 0.0013,
      "step": 27390
    },
    {
      "epoch": 4981.818181818182,
      "grad_norm": 0.47157022356987,
      "learning_rate": 9.793271695924621e-07,
      "loss": 0.0011,
      "step": 27400
    },
    {
      "epoch": 4983.636363636364,
      "grad_norm": 0.43478819727897644,
      "learning_rate": 9.792940450284696e-07,
      "loss": 0.0015,
      "step": 27410
    },
    {
      "epoch": 4985.454545454545,
      "grad_norm": 0.006631506606936455,
      "learning_rate": 9.792608945086879e-07,
      "loss": 0.0013,
      "step": 27420
    },
    {
      "epoch": 4987.272727272727,
      "grad_norm": 0.44379672408103943,
      "learning_rate": 9.792277180349122e-07,
      "loss": 0.001,
      "step": 27430
    },
    {
      "epoch": 4989.090909090909,
      "grad_norm": 0.5388143658638,
      "learning_rate": 9.79194515608939e-07,
      "loss": 0.0013,
      "step": 27440
    },
    {
      "epoch": 4990.909090909091,
      "grad_norm": 0.33775269985198975,
      "learning_rate": 9.791612872325666e-07,
      "loss": 0.0012,
      "step": 27450
    },
    {
      "epoch": 4992.727272727273,
      "grad_norm": 0.47025421261787415,
      "learning_rate": 9.791280329075942e-07,
      "loss": 0.0012,
      "step": 27460
    },
    {
      "epoch": 4994.545454545455,
      "grad_norm": 0.3208547532558441,
      "learning_rate": 9.790947526358228e-07,
      "loss": 0.0014,
      "step": 27470
    },
    {
      "epoch": 4996.363636363636,
      "grad_norm": 0.34963327646255493,
      "learning_rate": 9.790614464190547e-07,
      "loss": 0.0012,
      "step": 27480
    },
    {
      "epoch": 4998.181818181818,
      "grad_norm": 0.38347575068473816,
      "learning_rate": 9.790281142590936e-07,
      "loss": 0.0011,
      "step": 27490
    },
    {
      "epoch": 5000.0,
      "grad_norm": 0.4863048791885376,
      "learning_rate": 9.789947561577443e-07,
      "loss": 0.0012,
      "step": 27500
    },
    {
      "epoch": 5000.0,
      "eval_loss": 4.349274158477783,
      "eval_runtime": 0.9578,
      "eval_samples_per_second": 10.44,
      "eval_steps_per_second": 5.22,
      "step": 27500
    },
    {
      "epoch": 5001.818181818182,
      "grad_norm": 0.010933422483503819,
      "learning_rate": 9.789613721168138e-07,
      "loss": 0.0011,
      "step": 27510
    },
    {
      "epoch": 5003.636363636364,
      "grad_norm": 0.5209160447120667,
      "learning_rate": 9.789279621381094e-07,
      "loss": 0.0015,
      "step": 27520
    },
    {
      "epoch": 5005.454545454545,
      "grad_norm": 0.3547620475292206,
      "learning_rate": 9.788945262234406e-07,
      "loss": 0.0009,
      "step": 27530
    },
    {
      "epoch": 5007.272727272727,
      "grad_norm": 0.2772221565246582,
      "learning_rate": 9.788610643746182e-07,
      "loss": 0.0013,
      "step": 27540
    },
    {
      "epoch": 5009.090909090909,
      "grad_norm": 0.009326397441327572,
      "learning_rate": 9.788275765934543e-07,
      "loss": 0.0011,
      "step": 27550
    },
    {
      "epoch": 5010.909090909091,
      "grad_norm": 0.3442182242870331,
      "learning_rate": 9.787940628817627e-07,
      "loss": 0.0012,
      "step": 27560
    },
    {
      "epoch": 5012.727272727273,
      "grad_norm": 0.01256218645721674,
      "learning_rate": 9.787605232413575e-07,
      "loss": 0.0012,
      "step": 27570
    },
    {
      "epoch": 5014.545454545455,
      "grad_norm": 0.3292871415615082,
      "learning_rate": 9.787269576740556e-07,
      "loss": 0.0013,
      "step": 27580
    },
    {
      "epoch": 5016.363636363636,
      "grad_norm": 0.027983447536826134,
      "learning_rate": 9.786933661816745e-07,
      "loss": 0.0009,
      "step": 27590
    },
    {
      "epoch": 5018.181818181818,
      "grad_norm": 0.0061206091195344925,
      "learning_rate": 9.786597487660335e-07,
      "loss": 0.0013,
      "step": 27600
    },
    {
      "epoch": 5020.0,
      "grad_norm": 0.36300331354141235,
      "learning_rate": 9.786261054289531e-07,
      "loss": 0.0013,
      "step": 27610
    },
    {
      "epoch": 5021.818181818182,
      "grad_norm": 0.5693950057029724,
      "learning_rate": 9.78592436172255e-07,
      "loss": 0.0013,
      "step": 27620
    },
    {
      "epoch": 5023.636363636364,
      "grad_norm": 0.3428829610347748,
      "learning_rate": 9.78558740997763e-07,
      "loss": 0.001,
      "step": 27630
    },
    {
      "epoch": 5025.454545454545,
      "grad_norm": 0.004795791581273079,
      "learning_rate": 9.785250199073015e-07,
      "loss": 0.0011,
      "step": 27640
    },
    {
      "epoch": 5027.272727272727,
      "grad_norm": 0.0140494704246521,
      "learning_rate": 9.784912729026965e-07,
      "loss": 0.0016,
      "step": 27650
    },
    {
      "epoch": 5029.090909090909,
      "grad_norm": 0.015370244160294533,
      "learning_rate": 9.784574999857756e-07,
      "loss": 0.001,
      "step": 27660
    },
    {
      "epoch": 5030.909090909091,
      "grad_norm": 0.5100921988487244,
      "learning_rate": 9.78423701158368e-07,
      "loss": 0.0012,
      "step": 27670
    },
    {
      "epoch": 5032.727272727273,
      "grad_norm": 0.5325983762741089,
      "learning_rate": 9.783898764223039e-07,
      "loss": 0.001,
      "step": 27680
    },
    {
      "epoch": 5034.545454545455,
      "grad_norm": 0.010755226016044617,
      "learning_rate": 9.783560257794152e-07,
      "loss": 0.0015,
      "step": 27690
    },
    {
      "epoch": 5036.363636363636,
      "grad_norm": 0.012465202249586582,
      "learning_rate": 9.783221492315348e-07,
      "loss": 0.001,
      "step": 27700
    },
    {
      "epoch": 5038.181818181818,
      "grad_norm": 0.32696303725242615,
      "learning_rate": 9.782882467804974e-07,
      "loss": 0.0013,
      "step": 27710
    },
    {
      "epoch": 5040.0,
      "grad_norm": 0.39337193965911865,
      "learning_rate": 9.782543184281388e-07,
      "loss": 0.0013,
      "step": 27720
    },
    {
      "epoch": 5041.818181818182,
      "grad_norm": 0.008952585980296135,
      "learning_rate": 9.782203641762968e-07,
      "loss": 0.0009,
      "step": 27730
    },
    {
      "epoch": 5043.636363636364,
      "grad_norm": 0.31929367780685425,
      "learning_rate": 9.781863840268096e-07,
      "loss": 0.0015,
      "step": 27740
    },
    {
      "epoch": 5045.454545454545,
      "grad_norm": 0.5324215888977051,
      "learning_rate": 9.781523779815178e-07,
      "loss": 0.0013,
      "step": 27750
    },
    {
      "epoch": 5047.272727272727,
      "grad_norm": 0.32304647564888,
      "learning_rate": 9.781183460422626e-07,
      "loss": 0.0014,
      "step": 27760
    },
    {
      "epoch": 5049.090909090909,
      "grad_norm": 0.4281565248966217,
      "learning_rate": 9.780842882108872e-07,
      "loss": 0.0011,
      "step": 27770
    },
    {
      "epoch": 5050.909090909091,
      "grad_norm": 0.47963613271713257,
      "learning_rate": 9.78050204489236e-07,
      "loss": 0.0012,
      "step": 27780
    },
    {
      "epoch": 5052.727272727273,
      "grad_norm": 0.013543850742280483,
      "learning_rate": 9.78016094879155e-07,
      "loss": 0.001,
      "step": 27790
    },
    {
      "epoch": 5054.545454545455,
      "grad_norm": 0.005436878651380539,
      "learning_rate": 9.779819593824907e-07,
      "loss": 0.0013,
      "step": 27800
    },
    {
      "epoch": 5056.363636363636,
      "grad_norm": 0.020069953054189682,
      "learning_rate": 9.779477980010922e-07,
      "loss": 0.0014,
      "step": 27810
    },
    {
      "epoch": 5058.181818181818,
      "grad_norm": 0.03254805505275726,
      "learning_rate": 9.779136107368096e-07,
      "loss": 0.0011,
      "step": 27820
    },
    {
      "epoch": 5060.0,
      "grad_norm": 0.3774591386318207,
      "learning_rate": 9.778793975914938e-07,
      "loss": 0.0009,
      "step": 27830
    },
    {
      "epoch": 5061.818181818182,
      "grad_norm": 0.5385832190513611,
      "learning_rate": 9.77845158566998e-07,
      "loss": 0.0013,
      "step": 27840
    },
    {
      "epoch": 5063.636363636364,
      "grad_norm": 0.0051193502731621265,
      "learning_rate": 9.77810893665176e-07,
      "loss": 0.0011,
      "step": 27850
    },
    {
      "epoch": 5065.454545454545,
      "grad_norm": 0.004659919999539852,
      "learning_rate": 9.777766028878837e-07,
      "loss": 0.001,
      "step": 27860
    },
    {
      "epoch": 5067.272727272727,
      "grad_norm": 0.010875671170651913,
      "learning_rate": 9.777422862369783e-07,
      "loss": 0.0014,
      "step": 27870
    },
    {
      "epoch": 5069.090909090909,
      "grad_norm": 0.015215435065329075,
      "learning_rate": 9.777079437143176e-07,
      "loss": 0.001,
      "step": 27880
    },
    {
      "epoch": 5070.909090909091,
      "grad_norm": 0.37452930212020874,
      "learning_rate": 9.776735753217617e-07,
      "loss": 0.0012,
      "step": 27890
    },
    {
      "epoch": 5072.727272727273,
      "grad_norm": 0.4849729537963867,
      "learning_rate": 9.776391810611718e-07,
      "loss": 0.0013,
      "step": 27900
    },
    {
      "epoch": 5074.545454545455,
      "grad_norm": 0.007175810169428587,
      "learning_rate": 9.776047609344104e-07,
      "loss": 0.0012,
      "step": 27910
    },
    {
      "epoch": 5076.363636363636,
      "grad_norm": 0.007242729887366295,
      "learning_rate": 9.775703149433417e-07,
      "loss": 0.0011,
      "step": 27920
    },
    {
      "epoch": 5078.181818181818,
      "grad_norm": 0.00892671849578619,
      "learning_rate": 9.77535843089831e-07,
      "loss": 0.0012,
      "step": 27930
    },
    {
      "epoch": 5080.0,
      "grad_norm": 0.16211667656898499,
      "learning_rate": 9.77501345375745e-07,
      "loss": 0.0013,
      "step": 27940
    },
    {
      "epoch": 5081.818181818182,
      "grad_norm": 0.07076752185821533,
      "learning_rate": 9.774668218029519e-07,
      "loss": 0.0013,
      "step": 27950
    },
    {
      "epoch": 5083.636363636364,
      "grad_norm": 0.008569820784032345,
      "learning_rate": 9.774322723733214e-07,
      "loss": 0.0021,
      "step": 27960
    },
    {
      "epoch": 5085.454545454545,
      "grad_norm": 0.5173150300979614,
      "learning_rate": 9.773976970887245e-07,
      "loss": 0.0018,
      "step": 27970
    },
    {
      "epoch": 5087.272727272727,
      "grad_norm": 0.4093817472457886,
      "learning_rate": 9.773630959510336e-07,
      "loss": 0.0014,
      "step": 27980
    },
    {
      "epoch": 5089.090909090909,
      "grad_norm": 0.4278257489204407,
      "learning_rate": 9.773284689621222e-07,
      "loss": 0.0009,
      "step": 27990
    },
    {
      "epoch": 5090.909090909091,
      "grad_norm": 0.026619406417012215,
      "learning_rate": 9.77293816123866e-07,
      "loss": 0.0013,
      "step": 28000
    },
    {
      "epoch": 5090.909090909091,
      "eval_loss": 4.445250511169434,
      "eval_runtime": 0.953,
      "eval_samples_per_second": 10.493,
      "eval_steps_per_second": 5.247,
      "step": 28000
    },
    {
      "epoch": 5092.727272727273,
      "grad_norm": 0.027307016775012016,
      "learning_rate": 9.772591374381412e-07,
      "loss": 0.001,
      "step": 28010
    },
    {
      "epoch": 5094.545454545455,
      "grad_norm": 0.006596532184630632,
      "learning_rate": 9.77224432906826e-07,
      "loss": 0.0013,
      "step": 28020
    },
    {
      "epoch": 5096.363636363636,
      "grad_norm": 0.38405564427375793,
      "learning_rate": 9.771897025317997e-07,
      "loss": 0.0013,
      "step": 28030
    },
    {
      "epoch": 5098.181818181818,
      "grad_norm": 0.024767983704805374,
      "learning_rate": 9.771549463149431e-07,
      "loss": 0.0013,
      "step": 28040
    },
    {
      "epoch": 5100.0,
      "grad_norm": 0.03287940099835396,
      "learning_rate": 9.771201642581384e-07,
      "loss": 0.001,
      "step": 28050
    },
    {
      "epoch": 5101.818181818182,
      "grad_norm": 0.006900948006659746,
      "learning_rate": 9.770853563632693e-07,
      "loss": 0.0012,
      "step": 28060
    },
    {
      "epoch": 5103.636363636364,
      "grad_norm": 0.010474338196218014,
      "learning_rate": 9.770505226322208e-07,
      "loss": 0.001,
      "step": 28070
    },
    {
      "epoch": 5105.454545454545,
      "grad_norm": 0.4585023820400238,
      "learning_rate": 9.770156630668788e-07,
      "loss": 0.0013,
      "step": 28080
    },
    {
      "epoch": 5107.272727272727,
      "grad_norm": 0.010589390061795712,
      "learning_rate": 9.769807776691319e-07,
      "loss": 0.001,
      "step": 28090
    },
    {
      "epoch": 5109.090909090909,
      "grad_norm": 0.02105652168393135,
      "learning_rate": 9.769458664408686e-07,
      "loss": 0.0015,
      "step": 28100
    },
    {
      "epoch": 5110.909090909091,
      "grad_norm": 0.03441723436117172,
      "learning_rate": 9.769109293839802e-07,
      "loss": 0.001,
      "step": 28110
    },
    {
      "epoch": 5112.727272727273,
      "grad_norm": 0.009300128556787968,
      "learning_rate": 9.76875966500358e-07,
      "loss": 0.0012,
      "step": 28120
    },
    {
      "epoch": 5114.545454545455,
      "grad_norm": 0.04181583598256111,
      "learning_rate": 9.768409777918956e-07,
      "loss": 0.0014,
      "step": 28130
    },
    {
      "epoch": 5116.363636363636,
      "grad_norm": 0.39718255400657654,
      "learning_rate": 9.768059632604879e-07,
      "loss": 0.001,
      "step": 28140
    },
    {
      "epoch": 5118.181818181818,
      "grad_norm": 0.007920043542981148,
      "learning_rate": 9.76770922908031e-07,
      "loss": 0.0011,
      "step": 28150
    },
    {
      "epoch": 5120.0,
      "grad_norm": 0.004895200487226248,
      "learning_rate": 9.767358567364225e-07,
      "loss": 0.0013,
      "step": 28160
    },
    {
      "epoch": 5121.818181818182,
      "grad_norm": 0.006769638974219561,
      "learning_rate": 9.767007647475615e-07,
      "loss": 0.0012,
      "step": 28170
    },
    {
      "epoch": 5123.636363636364,
      "grad_norm": 0.2887157201766968,
      "learning_rate": 9.766656469433482e-07,
      "loss": 0.0012,
      "step": 28180
    },
    {
      "epoch": 5125.454545454545,
      "grad_norm": 0.3748144507408142,
      "learning_rate": 9.766305033256845e-07,
      "loss": 0.0011,
      "step": 28190
    },
    {
      "epoch": 5127.272727272727,
      "grad_norm": 0.007093712221831083,
      "learning_rate": 9.765953338964734e-07,
      "loss": 0.0011,
      "step": 28200
    },
    {
      "epoch": 5129.090909090909,
      "grad_norm": 1.0823770761489868,
      "learning_rate": 9.765601386576196e-07,
      "loss": 0.0015,
      "step": 28210
    },
    {
      "epoch": 5130.909090909091,
      "grad_norm": 0.30212366580963135,
      "learning_rate": 9.765249176110292e-07,
      "loss": 0.0011,
      "step": 28220
    },
    {
      "epoch": 5132.727272727273,
      "grad_norm": 0.44347333908081055,
      "learning_rate": 9.764896707586094e-07,
      "loss": 0.0013,
      "step": 28230
    },
    {
      "epoch": 5134.545454545455,
      "grad_norm": 0.3774779736995697,
      "learning_rate": 9.76454398102269e-07,
      "loss": 0.001,
      "step": 28240
    },
    {
      "epoch": 5136.363636363636,
      "grad_norm": 0.016108712181448936,
      "learning_rate": 9.76419099643918e-07,
      "loss": 0.0013,
      "step": 28250
    },
    {
      "epoch": 5138.181818181818,
      "grad_norm": 0.06964496523141861,
      "learning_rate": 9.763837753854682e-07,
      "loss": 0.0012,
      "step": 28260
    },
    {
      "epoch": 5140.0,
      "grad_norm": 0.004909145645797253,
      "learning_rate": 9.763484253288325e-07,
      "loss": 0.0013,
      "step": 28270
    },
    {
      "epoch": 5141.818181818182,
      "grad_norm": 0.017575306817889214,
      "learning_rate": 9.763130494759252e-07,
      "loss": 0.0011,
      "step": 28280
    },
    {
      "epoch": 5143.636363636364,
      "grad_norm": 0.38954317569732666,
      "learning_rate": 9.76277647828662e-07,
      "loss": 0.0012,
      "step": 28290
    },
    {
      "epoch": 5145.454545454545,
      "grad_norm": 0.011791289784014225,
      "learning_rate": 9.762422203889604e-07,
      "loss": 0.0013,
      "step": 28300
    },
    {
      "epoch": 5147.272727272727,
      "grad_norm": 0.01643882878124714,
      "learning_rate": 9.762067671587384e-07,
      "loss": 0.0012,
      "step": 28310
    },
    {
      "epoch": 5149.090909090909,
      "grad_norm": 0.4029315710067749,
      "learning_rate": 9.761712881399164e-07,
      "loss": 0.0013,
      "step": 28320
    },
    {
      "epoch": 5150.909090909091,
      "grad_norm": 0.397155225276947,
      "learning_rate": 9.761357833344153e-07,
      "loss": 0.0011,
      "step": 28330
    },
    {
      "epoch": 5152.727272727273,
      "grad_norm": 0.006575020961463451,
      "learning_rate": 9.761002527441584e-07,
      "loss": 0.0011,
      "step": 28340
    },
    {
      "epoch": 5154.545454545455,
      "grad_norm": 0.004985159263014793,
      "learning_rate": 9.760646963710692e-07,
      "loss": 0.0012,
      "step": 28350
    },
    {
      "epoch": 5156.363636363636,
      "grad_norm": 0.045545559376478195,
      "learning_rate": 9.760291142170738e-07,
      "loss": 0.0014,
      "step": 28360
    },
    {
      "epoch": 5158.181818181818,
      "grad_norm": 0.02233058772981167,
      "learning_rate": 9.759935062840989e-07,
      "loss": 0.0016,
      "step": 28370
    },
    {
      "epoch": 5160.0,
      "grad_norm": 17.269895553588867,
      "learning_rate": 9.759578725740727e-07,
      "loss": 0.0014,
      "step": 28380
    },
    {
      "epoch": 5161.818181818182,
      "grad_norm": 0.5667667984962463,
      "learning_rate": 9.75922213088925e-07,
      "loss": 0.0013,
      "step": 28390
    },
    {
      "epoch": 5163.636363636364,
      "grad_norm": 0.05493864417076111,
      "learning_rate": 9.758865278305867e-07,
      "loss": 0.0011,
      "step": 28400
    },
    {
      "epoch": 5165.454545454545,
      "grad_norm": 0.0713069885969162,
      "learning_rate": 9.758508168009907e-07,
      "loss": 0.0012,
      "step": 28410
    },
    {
      "epoch": 5167.272727272727,
      "grad_norm": 0.04778531566262245,
      "learning_rate": 9.75815080002071e-07,
      "loss": 0.0011,
      "step": 28420
    },
    {
      "epoch": 5169.090909090909,
      "grad_norm": 0.009915465489029884,
      "learning_rate": 9.757793174357621e-07,
      "loss": 0.0014,
      "step": 28430
    },
    {
      "epoch": 5170.909090909091,
      "grad_norm": 0.30457231402397156,
      "learning_rate": 9.757435291040015e-07,
      "loss": 0.0013,
      "step": 28440
    },
    {
      "epoch": 5172.727272727273,
      "grad_norm": 0.28057077527046204,
      "learning_rate": 9.75707715008727e-07,
      "loss": 0.0011,
      "step": 28450
    },
    {
      "epoch": 5174.545454545455,
      "grad_norm": 0.005964870098978281,
      "learning_rate": 9.75671875151878e-07,
      "loss": 0.0009,
      "step": 28460
    },
    {
      "epoch": 5176.363636363636,
      "grad_norm": 0.3649124801158905,
      "learning_rate": 9.756360095353957e-07,
      "loss": 0.0015,
      "step": 28470
    },
    {
      "epoch": 5178.181818181818,
      "grad_norm": 0.021734874695539474,
      "learning_rate": 9.75600118161222e-07,
      "loss": 0.001,
      "step": 28480
    },
    {
      "epoch": 5180.0,
      "grad_norm": 0.008378807455301285,
      "learning_rate": 9.755642010313005e-07,
      "loss": 0.0013,
      "step": 28490
    },
    {
      "epoch": 5181.818181818182,
      "grad_norm": 0.00857266504317522,
      "learning_rate": 9.755282581475767e-07,
      "loss": 0.0007,
      "step": 28500
    },
    {
      "epoch": 5181.818181818182,
      "eval_loss": 4.3287763595581055,
      "eval_runtime": 0.9466,
      "eval_samples_per_second": 10.564,
      "eval_steps_per_second": 5.282,
      "step": 28500
    },
    {
      "epoch": 5183.636363636364,
      "grad_norm": 0.36330580711364746,
      "learning_rate": 9.754922895119968e-07,
      "loss": 0.0014,
      "step": 28510
    },
    {
      "epoch": 5185.454545454545,
      "grad_norm": 0.32348155975341797,
      "learning_rate": 9.754562951265088e-07,
      "loss": 0.0016,
      "step": 28520
    },
    {
      "epoch": 5187.272727272727,
      "grad_norm": 0.006835167296230793,
      "learning_rate": 9.754202749930615e-07,
      "loss": 0.0009,
      "step": 28530
    },
    {
      "epoch": 5189.090909090909,
      "grad_norm": 0.2899913191795349,
      "learning_rate": 9.753842291136063e-07,
      "loss": 0.0014,
      "step": 28540
    },
    {
      "epoch": 5190.909090909091,
      "grad_norm": 0.047087281942367554,
      "learning_rate": 9.753481574900946e-07,
      "loss": 0.0011,
      "step": 28550
    },
    {
      "epoch": 5192.727272727273,
      "grad_norm": 0.30189627408981323,
      "learning_rate": 9.753120601244799e-07,
      "loss": 0.0012,
      "step": 28560
    },
    {
      "epoch": 5194.545454545455,
      "grad_norm": 0.007222524378448725,
      "learning_rate": 9.752759370187174e-07,
      "loss": 0.0011,
      "step": 28570
    },
    {
      "epoch": 5196.363636363636,
      "grad_norm": 0.46542155742645264,
      "learning_rate": 9.75239788174763e-07,
      "loss": 0.0011,
      "step": 28580
    },
    {
      "epoch": 5198.181818181818,
      "grad_norm": 0.4607332646846771,
      "learning_rate": 9.752036135945742e-07,
      "loss": 0.0013,
      "step": 28590
    },
    {
      "epoch": 5200.0,
      "grad_norm": 0.007959654554724693,
      "learning_rate": 9.751674132801105e-07,
      "loss": 0.0011,
      "step": 28600
    },
    {
      "epoch": 5201.818181818182,
      "grad_norm": 0.4351416826248169,
      "learning_rate": 9.751311872333319e-07,
      "loss": 0.0011,
      "step": 28610
    },
    {
      "epoch": 5203.636363636364,
      "grad_norm": 0.3156801462173462,
      "learning_rate": 9.750949354562004e-07,
      "loss": 0.0012,
      "step": 28620
    },
    {
      "epoch": 5205.454545454545,
      "grad_norm": 0.2980775535106659,
      "learning_rate": 9.75058657950679e-07,
      "loss": 0.0011,
      "step": 28630
    },
    {
      "epoch": 5207.272727272727,
      "grad_norm": 0.005479889456182718,
      "learning_rate": 9.750223547187322e-07,
      "loss": 0.001,
      "step": 28640
    },
    {
      "epoch": 5209.090909090909,
      "grad_norm": 0.006106398068368435,
      "learning_rate": 9.74986025762326e-07,
      "loss": 0.0013,
      "step": 28650
    },
    {
      "epoch": 5210.909090909091,
      "grad_norm": 0.006838751025497913,
      "learning_rate": 9.749496710834284e-07,
      "loss": 0.0013,
      "step": 28660
    },
    {
      "epoch": 5212.727272727273,
      "grad_norm": 0.011837011203169823,
      "learning_rate": 9.749132906840074e-07,
      "loss": 0.001,
      "step": 28670
    },
    {
      "epoch": 5214.545454545455,
      "grad_norm": 0.014905795454978943,
      "learning_rate": 9.748768845660333e-07,
      "loss": 0.0013,
      "step": 28680
    },
    {
      "epoch": 5216.363636363636,
      "grad_norm": 0.006344597786664963,
      "learning_rate": 9.74840452731478e-07,
      "loss": 0.001,
      "step": 28690
    },
    {
      "epoch": 5218.181818181818,
      "grad_norm": 0.005477897822856903,
      "learning_rate": 9.748039951823138e-07,
      "loss": 0.0012,
      "step": 28700
    },
    {
      "epoch": 5220.0,
      "grad_norm": 0.005469528026878834,
      "learning_rate": 9.74767511920516e-07,
      "loss": 0.0013,
      "step": 28710
    },
    {
      "epoch": 5221.818181818182,
      "grad_norm": 0.012785776518285275,
      "learning_rate": 9.747310029480594e-07,
      "loss": 0.0012,
      "step": 28720
    },
    {
      "epoch": 5223.636363636364,
      "grad_norm": 0.2891276776790619,
      "learning_rate": 9.746944682669213e-07,
      "loss": 0.0011,
      "step": 28730
    },
    {
      "epoch": 5225.454545454545,
      "grad_norm": 0.012242162600159645,
      "learning_rate": 9.746579078790807e-07,
      "loss": 0.0016,
      "step": 28740
    },
    {
      "epoch": 5227.272727272727,
      "grad_norm": 0.032392024993896484,
      "learning_rate": 9.74621321786517e-07,
      "loss": 0.0016,
      "step": 28750
    },
    {
      "epoch": 5229.090909090909,
      "grad_norm": 0.4088248610496521,
      "learning_rate": 9.745847099912116e-07,
      "loss": 0.0007,
      "step": 28760
    },
    {
      "epoch": 5230.909090909091,
      "grad_norm": 0.4767628610134125,
      "learning_rate": 9.745480724951473e-07,
      "loss": 0.0013,
      "step": 28770
    },
    {
      "epoch": 5232.727272727273,
      "grad_norm": 0.3849659860134125,
      "learning_rate": 9.74511409300308e-07,
      "loss": 0.0012,
      "step": 28780
    },
    {
      "epoch": 5234.545454545455,
      "grad_norm": 0.31170961260795593,
      "learning_rate": 9.744747204086794e-07,
      "loss": 0.0014,
      "step": 28790
    },
    {
      "epoch": 5236.363636363636,
      "grad_norm": 0.37289780378341675,
      "learning_rate": 9.744380058222482e-07,
      "loss": 0.0008,
      "step": 28800
    },
    {
      "epoch": 5238.181818181818,
      "grad_norm": 0.005496169440448284,
      "learning_rate": 9.744012655430027e-07,
      "loss": 0.0011,
      "step": 28810
    },
    {
      "epoch": 5240.0,
      "grad_norm": 0.32378849387168884,
      "learning_rate": 9.743644995729325e-07,
      "loss": 0.0013,
      "step": 28820
    },
    {
      "epoch": 5241.818181818182,
      "grad_norm": 0.4580235779285431,
      "learning_rate": 9.743277079140286e-07,
      "loss": 0.0012,
      "step": 28830
    },
    {
      "epoch": 5243.636363636364,
      "grad_norm": 0.005309706088155508,
      "learning_rate": 9.742908905682836e-07,
      "loss": 0.001,
      "step": 28840
    },
    {
      "epoch": 5245.454545454545,
      "grad_norm": 0.39597147703170776,
      "learning_rate": 9.742540475376912e-07,
      "loss": 0.0013,
      "step": 28850
    },
    {
      "epoch": 5247.272727272727,
      "grad_norm": 0.3164041042327881,
      "learning_rate": 9.742171788242467e-07,
      "loss": 0.0013,
      "step": 28860
    },
    {
      "epoch": 5249.090909090909,
      "grad_norm": 0.005895237438380718,
      "learning_rate": 9.741802844299463e-07,
      "loss": 0.0011,
      "step": 28870
    },
    {
      "epoch": 5250.909090909091,
      "grad_norm": 0.02062702365219593,
      "learning_rate": 9.741433643567884e-07,
      "loss": 0.0011,
      "step": 28880
    },
    {
      "epoch": 5252.727272727273,
      "grad_norm": 0.4155566692352295,
      "learning_rate": 9.741064186067722e-07,
      "loss": 0.0012,
      "step": 28890
    },
    {
      "epoch": 5254.545454545455,
      "grad_norm": 0.006401172373443842,
      "learning_rate": 9.740694471818986e-07,
      "loss": 0.0012,
      "step": 28900
    },
    {
      "epoch": 5256.363636363636,
      "grad_norm": 0.35365715622901917,
      "learning_rate": 9.740324500841698e-07,
      "loss": 0.0011,
      "step": 28910
    },
    {
      "epoch": 5258.181818181818,
      "grad_norm": 0.0036885528825223446,
      "learning_rate": 9.73995427315589e-07,
      "loss": 0.0011,
      "step": 28920
    },
    {
      "epoch": 5260.0,
      "grad_norm": 0.005157738924026489,
      "learning_rate": 9.739583788781615e-07,
      "loss": 0.0012,
      "step": 28930
    },
    {
      "epoch": 5261.818181818182,
      "grad_norm": 0.46804967522621155,
      "learning_rate": 9.739213047738937e-07,
      "loss": 0.0012,
      "step": 28940
    },
    {
      "epoch": 5263.636363636364,
      "grad_norm": 0.00616255821660161,
      "learning_rate": 9.738842050047928e-07,
      "loss": 0.0013,
      "step": 28950
    },
    {
      "epoch": 5265.454545454545,
      "grad_norm": 0.04491050913929939,
      "learning_rate": 9.738470795728684e-07,
      "loss": 0.0011,
      "step": 28960
    },
    {
      "epoch": 5267.272727272727,
      "grad_norm": 0.3712659478187561,
      "learning_rate": 9.738099284801306e-07,
      "loss": 0.0013,
      "step": 28970
    },
    {
      "epoch": 5269.090909090909,
      "grad_norm": 0.014489247463643551,
      "learning_rate": 9.73772751728592e-07,
      "loss": 0.001,
      "step": 28980
    },
    {
      "epoch": 5270.909090909091,
      "grad_norm": 0.2861640155315399,
      "learning_rate": 9.737355493202648e-07,
      "loss": 0.0013,
      "step": 28990
    },
    {
      "epoch": 5272.727272727273,
      "grad_norm": 0.005312787368893623,
      "learning_rate": 9.736983212571645e-07,
      "loss": 0.0011,
      "step": 29000
    },
    {
      "epoch": 5272.727272727273,
      "eval_loss": 4.353291988372803,
      "eval_runtime": 0.9559,
      "eval_samples_per_second": 10.461,
      "eval_steps_per_second": 5.23,
      "step": 29000
    },
    {
      "epoch": 5274.545454545455,
      "grad_norm": 0.30189913511276245,
      "learning_rate": 9.73661067541307e-07,
      "loss": 0.0014,
      "step": 29010
    },
    {
      "epoch": 5276.363636363636,
      "grad_norm": 0.31132006645202637,
      "learning_rate": 9.736237881747096e-07,
      "loss": 0.001,
      "step": 29020
    },
    {
      "epoch": 5278.181818181818,
      "grad_norm": 0.006223844364285469,
      "learning_rate": 9.735864831593914e-07,
      "loss": 0.0012,
      "step": 29030
    },
    {
      "epoch": 5280.0,
      "grad_norm": 0.011691421270370483,
      "learning_rate": 9.735491524973721e-07,
      "loss": 0.0013,
      "step": 29040
    },
    {
      "epoch": 5281.818181818182,
      "grad_norm": 0.38925766944885254,
      "learning_rate": 9.735117961906738e-07,
      "loss": 0.0012,
      "step": 29050
    },
    {
      "epoch": 5283.636363636364,
      "grad_norm": 0.2658863961696625,
      "learning_rate": 9.734744142413193e-07,
      "loss": 0.0011,
      "step": 29060
    },
    {
      "epoch": 5285.454545454545,
      "grad_norm": 0.0062093366868793964,
      "learning_rate": 9.734370066513328e-07,
      "loss": 0.001,
      "step": 29070
    },
    {
      "epoch": 5287.272727272727,
      "grad_norm": 0.38500872254371643,
      "learning_rate": 9.733995734227407e-07,
      "loss": 0.0014,
      "step": 29080
    },
    {
      "epoch": 5289.090909090909,
      "grad_norm": 0.0063844406977295876,
      "learning_rate": 9.733621145575695e-07,
      "loss": 0.001,
      "step": 29090
    },
    {
      "epoch": 5290.909090909091,
      "grad_norm": 0.0069341049529612064,
      "learning_rate": 9.733246300578482e-07,
      "loss": 0.0012,
      "step": 29100
    },
    {
      "epoch": 5292.727272727273,
      "grad_norm": 0.0047704000025987625,
      "learning_rate": 9.732871199256065e-07,
      "loss": 0.0011,
      "step": 29110
    },
    {
      "epoch": 5294.545454545455,
      "grad_norm": 0.3381613492965698,
      "learning_rate": 9.732495841628757e-07,
      "loss": 0.0013,
      "step": 29120
    },
    {
      "epoch": 5296.363636363636,
      "grad_norm": 0.49114561080932617,
      "learning_rate": 9.732120227716888e-07,
      "loss": 0.0012,
      "step": 29130
    },
    {
      "epoch": 5298.181818181818,
      "grad_norm": 0.29622581601142883,
      "learning_rate": 9.731744357540795e-07,
      "loss": 0.0011,
      "step": 29140
    },
    {
      "epoch": 5300.0,
      "grad_norm": 0.005694754887372255,
      "learning_rate": 9.731368231120835e-07,
      "loss": 0.0011,
      "step": 29150
    },
    {
      "epoch": 5301.818181818182,
      "grad_norm": 0.003958144225180149,
      "learning_rate": 9.73099184847738e-07,
      "loss": 0.0011,
      "step": 29160
    },
    {
      "epoch": 5303.636363636364,
      "grad_norm": 0.005273069255053997,
      "learning_rate": 9.730615209630807e-07,
      "loss": 0.0013,
      "step": 29170
    },
    {
      "epoch": 5305.454545454545,
      "grad_norm": 0.3643375337123871,
      "learning_rate": 9.730238314601517e-07,
      "loss": 0.001,
      "step": 29180
    },
    {
      "epoch": 5307.272727272727,
      "grad_norm": 0.32894840836524963,
      "learning_rate": 9.729861163409918e-07,
      "loss": 0.0013,
      "step": 29190
    },
    {
      "epoch": 5309.090909090909,
      "grad_norm": 0.010399986989796162,
      "learning_rate": 9.729483756076435e-07,
      "loss": 0.0011,
      "step": 29200
    },
    {
      "epoch": 5310.909090909091,
      "grad_norm": 0.28911882638931274,
      "learning_rate": 9.729106092621506e-07,
      "loss": 0.0013,
      "step": 29210
    },
    {
      "epoch": 5312.727272727273,
      "grad_norm": 0.013250207528471947,
      "learning_rate": 9.728728173065584e-07,
      "loss": 0.0011,
      "step": 29220
    },
    {
      "epoch": 5314.545454545455,
      "grad_norm": 0.006200551521033049,
      "learning_rate": 9.728349997429134e-07,
      "loss": 0.0011,
      "step": 29230
    },
    {
      "epoch": 5316.363636363636,
      "grad_norm": 0.005763832945376635,
      "learning_rate": 9.727971565732635e-07,
      "loss": 0.0011,
      "step": 29240
    },
    {
      "epoch": 5318.181818181818,
      "grad_norm": 0.358290433883667,
      "learning_rate": 9.727592877996584e-07,
      "loss": 0.0013,
      "step": 29250
    },
    {
      "epoch": 5320.0,
      "grad_norm": 0.37161538004875183,
      "learning_rate": 9.727213934241486e-07,
      "loss": 0.0011,
      "step": 29260
    },
    {
      "epoch": 5321.818181818182,
      "grad_norm": 0.013319166377186775,
      "learning_rate": 9.726834734487861e-07,
      "loss": 0.0011,
      "step": 29270
    },
    {
      "epoch": 5323.636363636364,
      "grad_norm": 0.3613177239894867,
      "learning_rate": 9.726455278756246e-07,
      "loss": 0.0013,
      "step": 29280
    },
    {
      "epoch": 5325.454545454545,
      "grad_norm": 0.005213756114244461,
      "learning_rate": 9.72607556706719e-07,
      "loss": 0.0008,
      "step": 29290
    },
    {
      "epoch": 5327.272727272727,
      "grad_norm": 0.005365179385989904,
      "learning_rate": 9.725695599441258e-07,
      "loss": 0.0012,
      "step": 29300
    },
    {
      "epoch": 5329.090909090909,
      "grad_norm": 0.290202260017395,
      "learning_rate": 9.725315375899023e-07,
      "loss": 0.0015,
      "step": 29310
    },
    {
      "epoch": 5330.909090909091,
      "grad_norm": 0.005864025559276342,
      "learning_rate": 9.724934896461078e-07,
      "loss": 0.0011,
      "step": 29320
    },
    {
      "epoch": 5332.727272727273,
      "grad_norm": 0.45040687918663025,
      "learning_rate": 9.724554161148029e-07,
      "loss": 0.0012,
      "step": 29330
    },
    {
      "epoch": 5334.545454545455,
      "grad_norm": 0.03611832857131958,
      "learning_rate": 9.72417316998049e-07,
      "loss": 0.0008,
      "step": 29340
    },
    {
      "epoch": 5336.363636363636,
      "grad_norm": 0.012387320399284363,
      "learning_rate": 9.723791922979099e-07,
      "loss": 0.0012,
      "step": 29350
    },
    {
      "epoch": 5338.181818181818,
      "grad_norm": 0.006848113611340523,
      "learning_rate": 9.723410420164496e-07,
      "loss": 0.0013,
      "step": 29360
    },
    {
      "epoch": 5340.0,
      "grad_norm": 0.004594242665916681,
      "learning_rate": 9.723028661557344e-07,
      "loss": 0.0013,
      "step": 29370
    },
    {
      "epoch": 5341.818181818182,
      "grad_norm": 0.3032257556915283,
      "learning_rate": 9.722646647178318e-07,
      "loss": 0.0009,
      "step": 29380
    },
    {
      "epoch": 5343.636363636364,
      "grad_norm": 0.35616281628608704,
      "learning_rate": 9.722264377048105e-07,
      "loss": 0.0013,
      "step": 29390
    },
    {
      "epoch": 5345.454545454545,
      "grad_norm": 0.5031594038009644,
      "learning_rate": 9.721881851187405e-07,
      "loss": 0.0013,
      "step": 29400
    },
    {
      "epoch": 5347.272727272727,
      "grad_norm": 0.26867160201072693,
      "learning_rate": 9.721499069616934e-07,
      "loss": 0.0011,
      "step": 29410
    },
    {
      "epoch": 5349.090909090909,
      "grad_norm": 0.2650531530380249,
      "learning_rate": 9.721116032357422e-07,
      "loss": 0.0014,
      "step": 29420
    },
    {
      "epoch": 5350.909090909091,
      "grad_norm": 0.28543731570243835,
      "learning_rate": 9.720732739429614e-07,
      "loss": 0.0012,
      "step": 29430
    },
    {
      "epoch": 5352.727272727273,
      "grad_norm": 0.006337381433695555,
      "learning_rate": 9.720349190854261e-07,
      "loss": 0.0015,
      "step": 29440
    },
    {
      "epoch": 5354.545454545455,
      "grad_norm": 0.25889313220977783,
      "learning_rate": 9.71996538665214e-07,
      "loss": 0.0011,
      "step": 29450
    },
    {
      "epoch": 5356.363636363636,
      "grad_norm": 0.3303945064544678,
      "learning_rate": 9.719581326844032e-07,
      "loss": 0.0011,
      "step": 29460
    },
    {
      "epoch": 5358.181818181818,
      "grad_norm": 0.008892357349395752,
      "learning_rate": 9.719197011450736e-07,
      "loss": 0.0012,
      "step": 29470
    },
    {
      "epoch": 5360.0,
      "grad_norm": 0.003400608664378524,
      "learning_rate": 9.718812440493065e-07,
      "loss": 0.0013,
      "step": 29480
    },
    {
      "epoch": 5361.818181818182,
      "grad_norm": 0.36935338377952576,
      "learning_rate": 9.718427613991846e-07,
      "loss": 0.0011,
      "step": 29490
    },
    {
      "epoch": 5363.636363636364,
      "grad_norm": 0.29076892137527466,
      "learning_rate": 9.718042531967916e-07,
      "loss": 0.001,
      "step": 29500
    },
    {
      "epoch": 5363.636363636364,
      "eval_loss": 4.3634772300720215,
      "eval_runtime": 0.9531,
      "eval_samples_per_second": 10.492,
      "eval_steps_per_second": 5.246,
      "step": 29500
    },
    {
      "epoch": 5365.454545454545,
      "grad_norm": 0.26629963517189026,
      "learning_rate": 9.717657194442133e-07,
      "loss": 0.0015,
      "step": 29510
    },
    {
      "epoch": 5367.272727272727,
      "grad_norm": 0.43255481123924255,
      "learning_rate": 9.71727160143536e-07,
      "loss": 0.0011,
      "step": 29520
    },
    {
      "epoch": 5369.090909090909,
      "grad_norm": 0.004915757570415735,
      "learning_rate": 9.716885752968483e-07,
      "loss": 0.001,
      "step": 29530
    },
    {
      "epoch": 5370.909090909091,
      "grad_norm": 0.011969175189733505,
      "learning_rate": 9.716499649062396e-07,
      "loss": 0.0012,
      "step": 29540
    },
    {
      "epoch": 5372.727272727273,
      "grad_norm": 0.006119136232882738,
      "learning_rate": 9.716113289738005e-07,
      "loss": 0.0011,
      "step": 29550
    },
    {
      "epoch": 5374.545454545455,
      "grad_norm": 0.00711832707747817,
      "learning_rate": 9.715726675016235e-07,
      "loss": 0.0011,
      "step": 29560
    },
    {
      "epoch": 5376.363636363636,
      "grad_norm": 0.005694427527487278,
      "learning_rate": 9.715339804918023e-07,
      "loss": 0.0011,
      "step": 29570
    },
    {
      "epoch": 5378.181818181818,
      "grad_norm": 0.02937140129506588,
      "learning_rate": 9.714952679464323e-07,
      "loss": 0.0013,
      "step": 29580
    },
    {
      "epoch": 5380.0,
      "grad_norm": 0.4863266050815582,
      "learning_rate": 9.714565298676094e-07,
      "loss": 0.0013,
      "step": 29590
    },
    {
      "epoch": 5381.818181818182,
      "grad_norm": 0.43508297204971313,
      "learning_rate": 9.714177662574315e-07,
      "loss": 0.001,
      "step": 29600
    },
    {
      "epoch": 5383.636363636364,
      "grad_norm": 0.011187867261469364,
      "learning_rate": 9.71378977117998e-07,
      "loss": 0.0016,
      "step": 29610
    },
    {
      "epoch": 5385.454545454545,
      "grad_norm": 0.37564653158187866,
      "learning_rate": 9.713401624514096e-07,
      "loss": 0.0009,
      "step": 29620
    },
    {
      "epoch": 5387.272727272727,
      "grad_norm": 0.2607404291629791,
      "learning_rate": 9.713013222597681e-07,
      "loss": 0.0011,
      "step": 29630
    },
    {
      "epoch": 5389.090909090909,
      "grad_norm": 0.4880175292491913,
      "learning_rate": 9.71262456545177e-07,
      "loss": 0.0013,
      "step": 29640
    },
    {
      "epoch": 5390.909090909091,
      "grad_norm": 0.01596258580684662,
      "learning_rate": 9.712235653097408e-07,
      "loss": 0.001,
      "step": 29650
    },
    {
      "epoch": 5392.727272727273,
      "grad_norm": 0.2558712661266327,
      "learning_rate": 9.711846485555659e-07,
      "loss": 0.0012,
      "step": 29660
    },
    {
      "epoch": 5394.545454545455,
      "grad_norm": 0.011290083639323711,
      "learning_rate": 9.711457062847595e-07,
      "loss": 0.0011,
      "step": 29670
    },
    {
      "epoch": 5396.363636363636,
      "grad_norm": 0.008680393919348717,
      "learning_rate": 9.711067384994307e-07,
      "loss": 0.001,
      "step": 29680
    },
    {
      "epoch": 5398.181818181818,
      "grad_norm": 0.5040829181671143,
      "learning_rate": 9.710677452016899e-07,
      "loss": 0.0016,
      "step": 29690
    },
    {
      "epoch": 5400.0,
      "grad_norm": 0.008049383759498596,
      "learning_rate": 9.710287263936483e-07,
      "loss": 0.001,
      "step": 29700
    },
    {
      "epoch": 5401.818181818182,
      "grad_norm": 0.006098481360822916,
      "learning_rate": 9.709896820774192e-07,
      "loss": 0.0012,
      "step": 29710
    },
    {
      "epoch": 5403.636363636364,
      "grad_norm": 0.006909190211445093,
      "learning_rate": 9.709506122551173e-07,
      "loss": 0.0011,
      "step": 29720
    },
    {
      "epoch": 5405.454545454545,
      "grad_norm": 0.006266055628657341,
      "learning_rate": 9.70911516928858e-07,
      "loss": 0.0011,
      "step": 29730
    },
    {
      "epoch": 5407.272727272727,
      "grad_norm": 0.005146377719938755,
      "learning_rate": 9.708723961007585e-07,
      "loss": 0.0011,
      "step": 29740
    },
    {
      "epoch": 5409.090909090909,
      "grad_norm": 0.38213813304901123,
      "learning_rate": 9.708332497729376e-07,
      "loss": 0.0013,
      "step": 29750
    },
    {
      "epoch": 5410.909090909091,
      "grad_norm": 0.40223297476768494,
      "learning_rate": 9.70794077947515e-07,
      "loss": 0.0013,
      "step": 29760
    },
    {
      "epoch": 5412.727272727273,
      "grad_norm": 0.005536258220672607,
      "learning_rate": 9.707548806266122e-07,
      "loss": 0.0009,
      "step": 29770
    },
    {
      "epoch": 5414.545454545455,
      "grad_norm": 0.4017651379108429,
      "learning_rate": 9.707156578123516e-07,
      "loss": 0.0013,
      "step": 29780
    },
    {
      "epoch": 5416.363636363636,
      "grad_norm": 0.4016611874103546,
      "learning_rate": 9.70676409506858e-07,
      "loss": 0.0012,
      "step": 29790
    },
    {
      "epoch": 5418.181818181818,
      "grad_norm": 0.3686794638633728,
      "learning_rate": 9.706371357122558e-07,
      "loss": 0.0013,
      "step": 29800
    },
    {
      "epoch": 5420.0,
      "grad_norm": 0.477617084980011,
      "learning_rate": 9.705978364306725e-07,
      "loss": 0.0011,
      "step": 29810
    },
    {
      "epoch": 5421.818181818182,
      "grad_norm": 0.2768625319004059,
      "learning_rate": 9.705585116642363e-07,
      "loss": 0.0011,
      "step": 29820
    },
    {
      "epoch": 5423.636363636364,
      "grad_norm": 0.37217843532562256,
      "learning_rate": 9.705191614150766e-07,
      "loss": 0.0013,
      "step": 29830
    },
    {
      "epoch": 5425.454545454545,
      "grad_norm": 0.4950870871543884,
      "learning_rate": 9.704797856853247e-07,
      "loss": 0.0012,
      "step": 29840
    },
    {
      "epoch": 5427.272727272727,
      "grad_norm": 0.006118678022176027,
      "learning_rate": 9.704403844771127e-07,
      "loss": 0.001,
      "step": 29850
    },
    {
      "epoch": 5429.090909090909,
      "grad_norm": 0.5552660226821899,
      "learning_rate": 9.704009577925743e-07,
      "loss": 0.0016,
      "step": 29860
    },
    {
      "epoch": 5430.909090909091,
      "grad_norm": 0.34706923365592957,
      "learning_rate": 9.703615056338448e-07,
      "loss": 0.0007,
      "step": 29870
    },
    {
      "epoch": 5432.727272727273,
      "grad_norm": 0.006017248146235943,
      "learning_rate": 9.703220280030607e-07,
      "loss": 0.0013,
      "step": 29880
    },
    {
      "epoch": 5434.545454545455,
      "grad_norm": 0.02101161703467369,
      "learning_rate": 9.702825249023596e-07,
      "loss": 0.0012,
      "step": 29890
    },
    {
      "epoch": 5436.363636363636,
      "grad_norm": 0.023944571614265442,
      "learning_rate": 9.702429963338813e-07,
      "loss": 0.0011,
      "step": 29900
    },
    {
      "epoch": 5438.181818181818,
      "grad_norm": 0.003417182480916381,
      "learning_rate": 9.702034422997656e-07,
      "loss": 0.001,
      "step": 29910
    },
    {
      "epoch": 5440.0,
      "grad_norm": 0.4205332100391388,
      "learning_rate": 9.701638628021553e-07,
      "loss": 0.0012,
      "step": 29920
    },
    {
      "epoch": 5441.818181818182,
      "grad_norm": 0.36380165815353394,
      "learning_rate": 9.701242578431936e-07,
      "loss": 0.0011,
      "step": 29930
    },
    {
      "epoch": 5443.636363636364,
      "grad_norm": 0.0046995277516543865,
      "learning_rate": 9.700846274250251e-07,
      "loss": 0.001,
      "step": 29940
    },
    {
      "epoch": 5445.454545454545,
      "grad_norm": 0.38137656450271606,
      "learning_rate": 9.70044971549796e-07,
      "loss": 0.0014,
      "step": 29950
    },
    {
      "epoch": 5447.272727272727,
      "grad_norm": 0.005250588990747929,
      "learning_rate": 9.70005290219654e-07,
      "loss": 0.001,
      "step": 29960
    },
    {
      "epoch": 5449.090909090909,
      "grad_norm": 0.37878814339637756,
      "learning_rate": 9.699655834367478e-07,
      "loss": 0.0014,
      "step": 29970
    },
    {
      "epoch": 5450.909090909091,
      "grad_norm": 0.007337283343076706,
      "learning_rate": 9.699258512032277e-07,
      "loss": 0.0011,
      "step": 29980
    },
    {
      "epoch": 5452.727272727273,
      "grad_norm": 0.027744131162762642,
      "learning_rate": 9.698860935212453e-07,
      "loss": 0.0013,
      "step": 29990
    },
    {
      "epoch": 5454.545454545455,
      "grad_norm": 0.3987371027469635,
      "learning_rate": 9.698463103929541e-07,
      "loss": 0.001,
      "step": 30000
    },
    {
      "epoch": 5454.545454545455,
      "eval_loss": 4.397106170654297,
      "eval_runtime": 0.9475,
      "eval_samples_per_second": 10.554,
      "eval_steps_per_second": 5.277,
      "step": 30000
    },
    {
      "epoch": 5456.363636363636,
      "grad_norm": 0.33149030804634094,
      "learning_rate": 9.69806501820508e-07,
      "loss": 0.0011,
      "step": 30010
    },
    {
      "epoch": 5458.181818181818,
      "grad_norm": 0.28364622592926025,
      "learning_rate": 9.697666678060631e-07,
      "loss": 0.0015,
      "step": 30020
    },
    {
      "epoch": 5460.0,
      "grad_norm": 0.31580817699432373,
      "learning_rate": 9.697268083517767e-07,
      "loss": 0.0008,
      "step": 30030
    },
    {
      "epoch": 5461.818181818182,
      "grad_norm": 0.007625415455549955,
      "learning_rate": 9.696869234598069e-07,
      "loss": 0.0011,
      "step": 30040
    },
    {
      "epoch": 5463.636363636364,
      "grad_norm": 0.003999871667474508,
      "learning_rate": 9.69647013132314e-07,
      "loss": 0.0009,
      "step": 30050
    },
    {
      "epoch": 5465.454545454545,
      "grad_norm": 0.03601688891649246,
      "learning_rate": 9.69607077371459e-07,
      "loss": 0.0016,
      "step": 30060
    },
    {
      "epoch": 5467.272727272727,
      "grad_norm": 0.31440994143486023,
      "learning_rate": 9.695671161794051e-07,
      "loss": 0.0009,
      "step": 30070
    },
    {
      "epoch": 5469.090909090909,
      "grad_norm": 0.00880931131541729,
      "learning_rate": 9.695271295583159e-07,
      "loss": 0.0011,
      "step": 30080
    },
    {
      "epoch": 5470.909090909091,
      "grad_norm": 0.0031686301808804274,
      "learning_rate": 9.69487117510357e-07,
      "loss": 0.001,
      "step": 30090
    },
    {
      "epoch": 5472.727272727273,
      "grad_norm": 0.0382939837872982,
      "learning_rate": 9.69447080037695e-07,
      "loss": 0.0014,
      "step": 30100
    },
    {
      "epoch": 5474.545454545455,
      "grad_norm": 0.3320635259151459,
      "learning_rate": 9.694070171424986e-07,
      "loss": 0.0014,
      "step": 30110
    },
    {
      "epoch": 5476.363636363636,
      "grad_norm": 0.007018404547125101,
      "learning_rate": 9.69366928826937e-07,
      "loss": 0.0007,
      "step": 30120
    },
    {
      "epoch": 5478.181818181818,
      "grad_norm": 0.3481665551662445,
      "learning_rate": 9.693268150931812e-07,
      "loss": 0.0014,
      "step": 30130
    },
    {
      "epoch": 5480.0,
      "grad_norm": 0.44260793924331665,
      "learning_rate": 9.692866759434037e-07,
      "loss": 0.0013,
      "step": 30140
    },
    {
      "epoch": 5481.818181818182,
      "grad_norm": 0.28973469138145447,
      "learning_rate": 9.692465113797778e-07,
      "loss": 0.0013,
      "step": 30150
    },
    {
      "epoch": 5483.636363636364,
      "grad_norm": 0.011232750490307808,
      "learning_rate": 9.692063214044791e-07,
      "loss": 0.0008,
      "step": 30160
    },
    {
      "epoch": 5485.454545454545,
      "grad_norm": 0.3635493218898773,
      "learning_rate": 9.691661060196838e-07,
      "loss": 0.0013,
      "step": 30170
    },
    {
      "epoch": 5487.272727272727,
      "grad_norm": 0.36128878593444824,
      "learning_rate": 9.691258652275697e-07,
      "loss": 0.0013,
      "step": 30180
    },
    {
      "epoch": 5489.090909090909,
      "grad_norm": 0.3201451301574707,
      "learning_rate": 9.690855990303159e-07,
      "loss": 0.0011,
      "step": 30190
    },
    {
      "epoch": 5490.909090909091,
      "grad_norm": 0.41206151247024536,
      "learning_rate": 9.690453074301033e-07,
      "loss": 0.0013,
      "step": 30200
    },
    {
      "epoch": 5492.727272727273,
      "grad_norm": 0.004914736840873957,
      "learning_rate": 9.690049904291137e-07,
      "loss": 0.0011,
      "step": 30210
    },
    {
      "epoch": 5494.545454545455,
      "grad_norm": 0.31324827671051025,
      "learning_rate": 9.689646480295304e-07,
      "loss": 0.0011,
      "step": 30220
    },
    {
      "epoch": 5496.363636363636,
      "grad_norm": 0.006364751141518354,
      "learning_rate": 9.689242802335381e-07,
      "loss": 0.0011,
      "step": 30230
    },
    {
      "epoch": 5498.181818181818,
      "grad_norm": 0.005071618594229221,
      "learning_rate": 9.688838870433228e-07,
      "loss": 0.0011,
      "step": 30240
    },
    {
      "epoch": 5500.0,
      "grad_norm": 0.003694982500746846,
      "learning_rate": 9.688434684610725e-07,
      "loss": 0.0013,
      "step": 30250
    },
    {
      "epoch": 5501.818181818182,
      "grad_norm": 0.2739594280719757,
      "learning_rate": 9.688030244889753e-07,
      "loss": 0.0012,
      "step": 30260
    },
    {
      "epoch": 5503.636363636364,
      "grad_norm": 0.24958743155002594,
      "learning_rate": 9.687625551292218e-07,
      "loss": 0.001,
      "step": 30270
    },
    {
      "epoch": 5505.454545454545,
      "grad_norm": 0.007977120578289032,
      "learning_rate": 9.687220603840034e-07,
      "loss": 0.0016,
      "step": 30280
    },
    {
      "epoch": 5507.272727272727,
      "grad_norm": 0.00513125117868185,
      "learning_rate": 9.686815402555133e-07,
      "loss": 0.0006,
      "step": 30290
    },
    {
      "epoch": 5509.090909090909,
      "grad_norm": 0.3536311388015747,
      "learning_rate": 9.686409947459457e-07,
      "loss": 0.0013,
      "step": 30300
    },
    {
      "epoch": 5510.909090909091,
      "grad_norm": 0.2364884614944458,
      "learning_rate": 9.686004238574962e-07,
      "loss": 0.0013,
      "step": 30310
    },
    {
      "epoch": 5512.727272727273,
      "grad_norm": 0.3132424056529999,
      "learning_rate": 9.685598275923623e-07,
      "loss": 0.0009,
      "step": 30320
    },
    {
      "epoch": 5514.545454545455,
      "grad_norm": 0.0037694426719099283,
      "learning_rate": 9.68519205952742e-07,
      "loss": 0.0013,
      "step": 30330
    },
    {
      "epoch": 5516.363636363636,
      "grad_norm": 0.30844026803970337,
      "learning_rate": 9.684785589408353e-07,
      "loss": 0.0013,
      "step": 30340
    },
    {
      "epoch": 5518.181818181818,
      "grad_norm": 0.016497701406478882,
      "learning_rate": 9.684378865588434e-07,
      "loss": 0.001,
      "step": 30350
    },
    {
      "epoch": 5520.0,
      "grad_norm": 0.27446556091308594,
      "learning_rate": 9.683971888089689e-07,
      "loss": 0.0013,
      "step": 30360
    },
    {
      "epoch": 5521.818181818182,
      "grad_norm": 0.008278322406113148,
      "learning_rate": 9.683564656934158e-07,
      "loss": 0.0011,
      "step": 30370
    },
    {
      "epoch": 5523.636363636364,
      "grad_norm": 0.01993182674050331,
      "learning_rate": 9.683157172143892e-07,
      "loss": 0.0012,
      "step": 30380
    },
    {
      "epoch": 5525.454545454545,
      "grad_norm": 0.24713502824306488,
      "learning_rate": 9.68274943374096e-07,
      "loss": 0.0013,
      "step": 30390
    },
    {
      "epoch": 5527.272727272727,
      "grad_norm": 0.004506013356149197,
      "learning_rate": 9.682341441747445e-07,
      "loss": 0.0012,
      "step": 30400
    },
    {
      "epoch": 5529.090909090909,
      "grad_norm": 0.4742541015148163,
      "learning_rate": 9.681933196185436e-07,
      "loss": 0.0013,
      "step": 30410
    },
    {
      "epoch": 5530.909090909091,
      "grad_norm": 0.005458944011479616,
      "learning_rate": 9.681524697077047e-07,
      "loss": 0.001,
      "step": 30420
    },
    {
      "epoch": 5532.727272727273,
      "grad_norm": 0.004021753557026386,
      "learning_rate": 9.681115944444395e-07,
      "loss": 0.0011,
      "step": 30430
    },
    {
      "epoch": 5534.545454545455,
      "grad_norm": 0.27072393894195557,
      "learning_rate": 9.680706938309618e-07,
      "loss": 0.0011,
      "step": 30440
    },
    {
      "epoch": 5536.363636363636,
      "grad_norm": 0.005497845355421305,
      "learning_rate": 9.680297678694867e-07,
      "loss": 0.0011,
      "step": 30450
    },
    {
      "epoch": 5538.181818181818,
      "grad_norm": 0.0038147505838423967,
      "learning_rate": 9.679888165622302e-07,
      "loss": 0.0012,
      "step": 30460
    },
    {
      "epoch": 5540.0,
      "grad_norm": 0.3142603933811188,
      "learning_rate": 9.6794783991141e-07,
      "loss": 0.0013,
      "step": 30470
    },
    {
      "epoch": 5541.818181818182,
      "grad_norm": 0.007692641578614712,
      "learning_rate": 9.679068379192455e-07,
      "loss": 0.0011,
      "step": 30480
    },
    {
      "epoch": 5543.636363636364,
      "grad_norm": 0.0037297485396265984,
      "learning_rate": 9.678658105879566e-07,
      "loss": 0.0012,
      "step": 30490
    },
    {
      "epoch": 5545.454545454545,
      "grad_norm": 0.47394704818725586,
      "learning_rate": 9.678247579197658e-07,
      "loss": 0.0015,
      "step": 30500
    },
    {
      "epoch": 5545.454545454545,
      "eval_loss": 4.426756858825684,
      "eval_runtime": 0.9501,
      "eval_samples_per_second": 10.526,
      "eval_steps_per_second": 5.263,
      "step": 30500
    },
    {
      "epoch": 5547.272727272727,
      "grad_norm": 0.27519744634628296,
      "learning_rate": 9.677836799168956e-07,
      "loss": 0.0011,
      "step": 30510
    },
    {
      "epoch": 5549.090909090909,
      "grad_norm": 0.012955893762409687,
      "learning_rate": 9.677425765815708e-07,
      "loss": 0.0009,
      "step": 30520
    },
    {
      "epoch": 5550.909090909091,
      "grad_norm": 0.022263791412115097,
      "learning_rate": 9.677014479160176e-07,
      "loss": 0.0013,
      "step": 30530
    },
    {
      "epoch": 5552.727272727273,
      "grad_norm": 0.34675073623657227,
      "learning_rate": 9.676602939224628e-07,
      "loss": 0.0009,
      "step": 30540
    },
    {
      "epoch": 5554.545454545455,
      "grad_norm": 0.4175267219543457,
      "learning_rate": 9.676191146031355e-07,
      "loss": 0.0016,
      "step": 30550
    },
    {
      "epoch": 5556.363636363636,
      "grad_norm": 0.26464998722076416,
      "learning_rate": 9.675779099602654e-07,
      "loss": 0.0009,
      "step": 30560
    },
    {
      "epoch": 5558.181818181818,
      "grad_norm": 0.369888037443161,
      "learning_rate": 9.67536679996084e-07,
      "loss": 0.0015,
      "step": 30570
    },
    {
      "epoch": 5560.0,
      "grad_norm": 0.26815930008888245,
      "learning_rate": 9.674954247128244e-07,
      "loss": 0.0012,
      "step": 30580
    },
    {
      "epoch": 5561.818181818182,
      "grad_norm": 0.5218125581741333,
      "learning_rate": 9.674541441127201e-07,
      "loss": 0.0013,
      "step": 30590
    },
    {
      "epoch": 5563.636363636364,
      "grad_norm": 0.46690797805786133,
      "learning_rate": 9.674128381980071e-07,
      "loss": 0.001,
      "step": 30600
    },
    {
      "epoch": 5565.454545454545,
      "grad_norm": 0.28867995738983154,
      "learning_rate": 9.673715069709223e-07,
      "loss": 0.0012,
      "step": 30610
    },
    {
      "epoch": 5567.272727272727,
      "grad_norm": 0.01637924462556839,
      "learning_rate": 9.673301504337037e-07,
      "loss": 0.0009,
      "step": 30620
    },
    {
      "epoch": 5569.090909090909,
      "grad_norm": 0.01297494862228632,
      "learning_rate": 9.67288768588591e-07,
      "loss": 0.0012,
      "step": 30630
    },
    {
      "epoch": 5570.909090909091,
      "grad_norm": 0.3351173400878906,
      "learning_rate": 9.672473614378254e-07,
      "loss": 0.0012,
      "step": 30640
    },
    {
      "epoch": 5572.727272727273,
      "grad_norm": 0.4639185965061188,
      "learning_rate": 9.672059289836491e-07,
      "loss": 0.0011,
      "step": 30650
    },
    {
      "epoch": 5574.545454545455,
      "grad_norm": 0.004254081752151251,
      "learning_rate": 9.67164471228306e-07,
      "loss": 0.0011,
      "step": 30660
    },
    {
      "epoch": 5576.363636363636,
      "grad_norm": 0.018550479784607887,
      "learning_rate": 9.67122988174041e-07,
      "loss": 0.0011,
      "step": 30670
    },
    {
      "epoch": 5578.181818181818,
      "grad_norm": 0.00472727045416832,
      "learning_rate": 9.670814798231005e-07,
      "loss": 0.0012,
      "step": 30680
    },
    {
      "epoch": 5580.0,
      "grad_norm": 0.004623292945325375,
      "learning_rate": 9.670399461777328e-07,
      "loss": 0.0013,
      "step": 30690
    },
    {
      "epoch": 5581.818181818182,
      "grad_norm": 0.45660364627838135,
      "learning_rate": 9.669983872401866e-07,
      "loss": 0.0011,
      "step": 30700
    },
    {
      "epoch": 5583.636363636364,
      "grad_norm": 0.3694270849227905,
      "learning_rate": 9.669568030127128e-07,
      "loss": 0.0014,
      "step": 30710
    },
    {
      "epoch": 5585.454545454545,
      "grad_norm": 0.4577295482158661,
      "learning_rate": 9.669151934975634e-07,
      "loss": 0.001,
      "step": 30720
    },
    {
      "epoch": 5587.272727272727,
      "grad_norm": 0.007461564615368843,
      "learning_rate": 9.668735586969915e-07,
      "loss": 0.001,
      "step": 30730
    },
    {
      "epoch": 5589.090909090909,
      "grad_norm": 0.3680168092250824,
      "learning_rate": 9.66831898613252e-07,
      "loss": 0.0014,
      "step": 30740
    },
    {
      "epoch": 5590.909090909091,
      "grad_norm": 0.38938674330711365,
      "learning_rate": 9.667902132486008e-07,
      "loss": 0.0011,
      "step": 30750
    },
    {
      "epoch": 5592.727272727273,
      "grad_norm": 0.48334503173828125,
      "learning_rate": 9.667485026052954e-07,
      "loss": 0.0013,
      "step": 30760
    },
    {
      "epoch": 5594.545454545455,
      "grad_norm": 0.3696749210357666,
      "learning_rate": 9.667067666855948e-07,
      "loss": 0.001,
      "step": 30770
    },
    {
      "epoch": 5596.363636363636,
      "grad_norm": 0.006064341403543949,
      "learning_rate": 9.66665005491759e-07,
      "loss": 0.0013,
      "step": 30780
    },
    {
      "epoch": 5598.181818181818,
      "grad_norm": 0.38692283630371094,
      "learning_rate": 9.666232190260495e-07,
      "loss": 0.0015,
      "step": 30790
    },
    {
      "epoch": 5600.0,
      "grad_norm": 0.004368258640170097,
      "learning_rate": 9.665814072907292e-07,
      "loss": 0.0011,
      "step": 30800
    },
    {
      "epoch": 5601.818181818182,
      "grad_norm": 0.01320677064359188,
      "learning_rate": 9.665395702880625e-07,
      "loss": 0.0013,
      "step": 30810
    },
    {
      "epoch": 5603.636363636364,
      "grad_norm": 0.38999345898628235,
      "learning_rate": 9.664977080203152e-07,
      "loss": 0.0012,
      "step": 30820
    },
    {
      "epoch": 5605.454545454545,
      "grad_norm": 0.46228229999542236,
      "learning_rate": 9.664558204897538e-07,
      "loss": 0.0011,
      "step": 30830
    },
    {
      "epoch": 5607.272727272727,
      "grad_norm": 0.003952697850763798,
      "learning_rate": 9.664139076986473e-07,
      "loss": 0.001,
      "step": 30840
    },
    {
      "epoch": 5609.090909090909,
      "grad_norm": 0.0055719902738928795,
      "learning_rate": 9.66371969649265e-07,
      "loss": 0.0012,
      "step": 30850
    },
    {
      "epoch": 5610.909090909091,
      "grad_norm": 0.004574504215270281,
      "learning_rate": 9.663300063438782e-07,
      "loss": 0.0012,
      "step": 30860
    },
    {
      "epoch": 5612.727272727273,
      "grad_norm": 0.38286325335502625,
      "learning_rate": 9.662880177847593e-07,
      "loss": 0.0011,
      "step": 30870
    },
    {
      "epoch": 5614.545454545455,
      "grad_norm": 0.0028211127500981092,
      "learning_rate": 9.662460039741824e-07,
      "loss": 0.001,
      "step": 30880
    },
    {
      "epoch": 5616.363636363636,
      "grad_norm": 0.287734717130661,
      "learning_rate": 9.662039649144224e-07,
      "loss": 0.0015,
      "step": 30890
    },
    {
      "epoch": 5618.181818181818,
      "grad_norm": 0.00487359706312418,
      "learning_rate": 9.661619006077561e-07,
      "loss": 0.0008,
      "step": 30900
    },
    {
      "epoch": 5620.0,
      "grad_norm": 0.286663681268692,
      "learning_rate": 9.661198110564614e-07,
      "loss": 0.0012,
      "step": 30910
    },
    {
      "epoch": 5621.818181818182,
      "grad_norm": 0.2993464767932892,
      "learning_rate": 9.660776962628177e-07,
      "loss": 0.0013,
      "step": 30920
    },
    {
      "epoch": 5623.636363636364,
      "grad_norm": 0.00531439483165741,
      "learning_rate": 9.660355562291054e-07,
      "loss": 0.0009,
      "step": 30930
    },
    {
      "epoch": 5625.454545454545,
      "grad_norm": 0.4382376968860626,
      "learning_rate": 9.65993390957607e-07,
      "loss": 0.0012,
      "step": 30940
    },
    {
      "epoch": 5627.272727272727,
      "grad_norm": 0.2996315062046051,
      "learning_rate": 9.659512004506056e-07,
      "loss": 0.0012,
      "step": 30950
    },
    {
      "epoch": 5629.090909090909,
      "grad_norm": 0.4354846179485321,
      "learning_rate": 9.659089847103861e-07,
      "loss": 0.0011,
      "step": 30960
    },
    {
      "epoch": 5630.909090909091,
      "grad_norm": 0.016340123489499092,
      "learning_rate": 9.658667437392348e-07,
      "loss": 0.0013,
      "step": 30970
    },
    {
      "epoch": 5632.727272727273,
      "grad_norm": 0.0055144610814750195,
      "learning_rate": 9.658244775394392e-07,
      "loss": 0.0008,
      "step": 30980
    },
    {
      "epoch": 5634.545454545455,
      "grad_norm": 0.00450298422947526,
      "learning_rate": 9.657821861132878e-07,
      "loss": 0.0014,
      "step": 30990
    },
    {
      "epoch": 5636.363636363636,
      "grad_norm": 0.3376300632953644,
      "learning_rate": 9.657398694630712e-07,
      "loss": 0.0013,
      "step": 31000
    },
    {
      "epoch": 5636.363636363636,
      "eval_loss": 4.44846773147583,
      "eval_runtime": 0.9535,
      "eval_samples_per_second": 10.488,
      "eval_steps_per_second": 5.244,
      "step": 31000
    },
    {
      "epoch": 5638.181818181818,
      "grad_norm": 0.014266309328377247,
      "learning_rate": 9.65697527591081e-07,
      "loss": 0.0009,
      "step": 31010
    },
    {
      "epoch": 5640.0,
      "grad_norm": 0.32066577672958374,
      "learning_rate": 9.656551604996102e-07,
      "loss": 0.0013,
      "step": 31020
    },
    {
      "epoch": 5641.818181818182,
      "grad_norm": 0.003353357780724764,
      "learning_rate": 9.656127681909528e-07,
      "loss": 0.0013,
      "step": 31030
    },
    {
      "epoch": 5643.636363636364,
      "grad_norm": 0.01018423680216074,
      "learning_rate": 9.655703506674054e-07,
      "loss": 0.0009,
      "step": 31040
    },
    {
      "epoch": 5645.454545454545,
      "grad_norm": 0.3663135766983032,
      "learning_rate": 9.655279079312643e-07,
      "loss": 0.0011,
      "step": 31050
    },
    {
      "epoch": 5647.272727272727,
      "grad_norm": 0.004354469478130341,
      "learning_rate": 9.65485439984828e-07,
      "loss": 0.0011,
      "step": 31060
    },
    {
      "epoch": 5649.090909090909,
      "grad_norm": 0.0057052564807236195,
      "learning_rate": 9.654429468303964e-07,
      "loss": 0.0013,
      "step": 31070
    },
    {
      "epoch": 5650.909090909091,
      "grad_norm": 0.004815227352082729,
      "learning_rate": 9.65400428470271e-07,
      "loss": 0.0013,
      "step": 31080
    },
    {
      "epoch": 5652.727272727273,
      "grad_norm": 0.02279444970190525,
      "learning_rate": 9.653578849067543e-07,
      "loss": 0.0013,
      "step": 31090
    },
    {
      "epoch": 5654.545454545455,
      "grad_norm": 0.0043775648809969425,
      "learning_rate": 9.653153161421497e-07,
      "loss": 0.0011,
      "step": 31100
    },
    {
      "epoch": 5656.363636363636,
      "grad_norm": 0.380930095911026,
      "learning_rate": 9.65272722178763e-07,
      "loss": 0.0012,
      "step": 31110
    },
    {
      "epoch": 5658.181818181818,
      "grad_norm": 0.33211472630500793,
      "learning_rate": 9.652301030189006e-07,
      "loss": 0.0013,
      "step": 31120
    },
    {
      "epoch": 5660.0,
      "grad_norm": 0.4337232708930969,
      "learning_rate": 9.651874586648706e-07,
      "loss": 0.0012,
      "step": 31130
    },
    {
      "epoch": 5661.818181818182,
      "grad_norm": 0.33144283294677734,
      "learning_rate": 9.651447891189823e-07,
      "loss": 0.0012,
      "step": 31140
    },
    {
      "epoch": 5663.636363636364,
      "grad_norm": 0.0049310047179460526,
      "learning_rate": 9.651020943835463e-07,
      "loss": 0.0012,
      "step": 31150
    },
    {
      "epoch": 5665.454545454545,
      "grad_norm": 0.24411934614181519,
      "learning_rate": 9.650593744608752e-07,
      "loss": 0.0012,
      "step": 31160
    },
    {
      "epoch": 5667.272727272727,
      "grad_norm": 0.0044850739650428295,
      "learning_rate": 9.65016629353282e-07,
      "loss": 0.0011,
      "step": 31170
    },
    {
      "epoch": 5669.090909090909,
      "grad_norm": 0.339531272649765,
      "learning_rate": 9.649738590630815e-07,
      "loss": 0.0014,
      "step": 31180
    },
    {
      "epoch": 5670.909090909091,
      "grad_norm": 0.014073572121560574,
      "learning_rate": 9.649310635925904e-07,
      "loss": 0.0009,
      "step": 31190
    },
    {
      "epoch": 5672.727272727273,
      "grad_norm": 0.30171555280685425,
      "learning_rate": 9.648882429441256e-07,
      "loss": 0.0013,
      "step": 31200
    },
    {
      "epoch": 5674.545454545455,
      "grad_norm": 0.2872193157672882,
      "learning_rate": 9.648453971200067e-07,
      "loss": 0.001,
      "step": 31210
    },
    {
      "epoch": 5676.363636363636,
      "grad_norm": 0.35780614614486694,
      "learning_rate": 9.648025261225533e-07,
      "loss": 0.0013,
      "step": 31220
    },
    {
      "epoch": 5678.181818181818,
      "grad_norm": 0.0048515633679926395,
      "learning_rate": 9.647596299540872e-07,
      "loss": 0.0011,
      "step": 31230
    },
    {
      "epoch": 5680.0,
      "grad_norm": 0.008729079738259315,
      "learning_rate": 9.647167086169317e-07,
      "loss": 0.0013,
      "step": 31240
    },
    {
      "epoch": 5681.818181818182,
      "grad_norm": 0.4063186049461365,
      "learning_rate": 9.64673762113411e-07,
      "loss": 0.0013,
      "step": 31250
    },
    {
      "epoch": 5683.636363636364,
      "grad_norm": 0.2951717674732208,
      "learning_rate": 9.646307904458512e-07,
      "loss": 0.001,
      "step": 31260
    },
    {
      "epoch": 5685.454545454545,
      "grad_norm": 0.34059566259384155,
      "learning_rate": 9.645877936165787e-07,
      "loss": 0.0011,
      "step": 31270
    },
    {
      "epoch": 5687.272727272727,
      "grad_norm": 0.004059477709233761,
      "learning_rate": 9.645447716279224e-07,
      "loss": 0.0012,
      "step": 31280
    },
    {
      "epoch": 5689.090909090909,
      "grad_norm": 0.2913530766963959,
      "learning_rate": 9.645017244822122e-07,
      "loss": 0.0012,
      "step": 31290
    },
    {
      "epoch": 5690.909090909091,
      "grad_norm": 0.3077460825443268,
      "learning_rate": 9.64458652181779e-07,
      "loss": 0.0013,
      "step": 31300
    },
    {
      "epoch": 5692.727272727273,
      "grad_norm": 0.28608521819114685,
      "learning_rate": 9.644155547289554e-07,
      "loss": 0.001,
      "step": 31310
    },
    {
      "epoch": 5694.545454545455,
      "grad_norm": 0.0045343125239014626,
      "learning_rate": 9.643724321260755e-07,
      "loss": 0.0012,
      "step": 31320
    },
    {
      "epoch": 5696.363636363636,
      "grad_norm": 0.004224687814712524,
      "learning_rate": 9.643292843754744e-07,
      "loss": 0.0013,
      "step": 31330
    },
    {
      "epoch": 5698.181818181818,
      "grad_norm": 0.0047128512524068356,
      "learning_rate": 9.64286111479489e-07,
      "loss": 0.0011,
      "step": 31340
    },
    {
      "epoch": 5700.0,
      "grad_norm": 0.009706460870802402,
      "learning_rate": 9.642429134404566e-07,
      "loss": 0.0013,
      "step": 31350
    },
    {
      "epoch": 5701.818181818182,
      "grad_norm": 0.026727663353085518,
      "learning_rate": 9.641996902607175e-07,
      "loss": 0.0011,
      "step": 31360
    },
    {
      "epoch": 5703.636363636364,
      "grad_norm": 0.004562890622764826,
      "learning_rate": 9.641564419426116e-07,
      "loss": 0.0008,
      "step": 31370
    },
    {
      "epoch": 5705.454545454545,
      "grad_norm": 0.0035129340831190348,
      "learning_rate": 9.641131684884815e-07,
      "loss": 0.0014,
      "step": 31380
    },
    {
      "epoch": 5707.272727272727,
      "grad_norm": 0.005763668101280928,
      "learning_rate": 9.640698699006705e-07,
      "loss": 0.001,
      "step": 31390
    },
    {
      "epoch": 5709.090909090909,
      "grad_norm": 0.33901873230934143,
      "learning_rate": 9.640265461815233e-07,
      "loss": 0.0013,
      "step": 31400
    },
    {
      "epoch": 5710.909090909091,
      "grad_norm": 0.3069148361682892,
      "learning_rate": 9.639831973333864e-07,
      "loss": 0.0013,
      "step": 31410
    },
    {
      "epoch": 5712.727272727273,
      "grad_norm": 0.3373890221118927,
      "learning_rate": 9.639398233586068e-07,
      "loss": 0.0011,
      "step": 31420
    },
    {
      "epoch": 5714.545454545455,
      "grad_norm": 0.4828762710094452,
      "learning_rate": 9.638964242595336e-07,
      "loss": 0.0011,
      "step": 31430
    },
    {
      "epoch": 5716.363636363636,
      "grad_norm": 0.4602573812007904,
      "learning_rate": 9.63853000038517e-07,
      "loss": 0.0013,
      "step": 31440
    },
    {
      "epoch": 5718.181818181818,
      "grad_norm": 0.0030469223856925964,
      "learning_rate": 9.638095506979089e-07,
      "loss": 0.001,
      "step": 31450
    },
    {
      "epoch": 5720.0,
      "grad_norm": 0.004173645284026861,
      "learning_rate": 9.637660762400618e-07,
      "loss": 0.0013,
      "step": 31460
    },
    {
      "epoch": 5721.818181818182,
      "grad_norm": 0.4450514614582062,
      "learning_rate": 9.637225766673306e-07,
      "loss": 0.0012,
      "step": 31470
    },
    {
      "epoch": 5723.636363636364,
      "grad_norm": 0.3754224479198456,
      "learning_rate": 9.636790519820707e-07,
      "loss": 0.0008,
      "step": 31480
    },
    {
      "epoch": 5725.454545454545,
      "grad_norm": 0.01302554365247488,
      "learning_rate": 9.636355021866386e-07,
      "loss": 0.0013,
      "step": 31490
    },
    {
      "epoch": 5727.272727272727,
      "grad_norm": 0.007920775562524796,
      "learning_rate": 9.635919272833937e-07,
      "loss": 0.0012,
      "step": 31500
    },
    {
      "epoch": 5727.272727272727,
      "eval_loss": 4.488545894622803,
      "eval_runtime": 0.9548,
      "eval_samples_per_second": 10.473,
      "eval_steps_per_second": 5.237,
      "step": 31500
    },
    {
      "epoch": 5729.090909090909,
      "grad_norm": 0.3528265058994293,
      "learning_rate": 9.63548327274695e-07,
      "loss": 0.0011,
      "step": 31510
    },
    {
      "epoch": 5730.909090909091,
      "grad_norm": 0.006416831165552139,
      "learning_rate": 9.63504702162904e-07,
      "loss": 0.0013,
      "step": 31520
    },
    {
      "epoch": 5732.727272727273,
      "grad_norm": 0.29970622062683105,
      "learning_rate": 9.634610519503833e-07,
      "loss": 0.0011,
      "step": 31530
    },
    {
      "epoch": 5734.545454545455,
      "grad_norm": 0.28106868267059326,
      "learning_rate": 9.634173766394962e-07,
      "loss": 0.0009,
      "step": 31540
    },
    {
      "epoch": 5736.363636363636,
      "grad_norm": 0.035064034163951874,
      "learning_rate": 9.633736762326081e-07,
      "loss": 0.0012,
      "step": 31550
    },
    {
      "epoch": 5738.181818181818,
      "grad_norm": 0.003440610598772764,
      "learning_rate": 9.63329950732086e-07,
      "loss": 0.0013,
      "step": 31560
    },
    {
      "epoch": 5740.0,
      "grad_norm": 0.006522353272885084,
      "learning_rate": 9.632862001402975e-07,
      "loss": 0.0013,
      "step": 31570
    },
    {
      "epoch": 5741.818181818182,
      "grad_norm": 0.36983224749565125,
      "learning_rate": 9.632424244596118e-07,
      "loss": 0.001,
      "step": 31580
    },
    {
      "epoch": 5743.636363636364,
      "grad_norm": 0.24121080338954926,
      "learning_rate": 9.631986236923996e-07,
      "loss": 0.0011,
      "step": 31590
    },
    {
      "epoch": 5745.454545454545,
      "grad_norm": 0.005943808704614639,
      "learning_rate": 9.63154797841033e-07,
      "loss": 0.0014,
      "step": 31600
    },
    {
      "epoch": 5747.272727272727,
      "grad_norm": 0.35726219415664673,
      "learning_rate": 9.631109469078851e-07,
      "loss": 0.0011,
      "step": 31610
    },
    {
      "epoch": 5749.090909090909,
      "grad_norm": 0.004445523954927921,
      "learning_rate": 9.63067070895331e-07,
      "loss": 0.0011,
      "step": 31620
    },
    {
      "epoch": 5750.909090909091,
      "grad_norm": 0.271462082862854,
      "learning_rate": 9.630231698057465e-07,
      "loss": 0.0013,
      "step": 31630
    },
    {
      "epoch": 5752.727272727273,
      "grad_norm": 0.2826283574104309,
      "learning_rate": 9.62979243641509e-07,
      "loss": 0.0013,
      "step": 31640
    },
    {
      "epoch": 5754.545454545455,
      "grad_norm": 0.35618799924850464,
      "learning_rate": 9.629352924049974e-07,
      "loss": 0.001,
      "step": 31650
    },
    {
      "epoch": 5756.363636363636,
      "grad_norm": 0.011619393713772297,
      "learning_rate": 9.628913160985919e-07,
      "loss": 0.0011,
      "step": 31660
    },
    {
      "epoch": 5758.181818181818,
      "grad_norm": 0.003936038818210363,
      "learning_rate": 9.628473147246736e-07,
      "loss": 0.0011,
      "step": 31670
    },
    {
      "epoch": 5760.0,
      "grad_norm": 0.25123271346092224,
      "learning_rate": 9.62803288285626e-07,
      "loss": 0.0013,
      "step": 31680
    },
    {
      "epoch": 5761.818181818182,
      "grad_norm": 0.004567085299640894,
      "learning_rate": 9.62759236783833e-07,
      "loss": 0.0012,
      "step": 31690
    },
    {
      "epoch": 5763.636363636364,
      "grad_norm": 0.34968897700309753,
      "learning_rate": 9.6271516022168e-07,
      "loss": 0.001,
      "step": 31700
    },
    {
      "epoch": 5765.454545454545,
      "grad_norm": 0.004979515448212624,
      "learning_rate": 9.626710586015543e-07,
      "loss": 0.0015,
      "step": 31710
    },
    {
      "epoch": 5767.272727272727,
      "grad_norm": 0.25796329975128174,
      "learning_rate": 9.626269319258437e-07,
      "loss": 0.0014,
      "step": 31720
    },
    {
      "epoch": 5769.090909090909,
      "grad_norm": 0.008877487853169441,
      "learning_rate": 9.625827801969384e-07,
      "loss": 0.001,
      "step": 31730
    },
    {
      "epoch": 5770.909090909091,
      "grad_norm": 2.550562620162964,
      "learning_rate": 9.62538603417229e-07,
      "loss": 0.0012,
      "step": 31740
    },
    {
      "epoch": 5772.727272727273,
      "grad_norm": 0.43745067715644836,
      "learning_rate": 9.62494401589108e-07,
      "loss": 0.0013,
      "step": 31750
    },
    {
      "epoch": 5774.545454545455,
      "grad_norm": 0.061435259878635406,
      "learning_rate": 9.62450174714969e-07,
      "loss": 0.002,
      "step": 31760
    },
    {
      "epoch": 5776.363636363636,
      "grad_norm": 0.02095027267932892,
      "learning_rate": 9.624059227972075e-07,
      "loss": 0.0036,
      "step": 31770
    },
    {
      "epoch": 5778.181818181818,
      "grad_norm": 0.01010892353951931,
      "learning_rate": 9.623616458382193e-07,
      "loss": 0.0015,
      "step": 31780
    },
    {
      "epoch": 5780.0,
      "grad_norm": 0.28981906175613403,
      "learning_rate": 9.623173438404026e-07,
      "loss": 0.0017,
      "step": 31790
    },
    {
      "epoch": 5781.818181818182,
      "grad_norm": 0.2923142611980438,
      "learning_rate": 9.622730168061567e-07,
      "loss": 0.0009,
      "step": 31800
    },
    {
      "epoch": 5783.636363636364,
      "grad_norm": 1.9534974098205566,
      "learning_rate": 9.622286647378814e-07,
      "loss": 0.0015,
      "step": 31810
    },
    {
      "epoch": 5785.454545454545,
      "grad_norm": 0.03135111555457115,
      "learning_rate": 9.62184287637979e-07,
      "loss": 0.0009,
      "step": 31820
    },
    {
      "epoch": 5787.272727272727,
      "grad_norm": 0.38450655341148376,
      "learning_rate": 9.62139885508853e-07,
      "loss": 0.0019,
      "step": 31830
    },
    {
      "epoch": 5789.090909090909,
      "grad_norm": 0.036318305879831314,
      "learning_rate": 9.620954583529075e-07,
      "loss": 0.0012,
      "step": 31840
    },
    {
      "epoch": 5790.909090909091,
      "grad_norm": 0.3014196455478668,
      "learning_rate": 9.620510061725485e-07,
      "loss": 0.0011,
      "step": 31850
    },
    {
      "epoch": 5792.727272727273,
      "grad_norm": 0.00720079755410552,
      "learning_rate": 9.620065289701833e-07,
      "loss": 0.0011,
      "step": 31860
    },
    {
      "epoch": 5794.545454545455,
      "grad_norm": 0.3506281077861786,
      "learning_rate": 9.619620267482208e-07,
      "loss": 0.0016,
      "step": 31870
    },
    {
      "epoch": 5796.363636363636,
      "grad_norm": 0.010646251030266285,
      "learning_rate": 9.619174995090705e-07,
      "loss": 0.0006,
      "step": 31880
    },
    {
      "epoch": 5798.181818181818,
      "grad_norm": 0.25407513976097107,
      "learning_rate": 9.61872947255144e-07,
      "loss": 0.0014,
      "step": 31890
    },
    {
      "epoch": 5800.0,
      "grad_norm": 0.005777477752417326,
      "learning_rate": 9.618283699888542e-07,
      "loss": 0.0011,
      "step": 31900
    },
    {
      "epoch": 5801.818181818182,
      "grad_norm": 0.34950166940689087,
      "learning_rate": 9.617837677126147e-07,
      "loss": 0.0013,
      "step": 31910
    },
    {
      "epoch": 5803.636363636364,
      "grad_norm": 0.020179955288767815,
      "learning_rate": 9.61739140428841e-07,
      "loss": 0.0011,
      "step": 31920
    },
    {
      "epoch": 5805.454545454545,
      "grad_norm": 0.016458740457892418,
      "learning_rate": 9.616944881399503e-07,
      "loss": 0.0008,
      "step": 31930
    },
    {
      "epoch": 5807.272727272727,
      "grad_norm": 0.016829950734972954,
      "learning_rate": 9.616498108483601e-07,
      "loss": 0.0017,
      "step": 31940
    },
    {
      "epoch": 5809.090909090909,
      "grad_norm": 0.3898462951183319,
      "learning_rate": 9.616051085564904e-07,
      "loss": 0.0008,
      "step": 31950
    },
    {
      "epoch": 5810.909090909091,
      "grad_norm": 0.006687789224088192,
      "learning_rate": 9.615603812667616e-07,
      "loss": 0.0013,
      "step": 31960
    },
    {
      "epoch": 5812.727272727273,
      "grad_norm": 0.4323923885822296,
      "learning_rate": 9.615156289815962e-07,
      "loss": 0.001,
      "step": 31970
    },
    {
      "epoch": 5814.545454545455,
      "grad_norm": 0.4200901389122009,
      "learning_rate": 9.614708517034175e-07,
      "loss": 0.0011,
      "step": 31980
    },
    {
      "epoch": 5816.363636363636,
      "grad_norm": 0.3429120182991028,
      "learning_rate": 9.614260494346503e-07,
      "loss": 0.0011,
      "step": 31990
    },
    {
      "epoch": 5818.181818181818,
      "grad_norm": 0.34519538283348083,
      "learning_rate": 9.613812221777212e-07,
      "loss": 0.0013,
      "step": 32000
    },
    {
      "epoch": 5818.181818181818,
      "eval_loss": 4.493298530578613,
      "eval_runtime": 0.9519,
      "eval_samples_per_second": 10.506,
      "eval_steps_per_second": 5.253,
      "step": 32000
    },
    {
      "epoch": 5820.0,
      "grad_norm": 0.0067074778489768505,
      "learning_rate": 9.613363699350574e-07,
      "loss": 0.0012,
      "step": 32010
    },
    {
      "epoch": 5821.818181818182,
      "grad_norm": 0.33944010734558105,
      "learning_rate": 9.61291492709088e-07,
      "loss": 0.0011,
      "step": 32020
    },
    {
      "epoch": 5823.636363636364,
      "grad_norm": 0.34600335359573364,
      "learning_rate": 9.612465905022433e-07,
      "loss": 0.0011,
      "step": 32030
    },
    {
      "epoch": 5825.454545454545,
      "grad_norm": 0.3204154968261719,
      "learning_rate": 9.612016633169549e-07,
      "loss": 0.0012,
      "step": 32040
    },
    {
      "epoch": 5827.272727272727,
      "grad_norm": 0.007554669864475727,
      "learning_rate": 9.61156711155656e-07,
      "loss": 0.001,
      "step": 32050
    },
    {
      "epoch": 5829.090909090909,
      "grad_norm": 0.0041642761789262295,
      "learning_rate": 9.611117340207807e-07,
      "loss": 0.0012,
      "step": 32060
    },
    {
      "epoch": 5830.909090909091,
      "grad_norm": 0.34132829308509827,
      "learning_rate": 9.610667319147646e-07,
      "loss": 0.0013,
      "step": 32070
    },
    {
      "epoch": 5832.727272727273,
      "grad_norm": 0.4313122034072876,
      "learning_rate": 9.61021704840045e-07,
      "loss": 0.0011,
      "step": 32080
    },
    {
      "epoch": 5834.545454545455,
      "grad_norm": 0.2652589678764343,
      "learning_rate": 9.609766527990604e-07,
      "loss": 0.0011,
      "step": 32090
    },
    {
      "epoch": 5836.363636363636,
      "grad_norm": 0.4022047817707062,
      "learning_rate": 9.609315757942502e-07,
      "loss": 0.0014,
      "step": 32100
    },
    {
      "epoch": 5838.181818181818,
      "grad_norm": 0.010914643295109272,
      "learning_rate": 9.608864738280557e-07,
      "loss": 0.0009,
      "step": 32110
    },
    {
      "epoch": 5840.0,
      "grad_norm": 0.2447729855775833,
      "learning_rate": 9.608413469029196e-07,
      "loss": 0.0012,
      "step": 32120
    },
    {
      "epoch": 5841.818181818182,
      "grad_norm": 0.3466808795928955,
      "learning_rate": 9.607961950212853e-07,
      "loss": 0.0013,
      "step": 32130
    },
    {
      "epoch": 5843.636363636364,
      "grad_norm": 0.007917549461126328,
      "learning_rate": 9.607510181855982e-07,
      "loss": 0.0011,
      "step": 32140
    },
    {
      "epoch": 5845.454545454545,
      "grad_norm": 0.005451010540127754,
      "learning_rate": 9.607058163983046e-07,
      "loss": 0.0008,
      "step": 32150
    },
    {
      "epoch": 5847.272727272727,
      "grad_norm": 0.26090380549430847,
      "learning_rate": 9.606605896618527e-07,
      "loss": 0.0014,
      "step": 32160
    },
    {
      "epoch": 5849.090909090909,
      "grad_norm": 0.004471097607165575,
      "learning_rate": 9.606153379786915e-07,
      "loss": 0.0011,
      "step": 32170
    },
    {
      "epoch": 5850.909090909091,
      "grad_norm": 0.2700832486152649,
      "learning_rate": 9.605700613512714e-07,
      "loss": 0.0013,
      "step": 32180
    },
    {
      "epoch": 5852.727272727273,
      "grad_norm": 0.38835787773132324,
      "learning_rate": 9.605247597820446e-07,
      "loss": 0.0013,
      "step": 32190
    },
    {
      "epoch": 5854.545454545455,
      "grad_norm": 0.00360308401286602,
      "learning_rate": 9.604794332734646e-07,
      "loss": 0.0008,
      "step": 32200
    },
    {
      "epoch": 5856.363636363636,
      "grad_norm": 0.3435911536216736,
      "learning_rate": 9.604340818279855e-07,
      "loss": 0.0014,
      "step": 32210
    },
    {
      "epoch": 5858.181818181818,
      "grad_norm": 0.005725631956011057,
      "learning_rate": 9.603887054480634e-07,
      "loss": 0.001,
      "step": 32220
    },
    {
      "epoch": 5860.0,
      "grad_norm": 0.3478619456291199,
      "learning_rate": 9.60343304136156e-07,
      "loss": 0.0012,
      "step": 32230
    },
    {
      "epoch": 5861.818181818182,
      "grad_norm": 0.006722213234752417,
      "learning_rate": 9.602978778947215e-07,
      "loss": 0.0011,
      "step": 32240
    },
    {
      "epoch": 5863.636363636364,
      "grad_norm": 0.004984652157872915,
      "learning_rate": 9.602524267262202e-07,
      "loss": 0.0012,
      "step": 32250
    },
    {
      "epoch": 5865.454545454545,
      "grad_norm": 0.004742128774523735,
      "learning_rate": 9.602069506331132e-07,
      "loss": 0.0011,
      "step": 32260
    },
    {
      "epoch": 5867.272727272727,
      "grad_norm": 0.3454093039035797,
      "learning_rate": 9.601614496178637e-07,
      "loss": 0.0013,
      "step": 32270
    },
    {
      "epoch": 5869.090909090909,
      "grad_norm": 0.014679688960313797,
      "learning_rate": 9.601159236829351e-07,
      "loss": 0.0012,
      "step": 32280
    },
    {
      "epoch": 5870.909090909091,
      "grad_norm": 0.003661594120785594,
      "learning_rate": 9.600703728307934e-07,
      "loss": 0.0012,
      "step": 32290
    },
    {
      "epoch": 5872.727272727273,
      "grad_norm": 0.0037520150654017925,
      "learning_rate": 9.600247970639052e-07,
      "loss": 0.0011,
      "step": 32300
    },
    {
      "epoch": 5874.545454545455,
      "grad_norm": 0.32340070605278015,
      "learning_rate": 9.599791963847385e-07,
      "loss": 0.001,
      "step": 32310
    },
    {
      "epoch": 5876.363636363636,
      "grad_norm": 0.26644256711006165,
      "learning_rate": 9.599335707957628e-07,
      "loss": 0.0014,
      "step": 32320
    },
    {
      "epoch": 5878.181818181818,
      "grad_norm": 0.008004692383110523,
      "learning_rate": 9.598879202994491e-07,
      "loss": 0.0009,
      "step": 32330
    },
    {
      "epoch": 5880.0,
      "grad_norm": 0.27661681175231934,
      "learning_rate": 9.598422448982696e-07,
      "loss": 0.0012,
      "step": 32340
    },
    {
      "epoch": 5881.818181818182,
      "grad_norm": 0.2594462037086487,
      "learning_rate": 9.597965445946972e-07,
      "loss": 0.0011,
      "step": 32350
    },
    {
      "epoch": 5883.636363636364,
      "grad_norm": 0.006168124731630087,
      "learning_rate": 9.597508193912076e-07,
      "loss": 0.0014,
      "step": 32360
    },
    {
      "epoch": 5885.454545454545,
      "grad_norm": 0.0054952604696154594,
      "learning_rate": 9.597050692902765e-07,
      "loss": 0.0008,
      "step": 32370
    },
    {
      "epoch": 5887.272727272727,
      "grad_norm": 0.0075012375600636005,
      "learning_rate": 9.596592942943815e-07,
      "loss": 0.0011,
      "step": 32380
    },
    {
      "epoch": 5889.090909090909,
      "grad_norm": 0.34501782059669495,
      "learning_rate": 9.596134944060017e-07,
      "loss": 0.0015,
      "step": 32390
    },
    {
      "epoch": 5890.909090909091,
      "grad_norm": 0.31200212240219116,
      "learning_rate": 9.595676696276171e-07,
      "loss": 0.0013,
      "step": 32400
    },
    {
      "epoch": 5892.727272727273,
      "grad_norm": 0.009286751039326191,
      "learning_rate": 9.595218199617097e-07,
      "loss": 0.001,
      "step": 32410
    },
    {
      "epoch": 5894.545454545455,
      "grad_norm": 0.004270628560334444,
      "learning_rate": 9.59475945410762e-07,
      "loss": 0.0015,
      "step": 32420
    },
    {
      "epoch": 5896.363636363636,
      "grad_norm": 0.30522552132606506,
      "learning_rate": 9.594300459772587e-07,
      "loss": 0.0014,
      "step": 32430
    },
    {
      "epoch": 5898.181818181818,
      "grad_norm": 0.36677849292755127,
      "learning_rate": 9.59384121663685e-07,
      "loss": 0.0011,
      "step": 32440
    },
    {
      "epoch": 5900.0,
      "grad_norm": 0.3400479853153229,
      "learning_rate": 9.593381724725284e-07,
      "loss": 0.0011,
      "step": 32450
    },
    {
      "epoch": 5901.818181818182,
      "grad_norm": 0.3175627589225769,
      "learning_rate": 9.59292198406277e-07,
      "loss": 0.0013,
      "step": 32460
    },
    {
      "epoch": 5903.636363636364,
      "grad_norm": 0.005181039683520794,
      "learning_rate": 9.592461994674204e-07,
      "loss": 0.001,
      "step": 32470
    },
    {
      "epoch": 5905.454545454545,
      "grad_norm": 0.0038919802755117416,
      "learning_rate": 9.592001756584497e-07,
      "loss": 0.0008,
      "step": 32480
    },
    {
      "epoch": 5907.272727272727,
      "grad_norm": 0.003635278670117259,
      "learning_rate": 9.591541269818572e-07,
      "loss": 0.0013,
      "step": 32490
    },
    {
      "epoch": 5909.090909090909,
      "grad_norm": 0.004769895225763321,
      "learning_rate": 9.591080534401371e-07,
      "loss": 0.0013,
      "step": 32500
    },
    {
      "epoch": 5909.090909090909,
      "eval_loss": 4.553934574127197,
      "eval_runtime": 0.9515,
      "eval_samples_per_second": 10.51,
      "eval_steps_per_second": 5.255,
      "step": 32500
    },
    {
      "epoch": 5910.909090909091,
      "grad_norm": 0.013491797260940075,
      "learning_rate": 9.590619550357838e-07,
      "loss": 0.0011,
      "step": 32510
    },
    {
      "epoch": 5912.727272727273,
      "grad_norm": 0.002711039735004306,
      "learning_rate": 9.59015831771294e-07,
      "loss": 0.0011,
      "step": 32520
    },
    {
      "epoch": 5914.545454545455,
      "grad_norm": 0.2740078866481781,
      "learning_rate": 9.589696836491657e-07,
      "loss": 0.0013,
      "step": 32530
    },
    {
      "epoch": 5916.363636363636,
      "grad_norm": 0.013948772102594376,
      "learning_rate": 9.589235106718976e-07,
      "loss": 0.0012,
      "step": 32540
    },
    {
      "epoch": 5918.181818181818,
      "grad_norm": 0.010011025704443455,
      "learning_rate": 9.588773128419905e-07,
      "loss": 0.0011,
      "step": 32550
    },
    {
      "epoch": 5920.0,
      "grad_norm": 0.36305952072143555,
      "learning_rate": 9.58831090161946e-07,
      "loss": 0.0013,
      "step": 32560
    },
    {
      "epoch": 5921.818181818182,
      "grad_norm": 0.006130971014499664,
      "learning_rate": 9.587848426342675e-07,
      "loss": 0.0013,
      "step": 32570
    },
    {
      "epoch": 5923.636363636364,
      "grad_norm": 0.2400597631931305,
      "learning_rate": 9.587385702614591e-07,
      "loss": 0.0011,
      "step": 32580
    },
    {
      "epoch": 5925.454545454545,
      "grad_norm": 0.31658023595809937,
      "learning_rate": 9.586922730460272e-07,
      "loss": 0.0013,
      "step": 32590
    },
    {
      "epoch": 5927.272727272727,
      "grad_norm": 0.00905443076044321,
      "learning_rate": 9.586459509904785e-07,
      "loss": 0.0008,
      "step": 32600
    },
    {
      "epoch": 5929.090909090909,
      "grad_norm": 0.4121483564376831,
      "learning_rate": 9.585996040973216e-07,
      "loss": 0.0014,
      "step": 32610
    },
    {
      "epoch": 5930.909090909091,
      "grad_norm": 0.008393710479140282,
      "learning_rate": 9.585532323690666e-07,
      "loss": 0.0011,
      "step": 32620
    },
    {
      "epoch": 5932.727272727273,
      "grad_norm": 0.2650970220565796,
      "learning_rate": 9.585068358082246e-07,
      "loss": 0.0011,
      "step": 32630
    },
    {
      "epoch": 5934.545454545455,
      "grad_norm": 0.009242596104741096,
      "learning_rate": 9.584604144173081e-07,
      "loss": 0.0011,
      "step": 32640
    },
    {
      "epoch": 5936.363636363636,
      "grad_norm": 0.005340438801795244,
      "learning_rate": 9.584139681988313e-07,
      "loss": 0.0012,
      "step": 32650
    },
    {
      "epoch": 5938.181818181818,
      "grad_norm": 0.003219180041924119,
      "learning_rate": 9.58367497155309e-07,
      "loss": 0.0011,
      "step": 32660
    },
    {
      "epoch": 5940.0,
      "grad_norm": 0.006959317252039909,
      "learning_rate": 9.58321001289258e-07,
      "loss": 0.0012,
      "step": 32670
    },
    {
      "epoch": 5941.818181818182,
      "grad_norm": 0.25538378953933716,
      "learning_rate": 9.582744806031965e-07,
      "loss": 0.0012,
      "step": 32680
    },
    {
      "epoch": 5943.636363636364,
      "grad_norm": 0.38706618547439575,
      "learning_rate": 9.582279350996437e-07,
      "loss": 0.0011,
      "step": 32690
    },
    {
      "epoch": 5945.454545454545,
      "grad_norm": 0.009788711555302143,
      "learning_rate": 9.581813647811197e-07,
      "loss": 0.0008,
      "step": 32700
    },
    {
      "epoch": 5947.272727272727,
      "grad_norm": 0.07372737675905228,
      "learning_rate": 9.581347696501471e-07,
      "loss": 0.0017,
      "step": 32710
    },
    {
      "epoch": 5949.090909090909,
      "grad_norm": 0.47522324323654175,
      "learning_rate": 9.580881497092491e-07,
      "loss": 0.001,
      "step": 32720
    },
    {
      "epoch": 5950.909090909091,
      "grad_norm": 0.0068811457604169846,
      "learning_rate": 9.580415049609502e-07,
      "loss": 0.001,
      "step": 32730
    },
    {
      "epoch": 5952.727272727273,
      "grad_norm": 0.38901641964912415,
      "learning_rate": 9.579948354077767e-07,
      "loss": 0.0013,
      "step": 32740
    },
    {
      "epoch": 5954.545454545455,
      "grad_norm": 0.0035672751255333424,
      "learning_rate": 9.579481410522556e-07,
      "loss": 0.0011,
      "step": 32750
    },
    {
      "epoch": 5956.363636363636,
      "grad_norm": 0.010656207799911499,
      "learning_rate": 9.579014218969157e-07,
      "loss": 0.0011,
      "step": 32760
    },
    {
      "epoch": 5958.181818181818,
      "grad_norm": 0.3144693374633789,
      "learning_rate": 9.578546779442871e-07,
      "loss": 0.0013,
      "step": 32770
    },
    {
      "epoch": 5960.0,
      "grad_norm": 0.3820239007472992,
      "learning_rate": 9.578079091969012e-07,
      "loss": 0.0011,
      "step": 32780
    },
    {
      "epoch": 5961.818181818182,
      "grad_norm": 0.009515897370874882,
      "learning_rate": 9.577611156572906e-07,
      "loss": 0.0013,
      "step": 32790
    },
    {
      "epoch": 5963.636363636364,
      "grad_norm": 0.3631286323070526,
      "learning_rate": 9.577142973279894e-07,
      "loss": 0.0011,
      "step": 32800
    },
    {
      "epoch": 5965.454545454545,
      "grad_norm": 0.3644176423549652,
      "learning_rate": 9.576674542115332e-07,
      "loss": 0.0012,
      "step": 32810
    },
    {
      "epoch": 5967.272727272727,
      "grad_norm": 0.33665940165519714,
      "learning_rate": 9.576205863104586e-07,
      "loss": 0.0011,
      "step": 32820
    },
    {
      "epoch": 5969.090909090909,
      "grad_norm": 0.2991873323917389,
      "learning_rate": 9.575736936273038e-07,
      "loss": 0.001,
      "step": 32830
    },
    {
      "epoch": 5970.909090909091,
      "grad_norm": 0.38764965534210205,
      "learning_rate": 9.575267761646082e-07,
      "loss": 0.0011,
      "step": 32840
    },
    {
      "epoch": 5972.727272727273,
      "grad_norm": 0.2537049651145935,
      "learning_rate": 9.574798339249123e-07,
      "loss": 0.0012,
      "step": 32850
    },
    {
      "epoch": 5974.545454545455,
      "grad_norm": 0.05659433826804161,
      "learning_rate": 9.574328669107586e-07,
      "loss": 0.0012,
      "step": 32860
    },
    {
      "epoch": 5976.363636363636,
      "grad_norm": 0.39533594250679016,
      "learning_rate": 9.573858751246903e-07,
      "loss": 0.0013,
      "step": 32870
    },
    {
      "epoch": 5978.181818181818,
      "grad_norm": 0.008758431300520897,
      "learning_rate": 9.573388585692524e-07,
      "loss": 0.0009,
      "step": 32880
    },
    {
      "epoch": 5980.0,
      "grad_norm": 0.3386072516441345,
      "learning_rate": 9.57291817246991e-07,
      "loss": 0.0013,
      "step": 32890
    },
    {
      "epoch": 5981.818181818182,
      "grad_norm": 0.34584176540374756,
      "learning_rate": 9.572447511604534e-07,
      "loss": 0.0012,
      "step": 32900
    },
    {
      "epoch": 5983.636363636364,
      "grad_norm": 0.24800018966197968,
      "learning_rate": 9.571976603121887e-07,
      "loss": 0.0013,
      "step": 32910
    },
    {
      "epoch": 5985.454545454545,
      "grad_norm": 0.0033280218485742807,
      "learning_rate": 9.571505447047467e-07,
      "loss": 0.0009,
      "step": 32920
    },
    {
      "epoch": 5987.272727272727,
      "grad_norm": 0.009574090130627155,
      "learning_rate": 9.571034043406796e-07,
      "loss": 0.0012,
      "step": 32930
    },
    {
      "epoch": 5989.090909090909,
      "grad_norm": 0.0051642670296132565,
      "learning_rate": 9.570562392225394e-07,
      "loss": 0.0011,
      "step": 32940
    },
    {
      "epoch": 5990.909090909091,
      "grad_norm": 0.00512454379349947,
      "learning_rate": 9.57009049352881e-07,
      "loss": 0.0013,
      "step": 32950
    },
    {
      "epoch": 5992.727272727273,
      "grad_norm": 0.34660568833351135,
      "learning_rate": 9.569618347342591e-07,
      "loss": 0.0013,
      "step": 32960
    },
    {
      "epoch": 5994.545454545455,
      "grad_norm": 0.368358314037323,
      "learning_rate": 9.569145953692314e-07,
      "loss": 0.0011,
      "step": 32970
    },
    {
      "epoch": 5996.363636363636,
      "grad_norm": 0.29107579588890076,
      "learning_rate": 9.56867331260356e-07,
      "loss": 0.0013,
      "step": 32980
    },
    {
      "epoch": 5998.181818181818,
      "grad_norm": 0.4315337538719177,
      "learning_rate": 9.568200424101918e-07,
      "loss": 0.0013,
      "step": 32990
    },
    {
      "epoch": 6000.0,
      "grad_norm": 0.4593850076198578,
      "learning_rate": 9.567727288213004e-07,
      "loss": 0.001,
      "step": 33000
    },
    {
      "epoch": 6000.0,
      "eval_loss": 4.4780473709106445,
      "eval_runtime": 0.9553,
      "eval_samples_per_second": 10.468,
      "eval_steps_per_second": 5.234,
      "step": 33000
    },
    {
      "epoch": 6001.818181818182,
      "grad_norm": 0.22792044281959534,
      "learning_rate": 9.567253904962439e-07,
      "loss": 0.0013,
      "step": 33010
    },
    {
      "epoch": 6003.636363636364,
      "grad_norm": 0.0037655599880963564,
      "learning_rate": 9.566780274375854e-07,
      "loss": 0.0009,
      "step": 33020
    },
    {
      "epoch": 6005.454545454545,
      "grad_norm": 0.003930587321519852,
      "learning_rate": 9.566306396478902e-07,
      "loss": 0.0013,
      "step": 33030
    },
    {
      "epoch": 6007.272727272727,
      "grad_norm": 0.006011962424963713,
      "learning_rate": 9.565832271297247e-07,
      "loss": 0.0009,
      "step": 33040
    },
    {
      "epoch": 6009.090909090909,
      "grad_norm": 0.3195541203022003,
      "learning_rate": 9.565357898856562e-07,
      "loss": 0.0014,
      "step": 33050
    },
    {
      "epoch": 6010.909090909091,
      "grad_norm": 0.3245621919631958,
      "learning_rate": 9.564883279182536e-07,
      "loss": 0.0011,
      "step": 33060
    },
    {
      "epoch": 6012.727272727273,
      "grad_norm": 0.2621651589870453,
      "learning_rate": 9.564408412300873e-07,
      "loss": 0.0012,
      "step": 33070
    },
    {
      "epoch": 6014.545454545455,
      "grad_norm": 0.2628767490386963,
      "learning_rate": 9.56393329823729e-07,
      "loss": 0.001,
      "step": 33080
    },
    {
      "epoch": 6016.363636363636,
      "grad_norm": 0.006827023811638355,
      "learning_rate": 9.563457937017514e-07,
      "loss": 0.0009,
      "step": 33090
    },
    {
      "epoch": 6018.181818181818,
      "grad_norm": 0.004223241005092859,
      "learning_rate": 9.562982328667288e-07,
      "loss": 0.0013,
      "step": 33100
    },
    {
      "epoch": 6020.0,
      "grad_norm": 0.32985129952430725,
      "learning_rate": 9.562506473212371e-07,
      "loss": 0.0013,
      "step": 33110
    },
    {
      "epoch": 6021.818181818182,
      "grad_norm": 0.35072723031044006,
      "learning_rate": 9.56203037067853e-07,
      "loss": 0.0013,
      "step": 33120
    },
    {
      "epoch": 6023.636363636364,
      "grad_norm": 0.0033977050334215164,
      "learning_rate": 9.561554021091549e-07,
      "loss": 0.0009,
      "step": 33130
    },
    {
      "epoch": 6025.454545454545,
      "grad_norm": 0.004505048040300608,
      "learning_rate": 9.561077424477223e-07,
      "loss": 0.0013,
      "step": 33140
    },
    {
      "epoch": 6027.272727272727,
      "grad_norm": 0.005831203889101744,
      "learning_rate": 9.560600580861365e-07,
      "loss": 0.0011,
      "step": 33150
    },
    {
      "epoch": 6029.090909090909,
      "grad_norm": 0.3856504261493683,
      "learning_rate": 9.560123490269793e-07,
      "loss": 0.0012,
      "step": 33160
    },
    {
      "epoch": 6030.909090909091,
      "grad_norm": 0.2329663187265396,
      "learning_rate": 9.55964615272835e-07,
      "loss": 0.0012,
      "step": 33170
    },
    {
      "epoch": 6032.727272727273,
      "grad_norm": 0.004302626010030508,
      "learning_rate": 9.559168568262878e-07,
      "loss": 0.001,
      "step": 33180
    },
    {
      "epoch": 6034.545454545455,
      "grad_norm": 0.00531098572537303,
      "learning_rate": 9.558690736899246e-07,
      "loss": 0.0015,
      "step": 33190
    },
    {
      "epoch": 6036.363636363636,
      "grad_norm": 0.011640017852187157,
      "learning_rate": 9.55821265866333e-07,
      "loss": 0.0011,
      "step": 33200
    },
    {
      "epoch": 6038.181818181818,
      "grad_norm": 0.23862464725971222,
      "learning_rate": 9.557734333581017e-07,
      "loss": 0.0011,
      "step": 33210
    },
    {
      "epoch": 6040.0,
      "grad_norm": 0.005008903797715902,
      "learning_rate": 9.557255761678214e-07,
      "loss": 0.0012,
      "step": 33220
    },
    {
      "epoch": 6041.818181818182,
      "grad_norm": 0.004738312680274248,
      "learning_rate": 9.556776942980836e-07,
      "loss": 0.0012,
      "step": 33230
    },
    {
      "epoch": 6043.636363636364,
      "grad_norm": 0.4169629216194153,
      "learning_rate": 9.55629787751481e-07,
      "loss": 0.0011,
      "step": 33240
    },
    {
      "epoch": 6045.454545454545,
      "grad_norm": 0.3352556824684143,
      "learning_rate": 9.555818565306084e-07,
      "loss": 0.0013,
      "step": 33250
    },
    {
      "epoch": 6047.272727272727,
      "grad_norm": 0.2646954357624054,
      "learning_rate": 9.555339006380614e-07,
      "loss": 0.0011,
      "step": 33260
    },
    {
      "epoch": 6049.090909090909,
      "grad_norm": 0.2558628022670746,
      "learning_rate": 9.554859200764368e-07,
      "loss": 0.0012,
      "step": 33270
    },
    {
      "epoch": 6050.909090909091,
      "grad_norm": 0.3588981330394745,
      "learning_rate": 9.554379148483332e-07,
      "loss": 0.0011,
      "step": 33280
    },
    {
      "epoch": 6052.727272727273,
      "grad_norm": 0.352344274520874,
      "learning_rate": 9.5538988495635e-07,
      "loss": 0.0011,
      "step": 33290
    },
    {
      "epoch": 6054.545454545455,
      "grad_norm": 0.25013965368270874,
      "learning_rate": 9.553418304030885e-07,
      "loss": 0.0012,
      "step": 33300
    },
    {
      "epoch": 6056.363636363636,
      "grad_norm": 0.23596997559070587,
      "learning_rate": 9.55293751191151e-07,
      "loss": 0.001,
      "step": 33310
    },
    {
      "epoch": 6058.181818181818,
      "grad_norm": 0.2804211974143982,
      "learning_rate": 9.55245647323141e-07,
      "loss": 0.0012,
      "step": 33320
    },
    {
      "epoch": 6060.0,
      "grad_norm": 0.007354936096817255,
      "learning_rate": 9.551975188016636e-07,
      "loss": 0.0011,
      "step": 33330
    },
    {
      "epoch": 6061.818181818182,
      "grad_norm": 0.004612362012267113,
      "learning_rate": 9.551493656293252e-07,
      "loss": 0.001,
      "step": 33340
    },
    {
      "epoch": 6063.636363636364,
      "grad_norm": 0.33400243520736694,
      "learning_rate": 9.551011878087337e-07,
      "loss": 0.0016,
      "step": 33350
    },
    {
      "epoch": 6065.454545454545,
      "grad_norm": 0.004391396418213844,
      "learning_rate": 9.550529853424978e-07,
      "loss": 0.0009,
      "step": 33360
    },
    {
      "epoch": 6067.272727272727,
      "grad_norm": 0.2458145171403885,
      "learning_rate": 9.55004758233228e-07,
      "loss": 0.0012,
      "step": 33370
    },
    {
      "epoch": 6069.090909090909,
      "grad_norm": 0.004068833775818348,
      "learning_rate": 9.549565064835361e-07,
      "loss": 0.0012,
      "step": 33380
    },
    {
      "epoch": 6070.909090909091,
      "grad_norm": 0.04167540743947029,
      "learning_rate": 9.549082300960349e-07,
      "loss": 0.0012,
      "step": 33390
    },
    {
      "epoch": 6072.727272727273,
      "grad_norm": 0.01729150302708149,
      "learning_rate": 9.54859929073339e-07,
      "loss": 0.002,
      "step": 33400
    },
    {
      "epoch": 6074.545454545455,
      "grad_norm": 0.048877887427806854,
      "learning_rate": 9.548116034180641e-07,
      "loss": 0.0009,
      "step": 33410
    },
    {
      "epoch": 6076.363636363636,
      "grad_norm": 0.015497932210564613,
      "learning_rate": 9.547632531328272e-07,
      "loss": 0.0015,
      "step": 33420
    },
    {
      "epoch": 6078.181818181818,
      "grad_norm": 0.34051749110221863,
      "learning_rate": 9.547148782202465e-07,
      "loss": 0.0011,
      "step": 33430
    },
    {
      "epoch": 6080.0,
      "grad_norm": 0.29897540807724,
      "learning_rate": 9.546664786829418e-07,
      "loss": 0.0013,
      "step": 33440
    },
    {
      "epoch": 6081.818181818182,
      "grad_norm": 0.27377355098724365,
      "learning_rate": 9.546180545235343e-07,
      "loss": 0.0011,
      "step": 33450
    },
    {
      "epoch": 6083.636363636364,
      "grad_norm": 0.36610084772109985,
      "learning_rate": 9.545696057446463e-07,
      "loss": 0.0016,
      "step": 33460
    },
    {
      "epoch": 6085.454545454545,
      "grad_norm": 0.2827785909175873,
      "learning_rate": 9.54521132348901e-07,
      "loss": 0.001,
      "step": 33470
    },
    {
      "epoch": 6087.272727272727,
      "grad_norm": 0.37069299817085266,
      "learning_rate": 9.544726343389243e-07,
      "loss": 0.0011,
      "step": 33480
    },
    {
      "epoch": 6089.090909090909,
      "grad_norm": 0.034764885902404785,
      "learning_rate": 9.54424111717342e-07,
      "loss": 0.0011,
      "step": 33490
    },
    {
      "epoch": 6090.909090909091,
      "grad_norm": 0.31384649872779846,
      "learning_rate": 9.543755644867822e-07,
      "loss": 0.0012,
      "step": 33500
    },
    {
      "epoch": 6090.909090909091,
      "eval_loss": 4.526725769042969,
      "eval_runtime": 0.9477,
      "eval_samples_per_second": 10.552,
      "eval_steps_per_second": 5.276,
      "step": 33500
    },
    {
      "epoch": 6092.727272727273,
      "grad_norm": 0.016079366207122803,
      "learning_rate": 9.543269926498733e-07,
      "loss": 0.0012,
      "step": 33510
    },
    {
      "epoch": 6094.545454545455,
      "grad_norm": 0.009177195839583874,
      "learning_rate": 9.542783962092465e-07,
      "loss": 0.001,
      "step": 33520
    },
    {
      "epoch": 6096.363636363636,
      "grad_norm": 0.31601595878601074,
      "learning_rate": 9.542297751675328e-07,
      "loss": 0.0015,
      "step": 33530
    },
    {
      "epoch": 6098.181818181818,
      "grad_norm": 0.006631635595113039,
      "learning_rate": 9.541811295273654e-07,
      "loss": 0.0008,
      "step": 33540
    },
    {
      "epoch": 6100.0,
      "grad_norm": 0.014627349562942982,
      "learning_rate": 9.541324592913791e-07,
      "loss": 0.0013,
      "step": 33550
    },
    {
      "epoch": 6101.818181818182,
      "grad_norm": 0.43558743596076965,
      "learning_rate": 9.54083764462209e-07,
      "loss": 0.0013,
      "step": 33560
    },
    {
      "epoch": 6103.636363636364,
      "grad_norm": 0.33479413390159607,
      "learning_rate": 9.540350450424925e-07,
      "loss": 0.0012,
      "step": 33570
    },
    {
      "epoch": 6105.454545454545,
      "grad_norm": 0.003168421797454357,
      "learning_rate": 9.53986301034868e-07,
      "loss": 0.0011,
      "step": 33580
    },
    {
      "epoch": 6107.272727272727,
      "grad_norm": 0.3257238566875458,
      "learning_rate": 9.539375324419747e-07,
      "loss": 0.0009,
      "step": 33590
    },
    {
      "epoch": 6109.090909090909,
      "grad_norm": 0.2516600787639618,
      "learning_rate": 9.538887392664543e-07,
      "loss": 0.0012,
      "step": 33600
    },
    {
      "epoch": 6110.909090909091,
      "grad_norm": 0.005745044443756342,
      "learning_rate": 9.538399215109486e-07,
      "loss": 0.0012,
      "step": 33610
    },
    {
      "epoch": 6112.727272727273,
      "grad_norm": 0.3725270628929138,
      "learning_rate": 9.537910791781018e-07,
      "loss": 0.0012,
      "step": 33620
    },
    {
      "epoch": 6114.545454545455,
      "grad_norm": 0.4360218048095703,
      "learning_rate": 9.537422122705583e-07,
      "loss": 0.0012,
      "step": 33630
    },
    {
      "epoch": 6116.363636363636,
      "grad_norm": 0.00569355022162199,
      "learning_rate": 9.53693320790965e-07,
      "loss": 0.0008,
      "step": 33640
    },
    {
      "epoch": 6118.181818181818,
      "grad_norm": 0.013285193592309952,
      "learning_rate": 9.536444047419694e-07,
      "loss": 0.0013,
      "step": 33650
    },
    {
      "epoch": 6120.0,
      "grad_norm": 0.00883909035474062,
      "learning_rate": 9.535954641262205e-07,
      "loss": 0.0012,
      "step": 33660
    },
    {
      "epoch": 6121.818181818182,
      "grad_norm": 0.005077608395367861,
      "learning_rate": 9.535464989463684e-07,
      "loss": 0.0011,
      "step": 33670
    },
    {
      "epoch": 6123.636363636364,
      "grad_norm": 0.00720279710367322,
      "learning_rate": 9.534975092050652e-07,
      "loss": 0.0009,
      "step": 33680
    },
    {
      "epoch": 6125.454545454545,
      "grad_norm": 0.005351516418159008,
      "learning_rate": 9.534484949049635e-07,
      "loss": 0.0013,
      "step": 33690
    },
    {
      "epoch": 6127.272727272727,
      "grad_norm": 0.33082282543182373,
      "learning_rate": 9.533994560487179e-07,
      "loss": 0.0011,
      "step": 33700
    },
    {
      "epoch": 6129.090909090909,
      "grad_norm": 0.27187955379486084,
      "learning_rate": 9.533503926389841e-07,
      "loss": 0.0011,
      "step": 33710
    },
    {
      "epoch": 6130.909090909091,
      "grad_norm": 0.29463768005371094,
      "learning_rate": 9.533013046784189e-07,
      "loss": 0.0012,
      "step": 33720
    },
    {
      "epoch": 6132.727272727273,
      "grad_norm": 0.011612159200012684,
      "learning_rate": 9.532521921696806e-07,
      "loss": 0.001,
      "step": 33730
    },
    {
      "epoch": 6134.545454545455,
      "grad_norm": 0.2214287966489792,
      "learning_rate": 9.532030551154289e-07,
      "loss": 0.0014,
      "step": 33740
    },
    {
      "epoch": 6136.363636363636,
      "grad_norm": 0.2896365225315094,
      "learning_rate": 9.531538935183249e-07,
      "loss": 0.0012,
      "step": 33750
    },
    {
      "epoch": 6138.181818181818,
      "grad_norm": 0.005616836249828339,
      "learning_rate": 9.531047073810308e-07,
      "loss": 0.0011,
      "step": 33760
    },
    {
      "epoch": 6140.0,
      "grad_norm": 0.19918332993984222,
      "learning_rate": 9.530554967062103e-07,
      "loss": 0.0014,
      "step": 33770
    },
    {
      "epoch": 6141.818181818182,
      "grad_norm": 0.014550959691405296,
      "learning_rate": 9.530062614965284e-07,
      "loss": 0.0013,
      "step": 33780
    },
    {
      "epoch": 6143.636363636364,
      "grad_norm": 0.4180399179458618,
      "learning_rate": 9.529570017546511e-07,
      "loss": 0.0011,
      "step": 33790
    },
    {
      "epoch": 6145.454545454545,
      "grad_norm": 0.4066869020462036,
      "learning_rate": 9.529077174832465e-07,
      "loss": 0.0014,
      "step": 33800
    },
    {
      "epoch": 6147.272727272727,
      "grad_norm": 0.004032505676150322,
      "learning_rate": 9.528584086849832e-07,
      "loss": 0.0006,
      "step": 33810
    },
    {
      "epoch": 6149.090909090909,
      "grad_norm": 0.3754355311393738,
      "learning_rate": 9.528090753625314e-07,
      "loss": 0.0013,
      "step": 33820
    },
    {
      "epoch": 6150.909090909091,
      "grad_norm": 0.24532130360603333,
      "learning_rate": 9.52759717518563e-07,
      "loss": 0.0012,
      "step": 33830
    },
    {
      "epoch": 6152.727272727273,
      "grad_norm": 0.003481966443359852,
      "learning_rate": 9.527103351557508e-07,
      "loss": 0.0011,
      "step": 33840
    },
    {
      "epoch": 6154.545454545455,
      "grad_norm": 0.004531671293079853,
      "learning_rate": 9.52660928276769e-07,
      "loss": 0.0011,
      "step": 33850
    },
    {
      "epoch": 6156.363636363636,
      "grad_norm": 0.33052152395248413,
      "learning_rate": 9.526114968842935e-07,
      "loss": 0.0012,
      "step": 33860
    },
    {
      "epoch": 6158.181818181818,
      "grad_norm": 0.006054179277271032,
      "learning_rate": 9.525620409810007e-07,
      "loss": 0.0011,
      "step": 33870
    },
    {
      "epoch": 6160.0,
      "grad_norm": 0.004213160369545221,
      "learning_rate": 9.525125605695693e-07,
      "loss": 0.0013,
      "step": 33880
    },
    {
      "epoch": 6161.818181818182,
      "grad_norm": 0.0031611525919288397,
      "learning_rate": 9.524630556526787e-07,
      "loss": 0.0011,
      "step": 33890
    },
    {
      "epoch": 6163.636363636364,
      "grad_norm": 0.2466639280319214,
      "learning_rate": 9.524135262330098e-07,
      "loss": 0.0013,
      "step": 33900
    },
    {
      "epoch": 6165.454545454545,
      "grad_norm": 0.4075509309768677,
      "learning_rate": 9.523639723132447e-07,
      "loss": 0.0012,
      "step": 33910
    },
    {
      "epoch": 6167.272727272727,
      "grad_norm": 0.006282470189034939,
      "learning_rate": 9.523143938960671e-07,
      "loss": 0.0008,
      "step": 33920
    },
    {
      "epoch": 6169.090909090909,
      "grad_norm": 0.2579268217086792,
      "learning_rate": 9.52264790984162e-07,
      "loss": 0.0013,
      "step": 33930
    },
    {
      "epoch": 6170.909090909091,
      "grad_norm": 0.2295890897512436,
      "learning_rate": 9.522151635802154e-07,
      "loss": 0.0012,
      "step": 33940
    },
    {
      "epoch": 6172.727272727273,
      "grad_norm": 0.004860702436417341,
      "learning_rate": 9.521655116869149e-07,
      "loss": 0.0011,
      "step": 33950
    },
    {
      "epoch": 6174.545454545455,
      "grad_norm": 0.3258454203605652,
      "learning_rate": 9.521158353069493e-07,
      "loss": 0.0012,
      "step": 33960
    },
    {
      "epoch": 6176.363636363636,
      "grad_norm": 0.3087271749973297,
      "learning_rate": 9.520661344430089e-07,
      "loss": 0.001,
      "step": 33970
    },
    {
      "epoch": 6178.181818181818,
      "grad_norm": 0.0033639571629464626,
      "learning_rate": 9.520164090977852e-07,
      "loss": 0.0011,
      "step": 33980
    },
    {
      "epoch": 6180.0,
      "grad_norm": 0.4023734927177429,
      "learning_rate": 9.519666592739708e-07,
      "loss": 0.0012,
      "step": 33990
    },
    {
      "epoch": 6181.818181818182,
      "grad_norm": 0.0027604640927165747,
      "learning_rate": 9.519168849742602e-07,
      "loss": 0.0011,
      "step": 34000
    },
    {
      "epoch": 6181.818181818182,
      "eval_loss": 4.507990837097168,
      "eval_runtime": 0.9486,
      "eval_samples_per_second": 10.542,
      "eval_steps_per_second": 5.271,
      "step": 34000
    },
    {
      "epoch": 6183.636363636364,
      "grad_norm": 0.009994370862841606,
      "learning_rate": 9.518670862013487e-07,
      "loss": 0.0013,
      "step": 34010
    },
    {
      "epoch": 6185.454545454545,
      "grad_norm": 0.24366599321365356,
      "learning_rate": 9.518172629579333e-07,
      "loss": 0.0012,
      "step": 34020
    },
    {
      "epoch": 6187.272727272727,
      "grad_norm": 0.25172850489616394,
      "learning_rate": 9.517674152467118e-07,
      "loss": 0.0011,
      "step": 34030
    },
    {
      "epoch": 6189.090909090909,
      "grad_norm": 0.014482862316071987,
      "learning_rate": 9.517175430703839e-07,
      "loss": 0.001,
      "step": 34040
    },
    {
      "epoch": 6190.909090909091,
      "grad_norm": 0.3037009835243225,
      "learning_rate": 9.516676464316504e-07,
      "loss": 0.0013,
      "step": 34050
    },
    {
      "epoch": 6192.727272727273,
      "grad_norm": 0.0040797279216349125,
      "learning_rate": 9.516177253332133e-07,
      "loss": 0.001,
      "step": 34060
    },
    {
      "epoch": 6194.545454545455,
      "grad_norm": 0.3010890781879425,
      "learning_rate": 9.515677797777762e-07,
      "loss": 0.001,
      "step": 34070
    },
    {
      "epoch": 6196.363636363636,
      "grad_norm": 0.22521235048770905,
      "learning_rate": 9.515178097680437e-07,
      "loss": 0.0011,
      "step": 34080
    },
    {
      "epoch": 6198.181818181818,
      "grad_norm": 0.33409354090690613,
      "learning_rate": 9.514678153067217e-07,
      "loss": 0.0013,
      "step": 34090
    },
    {
      "epoch": 6200.0,
      "grad_norm": 0.3494672477245331,
      "learning_rate": 9.514177963965181e-07,
      "loss": 0.0011,
      "step": 34100
    },
    {
      "epoch": 6201.818181818182,
      "grad_norm": 0.3284503221511841,
      "learning_rate": 9.513677530401413e-07,
      "loss": 0.0013,
      "step": 34110
    },
    {
      "epoch": 6203.636363636364,
      "grad_norm": 0.427890419960022,
      "learning_rate": 9.513176852403015e-07,
      "loss": 0.0013,
      "step": 34120
    },
    {
      "epoch": 6205.454545454545,
      "grad_norm": 0.22741280496120453,
      "learning_rate": 9.512675929997101e-07,
      "loss": 0.0007,
      "step": 34130
    },
    {
      "epoch": 6207.272727272727,
      "grad_norm": 0.030791306868195534,
      "learning_rate": 9.512174763210797e-07,
      "loss": 0.0015,
      "step": 34140
    },
    {
      "epoch": 6209.090909090909,
      "grad_norm": 0.006318599916994572,
      "learning_rate": 9.511673352071243e-07,
      "loss": 0.0008,
      "step": 34150
    },
    {
      "epoch": 6210.909090909091,
      "grad_norm": 0.4100417196750641,
      "learning_rate": 9.511171696605593e-07,
      "loss": 0.0012,
      "step": 34160
    },
    {
      "epoch": 6212.727272727273,
      "grad_norm": 0.007539333309978247,
      "learning_rate": 9.510669796841013e-07,
      "loss": 0.001,
      "step": 34170
    },
    {
      "epoch": 6214.545454545455,
      "grad_norm": 0.2138868123292923,
      "learning_rate": 9.510167652804686e-07,
      "loss": 0.0014,
      "step": 34180
    },
    {
      "epoch": 6216.363636363636,
      "grad_norm": 0.005151938181370497,
      "learning_rate": 9.509665264523802e-07,
      "loss": 0.0009,
      "step": 34190
    },
    {
      "epoch": 6218.181818181818,
      "grad_norm": 0.25709134340286255,
      "learning_rate": 9.509162632025569e-07,
      "loss": 0.0012,
      "step": 34200
    },
    {
      "epoch": 6220.0,
      "grad_norm": 0.005653403699398041,
      "learning_rate": 9.508659755337205e-07,
      "loss": 0.0012,
      "step": 34210
    },
    {
      "epoch": 6221.818181818182,
      "grad_norm": 0.005627221427857876,
      "learning_rate": 9.508156634485945e-07,
      "loss": 0.0012,
      "step": 34220
    },
    {
      "epoch": 6223.636363636364,
      "grad_norm": 0.2339840680360794,
      "learning_rate": 9.507653269499034e-07,
      "loss": 0.001,
      "step": 34230
    },
    {
      "epoch": 6225.454545454545,
      "grad_norm": 0.09586329758167267,
      "learning_rate": 9.507149660403732e-07,
      "loss": 0.0014,
      "step": 34240
    },
    {
      "epoch": 6227.272727272727,
      "grad_norm": 0.007936845533549786,
      "learning_rate": 9.506645807227311e-07,
      "loss": 0.0011,
      "step": 34250
    },
    {
      "epoch": 6229.090909090909,
      "grad_norm": 0.2590208351612091,
      "learning_rate": 9.506141709997057e-07,
      "loss": 0.0011,
      "step": 34260
    },
    {
      "epoch": 6230.909090909091,
      "grad_norm": 0.0047600604593753815,
      "learning_rate": 9.505637368740267e-07,
      "loss": 0.0013,
      "step": 34270
    },
    {
      "epoch": 6232.727272727273,
      "grad_norm": 0.02753518894314766,
      "learning_rate": 9.505132783484258e-07,
      "loss": 0.0012,
      "step": 34280
    },
    {
      "epoch": 6234.545454545455,
      "grad_norm": 0.008908305317163467,
      "learning_rate": 9.50462795425635e-07,
      "loss": 0.001,
      "step": 34290
    },
    {
      "epoch": 6236.363636363636,
      "grad_norm": 0.015290831215679646,
      "learning_rate": 9.504122881083885e-07,
      "loss": 0.0011,
      "step": 34300
    },
    {
      "epoch": 6238.181818181818,
      "grad_norm": 0.29607832431793213,
      "learning_rate": 9.503617563994213e-07,
      "loss": 0.0012,
      "step": 34310
    },
    {
      "epoch": 6240.0,
      "grad_norm": 0.003423523623496294,
      "learning_rate": 9.503112003014701e-07,
      "loss": 0.0011,
      "step": 34320
    },
    {
      "epoch": 6241.818181818182,
      "grad_norm": 0.005315108690410852,
      "learning_rate": 9.502606198172727e-07,
      "loss": 0.0011,
      "step": 34330
    },
    {
      "epoch": 6243.636363636364,
      "grad_norm": 0.003268049331381917,
      "learning_rate": 9.50210014949568e-07,
      "loss": 0.0011,
      "step": 34340
    },
    {
      "epoch": 6245.454545454545,
      "grad_norm": 0.3998797535896301,
      "learning_rate": 9.501593857010968e-07,
      "loss": 0.0014,
      "step": 34350
    },
    {
      "epoch": 6247.272727272727,
      "grad_norm": 0.008903959766030312,
      "learning_rate": 9.501087320746007e-07,
      "loss": 0.0011,
      "step": 34360
    },
    {
      "epoch": 6249.090909090909,
      "grad_norm": 0.3323065936565399,
      "learning_rate": 9.500580540728226e-07,
      "loss": 0.001,
      "step": 34370
    },
    {
      "epoch": 6250.909090909091,
      "grad_norm": 0.3286014795303345,
      "learning_rate": 9.500073516985073e-07,
      "loss": 0.001,
      "step": 34380
    },
    {
      "epoch": 6252.727272727273,
      "grad_norm": 0.0027435647789388895,
      "learning_rate": 9.499566249544004e-07,
      "loss": 0.0012,
      "step": 34390
    },
    {
      "epoch": 6254.545454545455,
      "grad_norm": 0.0035343486815690994,
      "learning_rate": 9.499058738432491e-07,
      "loss": 0.001,
      "step": 34400
    },
    {
      "epoch": 6256.363636363636,
      "grad_norm": 0.25372084975242615,
      "learning_rate": 9.498550983678015e-07,
      "loss": 0.0013,
      "step": 34410
    },
    {
      "epoch": 6258.181818181818,
      "grad_norm": 0.037250664085149765,
      "learning_rate": 9.498042985308074e-07,
      "loss": 0.0014,
      "step": 34420
    },
    {
      "epoch": 6260.0,
      "grad_norm": 0.26591184735298157,
      "learning_rate": 9.497534743350179e-07,
      "loss": 0.001,
      "step": 34430
    },
    {
      "epoch": 6261.818181818182,
      "grad_norm": 0.43324291706085205,
      "learning_rate": 9.497026257831855e-07,
      "loss": 0.0011,
      "step": 34440
    },
    {
      "epoch": 6263.636363636364,
      "grad_norm": 0.013876033015549183,
      "learning_rate": 9.496517528780637e-07,
      "loss": 0.0011,
      "step": 34450
    },
    {
      "epoch": 6265.454545454545,
      "grad_norm": 0.005644533783197403,
      "learning_rate": 9.496008556224073e-07,
      "loss": 0.0014,
      "step": 34460
    },
    {
      "epoch": 6267.272727272727,
      "grad_norm": 0.2523585259914398,
      "learning_rate": 9.495499340189727e-07,
      "loss": 0.0009,
      "step": 34470
    },
    {
      "epoch": 6269.090909090909,
      "grad_norm": 0.003120986046269536,
      "learning_rate": 9.494989880705177e-07,
      "loss": 0.0011,
      "step": 34480
    },
    {
      "epoch": 6270.909090909091,
      "grad_norm": 0.4121963679790497,
      "learning_rate": 9.494480177798012e-07,
      "loss": 0.0013,
      "step": 34490
    },
    {
      "epoch": 6272.727272727273,
      "grad_norm": 0.00667436933144927,
      "learning_rate": 9.493970231495834e-07,
      "loss": 0.0009,
      "step": 34500
    },
    {
      "epoch": 6272.727272727273,
      "eval_loss": 4.56458854675293,
      "eval_runtime": 0.9517,
      "eval_samples_per_second": 10.507,
      "eval_steps_per_second": 5.254,
      "step": 34500
    },
    {
      "epoch": 6274.545454545455,
      "grad_norm": 0.26042523980140686,
      "learning_rate": 9.493460041826259e-07,
      "loss": 0.0015,
      "step": 34510
    },
    {
      "epoch": 6276.363636363636,
      "grad_norm": 0.2571341395378113,
      "learning_rate": 9.492949608816914e-07,
      "loss": 0.0008,
      "step": 34520
    },
    {
      "epoch": 6278.181818181818,
      "grad_norm": 0.24041971564292908,
      "learning_rate": 9.492438932495443e-07,
      "loss": 0.0012,
      "step": 34530
    },
    {
      "epoch": 6280.0,
      "grad_norm": 0.006800735369324684,
      "learning_rate": 9.491928012889501e-07,
      "loss": 0.0011,
      "step": 34540
    },
    {
      "epoch": 6281.818181818182,
      "grad_norm": 0.006344443652778864,
      "learning_rate": 9.491416850026757e-07,
      "loss": 0.0011,
      "step": 34550
    },
    {
      "epoch": 6283.636363636364,
      "grad_norm": 0.44752806425094604,
      "learning_rate": 9.490905443934891e-07,
      "loss": 0.0014,
      "step": 34560
    },
    {
      "epoch": 6285.454545454545,
      "grad_norm": 0.01762308180332184,
      "learning_rate": 9.4903937946416e-07,
      "loss": 0.0009,
      "step": 34570
    },
    {
      "epoch": 6287.272727272727,
      "grad_norm": 0.0028544238302856684,
      "learning_rate": 9.489881902174591e-07,
      "loss": 0.001,
      "step": 34580
    },
    {
      "epoch": 6289.090909090909,
      "grad_norm": 0.3984938859939575,
      "learning_rate": 9.489369766561583e-07,
      "loss": 0.0014,
      "step": 34590
    },
    {
      "epoch": 6290.909090909091,
      "grad_norm": 0.32651492953300476,
      "learning_rate": 9.488857387830313e-07,
      "loss": 0.0011,
      "step": 34600
    },
    {
      "epoch": 6292.727272727273,
      "grad_norm": 0.00388177577406168,
      "learning_rate": 9.488344766008527e-07,
      "loss": 0.0009,
      "step": 34610
    },
    {
      "epoch": 6294.545454545455,
      "grad_norm": 0.2552664875984192,
      "learning_rate": 9.487831901123988e-07,
      "loss": 0.0016,
      "step": 34620
    },
    {
      "epoch": 6296.363636363636,
      "grad_norm": 0.004387366585433483,
      "learning_rate": 9.487318793204466e-07,
      "loss": 0.0008,
      "step": 34630
    },
    {
      "epoch": 6298.181818181818,
      "grad_norm": 0.05263489857316017,
      "learning_rate": 9.486805442277751e-07,
      "loss": 0.0011,
      "step": 34640
    },
    {
      "epoch": 6300.0,
      "grad_norm": 0.4205055832862854,
      "learning_rate": 9.486291848371642e-07,
      "loss": 0.0013,
      "step": 34650
    },
    {
      "epoch": 6301.818181818182,
      "grad_norm": 0.2454870045185089,
      "learning_rate": 9.485778011513952e-07,
      "loss": 0.0011,
      "step": 34660
    },
    {
      "epoch": 6303.636363636364,
      "grad_norm": 0.003903078380972147,
      "learning_rate": 9.485263931732508e-07,
      "loss": 0.0012,
      "step": 34670
    },
    {
      "epoch": 6305.454545454545,
      "grad_norm": 0.003740164451301098,
      "learning_rate": 9.48474960905515e-07,
      "loss": 0.001,
      "step": 34680
    },
    {
      "epoch": 6307.272727272727,
      "grad_norm": 0.007344442885369062,
      "learning_rate": 9.48423504350973e-07,
      "loss": 0.0012,
      "step": 34690
    },
    {
      "epoch": 6309.090909090909,
      "grad_norm": 0.2471083104610443,
      "learning_rate": 9.483720235124113e-07,
      "loss": 0.0013,
      "step": 34700
    },
    {
      "epoch": 6310.909090909091,
      "grad_norm": 0.002688836306333542,
      "learning_rate": 9.48320518392618e-07,
      "loss": 0.0011,
      "step": 34710
    },
    {
      "epoch": 6312.727272727273,
      "grad_norm": 0.00842138659209013,
      "learning_rate": 9.48268988994382e-07,
      "loss": 0.0011,
      "step": 34720
    },
    {
      "epoch": 6314.545454545455,
      "grad_norm": 0.24589408934116364,
      "learning_rate": 9.482174353204944e-07,
      "loss": 0.0011,
      "step": 34730
    },
    {
      "epoch": 6316.363636363636,
      "grad_norm": 0.03594065085053444,
      "learning_rate": 9.481658573737464e-07,
      "loss": 0.001,
      "step": 34740
    },
    {
      "epoch": 6318.181818181818,
      "grad_norm": 0.004993657115846872,
      "learning_rate": 9.481142551569317e-07,
      "loss": 0.0012,
      "step": 34750
    },
    {
      "epoch": 6320.0,
      "grad_norm": 0.31301379203796387,
      "learning_rate": 9.480626286728444e-07,
      "loss": 0.0012,
      "step": 34760
    },
    {
      "epoch": 6321.818181818182,
      "grad_norm": 0.002724249614402652,
      "learning_rate": 9.480109779242804e-07,
      "loss": 0.0011,
      "step": 34770
    },
    {
      "epoch": 6323.636363636364,
      "grad_norm": 0.24837812781333923,
      "learning_rate": 9.47959302914037e-07,
      "loss": 0.0014,
      "step": 34780
    },
    {
      "epoch": 6325.454545454545,
      "grad_norm": 0.006152313202619553,
      "learning_rate": 9.479076036449124e-07,
      "loss": 0.001,
      "step": 34790
    },
    {
      "epoch": 6327.272727272727,
      "grad_norm": 0.004634360317140818,
      "learning_rate": 9.478558801197064e-07,
      "loss": 0.0011,
      "step": 34800
    },
    {
      "epoch": 6329.090909090909,
      "grad_norm": 0.2544732689857483,
      "learning_rate": 9.4780413234122e-07,
      "loss": 0.0014,
      "step": 34810
    },
    {
      "epoch": 6330.909090909091,
      "grad_norm": 0.009182987734675407,
      "learning_rate": 9.477523603122557e-07,
      "loss": 0.0009,
      "step": 34820
    },
    {
      "epoch": 6332.727272727273,
      "grad_norm": 0.2542053163051605,
      "learning_rate": 9.47700564035617e-07,
      "loss": 0.0012,
      "step": 34830
    },
    {
      "epoch": 6334.545454545455,
      "grad_norm": 0.008763548918068409,
      "learning_rate": 9.476487435141089e-07,
      "loss": 0.0011,
      "step": 34840
    },
    {
      "epoch": 6336.363636363636,
      "grad_norm": 0.003118949243798852,
      "learning_rate": 9.475968987505378e-07,
      "loss": 0.0012,
      "step": 34850
    },
    {
      "epoch": 6338.181818181818,
      "grad_norm": 0.004226667806506157,
      "learning_rate": 9.475450297477112e-07,
      "loss": 0.0013,
      "step": 34860
    },
    {
      "epoch": 6340.0,
      "grad_norm": 0.37820395827293396,
      "learning_rate": 9.474931365084382e-07,
      "loss": 0.0012,
      "step": 34870
    },
    {
      "epoch": 6341.818181818182,
      "grad_norm": 0.3486248254776001,
      "learning_rate": 9.47441219035529e-07,
      "loss": 0.0012,
      "step": 34880
    },
    {
      "epoch": 6343.636363636364,
      "grad_norm": 0.40759822726249695,
      "learning_rate": 9.473892773317951e-07,
      "loss": 0.0011,
      "step": 34890
    },
    {
      "epoch": 6345.454545454545,
      "grad_norm": 0.00428394740447402,
      "learning_rate": 9.473373114000492e-07,
      "loss": 0.0008,
      "step": 34900
    },
    {
      "epoch": 6347.272727272727,
      "grad_norm": 0.2557951509952545,
      "learning_rate": 9.472853212431056e-07,
      "loss": 0.0013,
      "step": 34910
    },
    {
      "epoch": 6349.090909090909,
      "grad_norm": 0.002479350194334984,
      "learning_rate": 9.472333068637799e-07,
      "loss": 0.0011,
      "step": 34920
    },
    {
      "epoch": 6350.909090909091,
      "grad_norm": 0.003109539160504937,
      "learning_rate": 9.471812682648887e-07,
      "loss": 0.0012,
      "step": 34930
    },
    {
      "epoch": 6352.727272727273,
      "grad_norm": 0.0027813485357910395,
      "learning_rate": 9.471292054492504e-07,
      "loss": 0.0012,
      "step": 34940
    },
    {
      "epoch": 6354.545454545455,
      "grad_norm": 0.3127131164073944,
      "learning_rate": 9.47077118419684e-07,
      "loss": 0.0011,
      "step": 34950
    },
    {
      "epoch": 6356.363636363636,
      "grad_norm": 0.22107452154159546,
      "learning_rate": 9.470250071790106e-07,
      "loss": 0.0012,
      "step": 34960
    },
    {
      "epoch": 6358.181818181818,
      "grad_norm": 0.258004754781723,
      "learning_rate": 9.46972871730052e-07,
      "loss": 0.0011,
      "step": 34970
    },
    {
      "epoch": 6360.0,
      "grad_norm": 0.007111815735697746,
      "learning_rate": 9.469207120756318e-07,
      "loss": 0.0011,
      "step": 34980
    },
    {
      "epoch": 6361.818181818182,
      "grad_norm": 0.007355955429375172,
      "learning_rate": 9.468685282185745e-07,
      "loss": 0.0013,
      "step": 34990
    },
    {
      "epoch": 6363.636363636364,
      "grad_norm": 0.011282235383987427,
      "learning_rate": 9.468163201617061e-07,
      "loss": 0.0013,
      "step": 35000
    },
    {
      "epoch": 6363.636363636364,
      "eval_loss": 4.564818382263184,
      "eval_runtime": 0.9552,
      "eval_samples_per_second": 10.469,
      "eval_steps_per_second": 5.234,
      "step": 35000
    },
    {
      "epoch": 6365.454545454545,
      "grad_norm": 0.005671039689332247,
      "learning_rate": 9.467640879078538e-07,
      "loss": 0.0011,
      "step": 35010
    },
    {
      "epoch": 6367.272727272727,
      "grad_norm": 0.01735731028020382,
      "learning_rate": 9.467118314598464e-07,
      "loss": 0.0009,
      "step": 35020
    },
    {
      "epoch": 6369.090909090909,
      "grad_norm": 0.29470258951187134,
      "learning_rate": 9.466595508205135e-07,
      "loss": 0.0014,
      "step": 35030
    },
    {
      "epoch": 6370.909090909091,
      "grad_norm": 0.0060497974045574665,
      "learning_rate": 9.466072459926868e-07,
      "loss": 0.0009,
      "step": 35040
    },
    {
      "epoch": 6372.727272727273,
      "grad_norm": 0.009966445155441761,
      "learning_rate": 9.465549169791984e-07,
      "loss": 0.0011,
      "step": 35050
    },
    {
      "epoch": 6374.545454545455,
      "grad_norm": 0.3680287003517151,
      "learning_rate": 9.465025637828821e-07,
      "loss": 0.0014,
      "step": 35060
    },
    {
      "epoch": 6376.363636363636,
      "grad_norm": 0.009970327839255333,
      "learning_rate": 9.464501864065734e-07,
      "loss": 0.001,
      "step": 35070
    },
    {
      "epoch": 6378.181818181818,
      "grad_norm": 0.0062322537414729595,
      "learning_rate": 9.463977848531085e-07,
      "loss": 0.0011,
      "step": 35080
    },
    {
      "epoch": 6380.0,
      "grad_norm": 0.0032805202063173056,
      "learning_rate": 9.463453591253252e-07,
      "loss": 0.0012,
      "step": 35090
    },
    {
      "epoch": 6381.818181818182,
      "grad_norm": 0.46654462814331055,
      "learning_rate": 9.462929092260628e-07,
      "loss": 0.0013,
      "step": 35100
    },
    {
      "epoch": 6383.636363636364,
      "grad_norm": 0.002923469990491867,
      "learning_rate": 9.462404351581612e-07,
      "loss": 0.0006,
      "step": 35110
    },
    {
      "epoch": 6385.454545454545,
      "grad_norm": 0.0034348852932453156,
      "learning_rate": 9.461879369244626e-07,
      "loss": 0.0014,
      "step": 35120
    },
    {
      "epoch": 6387.272727272727,
      "grad_norm": 0.26369526982307434,
      "learning_rate": 9.461354145278098e-07,
      "loss": 0.0014,
      "step": 35130
    },
    {
      "epoch": 6389.090909090909,
      "grad_norm": 0.40520721673965454,
      "learning_rate": 9.460828679710469e-07,
      "loss": 0.0012,
      "step": 35140
    },
    {
      "epoch": 6390.909090909091,
      "grad_norm": 0.007796884048730135,
      "learning_rate": 9.460302972570198e-07,
      "loss": 0.0012,
      "step": 35150
    },
    {
      "epoch": 6392.727272727273,
      "grad_norm": 0.00317197828553617,
      "learning_rate": 9.459777023885753e-07,
      "loss": 0.0009,
      "step": 35160
    },
    {
      "epoch": 6394.545454545455,
      "grad_norm": 0.006715039722621441,
      "learning_rate": 9.459250833685618e-07,
      "loss": 0.0013,
      "step": 35170
    },
    {
      "epoch": 6396.363636363636,
      "grad_norm": 0.0036138794384896755,
      "learning_rate": 9.458724401998286e-07,
      "loss": 0.0009,
      "step": 35180
    },
    {
      "epoch": 6398.181818181818,
      "grad_norm": 0.003117917338386178,
      "learning_rate": 9.458197728852267e-07,
      "loss": 0.0013,
      "step": 35190
    },
    {
      "epoch": 6400.0,
      "grad_norm": 0.2226010262966156,
      "learning_rate": 9.457670814276082e-07,
      "loss": 0.0012,
      "step": 35200
    },
    {
      "epoch": 6401.818181818182,
      "grad_norm": 0.3236795663833618,
      "learning_rate": 9.457143658298266e-07,
      "loss": 0.0012,
      "step": 35210
    },
    {
      "epoch": 6403.636363636364,
      "grad_norm": 0.15415632724761963,
      "learning_rate": 9.456616260947365e-07,
      "loss": 0.0009,
      "step": 35220
    },
    {
      "epoch": 6405.454545454545,
      "grad_norm": 0.34233763813972473,
      "learning_rate": 9.456088622251944e-07,
      "loss": 0.0014,
      "step": 35230
    },
    {
      "epoch": 6407.272727272727,
      "grad_norm": 0.3483802080154419,
      "learning_rate": 9.455560742240572e-07,
      "loss": 0.001,
      "step": 35240
    },
    {
      "epoch": 6409.090909090909,
      "grad_norm": 0.002875814214348793,
      "learning_rate": 9.455032620941839e-07,
      "loss": 0.0011,
      "step": 35250
    },
    {
      "epoch": 6410.909090909091,
      "grad_norm": 0.2565748989582062,
      "learning_rate": 9.454504258384343e-07,
      "loss": 0.0012,
      "step": 35260
    },
    {
      "epoch": 6412.727272727273,
      "grad_norm": 0.003635865170508623,
      "learning_rate": 9.4539756545967e-07,
      "loss": 0.0011,
      "step": 35270
    },
    {
      "epoch": 6414.545454545455,
      "grad_norm": 0.22979305684566498,
      "learning_rate": 9.453446809607532e-07,
      "loss": 0.001,
      "step": 35280
    },
    {
      "epoch": 6416.363636363636,
      "grad_norm": 0.23482131958007812,
      "learning_rate": 9.452917723445483e-07,
      "loss": 0.0013,
      "step": 35290
    },
    {
      "epoch": 6418.181818181818,
      "grad_norm": 0.003809791523963213,
      "learning_rate": 9.4523883961392e-07,
      "loss": 0.0009,
      "step": 35300
    },
    {
      "epoch": 6420.0,
      "grad_norm": 0.24750106036663055,
      "learning_rate": 9.451858827717353e-07,
      "loss": 0.0012,
      "step": 35310
    },
    {
      "epoch": 6421.818181818182,
      "grad_norm": 0.0037135607562959194,
      "learning_rate": 9.451329018208617e-07,
      "loss": 0.0013,
      "step": 35320
    },
    {
      "epoch": 6423.636363636364,
      "grad_norm": 0.005060940980911255,
      "learning_rate": 9.450798967641685e-07,
      "loss": 0.0011,
      "step": 35330
    },
    {
      "epoch": 6425.454545454545,
      "grad_norm": 0.29900258779525757,
      "learning_rate": 9.450268676045261e-07,
      "loss": 0.0011,
      "step": 35340
    },
    {
      "epoch": 6427.272727272727,
      "grad_norm": 0.23884253203868866,
      "learning_rate": 9.449738143448064e-07,
      "loss": 0.0011,
      "step": 35350
    },
    {
      "epoch": 6429.090909090909,
      "grad_norm": 0.008819356560707092,
      "learning_rate": 9.449207369878822e-07,
      "loss": 0.0011,
      "step": 35360
    },
    {
      "epoch": 6430.909090909091,
      "grad_norm": 0.0038825394585728645,
      "learning_rate": 9.44867635536628e-07,
      "loss": 0.0013,
      "step": 35370
    },
    {
      "epoch": 6432.727272727273,
      "grad_norm": 0.005393753759562969,
      "learning_rate": 9.448145099939195e-07,
      "loss": 0.0012,
      "step": 35380
    },
    {
      "epoch": 6434.545454545455,
      "grad_norm": 0.2103862315416336,
      "learning_rate": 9.447613603626336e-07,
      "loss": 0.0011,
      "step": 35390
    },
    {
      "epoch": 6436.363636363636,
      "grad_norm": 0.4048556089401245,
      "learning_rate": 9.447081866456487e-07,
      "loss": 0.0013,
      "step": 35400
    },
    {
      "epoch": 6438.181818181818,
      "grad_norm": 0.021763497963547707,
      "learning_rate": 9.446549888458443e-07,
      "loss": 0.0007,
      "step": 35410
    },
    {
      "epoch": 6440.0,
      "grad_norm": 0.02618853747844696,
      "learning_rate": 9.446017669661011e-07,
      "loss": 0.0013,
      "step": 35420
    },
    {
      "epoch": 6441.818181818182,
      "grad_norm": 0.31821611523628235,
      "learning_rate": 9.445485210093016e-07,
      "loss": 0.0012,
      "step": 35430
    },
    {
      "epoch": 6443.636363636364,
      "grad_norm": 0.24562907218933105,
      "learning_rate": 9.44495250978329e-07,
      "loss": 0.0011,
      "step": 35440
    },
    {
      "epoch": 6445.454545454545,
      "grad_norm": 0.30360379815101624,
      "learning_rate": 9.444419568760684e-07,
      "loss": 0.0012,
      "step": 35450
    },
    {
      "epoch": 6447.272727272727,
      "grad_norm": 0.3024224042892456,
      "learning_rate": 9.443886387054056e-07,
      "loss": 0.0013,
      "step": 35460
    },
    {
      "epoch": 6449.090909090909,
      "grad_norm": 0.005475855432450771,
      "learning_rate": 9.443352964692283e-07,
      "loss": 0.0011,
      "step": 35470
    },
    {
      "epoch": 6450.909090909091,
      "grad_norm": 0.25670185685157776,
      "learning_rate": 9.44281930170425e-07,
      "loss": 0.0012,
      "step": 35480
    },
    {
      "epoch": 6452.727272727273,
      "grad_norm": 0.5658534169197083,
      "learning_rate": 9.442285398118859e-07,
      "loss": 0.0012,
      "step": 35490
    },
    {
      "epoch": 6454.545454545455,
      "grad_norm": 0.22408083081245422,
      "learning_rate": 9.44175125396502e-07,
      "loss": 0.0011,
      "step": 35500
    },
    {
      "epoch": 6454.545454545455,
      "eval_loss": 4.501787185668945,
      "eval_runtime": 0.9583,
      "eval_samples_per_second": 10.435,
      "eval_steps_per_second": 5.217,
      "step": 35500
    },
    {
      "epoch": 6456.363636363636,
      "grad_norm": 0.24818851053714752,
      "learning_rate": 9.441216869271662e-07,
      "loss": 0.0011,
      "step": 35510
    },
    {
      "epoch": 6458.181818181818,
      "grad_norm": 0.32100439071655273,
      "learning_rate": 9.440682244067722e-07,
      "loss": 0.0011,
      "step": 35520
    },
    {
      "epoch": 6460.0,
      "grad_norm": 0.004475093446671963,
      "learning_rate": 9.440147378382154e-07,
      "loss": 0.0011,
      "step": 35530
    },
    {
      "epoch": 6461.818181818182,
      "grad_norm": 0.004694211296737194,
      "learning_rate": 9.439612272243923e-07,
      "loss": 0.0012,
      "step": 35540
    },
    {
      "epoch": 6463.636363636364,
      "grad_norm": 0.24050794541835785,
      "learning_rate": 9.439076925682005e-07,
      "loss": 0.001,
      "step": 35550
    },
    {
      "epoch": 6465.454545454545,
      "grad_norm": 0.31889310479164124,
      "learning_rate": 9.438541338725397e-07,
      "loss": 0.0011,
      "step": 35560
    },
    {
      "epoch": 6467.272727272727,
      "grad_norm": 0.2843710482120514,
      "learning_rate": 9.438005511403096e-07,
      "loss": 0.0012,
      "step": 35570
    },
    {
      "epoch": 6469.090909090909,
      "grad_norm": 0.026673337444663048,
      "learning_rate": 9.437469443744124e-07,
      "loss": 0.0011,
      "step": 35580
    },
    {
      "epoch": 6470.909090909091,
      "grad_norm": 0.34085965156555176,
      "learning_rate": 9.43693313577751e-07,
      "loss": 0.0012,
      "step": 35590
    },
    {
      "epoch": 6472.727272727273,
      "grad_norm": 0.015829527750611305,
      "learning_rate": 9.436396587532296e-07,
      "loss": 0.0011,
      "step": 35600
    },
    {
      "epoch": 6474.545454545455,
      "grad_norm": 0.2647645175457001,
      "learning_rate": 9.43585979903754e-07,
      "loss": 0.0012,
      "step": 35610
    },
    {
      "epoch": 6476.363636363636,
      "grad_norm": 0.00436435965821147,
      "learning_rate": 9.435322770322311e-07,
      "loss": 0.0011,
      "step": 35620
    },
    {
      "epoch": 6478.181818181818,
      "grad_norm": 0.2881286144256592,
      "learning_rate": 9.434785501415693e-07,
      "loss": 0.0012,
      "step": 35630
    },
    {
      "epoch": 6480.0,
      "grad_norm": 0.42302048206329346,
      "learning_rate": 9.434247992346779e-07,
      "loss": 0.0011,
      "step": 35640
    },
    {
      "epoch": 6481.818181818182,
      "grad_norm": 0.278942734003067,
      "learning_rate": 9.433710243144678e-07,
      "loss": 0.0013,
      "step": 35650
    },
    {
      "epoch": 6483.636363636364,
      "grad_norm": 0.005687866359949112,
      "learning_rate": 9.43317225383851e-07,
      "loss": 0.0011,
      "step": 35660
    },
    {
      "epoch": 6485.454545454545,
      "grad_norm": 0.2857295274734497,
      "learning_rate": 9.432634024457413e-07,
      "loss": 0.0013,
      "step": 35670
    },
    {
      "epoch": 6487.272727272727,
      "grad_norm": 0.3796471953392029,
      "learning_rate": 9.432095555030533e-07,
      "loss": 0.0016,
      "step": 35680
    },
    {
      "epoch": 6489.090909090909,
      "grad_norm": 0.0037344039883464575,
      "learning_rate": 9.431556845587027e-07,
      "loss": 0.0008,
      "step": 35690
    },
    {
      "epoch": 6490.909090909091,
      "grad_norm": 0.004568182863295078,
      "learning_rate": 9.431017896156073e-07,
      "loss": 0.0013,
      "step": 35700
    },
    {
      "epoch": 6492.727272727273,
      "grad_norm": 0.2482994943857193,
      "learning_rate": 9.430478706766855e-07,
      "loss": 0.0011,
      "step": 35710
    },
    {
      "epoch": 6494.545454545455,
      "grad_norm": 0.2210400402545929,
      "learning_rate": 9.429939277448573e-07,
      "loss": 0.0012,
      "step": 35720
    },
    {
      "epoch": 6496.363636363636,
      "grad_norm": 0.07543583959341049,
      "learning_rate": 9.429399608230439e-07,
      "loss": 0.001,
      "step": 35730
    },
    {
      "epoch": 6498.181818181818,
      "grad_norm": 0.35997042059898376,
      "learning_rate": 9.428859699141678e-07,
      "loss": 0.0015,
      "step": 35740
    },
    {
      "epoch": 6500.0,
      "grad_norm": 0.394802451133728,
      "learning_rate": 9.428319550211529e-07,
      "loss": 0.0009,
      "step": 35750
    },
    {
      "epoch": 6501.818181818182,
      "grad_norm": 0.46843597292900085,
      "learning_rate": 9.427779161469245e-07,
      "loss": 0.0013,
      "step": 35760
    },
    {
      "epoch": 6503.636363636364,
      "grad_norm": 0.34653064608573914,
      "learning_rate": 9.427238532944087e-07,
      "loss": 0.0009,
      "step": 35770
    },
    {
      "epoch": 6505.454545454545,
      "grad_norm": 0.005920759867876768,
      "learning_rate": 9.426697664665333e-07,
      "loss": 0.0012,
      "step": 35780
    },
    {
      "epoch": 6507.272727272727,
      "grad_norm": 0.008071398362517357,
      "learning_rate": 9.426156556662275e-07,
      "loss": 0.001,
      "step": 35790
    },
    {
      "epoch": 6509.090909090909,
      "grad_norm": 0.028766022995114326,
      "learning_rate": 9.425615208964216e-07,
      "loss": 0.0013,
      "step": 35800
    },
    {
      "epoch": 6510.909090909091,
      "grad_norm": 0.3280195891857147,
      "learning_rate": 9.425073621600472e-07,
      "loss": 0.0013,
      "step": 35810
    },
    {
      "epoch": 6512.727272727273,
      "grad_norm": 0.34607818722724915,
      "learning_rate": 9.424531794600371e-07,
      "loss": 0.0013,
      "step": 35820
    },
    {
      "epoch": 6514.545454545455,
      "grad_norm": 0.0037162844091653824,
      "learning_rate": 9.423989727993256e-07,
      "loss": 0.0009,
      "step": 35830
    },
    {
      "epoch": 6516.363636363636,
      "grad_norm": 0.005254484247416258,
      "learning_rate": 9.423447421808484e-07,
      "loss": 0.0013,
      "step": 35840
    },
    {
      "epoch": 6518.181818181818,
      "grad_norm": 0.5775051116943359,
      "learning_rate": 9.42290487607542e-07,
      "loss": 0.0011,
      "step": 35850
    },
    {
      "epoch": 6520.0,
      "grad_norm": 0.2599744200706482,
      "learning_rate": 9.422362090823445e-07,
      "loss": 0.0011,
      "step": 35860
    },
    {
      "epoch": 6521.818181818182,
      "grad_norm": 0.3222690224647522,
      "learning_rate": 9.421819066081958e-07,
      "loss": 0.0013,
      "step": 35870
    },
    {
      "epoch": 6523.636363636364,
      "grad_norm": 0.006332768592983484,
      "learning_rate": 9.421275801880362e-07,
      "loss": 0.0011,
      "step": 35880
    },
    {
      "epoch": 6525.454545454545,
      "grad_norm": 0.01869114674627781,
      "learning_rate": 9.420732298248077e-07,
      "loss": 0.0011,
      "step": 35890
    },
    {
      "epoch": 6527.272727272727,
      "grad_norm": 0.29211488366127014,
      "learning_rate": 9.420188555214536e-07,
      "loss": 0.0013,
      "step": 35900
    },
    {
      "epoch": 6529.090909090909,
      "grad_norm": 0.25859904289245605,
      "learning_rate": 9.419644572809188e-07,
      "loss": 0.0009,
      "step": 35910
    },
    {
      "epoch": 6530.909090909091,
      "grad_norm": 0.34678223729133606,
      "learning_rate": 9.419100351061488e-07,
      "loss": 0.0011,
      "step": 35920
    },
    {
      "epoch": 6532.727272727273,
      "grad_norm": 0.4044511318206787,
      "learning_rate": 9.41855589000091e-07,
      "loss": 0.0011,
      "step": 35930
    },
    {
      "epoch": 6534.545454545455,
      "grad_norm": 0.4021245241165161,
      "learning_rate": 9.418011189656941e-07,
      "loss": 0.0014,
      "step": 35940
    },
    {
      "epoch": 6536.363636363636,
      "grad_norm": 0.011205131188035011,
      "learning_rate": 9.417466250059073e-07,
      "loss": 0.0009,
      "step": 35950
    },
    {
      "epoch": 6538.181818181818,
      "grad_norm": 0.34787458181381226,
      "learning_rate": 9.41692107123682e-07,
      "loss": 0.0012,
      "step": 35960
    },
    {
      "epoch": 6540.0,
      "grad_norm": 0.31919047236442566,
      "learning_rate": 9.416375653219708e-07,
      "loss": 0.0011,
      "step": 35970
    },
    {
      "epoch": 6541.818181818182,
      "grad_norm": 0.006926983594894409,
      "learning_rate": 9.415829996037271e-07,
      "loss": 0.0013,
      "step": 35980
    },
    {
      "epoch": 6543.636363636364,
      "grad_norm": 0.31434038281440735,
      "learning_rate": 9.415284099719059e-07,
      "loss": 0.0012,
      "step": 35990
    },
    {
      "epoch": 6545.454545454545,
      "grad_norm": 0.4564018249511719,
      "learning_rate": 9.414737964294634e-07,
      "loss": 0.0011,
      "step": 36000
    },
    {
      "epoch": 6545.454545454545,
      "eval_loss": 4.614027976989746,
      "eval_runtime": 0.9518,
      "eval_samples_per_second": 10.506,
      "eval_steps_per_second": 5.253,
      "step": 36000
    },
    {
      "epoch": 6547.272727272727,
      "grad_norm": 0.0081334188580513,
      "learning_rate": 9.414191589793573e-07,
      "loss": 0.001,
      "step": 36010
    },
    {
      "epoch": 6549.090909090909,
      "grad_norm": 1.1390225887298584,
      "learning_rate": 9.413644976245464e-07,
      "loss": 0.0012,
      "step": 36020
    },
    {
      "epoch": 6550.909090909091,
      "grad_norm": 0.3199600577354431,
      "learning_rate": 9.413098123679908e-07,
      "loss": 0.0009,
      "step": 36030
    },
    {
      "epoch": 6552.727272727273,
      "grad_norm": 0.014522907324135303,
      "learning_rate": 9.412551032126519e-07,
      "loss": 0.0015,
      "step": 36040
    },
    {
      "epoch": 6554.545454545455,
      "grad_norm": 0.0074708424508571625,
      "learning_rate": 9.412003701614926e-07,
      "loss": 0.0007,
      "step": 36050
    },
    {
      "epoch": 6556.363636363636,
      "grad_norm": 0.2615189552307129,
      "learning_rate": 9.411456132174766e-07,
      "loss": 0.0014,
      "step": 36060
    },
    {
      "epoch": 6558.181818181818,
      "grad_norm": 0.3369755148887634,
      "learning_rate": 9.410908323835695e-07,
      "loss": 0.0011,
      "step": 36070
    },
    {
      "epoch": 6560.0,
      "grad_norm": 0.005358294118195772,
      "learning_rate": 9.41036027662738e-07,
      "loss": 0.0011,
      "step": 36080
    },
    {
      "epoch": 6561.818181818182,
      "grad_norm": 0.005595264956355095,
      "learning_rate": 9.409811990579497e-07,
      "loss": 0.0012,
      "step": 36090
    },
    {
      "epoch": 6563.636363636364,
      "grad_norm": 0.026186654344201088,
      "learning_rate": 9.40926346572174e-07,
      "loss": 0.0011,
      "step": 36100
    },
    {
      "epoch": 6565.454545454545,
      "grad_norm": 0.004188433289527893,
      "learning_rate": 9.408714702083813e-07,
      "loss": 0.0012,
      "step": 36110
    },
    {
      "epoch": 6567.272727272727,
      "grad_norm": 0.02653086930513382,
      "learning_rate": 9.408165699695434e-07,
      "loss": 0.0012,
      "step": 36120
    },
    {
      "epoch": 6569.090909090909,
      "grad_norm": 0.007567262277007103,
      "learning_rate": 9.407616458586335e-07,
      "loss": 0.0009,
      "step": 36130
    },
    {
      "epoch": 6570.909090909091,
      "grad_norm": 0.00962830986827612,
      "learning_rate": 9.407066978786258e-07,
      "loss": 0.0012,
      "step": 36140
    },
    {
      "epoch": 6572.727272727273,
      "grad_norm": 0.2814808785915375,
      "learning_rate": 9.40651726032496e-07,
      "loss": 0.0012,
      "step": 36150
    },
    {
      "epoch": 6574.545454545455,
      "grad_norm": 0.2690799832344055,
      "learning_rate": 9.405967303232211e-07,
      "loss": 0.0009,
      "step": 36160
    },
    {
      "epoch": 6576.363636363636,
      "grad_norm": 0.0029892444144934416,
      "learning_rate": 9.405417107537795e-07,
      "loss": 0.0009,
      "step": 36170
    },
    {
      "epoch": 6578.181818181818,
      "grad_norm": 0.0045303236693143845,
      "learning_rate": 9.404866673271505e-07,
      "loss": 0.0012,
      "step": 36180
    },
    {
      "epoch": 6580.0,
      "grad_norm": 0.38085564970970154,
      "learning_rate": 9.40431600046315e-07,
      "loss": 0.0012,
      "step": 36190
    },
    {
      "epoch": 6581.818181818182,
      "grad_norm": 0.29593607783317566,
      "learning_rate": 9.403765089142552e-07,
      "loss": 0.0012,
      "step": 36200
    },
    {
      "epoch": 6583.636363636364,
      "grad_norm": 0.00608805799856782,
      "learning_rate": 9.403213939339545e-07,
      "loss": 0.0011,
      "step": 36210
    },
    {
      "epoch": 6585.454545454545,
      "grad_norm": 0.32572296261787415,
      "learning_rate": 9.402662551083976e-07,
      "loss": 0.0011,
      "step": 36220
    },
    {
      "epoch": 6587.272727272727,
      "grad_norm": 0.31914573907852173,
      "learning_rate": 9.402110924405701e-07,
      "loss": 0.0011,
      "step": 36230
    },
    {
      "epoch": 6589.090909090909,
      "grad_norm": 0.2709050476551056,
      "learning_rate": 9.401559059334601e-07,
      "loss": 0.0011,
      "step": 36240
    },
    {
      "epoch": 6590.909090909091,
      "grad_norm": 0.2993900775909424,
      "learning_rate": 9.401006955900554e-07,
      "loss": 0.0013,
      "step": 36250
    },
    {
      "epoch": 6592.727272727273,
      "grad_norm": 0.006212170701473951,
      "learning_rate": 9.400454614133465e-07,
      "loss": 0.0011,
      "step": 36260
    },
    {
      "epoch": 6594.545454545455,
      "grad_norm": 0.010554688051342964,
      "learning_rate": 9.399902034063242e-07,
      "loss": 0.001,
      "step": 36270
    },
    {
      "epoch": 6596.363636363636,
      "grad_norm": 0.26352939009666443,
      "learning_rate": 9.39934921571981e-07,
      "loss": 0.0014,
      "step": 36280
    },
    {
      "epoch": 6598.181818181818,
      "grad_norm": 0.06811001151800156,
      "learning_rate": 9.398796159133107e-07,
      "loss": 0.0011,
      "step": 36290
    },
    {
      "epoch": 6600.0,
      "grad_norm": 0.3709760308265686,
      "learning_rate": 9.398242864333083e-07,
      "loss": 0.0013,
      "step": 36300
    },
    {
      "epoch": 6601.818181818182,
      "grad_norm": 0.38959744572639465,
      "learning_rate": 9.397689331349701e-07,
      "loss": 0.0013,
      "step": 36310
    },
    {
      "epoch": 6603.636363636364,
      "grad_norm": 0.051672711968421936,
      "learning_rate": 9.397135560212937e-07,
      "loss": 0.0009,
      "step": 36320
    },
    {
      "epoch": 6605.454545454545,
      "grad_norm": 0.28904205560684204,
      "learning_rate": 9.39658155095278e-07,
      "loss": 0.0011,
      "step": 36330
    },
    {
      "epoch": 6607.272727272727,
      "grad_norm": 0.010434063151478767,
      "learning_rate": 9.396027303599234e-07,
      "loss": 0.0011,
      "step": 36340
    },
    {
      "epoch": 6609.090909090909,
      "grad_norm": 0.35177233815193176,
      "learning_rate": 9.395472818182312e-07,
      "loss": 0.0013,
      "step": 36350
    },
    {
      "epoch": 6610.909090909091,
      "grad_norm": 0.32947295904159546,
      "learning_rate": 9.394918094732043e-07,
      "loss": 0.0011,
      "step": 36360
    },
    {
      "epoch": 6612.727272727273,
      "grad_norm": 0.28049904108047485,
      "learning_rate": 9.394363133278465e-07,
      "loss": 0.0012,
      "step": 36370
    },
    {
      "epoch": 6614.545454545455,
      "grad_norm": 0.2872743606567383,
      "learning_rate": 9.393807933851633e-07,
      "loss": 0.0011,
      "step": 36380
    },
    {
      "epoch": 6616.363636363636,
      "grad_norm": 0.32734015583992004,
      "learning_rate": 9.393252496481614e-07,
      "loss": 0.0013,
      "step": 36390
    },
    {
      "epoch": 6618.181818181818,
      "grad_norm": 0.35079026222229004,
      "learning_rate": 9.392696821198487e-07,
      "loss": 0.0013,
      "step": 36400
    },
    {
      "epoch": 6620.0,
      "grad_norm": 0.011735394597053528,
      "learning_rate": 9.392140908032344e-07,
      "loss": 0.001,
      "step": 36410
    },
    {
      "epoch": 6621.818181818182,
      "grad_norm": 0.28975561261177063,
      "learning_rate": 9.391584757013289e-07,
      "loss": 0.0012,
      "step": 36420
    },
    {
      "epoch": 6623.636363636364,
      "grad_norm": 0.26193442940711975,
      "learning_rate": 9.391028368171441e-07,
      "loss": 0.0012,
      "step": 36430
    },
    {
      "epoch": 6625.454545454545,
      "grad_norm": 0.2537522315979004,
      "learning_rate": 9.390471741536931e-07,
      "loss": 0.001,
      "step": 36440
    },
    {
      "epoch": 6627.272727272727,
      "grad_norm": 0.008023476228117943,
      "learning_rate": 9.389914877139902e-07,
      "loss": 0.0011,
      "step": 36450
    },
    {
      "epoch": 6629.090909090909,
      "grad_norm": 0.00619439035654068,
      "learning_rate": 9.389357775010511e-07,
      "loss": 0.0012,
      "step": 36460
    },
    {
      "epoch": 6630.909090909091,
      "grad_norm": 0.005292633548378944,
      "learning_rate": 9.388800435178928e-07,
      "loss": 0.0011,
      "step": 36470
    },
    {
      "epoch": 6632.727272727273,
      "grad_norm": 0.29068872332572937,
      "learning_rate": 9.388242857675335e-07,
      "loss": 0.0011,
      "step": 36480
    },
    {
      "epoch": 6634.545454545455,
      "grad_norm": 0.002715745475143194,
      "learning_rate": 9.387685042529924e-07,
      "loss": 0.001,
      "step": 36490
    },
    {
      "epoch": 6636.363636363636,
      "grad_norm": 0.0027743694372475147,
      "learning_rate": 9.387126989772909e-07,
      "loss": 0.0015,
      "step": 36500
    },
    {
      "epoch": 6636.363636363636,
      "eval_loss": 4.602992057800293,
      "eval_runtime": 0.9534,
      "eval_samples_per_second": 10.489,
      "eval_steps_per_second": 5.244,
      "step": 36500
    },
    {
      "epoch": 6638.181818181818,
      "grad_norm": 0.0035746886860579252,
      "learning_rate": 9.386568699434507e-07,
      "loss": 0.001,
      "step": 36510
    },
    {
      "epoch": 6640.0,
      "grad_norm": 0.0037006083875894547,
      "learning_rate": 9.386010171544951e-07,
      "loss": 0.0012,
      "step": 36520
    },
    {
      "epoch": 6641.818181818182,
      "grad_norm": 0.003224989864975214,
      "learning_rate": 9.38545140613449e-07,
      "loss": 0.0012,
      "step": 36530
    },
    {
      "epoch": 6643.636363636364,
      "grad_norm": 0.2434326410293579,
      "learning_rate": 9.384892403233383e-07,
      "loss": 0.001,
      "step": 36540
    },
    {
      "epoch": 6645.454545454545,
      "grad_norm": 0.278125524520874,
      "learning_rate": 9.384333162871903e-07,
      "loss": 0.0013,
      "step": 36550
    },
    {
      "epoch": 6647.272727272727,
      "grad_norm": 0.28723442554473877,
      "learning_rate": 9.383773685080332e-07,
      "loss": 0.0009,
      "step": 36560
    },
    {
      "epoch": 6649.090909090909,
      "grad_norm": 0.35696497559547424,
      "learning_rate": 9.383213969888971e-07,
      "loss": 0.0013,
      "step": 36570
    },
    {
      "epoch": 6650.909090909091,
      "grad_norm": 0.004302728921175003,
      "learning_rate": 9.382654017328131e-07,
      "loss": 0.001,
      "step": 36580
    },
    {
      "epoch": 6652.727272727273,
      "grad_norm": 0.0021530031226575375,
      "learning_rate": 9.382093827428134e-07,
      "loss": 0.0011,
      "step": 36590
    },
    {
      "epoch": 6654.545454545455,
      "grad_norm": 0.27587956190109253,
      "learning_rate": 9.381533400219317e-07,
      "loss": 0.0011,
      "step": 36600
    },
    {
      "epoch": 6656.363636363636,
      "grad_norm": 0.0055575016885995865,
      "learning_rate": 9.380972735732031e-07,
      "loss": 0.0013,
      "step": 36610
    },
    {
      "epoch": 6658.181818181818,
      "grad_norm": 0.40409189462661743,
      "learning_rate": 9.380411833996638e-07,
      "loss": 0.0012,
      "step": 36620
    },
    {
      "epoch": 6660.0,
      "grad_norm": 0.008187748491764069,
      "learning_rate": 9.379850695043512e-07,
      "loss": 0.001,
      "step": 36630
    },
    {
      "epoch": 6661.818181818182,
      "grad_norm": 0.004051342606544495,
      "learning_rate": 9.379289318903043e-07,
      "loss": 0.0011,
      "step": 36640
    },
    {
      "epoch": 6663.636363636364,
      "grad_norm": 0.3117815852165222,
      "learning_rate": 9.378727705605629e-07,
      "loss": 0.0011,
      "step": 36650
    },
    {
      "epoch": 6665.454545454545,
      "grad_norm": 0.37088561058044434,
      "learning_rate": 9.378165855181686e-07,
      "loss": 0.0013,
      "step": 36660
    },
    {
      "epoch": 6667.272727272727,
      "grad_norm": 0.1985570788383484,
      "learning_rate": 9.37760376766164e-07,
      "loss": 0.0011,
      "step": 36670
    },
    {
      "epoch": 6669.090909090909,
      "grad_norm": 0.009032650850713253,
      "learning_rate": 9.377041443075929e-07,
      "loss": 0.001,
      "step": 36680
    },
    {
      "epoch": 6670.909090909091,
      "grad_norm": 0.295755535364151,
      "learning_rate": 9.376478881455008e-07,
      "loss": 0.0012,
      "step": 36690
    },
    {
      "epoch": 6672.727272727273,
      "grad_norm": 0.22062353789806366,
      "learning_rate": 9.37591608282934e-07,
      "loss": 0.0008,
      "step": 36700
    },
    {
      "epoch": 6674.545454545455,
      "grad_norm": 0.003221831750124693,
      "learning_rate": 9.375353047229402e-07,
      "loss": 0.0013,
      "step": 36710
    },
    {
      "epoch": 6676.363636363636,
      "grad_norm": 0.0028189034201204777,
      "learning_rate": 9.374789774685689e-07,
      "loss": 0.0014,
      "step": 36720
    },
    {
      "epoch": 6678.181818181818,
      "grad_norm": 0.2509385943412781,
      "learning_rate": 9.3742262652287e-07,
      "loss": 0.0012,
      "step": 36730
    },
    {
      "epoch": 6680.0,
      "grad_norm": 0.22437581419944763,
      "learning_rate": 9.373662518888954e-07,
      "loss": 0.0011,
      "step": 36740
    },
    {
      "epoch": 6681.818181818182,
      "grad_norm": 0.2499333769083023,
      "learning_rate": 9.373098535696978e-07,
      "loss": 0.0011,
      "step": 36750
    },
    {
      "epoch": 6683.636363636364,
      "grad_norm": 0.003804718842729926,
      "learning_rate": 9.372534315683319e-07,
      "loss": 0.0009,
      "step": 36760
    },
    {
      "epoch": 6685.454545454545,
      "grad_norm": 0.22539867460727692,
      "learning_rate": 9.371969858878525e-07,
      "loss": 0.0015,
      "step": 36770
    },
    {
      "epoch": 6687.272727272727,
      "grad_norm": 0.003713335609063506,
      "learning_rate": 9.371405165313168e-07,
      "loss": 0.0011,
      "step": 36780
    },
    {
      "epoch": 6689.090909090909,
      "grad_norm": 0.0069142780266702175,
      "learning_rate": 9.370840235017827e-07,
      "loss": 0.0011,
      "step": 36790
    },
    {
      "epoch": 6690.909090909091,
      "grad_norm": 0.3126871883869171,
      "learning_rate": 9.370275068023096e-07,
      "loss": 0.0012,
      "step": 36800
    },
    {
      "epoch": 6692.727272727273,
      "grad_norm": 0.2803807854652405,
      "learning_rate": 9.369709664359584e-07,
      "loss": 0.0013,
      "step": 36810
    },
    {
      "epoch": 6694.545454545455,
      "grad_norm": 0.0023265692871063948,
      "learning_rate": 9.369144024057904e-07,
      "loss": 0.0009,
      "step": 36820
    },
    {
      "epoch": 6696.363636363636,
      "grad_norm": 0.0037822313606739044,
      "learning_rate": 9.368578147148694e-07,
      "loss": 0.001,
      "step": 36830
    },
    {
      "epoch": 6698.181818181818,
      "grad_norm": 0.2690736949443817,
      "learning_rate": 9.368012033662594e-07,
      "loss": 0.0013,
      "step": 36840
    },
    {
      "epoch": 6700.0,
      "grad_norm": 0.002967769280076027,
      "learning_rate": 9.367445683630262e-07,
      "loss": 0.0012,
      "step": 36850
    },
    {
      "epoch": 6701.818181818182,
      "grad_norm": 0.3639656901359558,
      "learning_rate": 9.36687909708237e-07,
      "loss": 0.0014,
      "step": 36860
    },
    {
      "epoch": 6703.636363636364,
      "grad_norm": 0.0030602826736867428,
      "learning_rate": 9.3663122740496e-07,
      "loss": 0.0011,
      "step": 36870
    },
    {
      "epoch": 6705.454545454545,
      "grad_norm": 0.24905678629875183,
      "learning_rate": 9.36574521456265e-07,
      "loss": 0.0011,
      "step": 36880
    },
    {
      "epoch": 6707.272727272727,
      "grad_norm": 0.004937370773404837,
      "learning_rate": 9.365177918652225e-07,
      "loss": 0.001,
      "step": 36890
    },
    {
      "epoch": 6709.090909090909,
      "grad_norm": 0.004854164551943541,
      "learning_rate": 9.364610386349047e-07,
      "loss": 0.0013,
      "step": 36900
    },
    {
      "epoch": 6710.909090909091,
      "grad_norm": 0.005322735290974379,
      "learning_rate": 9.364042617683853e-07,
      "loss": 0.0012,
      "step": 36910
    },
    {
      "epoch": 6712.727272727273,
      "grad_norm": 0.004947252571582794,
      "learning_rate": 9.363474612687389e-07,
      "loss": 0.001,
      "step": 36920
    },
    {
      "epoch": 6714.545454545455,
      "grad_norm": 0.005720589775592089,
      "learning_rate": 9.362906371390416e-07,
      "loss": 0.0014,
      "step": 36930
    },
    {
      "epoch": 6716.363636363636,
      "grad_norm": 0.0024892783258110285,
      "learning_rate": 9.362337893823701e-07,
      "loss": 0.0008,
      "step": 36940
    },
    {
      "epoch": 6718.181818181818,
      "grad_norm": 0.008228708989918232,
      "learning_rate": 9.361769180018038e-07,
      "loss": 0.0012,
      "step": 36950
    },
    {
      "epoch": 6720.0,
      "grad_norm": 0.2563447952270508,
      "learning_rate": 9.361200230004218e-07,
      "loss": 0.0012,
      "step": 36960
    },
    {
      "epoch": 6721.818181818182,
      "grad_norm": 0.27270033955574036,
      "learning_rate": 9.360631043813056e-07,
      "loss": 0.0013,
      "step": 36970
    },
    {
      "epoch": 6723.636363636364,
      "grad_norm": 0.0026381935458630323,
      "learning_rate": 9.360061621475374e-07,
      "loss": 0.0011,
      "step": 36980
    },
    {
      "epoch": 6725.454545454545,
      "grad_norm": 0.004282715730369091,
      "learning_rate": 9.359491963022009e-07,
      "loss": 0.0008,
      "step": 36990
    },
    {
      "epoch": 6727.272727272727,
      "grad_norm": 0.31308630108833313,
      "learning_rate": 9.358922068483811e-07,
      "loss": 0.0016,
      "step": 37000
    },
    {
      "epoch": 6727.272727272727,
      "eval_loss": 4.495966911315918,
      "eval_runtime": 0.9517,
      "eval_samples_per_second": 10.508,
      "eval_steps_per_second": 5.254,
      "step": 37000
    },
    {
      "epoch": 6729.090909090909,
      "grad_norm": 0.3085528612136841,
      "learning_rate": 9.358351937891642e-07,
      "loss": 0.0008,
      "step": 37010
    },
    {
      "epoch": 6730.909090909091,
      "grad_norm": 0.003183792345225811,
      "learning_rate": 9.357781571276378e-07,
      "loss": 0.0012,
      "step": 37020
    },
    {
      "epoch": 6732.727272727273,
      "grad_norm": 0.0033692875877022743,
      "learning_rate": 9.357210968668904e-07,
      "loss": 0.0012,
      "step": 37030
    },
    {
      "epoch": 6734.545454545455,
      "grad_norm": 0.002207488054409623,
      "learning_rate": 9.356640130100121e-07,
      "loss": 0.0011,
      "step": 37040
    },
    {
      "epoch": 6736.363636363636,
      "grad_norm": 0.3891107439994812,
      "learning_rate": 9.356069055600947e-07,
      "loss": 0.0012,
      "step": 37050
    },
    {
      "epoch": 6738.181818181818,
      "grad_norm": 0.00245232624001801,
      "learning_rate": 9.355497745202303e-07,
      "loss": 0.0008,
      "step": 37060
    },
    {
      "epoch": 6740.0,
      "grad_norm": 0.01668795756995678,
      "learning_rate": 9.354926198935129e-07,
      "loss": 0.0012,
      "step": 37070
    },
    {
      "epoch": 6741.818181818182,
      "grad_norm": 0.3371719419956207,
      "learning_rate": 9.354354416830376e-07,
      "loss": 0.0012,
      "step": 37080
    },
    {
      "epoch": 6743.636363636364,
      "grad_norm": 0.2976188361644745,
      "learning_rate": 9.353782398919011e-07,
      "loss": 0.0009,
      "step": 37090
    },
    {
      "epoch": 6745.454545454545,
      "grad_norm": 0.0029447174165397882,
      "learning_rate": 9.353210145232009e-07,
      "loss": 0.0009,
      "step": 37100
    },
    {
      "epoch": 6747.272727272727,
      "grad_norm": 0.29245874285697937,
      "learning_rate": 9.35263765580036e-07,
      "loss": 0.0014,
      "step": 37110
    },
    {
      "epoch": 6749.090909090909,
      "grad_norm": 0.004270719364285469,
      "learning_rate": 9.352064930655068e-07,
      "loss": 0.001,
      "step": 37120
    },
    {
      "epoch": 6750.909090909091,
      "grad_norm": 0.002864598063752055,
      "learning_rate": 9.351491969827148e-07,
      "loss": 0.0012,
      "step": 37130
    },
    {
      "epoch": 6752.727272727273,
      "grad_norm": 0.003558262251317501,
      "learning_rate": 9.350918773347628e-07,
      "loss": 0.0009,
      "step": 37140
    },
    {
      "epoch": 6754.545454545455,
      "grad_norm": 0.002587706781923771,
      "learning_rate": 9.350345341247549e-07,
      "loss": 0.0013,
      "step": 37150
    },
    {
      "epoch": 6756.363636363636,
      "grad_norm": 0.015597331337630749,
      "learning_rate": 9.349771673557964e-07,
      "loss": 0.0011,
      "step": 37160
    },
    {
      "epoch": 6758.181818181818,
      "grad_norm": 0.00391807546839118,
      "learning_rate": 9.34919777030994e-07,
      "loss": 0.0011,
      "step": 37170
    },
    {
      "epoch": 6760.0,
      "grad_norm": 0.34677577018737793,
      "learning_rate": 9.348623631534558e-07,
      "loss": 0.0012,
      "step": 37180
    },
    {
      "epoch": 6761.818181818182,
      "grad_norm": 0.23737919330596924,
      "learning_rate": 9.348049257262907e-07,
      "loss": 0.0012,
      "step": 37190
    },
    {
      "epoch": 6763.636363636364,
      "grad_norm": 0.003505873493850231,
      "learning_rate": 9.347474647526095e-07,
      "loss": 0.0011,
      "step": 37200
    },
    {
      "epoch": 6765.454545454545,
      "grad_norm": 0.3786220848560333,
      "learning_rate": 9.346899802355238e-07,
      "loss": 0.0013,
      "step": 37210
    },
    {
      "epoch": 6767.272727272727,
      "grad_norm": 0.285369336605072,
      "learning_rate": 9.346324721781465e-07,
      "loss": 0.0011,
      "step": 37220
    },
    {
      "epoch": 6769.090909090909,
      "grad_norm": 0.003084638388827443,
      "learning_rate": 9.345749405835919e-07,
      "loss": 0.0011,
      "step": 37230
    },
    {
      "epoch": 6770.909090909091,
      "grad_norm": 0.0031005421187728643,
      "learning_rate": 9.34517385454976e-07,
      "loss": 0.0013,
      "step": 37240
    },
    {
      "epoch": 6772.727272727273,
      "grad_norm": 0.005275816190987825,
      "learning_rate": 9.344598067954151e-07,
      "loss": 0.001,
      "step": 37250
    },
    {
      "epoch": 6774.545454545455,
      "grad_norm": 0.002523396397009492,
      "learning_rate": 9.344022046080275e-07,
      "loss": 0.0011,
      "step": 37260
    },
    {
      "epoch": 6776.363636363636,
      "grad_norm": 0.002655231626704335,
      "learning_rate": 9.34344578895933e-07,
      "loss": 0.0009,
      "step": 37270
    },
    {
      "epoch": 6778.181818181818,
      "grad_norm": 0.003919481765478849,
      "learning_rate": 9.342869296622518e-07,
      "loss": 0.0012,
      "step": 37280
    },
    {
      "epoch": 6780.0,
      "grad_norm": 0.0030941294971853495,
      "learning_rate": 9.34229256910106e-07,
      "loss": 0.0012,
      "step": 37290
    },
    {
      "epoch": 6781.818181818182,
      "grad_norm": 0.24660947918891907,
      "learning_rate": 9.341715606426189e-07,
      "loss": 0.0013,
      "step": 37300
    },
    {
      "epoch": 6783.636363636364,
      "grad_norm": 0.0031473624985665083,
      "learning_rate": 9.341138408629148e-07,
      "loss": 0.0012,
      "step": 37310
    },
    {
      "epoch": 6785.454545454545,
      "grad_norm": 0.004029024858027697,
      "learning_rate": 9.340560975741196e-07,
      "loss": 0.0011,
      "step": 37320
    },
    {
      "epoch": 6787.272727272727,
      "grad_norm": 0.002409803681075573,
      "learning_rate": 9.339983307793603e-07,
      "loss": 0.0013,
      "step": 37330
    },
    {
      "epoch": 6789.090909090909,
      "grad_norm": 0.0028867791406810284,
      "learning_rate": 9.339405404817653e-07,
      "loss": 0.001,
      "step": 37340
    },
    {
      "epoch": 6790.909090909091,
      "grad_norm": 0.003389899153262377,
      "learning_rate": 9.338827266844642e-07,
      "loss": 0.0013,
      "step": 37350
    },
    {
      "epoch": 6792.727272727273,
      "grad_norm": 0.3735545575618744,
      "learning_rate": 9.338248893905878e-07,
      "loss": 0.0012,
      "step": 37360
    },
    {
      "epoch": 6794.545454545455,
      "grad_norm": 0.0063315387815237045,
      "learning_rate": 9.337670286032681e-07,
      "loss": 0.0011,
      "step": 37370
    },
    {
      "epoch": 6796.363636363636,
      "grad_norm": 0.2022705227136612,
      "learning_rate": 9.337091443256386e-07,
      "loss": 0.0009,
      "step": 37380
    },
    {
      "epoch": 6798.181818181818,
      "grad_norm": 0.004233177285641432,
      "learning_rate": 9.336512365608343e-07,
      "loss": 0.001,
      "step": 37390
    },
    {
      "epoch": 6800.0,
      "grad_norm": 0.0027945260517299175,
      "learning_rate": 9.335933053119905e-07,
      "loss": 0.0013,
      "step": 37400
    },
    {
      "epoch": 6801.818181818182,
      "grad_norm": 0.004504694137722254,
      "learning_rate": 9.33535350582245e-07,
      "loss": 0.0009,
      "step": 37410
    },
    {
      "epoch": 6803.636363636364,
      "grad_norm": 0.34769773483276367,
      "learning_rate": 9.33477372374736e-07,
      "loss": 0.0015,
      "step": 37420
    },
    {
      "epoch": 6805.454545454545,
      "grad_norm": 0.002654734766110778,
      "learning_rate": 9.334193706926034e-07,
      "loss": 0.0011,
      "step": 37430
    },
    {
      "epoch": 6807.272727272727,
      "grad_norm": 0.0048868837766349316,
      "learning_rate": 9.333613455389881e-07,
      "loss": 0.0012,
      "step": 37440
    },
    {
      "epoch": 6809.090909090909,
      "grad_norm": 0.0025308458134531975,
      "learning_rate": 9.333032969170325e-07,
      "loss": 0.0011,
      "step": 37450
    },
    {
      "epoch": 6810.909090909091,
      "grad_norm": 0.0027507569175213575,
      "learning_rate": 9.332452248298801e-07,
      "loss": 0.0013,
      "step": 37460
    },
    {
      "epoch": 6812.727272727273,
      "grad_norm": 0.19646787643432617,
      "learning_rate": 9.331871292806759e-07,
      "loss": 0.0011,
      "step": 37470
    },
    {
      "epoch": 6814.545454545455,
      "grad_norm": 0.21848206222057343,
      "learning_rate": 9.331290102725659e-07,
      "loss": 0.0014,
      "step": 37480
    },
    {
      "epoch": 6816.363636363636,
      "grad_norm": 0.002356497570872307,
      "learning_rate": 9.330708678086974e-07,
      "loss": 0.001,
      "step": 37490
    },
    {
      "epoch": 6818.181818181818,
      "grad_norm": 0.27426186203956604,
      "learning_rate": 9.330127018922193e-07,
      "loss": 0.0012,
      "step": 37500
    },
    {
      "epoch": 6818.181818181818,
      "eval_loss": 4.582085609436035,
      "eval_runtime": 0.9528,
      "eval_samples_per_second": 10.495,
      "eval_steps_per_second": 5.248,
      "step": 37500
    },
    {
      "epoch": 6820.0,
      "grad_norm": 0.005672885570675135,
      "learning_rate": 9.329545125262813e-07,
      "loss": 0.0011,
      "step": 37510
    },
    {
      "epoch": 6821.818181818182,
      "grad_norm": 0.008489933796226978,
      "learning_rate": 9.328962997140348e-07,
      "loss": 0.0011,
      "step": 37520
    },
    {
      "epoch": 6823.636363636364,
      "grad_norm": 0.2931252717971802,
      "learning_rate": 9.328380634586321e-07,
      "loss": 0.0012,
      "step": 37530
    },
    {
      "epoch": 6825.454545454545,
      "grad_norm": 0.004099101759493351,
      "learning_rate": 9.327798037632271e-07,
      "loss": 0.0009,
      "step": 37540
    },
    {
      "epoch": 6827.272727272727,
      "grad_norm": 0.0022978780325502157,
      "learning_rate": 9.327215206309745e-07,
      "loss": 0.0011,
      "step": 37550
    },
    {
      "epoch": 6829.090909090909,
      "grad_norm": 0.28408223390579224,
      "learning_rate": 9.32663214065031e-07,
      "loss": 0.0014,
      "step": 37560
    },
    {
      "epoch": 6830.909090909091,
      "grad_norm": 0.21704541146755219,
      "learning_rate": 9.326048840685538e-07,
      "loss": 0.0011,
      "step": 37570
    },
    {
      "epoch": 6832.727272727273,
      "grad_norm": 0.42721205949783325,
      "learning_rate": 9.325465306447019e-07,
      "loss": 0.0012,
      "step": 37580
    },
    {
      "epoch": 6834.545454545455,
      "grad_norm": 0.341933012008667,
      "learning_rate": 9.324881537966354e-07,
      "loss": 0.001,
      "step": 37590
    },
    {
      "epoch": 6836.363636363636,
      "grad_norm": 0.30760514736175537,
      "learning_rate": 9.324297535275154e-07,
      "loss": 0.0011,
      "step": 37600
    },
    {
      "epoch": 6838.181818181818,
      "grad_norm": 0.003095826832577586,
      "learning_rate": 9.323713298405048e-07,
      "loss": 0.001,
      "step": 37610
    },
    {
      "epoch": 6840.0,
      "grad_norm": 0.0029469539877027273,
      "learning_rate": 9.323128827387674e-07,
      "loss": 0.0012,
      "step": 37620
    },
    {
      "epoch": 6841.818181818182,
      "grad_norm": 0.23596453666687012,
      "learning_rate": 9.322544122254684e-07,
      "loss": 0.0011,
      "step": 37630
    },
    {
      "epoch": 6843.636363636364,
      "grad_norm": 0.0049657681956887245,
      "learning_rate": 9.321959183037742e-07,
      "loss": 0.0012,
      "step": 37640
    },
    {
      "epoch": 6845.454545454545,
      "grad_norm": 0.0064079794101417065,
      "learning_rate": 9.321374009768524e-07,
      "loss": 0.0011,
      "step": 37650
    },
    {
      "epoch": 6847.272727272727,
      "grad_norm": 0.0027020866982638836,
      "learning_rate": 9.320788602478719e-07,
      "loss": 0.0009,
      "step": 37660
    },
    {
      "epoch": 6849.090909090909,
      "grad_norm": 0.3000517785549164,
      "learning_rate": 9.320202961200033e-07,
      "loss": 0.0012,
      "step": 37670
    },
    {
      "epoch": 6850.909090909091,
      "grad_norm": 0.003005834762006998,
      "learning_rate": 9.319617085964175e-07,
      "loss": 0.0012,
      "step": 37680
    },
    {
      "epoch": 6852.727272727273,
      "grad_norm": 0.06850167363882065,
      "learning_rate": 9.31903097680288e-07,
      "loss": 0.001,
      "step": 37690
    },
    {
      "epoch": 6854.545454545455,
      "grad_norm": 1.172201156616211,
      "learning_rate": 9.318444633747882e-07,
      "loss": 0.0012,
      "step": 37700
    },
    {
      "epoch": 6856.363636363636,
      "grad_norm": 60.01349639892578,
      "learning_rate": 9.317858056830936e-07,
      "loss": 0.0038,
      "step": 37710
    },
    {
      "epoch": 6858.181818181818,
      "grad_norm": 0.05309288203716278,
      "learning_rate": 9.317271246083809e-07,
      "loss": 0.001,
      "step": 37720
    },
    {
      "epoch": 6860.0,
      "grad_norm": 0.07897137105464935,
      "learning_rate": 9.316684201538279e-07,
      "loss": 0.0014,
      "step": 37730
    },
    {
      "epoch": 6861.818181818182,
      "grad_norm": 1.5750385522842407,
      "learning_rate": 9.316096923226135e-07,
      "loss": 0.0036,
      "step": 37740
    },
    {
      "epoch": 6863.636363636364,
      "grad_norm": 0.2083081156015396,
      "learning_rate": 9.315509411179181e-07,
      "loss": 0.0017,
      "step": 37750
    },
    {
      "epoch": 6865.454545454545,
      "grad_norm": 0.2266680896282196,
      "learning_rate": 9.314921665429235e-07,
      "loss": 0.001,
      "step": 37760
    },
    {
      "epoch": 6867.272727272727,
      "grad_norm": 0.0046339076943695545,
      "learning_rate": 9.314333686008124e-07,
      "loss": 0.0011,
      "step": 37770
    },
    {
      "epoch": 6869.090909090909,
      "grad_norm": 0.24470531940460205,
      "learning_rate": 9.313745472947691e-07,
      "loss": 0.0014,
      "step": 37780
    },
    {
      "epoch": 6870.909090909091,
      "grad_norm": 0.2422664761543274,
      "learning_rate": 9.31315702627979e-07,
      "loss": 0.0011,
      "step": 37790
    },
    {
      "epoch": 6872.727272727273,
      "grad_norm": 0.23539739847183228,
      "learning_rate": 9.312568346036287e-07,
      "loss": 0.0013,
      "step": 37800
    },
    {
      "epoch": 6874.545454545455,
      "grad_norm": 0.222139373421669,
      "learning_rate": 9.311979432249062e-07,
      "loss": 0.0012,
      "step": 37810
    },
    {
      "epoch": 6876.363636363636,
      "grad_norm": 0.19696776568889618,
      "learning_rate": 9.311390284950007e-07,
      "loss": 0.0009,
      "step": 37820
    },
    {
      "epoch": 6878.181818181818,
      "grad_norm": 0.28108203411102295,
      "learning_rate": 9.310800904171028e-07,
      "loss": 0.0011,
      "step": 37830
    },
    {
      "epoch": 6880.0,
      "grad_norm": 0.0063467081636190414,
      "learning_rate": 9.31021128994404e-07,
      "loss": 0.0012,
      "step": 37840
    },
    {
      "epoch": 6881.818181818182,
      "grad_norm": 0.24536460638046265,
      "learning_rate": 9.309621442300974e-07,
      "loss": 0.0012,
      "step": 37850
    },
    {
      "epoch": 6883.636363636364,
      "grad_norm": 0.3722473680973053,
      "learning_rate": 9.309031361273774e-07,
      "loss": 0.0011,
      "step": 37860
    },
    {
      "epoch": 6885.454545454545,
      "grad_norm": 0.007729521486908197,
      "learning_rate": 9.308441046894396e-07,
      "loss": 0.001,
      "step": 37870
    },
    {
      "epoch": 6887.272727272727,
      "grad_norm": 0.39045029878616333,
      "learning_rate": 9.307850499194804e-07,
      "loss": 0.0014,
      "step": 37880
    },
    {
      "epoch": 6889.090909090909,
      "grad_norm": 0.006640076637268066,
      "learning_rate": 9.307259718206983e-07,
      "loss": 0.001,
      "step": 37890
    },
    {
      "epoch": 6890.909090909091,
      "grad_norm": 0.24621640145778656,
      "learning_rate": 9.306668703962926e-07,
      "loss": 0.0012,
      "step": 37900
    },
    {
      "epoch": 6892.727272727273,
      "grad_norm": 0.35441675782203674,
      "learning_rate": 9.306077456494634e-07,
      "loss": 0.0011,
      "step": 37910
    },
    {
      "epoch": 6894.545454545455,
      "grad_norm": 0.002898718696087599,
      "learning_rate": 9.305485975834131e-07,
      "loss": 0.0009,
      "step": 37920
    },
    {
      "epoch": 6896.363636363636,
      "grad_norm": 0.004639328457415104,
      "learning_rate": 9.304894262013445e-07,
      "loss": 0.0013,
      "step": 37930
    },
    {
      "epoch": 6898.181818181818,
      "grad_norm": 0.00702564837411046,
      "learning_rate": 9.304302315064623e-07,
      "loss": 0.0011,
      "step": 37940
    },
    {
      "epoch": 6900.0,
      "grad_norm": 0.28146499395370483,
      "learning_rate": 9.303710135019717e-07,
      "loss": 0.0013,
      "step": 37950
    },
    {
      "epoch": 6901.818181818182,
      "grad_norm": 0.30186739563941956,
      "learning_rate": 9.3031177219108e-07,
      "loss": 0.0012,
      "step": 37960
    },
    {
      "epoch": 6903.636363636364,
      "grad_norm": 0.002616040874272585,
      "learning_rate": 9.302525075769951e-07,
      "loss": 0.0009,
      "step": 37970
    },
    {
      "epoch": 6905.454545454545,
      "grad_norm": 0.1293507069349289,
      "learning_rate": 9.301932196629266e-07,
      "loss": 0.0016,
      "step": 37980
    },
    {
      "epoch": 6907.272727272727,
      "grad_norm": 0.27874666452407837,
      "learning_rate": 9.301339084520852e-07,
      "loss": 0.001,
      "step": 37990
    },
    {
      "epoch": 6909.090909090909,
      "grad_norm": 0.008922085165977478,
      "learning_rate": 9.300745739476828e-07,
      "loss": 0.0011,
      "step": 38000
    },
    {
      "epoch": 6909.090909090909,
      "eval_loss": 4.539914608001709,
      "eval_runtime": 0.9504,
      "eval_samples_per_second": 10.522,
      "eval_steps_per_second": 5.261,
      "step": 38000
    },
    {
      "epoch": 6910.909090909091,
      "grad_norm": 0.011143933981657028,
      "learning_rate": 9.300152161529324e-07,
      "loss": 0.0013,
      "step": 38010
    },
    {
      "epoch": 6912.727272727273,
      "grad_norm": 0.0030816644430160522,
      "learning_rate": 9.299558350710486e-07,
      "loss": 0.0011,
      "step": 38020
    },
    {
      "epoch": 6914.545454545455,
      "grad_norm": 0.0028301789425313473,
      "learning_rate": 9.298964307052474e-07,
      "loss": 0.0011,
      "step": 38030
    },
    {
      "epoch": 6916.363636363636,
      "grad_norm": 0.0029543584678322077,
      "learning_rate": 9.298370030587455e-07,
      "loss": 0.0011,
      "step": 38040
    },
    {
      "epoch": 6918.181818181818,
      "grad_norm": 0.2946685254573822,
      "learning_rate": 9.297775521347613e-07,
      "loss": 0.0012,
      "step": 38050
    },
    {
      "epoch": 6920.0,
      "grad_norm": 0.009851595386862755,
      "learning_rate": 9.297180779365143e-07,
      "loss": 0.0011,
      "step": 38060
    },
    {
      "epoch": 6921.818181818182,
      "grad_norm": 0.01159097533673048,
      "learning_rate": 9.296585804672251e-07,
      "loss": 0.0012,
      "step": 38070
    },
    {
      "epoch": 6923.636363636364,
      "grad_norm": 0.3533805012702942,
      "learning_rate": 9.29599059730116e-07,
      "loss": 0.0011,
      "step": 38080
    },
    {
      "epoch": 6925.454545454545,
      "grad_norm": 0.025419453158974648,
      "learning_rate": 9.295395157284102e-07,
      "loss": 0.0011,
      "step": 38090
    },
    {
      "epoch": 6927.272727272727,
      "grad_norm": 0.2986868619918823,
      "learning_rate": 9.294799484653322e-07,
      "loss": 0.0011,
      "step": 38100
    },
    {
      "epoch": 6929.090909090909,
      "grad_norm": 0.3083759546279907,
      "learning_rate": 9.294203579441078e-07,
      "loss": 0.0011,
      "step": 38110
    },
    {
      "epoch": 6930.909090909091,
      "grad_norm": 0.003100145375356078,
      "learning_rate": 9.293607441679643e-07,
      "loss": 0.0012,
      "step": 38120
    },
    {
      "epoch": 6932.727272727273,
      "grad_norm": 0.28223904967308044,
      "learning_rate": 9.293011071401298e-07,
      "loss": 0.0009,
      "step": 38130
    },
    {
      "epoch": 6934.545454545455,
      "grad_norm": 0.19059069454669952,
      "learning_rate": 9.292414468638339e-07,
      "loss": 0.0014,
      "step": 38140
    },
    {
      "epoch": 6936.363636363636,
      "grad_norm": 0.22543062269687653,
      "learning_rate": 9.291817633423076e-07,
      "loss": 0.0011,
      "step": 38150
    },
    {
      "epoch": 6938.181818181818,
      "grad_norm": 0.017011892050504684,
      "learning_rate": 9.291220565787828e-07,
      "loss": 0.0012,
      "step": 38160
    },
    {
      "epoch": 6940.0,
      "grad_norm": 0.2176627516746521,
      "learning_rate": 9.290623265764932e-07,
      "loss": 0.0009,
      "step": 38170
    },
    {
      "epoch": 6941.818181818182,
      "grad_norm": 0.25933393836021423,
      "learning_rate": 9.290025733386731e-07,
      "loss": 0.0009,
      "step": 38180
    },
    {
      "epoch": 6943.636363636364,
      "grad_norm": 0.007975857704877853,
      "learning_rate": 9.289427968685586e-07,
      "loss": 0.0015,
      "step": 38190
    },
    {
      "epoch": 6945.454545454545,
      "grad_norm": 0.33111435174942017,
      "learning_rate": 9.288829971693867e-07,
      "loss": 0.0012,
      "step": 38200
    },
    {
      "epoch": 6947.272727272727,
      "grad_norm": 0.2249077707529068,
      "learning_rate": 9.288231742443961e-07,
      "loss": 0.0009,
      "step": 38210
    },
    {
      "epoch": 6949.090909090909,
      "grad_norm": 0.33850395679473877,
      "learning_rate": 9.287633280968261e-07,
      "loss": 0.0012,
      "step": 38220
    },
    {
      "epoch": 6950.909090909091,
      "grad_norm": 0.004680795595049858,
      "learning_rate": 9.287034587299179e-07,
      "loss": 0.0009,
      "step": 38230
    },
    {
      "epoch": 6952.727272727273,
      "grad_norm": 0.006593168713152409,
      "learning_rate": 9.286435661469133e-07,
      "loss": 0.0016,
      "step": 38240
    },
    {
      "epoch": 6954.545454545455,
      "grad_norm": 0.00522428285330534,
      "learning_rate": 9.285836503510562e-07,
      "loss": 0.0009,
      "step": 38250
    },
    {
      "epoch": 6956.363636363636,
      "grad_norm": 0.3702815771102905,
      "learning_rate": 9.285237113455908e-07,
      "loss": 0.0012,
      "step": 38260
    },
    {
      "epoch": 6958.181818181818,
      "grad_norm": 0.31767168641090393,
      "learning_rate": 9.284637491337635e-07,
      "loss": 0.0012,
      "step": 38270
    },
    {
      "epoch": 6960.0,
      "grad_norm": 0.023575156927108765,
      "learning_rate": 9.284037637188213e-07,
      "loss": 0.0011,
      "step": 38280
    },
    {
      "epoch": 6961.818181818182,
      "grad_norm": 0.016614267602562904,
      "learning_rate": 9.283437551040129e-07,
      "loss": 0.001,
      "step": 38290
    },
    {
      "epoch": 6963.636363636364,
      "grad_norm": 0.0029802757780998945,
      "learning_rate": 9.282837232925875e-07,
      "loss": 0.0011,
      "step": 38300
    },
    {
      "epoch": 6965.454545454545,
      "grad_norm": 0.3623166084289551,
      "learning_rate": 9.282236682877967e-07,
      "loss": 0.0014,
      "step": 38310
    },
    {
      "epoch": 6967.272727272727,
      "grad_norm": 0.23628519475460052,
      "learning_rate": 9.281635900928921e-07,
      "loss": 0.001,
      "step": 38320
    },
    {
      "epoch": 6969.090909090909,
      "grad_norm": 0.006184759084135294,
      "learning_rate": 9.281034887111277e-07,
      "loss": 0.0011,
      "step": 38330
    },
    {
      "epoch": 6970.909090909091,
      "grad_norm": 0.0029005729593336582,
      "learning_rate": 9.28043364145758e-07,
      "loss": 0.0012,
      "step": 38340
    },
    {
      "epoch": 6972.727272727273,
      "grad_norm": 0.005381404422223568,
      "learning_rate": 9.279832164000391e-07,
      "loss": 0.0011,
      "step": 38350
    },
    {
      "epoch": 6974.545454545455,
      "grad_norm": 0.00331279169768095,
      "learning_rate": 9.279230454772281e-07,
      "loss": 0.0011,
      "step": 38360
    },
    {
      "epoch": 6976.363636363636,
      "grad_norm": 0.24404481053352356,
      "learning_rate": 9.278628513805837e-07,
      "loss": 0.0012,
      "step": 38370
    },
    {
      "epoch": 6978.181818181818,
      "grad_norm": 0.2835755944252014,
      "learning_rate": 9.278026341133656e-07,
      "loss": 0.0014,
      "step": 38380
    },
    {
      "epoch": 6980.0,
      "grad_norm": 0.31258177757263184,
      "learning_rate": 9.277423936788346e-07,
      "loss": 0.0011,
      "step": 38390
    },
    {
      "epoch": 6981.818181818182,
      "grad_norm": 0.00605462770909071,
      "learning_rate": 9.276821300802533e-07,
      "loss": 0.0013,
      "step": 38400
    },
    {
      "epoch": 6983.636363636364,
      "grad_norm": 0.01083355862647295,
      "learning_rate": 9.276218433208852e-07,
      "loss": 0.0011,
      "step": 38410
    },
    {
      "epoch": 6985.454545454545,
      "grad_norm": 0.0020544712897390127,
      "learning_rate": 9.275615334039947e-07,
      "loss": 0.001,
      "step": 38420
    },
    {
      "epoch": 6987.272727272727,
      "grad_norm": 0.0024204105138778687,
      "learning_rate": 9.275012003328482e-07,
      "loss": 0.0011,
      "step": 38430
    },
    {
      "epoch": 6989.090909090909,
      "grad_norm": 0.004720423370599747,
      "learning_rate": 9.27440844110713e-07,
      "loss": 0.0012,
      "step": 38440
    },
    {
      "epoch": 6990.909090909091,
      "grad_norm": 0.26541200280189514,
      "learning_rate": 9.273804647408574e-07,
      "loss": 0.0012,
      "step": 38450
    },
    {
      "epoch": 6992.727272727273,
      "grad_norm": 0.0033824164420366287,
      "learning_rate": 9.273200622265515e-07,
      "loss": 0.0008,
      "step": 38460
    },
    {
      "epoch": 6994.545454545455,
      "grad_norm": 0.23968474566936493,
      "learning_rate": 9.272596365710661e-07,
      "loss": 0.0014,
      "step": 38470
    },
    {
      "epoch": 6996.363636363636,
      "grad_norm": 0.3594297766685486,
      "learning_rate": 9.271991877776736e-07,
      "loss": 0.0013,
      "step": 38480
    },
    {
      "epoch": 6998.181818181818,
      "grad_norm": 0.2649056911468506,
      "learning_rate": 9.271387158496476e-07,
      "loss": 0.0009,
      "step": 38490
    },
    {
      "epoch": 7000.0,
      "grad_norm": 0.004843853414058685,
      "learning_rate": 9.270782207902627e-07,
      "loss": 0.0011,
      "step": 38500
    },
    {
      "epoch": 7000.0,
      "eval_loss": 4.672787666320801,
      "eval_runtime": 0.9506,
      "eval_samples_per_second": 10.52,
      "eval_steps_per_second": 5.26,
      "step": 38500
    },
    {
      "epoch": 7001.818181818182,
      "grad_norm": 0.30624160170555115,
      "learning_rate": 9.270177026027953e-07,
      "loss": 0.0012,
      "step": 38510
    },
    {
      "epoch": 7003.636363636364,
      "grad_norm": 0.0023020782973617315,
      "learning_rate": 9.269571612905225e-07,
      "loss": 0.0009,
      "step": 38520
    },
    {
      "epoch": 7005.454545454545,
      "grad_norm": 0.0033176057040691376,
      "learning_rate": 9.268965968567228e-07,
      "loss": 0.0014,
      "step": 38530
    },
    {
      "epoch": 7007.272727272727,
      "grad_norm": 0.004401078913360834,
      "learning_rate": 9.268360093046762e-07,
      "loss": 0.001,
      "step": 38540
    },
    {
      "epoch": 7009.090909090909,
      "grad_norm": 0.24626290798187256,
      "learning_rate": 9.267753986376636e-07,
      "loss": 0.0012,
      "step": 38550
    },
    {
      "epoch": 7010.909090909091,
      "grad_norm": 0.002712889341637492,
      "learning_rate": 9.267147648589676e-07,
      "loss": 0.0013,
      "step": 38560
    },
    {
      "epoch": 7012.727272727273,
      "grad_norm": 0.41188448667526245,
      "learning_rate": 9.266541079718714e-07,
      "loss": 0.0012,
      "step": 38570
    },
    {
      "epoch": 7014.545454545455,
      "grad_norm": 0.005043167620897293,
      "learning_rate": 9.265934279796601e-07,
      "loss": 0.001,
      "step": 38580
    },
    {
      "epoch": 7016.363636363636,
      "grad_norm": 0.3929395079612732,
      "learning_rate": 9.265327248856196e-07,
      "loss": 0.0013,
      "step": 38590
    },
    {
      "epoch": 7018.181818181818,
      "grad_norm": 0.02088945358991623,
      "learning_rate": 9.264719986930375e-07,
      "loss": 0.001,
      "step": 38600
    },
    {
      "epoch": 7020.0,
      "grad_norm": 0.003442394779995084,
      "learning_rate": 9.264112494052021e-07,
      "loss": 0.0013,
      "step": 38610
    },
    {
      "epoch": 7021.818181818182,
      "grad_norm": 0.35325223207473755,
      "learning_rate": 9.263504770254033e-07,
      "loss": 0.0012,
      "step": 38620
    },
    {
      "epoch": 7023.636363636364,
      "grad_norm": 0.010954918339848518,
      "learning_rate": 9.262896815569322e-07,
      "loss": 0.0009,
      "step": 38630
    },
    {
      "epoch": 7025.454545454545,
      "grad_norm": 0.017199236899614334,
      "learning_rate": 9.262288630030812e-07,
      "loss": 0.0014,
      "step": 38640
    },
    {
      "epoch": 7027.272727272727,
      "grad_norm": 0.0025455050636082888,
      "learning_rate": 9.261680213671437e-07,
      "loss": 0.0008,
      "step": 38650
    },
    {
      "epoch": 7029.090909090909,
      "grad_norm": 0.22411088645458221,
      "learning_rate": 9.261071566524148e-07,
      "loss": 0.0014,
      "step": 38660
    },
    {
      "epoch": 7030.909090909091,
      "grad_norm": 0.31853652000427246,
      "learning_rate": 9.260462688621904e-07,
      "loss": 0.0011,
      "step": 38670
    },
    {
      "epoch": 7032.727272727273,
      "grad_norm": 0.3605384826660156,
      "learning_rate": 9.259853579997679e-07,
      "loss": 0.0011,
      "step": 38680
    },
    {
      "epoch": 7034.545454545455,
      "grad_norm": 0.22805064916610718,
      "learning_rate": 9.259244240684456e-07,
      "loss": 0.0011,
      "step": 38690
    },
    {
      "epoch": 7036.363636363636,
      "grad_norm": 0.0030787368305027485,
      "learning_rate": 9.258634670715237e-07,
      "loss": 0.001,
      "step": 38700
    },
    {
      "epoch": 7038.181818181818,
      "grad_norm": 0.0025069278199225664,
      "learning_rate": 9.258024870123033e-07,
      "loss": 0.0012,
      "step": 38710
    },
    {
      "epoch": 7040.0,
      "grad_norm": 0.3161502480506897,
      "learning_rate": 9.257414838940864e-07,
      "loss": 0.0012,
      "step": 38720
    },
    {
      "epoch": 7041.818181818182,
      "grad_norm": 0.00709320604801178,
      "learning_rate": 9.256804577201767e-07,
      "loss": 0.0012,
      "step": 38730
    },
    {
      "epoch": 7043.636363636364,
      "grad_norm": 0.22585546970367432,
      "learning_rate": 9.256194084938793e-07,
      "loss": 0.0011,
      "step": 38740
    },
    {
      "epoch": 7045.454545454545,
      "grad_norm": 0.21741819381713867,
      "learning_rate": 9.255583362184998e-07,
      "loss": 0.001,
      "step": 38750
    },
    {
      "epoch": 7047.272727272727,
      "grad_norm": 0.27705493569374084,
      "learning_rate": 9.25497240897346e-07,
      "loss": 0.0014,
      "step": 38760
    },
    {
      "epoch": 7049.090909090909,
      "grad_norm": 0.3005078434944153,
      "learning_rate": 9.25436122533726e-07,
      "loss": 0.001,
      "step": 38770
    },
    {
      "epoch": 7050.909090909091,
      "grad_norm": 0.30163654685020447,
      "learning_rate": 9.2537498113095e-07,
      "loss": 0.0013,
      "step": 38780
    },
    {
      "epoch": 7052.727272727273,
      "grad_norm": 0.36864733695983887,
      "learning_rate": 9.253138166923289e-07,
      "loss": 0.0012,
      "step": 38790
    },
    {
      "epoch": 7054.545454545455,
      "grad_norm": 0.0026297515723854303,
      "learning_rate": 9.252526292211749e-07,
      "loss": 0.0009,
      "step": 38800
    },
    {
      "epoch": 7056.363636363636,
      "grad_norm": 0.009014160372316837,
      "learning_rate": 9.251914187208018e-07,
      "loss": 0.0012,
      "step": 38810
    },
    {
      "epoch": 7058.181818181818,
      "grad_norm": 0.31517595052719116,
      "learning_rate": 9.251301851945242e-07,
      "loss": 0.0013,
      "step": 38820
    },
    {
      "epoch": 7060.0,
      "grad_norm": 0.0019626340363174677,
      "learning_rate": 9.250689286456584e-07,
      "loss": 0.0011,
      "step": 38830
    },
    {
      "epoch": 7061.818181818182,
      "grad_norm": 0.0021625575609505177,
      "learning_rate": 9.250076490775213e-07,
      "loss": 0.0012,
      "step": 38840
    },
    {
      "epoch": 7063.636363636364,
      "grad_norm": 0.21749351918697357,
      "learning_rate": 9.24946346493432e-07,
      "loss": 0.0011,
      "step": 38850
    },
    {
      "epoch": 7065.454545454545,
      "grad_norm": 0.0022555324248969555,
      "learning_rate": 9.248850208967097e-07,
      "loss": 0.0011,
      "step": 38860
    },
    {
      "epoch": 7067.272727272727,
      "grad_norm": 0.0030397309456020594,
      "learning_rate": 9.248236722906759e-07,
      "loss": 0.001,
      "step": 38870
    },
    {
      "epoch": 7069.090909090909,
      "grad_norm": 0.002457383321598172,
      "learning_rate": 9.247623006786527e-07,
      "loss": 0.0015,
      "step": 38880
    },
    {
      "epoch": 7070.909090909091,
      "grad_norm": 0.1704934984445572,
      "learning_rate": 9.247009060639636e-07,
      "loss": 0.0013,
      "step": 38890
    },
    {
      "epoch": 7072.727272727273,
      "grad_norm": 1.0286496877670288,
      "learning_rate": 9.246394884499333e-07,
      "loss": 0.001,
      "step": 38900
    },
    {
      "epoch": 7074.545454545455,
      "grad_norm": 0.35906484723091125,
      "learning_rate": 9.245780478398881e-07,
      "loss": 0.001,
      "step": 38910
    },
    {
      "epoch": 7076.363636363636,
      "grad_norm": 0.006163818296045065,
      "learning_rate": 9.245165842371551e-07,
      "loss": 0.0015,
      "step": 38920
    },
    {
      "epoch": 7078.181818181818,
      "grad_norm": 0.3056384325027466,
      "learning_rate": 9.244550976450627e-07,
      "loss": 0.0011,
      "step": 38930
    },
    {
      "epoch": 7080.0,
      "grad_norm": 0.32711243629455566,
      "learning_rate": 9.243935880669409e-07,
      "loss": 0.0011,
      "step": 38940
    },
    {
      "epoch": 7081.818181818182,
      "grad_norm": 0.2878161370754242,
      "learning_rate": 9.243320555061205e-07,
      "loss": 0.0009,
      "step": 38950
    },
    {
      "epoch": 7083.636363636364,
      "grad_norm": 0.3697413206100464,
      "learning_rate": 9.242704999659338e-07,
      "loss": 0.0013,
      "step": 38960
    },
    {
      "epoch": 7085.454545454545,
      "grad_norm": 0.01835617981851101,
      "learning_rate": 9.242089214497144e-07,
      "loss": 0.0012,
      "step": 38970
    },
    {
      "epoch": 7087.272727272727,
      "grad_norm": 0.36036062240600586,
      "learning_rate": 9.24147319960797e-07,
      "loss": 0.0014,
      "step": 38980
    },
    {
      "epoch": 7089.090909090909,
      "grad_norm": 0.27027568221092224,
      "learning_rate": 9.240856955025174e-07,
      "loss": 0.0009,
      "step": 38990
    },
    {
      "epoch": 7090.909090909091,
      "grad_norm": 0.009275330230593681,
      "learning_rate": 9.240240480782129e-07,
      "loss": 0.0009,
      "step": 39000
    },
    {
      "epoch": 7090.909090909091,
      "eval_loss": 4.68947696685791,
      "eval_runtime": 0.9532,
      "eval_samples_per_second": 10.491,
      "eval_steps_per_second": 5.245,
      "step": 39000
    },
    {
      "epoch": 7092.727272727273,
      "grad_norm": 0.02442859672009945,
      "learning_rate": 9.239623776912222e-07,
      "loss": 0.0012,
      "step": 39010
    },
    {
      "epoch": 7094.545454545455,
      "grad_norm": 0.00243469444103539,
      "learning_rate": 9.239006843448847e-07,
      "loss": 0.0009,
      "step": 39020
    },
    {
      "epoch": 7096.363636363636,
      "grad_norm": 0.006531576160341501,
      "learning_rate": 9.238389680425415e-07,
      "loss": 0.0012,
      "step": 39030
    },
    {
      "epoch": 7098.181818181818,
      "grad_norm": 0.2171807587146759,
      "learning_rate": 9.23777228787535e-07,
      "loss": 0.0012,
      "step": 39040
    },
    {
      "epoch": 7100.0,
      "grad_norm": 0.27291780710220337,
      "learning_rate": 9.237154665832082e-07,
      "loss": 0.0012,
      "step": 39050
    },
    {
      "epoch": 7101.818181818182,
      "grad_norm": 0.18770070374011993,
      "learning_rate": 9.23653681432906e-07,
      "loss": 0.0012,
      "step": 39060
    },
    {
      "epoch": 7103.636363636364,
      "grad_norm": 0.002411821624264121,
      "learning_rate": 9.235918733399745e-07,
      "loss": 0.0008,
      "step": 39070
    },
    {
      "epoch": 7105.454545454545,
      "grad_norm": 0.3568236529827118,
      "learning_rate": 9.235300423077606e-07,
      "loss": 0.0017,
      "step": 39080
    },
    {
      "epoch": 7107.272727272727,
      "grad_norm": 0.0035768775269389153,
      "learning_rate": 9.234681883396127e-07,
      "loss": 0.0006,
      "step": 39090
    },
    {
      "epoch": 7109.090909090909,
      "grad_norm": 0.29027366638183594,
      "learning_rate": 9.234063114388809e-07,
      "loss": 0.0014,
      "step": 39100
    },
    {
      "epoch": 7110.909090909091,
      "grad_norm": 0.2918914556503296,
      "learning_rate": 9.233444116089154e-07,
      "loss": 0.0011,
      "step": 39110
    },
    {
      "epoch": 7112.727272727273,
      "grad_norm": 0.003406342351809144,
      "learning_rate": 9.232824888530688e-07,
      "loss": 0.0011,
      "step": 39120
    },
    {
      "epoch": 7114.545454545455,
      "grad_norm": 0.22686976194381714,
      "learning_rate": 9.232205431746944e-07,
      "loss": 0.0014,
      "step": 39130
    },
    {
      "epoch": 7116.363636363636,
      "grad_norm": 0.24367506802082062,
      "learning_rate": 9.231585745771468e-07,
      "loss": 0.0009,
      "step": 39140
    },
    {
      "epoch": 7118.181818181818,
      "grad_norm": 0.27476057410240173,
      "learning_rate": 9.23096583063782e-07,
      "loss": 0.0011,
      "step": 39150
    },
    {
      "epoch": 7120.0,
      "grad_norm": 0.2675042450428009,
      "learning_rate": 9.230345686379568e-07,
      "loss": 0.0011,
      "step": 39160
    },
    {
      "epoch": 7121.818181818182,
      "grad_norm": 0.3965096175670624,
      "learning_rate": 9.229725313030296e-07,
      "loss": 0.0012,
      "step": 39170
    },
    {
      "epoch": 7123.636363636364,
      "grad_norm": 0.26728853583335876,
      "learning_rate": 9.229104710623603e-07,
      "loss": 0.0008,
      "step": 39180
    },
    {
      "epoch": 7125.454545454545,
      "grad_norm": 0.21749275922775269,
      "learning_rate": 9.228483879193094e-07,
      "loss": 0.0015,
      "step": 39190
    },
    {
      "epoch": 7127.272727272727,
      "grad_norm": 0.2776721715927124,
      "learning_rate": 9.227862818772392e-07,
      "loss": 0.0009,
      "step": 39200
    },
    {
      "epoch": 7129.090909090909,
      "grad_norm": 0.24691615998744965,
      "learning_rate": 9.227241529395127e-07,
      "loss": 0.0012,
      "step": 39210
    },
    {
      "epoch": 7130.909090909091,
      "grad_norm": 0.407438725233078,
      "learning_rate": 9.226620011094947e-07,
      "loss": 0.0011,
      "step": 39220
    },
    {
      "epoch": 7132.727272727273,
      "grad_norm": 0.2366410791873932,
      "learning_rate": 9.225998263905508e-07,
      "loss": 0.0012,
      "step": 39230
    },
    {
      "epoch": 7134.545454545455,
      "grad_norm": 0.2676820158958435,
      "learning_rate": 9.225376287860482e-07,
      "loss": 0.0008,
      "step": 39240
    },
    {
      "epoch": 7136.363636363636,
      "grad_norm": 0.20317935943603516,
      "learning_rate": 9.224754082993551e-07,
      "loss": 0.0012,
      "step": 39250
    },
    {
      "epoch": 7138.181818181818,
      "grad_norm": 0.2886103391647339,
      "learning_rate": 9.22413164933841e-07,
      "loss": 0.0013,
      "step": 39260
    },
    {
      "epoch": 7140.0,
      "grad_norm": 0.006030172109603882,
      "learning_rate": 9.223508986928765e-07,
      "loss": 0.0012,
      "step": 39270
    },
    {
      "epoch": 7141.818181818182,
      "grad_norm": 0.32234248518943787,
      "learning_rate": 9.222886095798337e-07,
      "loss": 0.0012,
      "step": 39280
    },
    {
      "epoch": 7143.636363636364,
      "grad_norm": 0.003171463031321764,
      "learning_rate": 9.222262975980859e-07,
      "loss": 0.0009,
      "step": 39290
    },
    {
      "epoch": 7145.454545454545,
      "grad_norm": 0.004544020630419254,
      "learning_rate": 9.221639627510075e-07,
      "loss": 0.0014,
      "step": 39300
    },
    {
      "epoch": 7147.272727272727,
      "grad_norm": 0.013021818362176418,
      "learning_rate": 9.221016050419741e-07,
      "loss": 0.0009,
      "step": 39310
    },
    {
      "epoch": 7149.090909090909,
      "grad_norm": 0.004291947465389967,
      "learning_rate": 9.220392244743628e-07,
      "loss": 0.0013,
      "step": 39320
    },
    {
      "epoch": 7150.909090909091,
      "grad_norm": 0.33057838678359985,
      "learning_rate": 9.219768210515516e-07,
      "loss": 0.0012,
      "step": 39330
    },
    {
      "epoch": 7152.727272727273,
      "grad_norm": 0.26308250427246094,
      "learning_rate": 9.2191439477692e-07,
      "loss": 0.0012,
      "step": 39340
    },
    {
      "epoch": 7154.545454545455,
      "grad_norm": 0.0022627669386565685,
      "learning_rate": 9.218519456538486e-07,
      "loss": 0.0009,
      "step": 39350
    },
    {
      "epoch": 7156.363636363636,
      "grad_norm": 0.2213139683008194,
      "learning_rate": 9.217894736857194e-07,
      "loss": 0.0012,
      "step": 39360
    },
    {
      "epoch": 7158.181818181818,
      "grad_norm": 0.00533631257712841,
      "learning_rate": 9.217269788759153e-07,
      "loss": 0.0009,
      "step": 39370
    },
    {
      "epoch": 7160.0,
      "grad_norm": 0.003462724620476365,
      "learning_rate": 9.216644612278208e-07,
      "loss": 0.0012,
      "step": 39380
    },
    {
      "epoch": 7161.818181818182,
      "grad_norm": 0.28691038489341736,
      "learning_rate": 9.216019207448216e-07,
      "loss": 0.0011,
      "step": 39390
    },
    {
      "epoch": 7163.636363636364,
      "grad_norm": 0.011880925856530666,
      "learning_rate": 9.215393574303043e-07,
      "loss": 0.0012,
      "step": 39400
    },
    {
      "epoch": 7165.454545454545,
      "grad_norm": 0.003970396239310503,
      "learning_rate": 9.214767712876572e-07,
      "loss": 0.0008,
      "step": 39410
    },
    {
      "epoch": 7167.272727272727,
      "grad_norm": 0.20806533098220825,
      "learning_rate": 9.214141623202694e-07,
      "loss": 0.0015,
      "step": 39420
    },
    {
      "epoch": 7169.090909090909,
      "grad_norm": 0.006718325428664684,
      "learning_rate": 9.213515305315314e-07,
      "loss": 0.0011,
      "step": 39430
    },
    {
      "epoch": 7170.909090909091,
      "grad_norm": 0.002424441045150161,
      "learning_rate": 9.212888759248352e-07,
      "loss": 0.0011,
      "step": 39440
    },
    {
      "epoch": 7172.727272727273,
      "grad_norm": 0.0030099134892225266,
      "learning_rate": 9.212261985035737e-07,
      "loss": 0.0012,
      "step": 39450
    },
    {
      "epoch": 7174.545454545455,
      "grad_norm": 0.3512960374355316,
      "learning_rate": 9.211634982711412e-07,
      "loss": 0.0014,
      "step": 39460
    },
    {
      "epoch": 7176.363636363636,
      "grad_norm": 0.0028401468880474567,
      "learning_rate": 9.211007752309331e-07,
      "loss": 0.0007,
      "step": 39470
    },
    {
      "epoch": 7178.181818181818,
      "grad_norm": 0.271281898021698,
      "learning_rate": 9.210380293863461e-07,
      "loss": 0.0013,
      "step": 39480
    },
    {
      "epoch": 7180.0,
      "grad_norm": 0.26953253149986267,
      "learning_rate": 9.209752607407783e-07,
      "loss": 0.0011,
      "step": 39490
    },
    {
      "epoch": 7181.818181818182,
      "grad_norm": 0.1728896051645279,
      "learning_rate": 9.209124692976287e-07,
      "loss": 0.0012,
      "step": 39500
    },
    {
      "epoch": 7181.818181818182,
      "eval_loss": 4.620457649230957,
      "eval_runtime": 0.9519,
      "eval_samples_per_second": 10.505,
      "eval_steps_per_second": 5.252,
      "step": 39500
    },
    {
      "epoch": 7183.636363636364,
      "grad_norm": 0.2645946443080902,
      "learning_rate": 9.208496550602978e-07,
      "loss": 0.0009,
      "step": 39510
    },
    {
      "epoch": 7185.454545454545,
      "grad_norm": 0.02350425161421299,
      "learning_rate": 9.207868180321874e-07,
      "loss": 0.0015,
      "step": 39520
    },
    {
      "epoch": 7187.272727272727,
      "grad_norm": 0.0067046103067696095,
      "learning_rate": 9.207239582167002e-07,
      "loss": 0.0008,
      "step": 39530
    },
    {
      "epoch": 7189.090909090909,
      "grad_norm": 0.2037504017353058,
      "learning_rate": 9.206610756172402e-07,
      "loss": 0.0015,
      "step": 39540
    },
    {
      "epoch": 7190.909090909091,
      "grad_norm": 0.34797897934913635,
      "learning_rate": 9.20598170237213e-07,
      "loss": 0.0005,
      "step": 39550
    },
    {
      "epoch": 7192.727272727273,
      "grad_norm": 0.2246355563402176,
      "learning_rate": 9.205352420800252e-07,
      "loss": 0.0015,
      "step": 39560
    },
    {
      "epoch": 7194.545454545455,
      "grad_norm": 0.2654836177825928,
      "learning_rate": 9.204722911490846e-07,
      "loss": 0.0009,
      "step": 39570
    },
    {
      "epoch": 7196.363636363636,
      "grad_norm": 0.27142614126205444,
      "learning_rate": 9.204093174478e-07,
      "loss": 0.0013,
      "step": 39580
    },
    {
      "epoch": 7198.181818181818,
      "grad_norm": 0.0054470873437821865,
      "learning_rate": 9.203463209795821e-07,
      "loss": 0.0009,
      "step": 39590
    },
    {
      "epoch": 7200.0,
      "grad_norm": 0.23254036903381348,
      "learning_rate": 9.202833017478421e-07,
      "loss": 0.0012,
      "step": 39600
    },
    {
      "epoch": 7201.818181818182,
      "grad_norm": 0.0037912174593657255,
      "learning_rate": 9.202202597559929e-07,
      "loss": 0.001,
      "step": 39610
    },
    {
      "epoch": 7203.636363636364,
      "grad_norm": 0.28628817200660706,
      "learning_rate": 9.201571950074485e-07,
      "loss": 0.0011,
      "step": 39620
    },
    {
      "epoch": 7205.454545454545,
      "grad_norm": 0.011271812953054905,
      "learning_rate": 9.200941075056241e-07,
      "loss": 0.0014,
      "step": 39630
    },
    {
      "epoch": 7207.272727272727,
      "grad_norm": 0.37630748748779297,
      "learning_rate": 9.200309972539361e-07,
      "loss": 0.0011,
      "step": 39640
    },
    {
      "epoch": 7209.090909090909,
      "grad_norm": 0.20802217721939087,
      "learning_rate": 9.199678642558022e-07,
      "loss": 0.0011,
      "step": 39650
    },
    {
      "epoch": 7210.909090909091,
      "grad_norm": 0.00498109171167016,
      "learning_rate": 9.199047085146414e-07,
      "loss": 0.0011,
      "step": 39660
    },
    {
      "epoch": 7212.727272727273,
      "grad_norm": 0.006106301210820675,
      "learning_rate": 9.198415300338737e-07,
      "loss": 0.0011,
      "step": 39670
    },
    {
      "epoch": 7214.545454545455,
      "grad_norm": 0.004338831175118685,
      "learning_rate": 9.197783288169207e-07,
      "loss": 0.0011,
      "step": 39680
    },
    {
      "epoch": 7216.363636363636,
      "grad_norm": 0.0030088413041085005,
      "learning_rate": 9.197151048672049e-07,
      "loss": 0.001,
      "step": 39690
    },
    {
      "epoch": 7218.181818181818,
      "grad_norm": 0.22088523209095,
      "learning_rate": 9.196518581881501e-07,
      "loss": 0.0014,
      "step": 39700
    },
    {
      "epoch": 7220.0,
      "grad_norm": 0.2911912202835083,
      "learning_rate": 9.195885887831813e-07,
      "loss": 0.0012,
      "step": 39710
    },
    {
      "epoch": 7221.818181818182,
      "grad_norm": 0.21477010846138,
      "learning_rate": 9.19525296655725e-07,
      "loss": 0.0012,
      "step": 39720
    },
    {
      "epoch": 7223.636363636364,
      "grad_norm": 0.2761552035808563,
      "learning_rate": 9.194619818092087e-07,
      "loss": 0.0009,
      "step": 39730
    },
    {
      "epoch": 7225.454545454545,
      "grad_norm": 0.21656373143196106,
      "learning_rate": 9.19398644247061e-07,
      "loss": 0.0012,
      "step": 39740
    },
    {
      "epoch": 7227.272727272727,
      "grad_norm": 0.4083782732486725,
      "learning_rate": 9.19335283972712e-07,
      "loss": 0.0014,
      "step": 39750
    },
    {
      "epoch": 7229.090909090909,
      "grad_norm": 0.26368942856788635,
      "learning_rate": 9.19271900989593e-07,
      "loss": 0.0011,
      "step": 39760
    },
    {
      "epoch": 7230.909090909091,
      "grad_norm": 0.2328536957502365,
      "learning_rate": 9.192084953011363e-07,
      "loss": 0.0011,
      "step": 39770
    },
    {
      "epoch": 7232.727272727273,
      "grad_norm": 0.0029257761780172586,
      "learning_rate": 9.191450669107757e-07,
      "loss": 0.0009,
      "step": 39780
    },
    {
      "epoch": 7234.545454545455,
      "grad_norm": 0.2667546570301056,
      "learning_rate": 9.190816158219461e-07,
      "loss": 0.0015,
      "step": 39790
    },
    {
      "epoch": 7236.363636363636,
      "grad_norm": 0.29204872250556946,
      "learning_rate": 9.190181420380836e-07,
      "loss": 0.0008,
      "step": 39800
    },
    {
      "epoch": 7238.181818181818,
      "grad_norm": 0.004033075179904699,
      "learning_rate": 9.189546455626256e-07,
      "loss": 0.001,
      "step": 39810
    },
    {
      "epoch": 7240.0,
      "grad_norm": 0.04883512854576111,
      "learning_rate": 9.188911263990106e-07,
      "loss": 0.0012,
      "step": 39820
    },
    {
      "epoch": 7241.818181818182,
      "grad_norm": 0.34333622455596924,
      "learning_rate": 9.188275845506788e-07,
      "loss": 0.0012,
      "step": 39830
    },
    {
      "epoch": 7243.636363636364,
      "grad_norm": 0.004940837621688843,
      "learning_rate": 9.187640200210709e-07,
      "loss": 0.001,
      "step": 39840
    },
    {
      "epoch": 7245.454545454545,
      "grad_norm": 0.005774935241788626,
      "learning_rate": 9.187004328136292e-07,
      "loss": 0.0011,
      "step": 39850
    },
    {
      "epoch": 7247.272727272727,
      "grad_norm": 0.01093740202486515,
      "learning_rate": 9.186368229317973e-07,
      "loss": 0.0014,
      "step": 39860
    },
    {
      "epoch": 7249.090909090909,
      "grad_norm": 0.030439672991633415,
      "learning_rate": 9.1857319037902e-07,
      "loss": 0.0012,
      "step": 39870
    },
    {
      "epoch": 7250.909090909091,
      "grad_norm": 0.27280011773109436,
      "learning_rate": 9.185095351587429e-07,
      "loss": 0.0009,
      "step": 39880
    },
    {
      "epoch": 7252.727272727273,
      "grad_norm": 0.003981525544077158,
      "learning_rate": 9.184458572744138e-07,
      "loss": 0.0009,
      "step": 39890
    },
    {
      "epoch": 7254.545454545455,
      "grad_norm": 0.0028825688641518354,
      "learning_rate": 9.183821567294808e-07,
      "loss": 0.0012,
      "step": 39900
    },
    {
      "epoch": 7256.363636363636,
      "grad_norm": 0.22722330689430237,
      "learning_rate": 9.183184335273935e-07,
      "loss": 0.0012,
      "step": 39910
    },
    {
      "epoch": 7258.181818181818,
      "grad_norm": 0.0020460630767047405,
      "learning_rate": 9.18254687671603e-07,
      "loss": 0.0011,
      "step": 39920
    },
    {
      "epoch": 7260.0,
      "grad_norm": 0.3315463662147522,
      "learning_rate": 9.181909191655611e-07,
      "loss": 0.0012,
      "step": 39930
    },
    {
      "epoch": 7261.818181818182,
      "grad_norm": 0.029953036457300186,
      "learning_rate": 9.181271280127214e-07,
      "loss": 0.0012,
      "step": 39940
    },
    {
      "epoch": 7263.636363636364,
      "grad_norm": 0.30236944556236267,
      "learning_rate": 9.180633142165384e-07,
      "loss": 0.0012,
      "step": 39950
    },
    {
      "epoch": 7265.454545454545,
      "grad_norm": 0.00239416747353971,
      "learning_rate": 9.179994777804676e-07,
      "loss": 0.0006,
      "step": 39960
    },
    {
      "epoch": 7267.272727272727,
      "grad_norm": 0.006528228521347046,
      "learning_rate": 9.179356187079665e-07,
      "loss": 0.0012,
      "step": 39970
    },
    {
      "epoch": 7269.090909090909,
      "grad_norm": 0.00524677149951458,
      "learning_rate": 9.17871737002493e-07,
      "loss": 0.0012,
      "step": 39980
    },
    {
      "epoch": 7270.909090909091,
      "grad_norm": 0.0019307670881971717,
      "learning_rate": 9.178078326675067e-07,
      "loss": 0.0011,
      "step": 39990
    },
    {
      "epoch": 7272.727272727273,
      "grad_norm": 0.20625747740268707,
      "learning_rate": 9.177439057064682e-07,
      "loss": 0.0012,
      "step": 40000
    },
    {
      "epoch": 7272.727272727273,
      "eval_loss": 4.636606693267822,
      "eval_runtime": 0.9529,
      "eval_samples_per_second": 10.494,
      "eval_steps_per_second": 5.247,
      "step": 40000
    },
    {
      "epoch": 7274.545454545455,
      "grad_norm": 0.32437893748283386,
      "learning_rate": 9.176799561228394e-07,
      "loss": 0.0013,
      "step": 40010
    },
    {
      "epoch": 7276.363636363636,
      "grad_norm": 0.336478590965271,
      "learning_rate": 9.176159839200837e-07,
      "loss": 0.0012,
      "step": 40020
    },
    {
      "epoch": 7278.181818181818,
      "grad_norm": 0.2595162093639374,
      "learning_rate": 9.175519891016649e-07,
      "loss": 0.0011,
      "step": 40030
    },
    {
      "epoch": 7280.0,
      "grad_norm": 0.004002372268587351,
      "learning_rate": 9.174879716710493e-07,
      "loss": 0.001,
      "step": 40040
    },
    {
      "epoch": 7281.818181818182,
      "grad_norm": 0.2211253046989441,
      "learning_rate": 9.174239316317032e-07,
      "loss": 0.0012,
      "step": 40050
    },
    {
      "epoch": 7283.636363636364,
      "grad_norm": 0.26638007164001465,
      "learning_rate": 9.173598689870949e-07,
      "loss": 0.0008,
      "step": 40060
    },
    {
      "epoch": 7285.454545454545,
      "grad_norm": 0.003922516945749521,
      "learning_rate": 9.172957837406934e-07,
      "loss": 0.0012,
      "step": 40070
    },
    {
      "epoch": 7287.272727272727,
      "grad_norm": 0.38572949171066284,
      "learning_rate": 9.172316758959695e-07,
      "loss": 0.0015,
      "step": 40080
    },
    {
      "epoch": 7289.090909090909,
      "grad_norm": 0.0018064591567963362,
      "learning_rate": 9.171675454563948e-07,
      "loss": 0.0008,
      "step": 40090
    },
    {
      "epoch": 7290.909090909091,
      "grad_norm": 0.19437657296657562,
      "learning_rate": 9.17103392425442e-07,
      "loss": 0.001,
      "step": 40100
    },
    {
      "epoch": 7292.727272727273,
      "grad_norm": 0.005178060382604599,
      "learning_rate": 9.170392168065856e-07,
      "loss": 0.0011,
      "step": 40110
    },
    {
      "epoch": 7294.545454545455,
      "grad_norm": 0.28626373410224915,
      "learning_rate": 9.169750186033008e-07,
      "loss": 0.0012,
      "step": 40120
    },
    {
      "epoch": 7296.363636363636,
      "grad_norm": 0.3669412136077881,
      "learning_rate": 9.169107978190641e-07,
      "loss": 0.0013,
      "step": 40130
    },
    {
      "epoch": 7298.181818181818,
      "grad_norm": 0.013232535682618618,
      "learning_rate": 9.168465544573536e-07,
      "loss": 0.0012,
      "step": 40140
    },
    {
      "epoch": 7300.0,
      "grad_norm": 0.0031600964721292257,
      "learning_rate": 9.167822885216481e-07,
      "loss": 0.0009,
      "step": 40150
    },
    {
      "epoch": 7301.818181818182,
      "grad_norm": 0.003111929865553975,
      "learning_rate": 9.16718000015428e-07,
      "loss": 0.0011,
      "step": 40160
    },
    {
      "epoch": 7303.636363636364,
      "grad_norm": 0.002846331335604191,
      "learning_rate": 9.166536889421748e-07,
      "loss": 0.001,
      "step": 40170
    },
    {
      "epoch": 7305.454545454545,
      "grad_norm": 0.0030024617444723845,
      "learning_rate": 9.165893553053712e-07,
      "loss": 0.0011,
      "step": 40180
    },
    {
      "epoch": 7307.272727272727,
      "grad_norm": 0.0033273273147642612,
      "learning_rate": 9.165249991085011e-07,
      "loss": 0.0012,
      "step": 40190
    },
    {
      "epoch": 7309.090909090909,
      "grad_norm": 0.3038025200366974,
      "learning_rate": 9.164606203550497e-07,
      "loss": 0.0011,
      "step": 40200
    },
    {
      "epoch": 7310.909090909091,
      "grad_norm": 0.3869459629058838,
      "learning_rate": 9.163962190485033e-07,
      "loss": 0.0012,
      "step": 40210
    },
    {
      "epoch": 7312.727272727273,
      "grad_norm": 0.0018499180441722274,
      "learning_rate": 9.163317951923496e-07,
      "loss": 0.001,
      "step": 40220
    },
    {
      "epoch": 7314.545454545455,
      "grad_norm": 0.008073936216533184,
      "learning_rate": 9.162673487900773e-07,
      "loss": 0.0011,
      "step": 40230
    },
    {
      "epoch": 7316.363636363636,
      "grad_norm": 0.0023165661841630936,
      "learning_rate": 9.162028798451766e-07,
      "loss": 0.0011,
      "step": 40240
    },
    {
      "epoch": 7318.181818181818,
      "grad_norm": 0.3862411081790924,
      "learning_rate": 9.161383883611389e-07,
      "loss": 0.0014,
      "step": 40250
    },
    {
      "epoch": 7320.0,
      "grad_norm": 0.003673301776871085,
      "learning_rate": 9.160738743414562e-07,
      "loss": 0.0009,
      "step": 40260
    },
    {
      "epoch": 7321.818181818182,
      "grad_norm": 0.2597779333591461,
      "learning_rate": 9.160093377896225e-07,
      "loss": 0.0011,
      "step": 40270
    },
    {
      "epoch": 7323.636363636364,
      "grad_norm": 0.3166823387145996,
      "learning_rate": 9.159447787091328e-07,
      "loss": 0.001,
      "step": 40280
    },
    {
      "epoch": 7325.454545454545,
      "grad_norm": 0.004307617899030447,
      "learning_rate": 9.158801971034832e-07,
      "loss": 0.0012,
      "step": 40290
    },
    {
      "epoch": 7327.272727272727,
      "grad_norm": 0.2032252699136734,
      "learning_rate": 9.158155929761709e-07,
      "loss": 0.0016,
      "step": 40300
    },
    {
      "epoch": 7329.090909090909,
      "grad_norm": 0.002558769891038537,
      "learning_rate": 9.157509663306947e-07,
      "loss": 0.001,
      "step": 40310
    },
    {
      "epoch": 7330.909090909091,
      "grad_norm": 0.22147148847579956,
      "learning_rate": 9.156863171705542e-07,
      "loss": 0.0012,
      "step": 40320
    },
    {
      "epoch": 7332.727272727273,
      "grad_norm": 0.0025958537589758635,
      "learning_rate": 9.156216454992507e-07,
      "loss": 0.0012,
      "step": 40330
    },
    {
      "epoch": 7334.545454545455,
      "grad_norm": 0.24808160960674286,
      "learning_rate": 9.155569513202861e-07,
      "loss": 0.001,
      "step": 40340
    },
    {
      "epoch": 7336.363636363636,
      "grad_norm": 0.03384966030716896,
      "learning_rate": 9.154922346371641e-07,
      "loss": 0.0015,
      "step": 40350
    },
    {
      "epoch": 7338.181818181818,
      "grad_norm": 0.023277416825294495,
      "learning_rate": 9.154274954533894e-07,
      "loss": 0.0011,
      "step": 40360
    },
    {
      "epoch": 7340.0,
      "grad_norm": 0.003100051311776042,
      "learning_rate": 9.153627337724677e-07,
      "loss": 0.0009,
      "step": 40370
    },
    {
      "epoch": 7341.818181818182,
      "grad_norm": 0.26111194491386414,
      "learning_rate": 9.152979495979063e-07,
      "loss": 0.0012,
      "step": 40380
    },
    {
      "epoch": 7343.636363636364,
      "grad_norm": 0.24158354103565216,
      "learning_rate": 9.152331429332135e-07,
      "loss": 0.0012,
      "step": 40390
    },
    {
      "epoch": 7345.454545454545,
      "grad_norm": 0.00408818619325757,
      "learning_rate": 9.151683137818987e-07,
      "loss": 0.0013,
      "step": 40400
    },
    {
      "epoch": 7347.272727272727,
      "grad_norm": 0.25567612051963806,
      "learning_rate": 9.151034621474729e-07,
      "loss": 0.001,
      "step": 40410
    },
    {
      "epoch": 7349.090909090909,
      "grad_norm": 0.0025270625483244658,
      "learning_rate": 9.150385880334479e-07,
      "loss": 0.0011,
      "step": 40420
    },
    {
      "epoch": 7350.909090909091,
      "grad_norm": 0.004738575778901577,
      "learning_rate": 9.149736914433371e-07,
      "loss": 0.0011,
      "step": 40430
    },
    {
      "epoch": 7352.727272727273,
      "grad_norm": 0.002312970580533147,
      "learning_rate": 9.149087723806548e-07,
      "loss": 0.0014,
      "step": 40440
    },
    {
      "epoch": 7354.545454545455,
      "grad_norm": 0.26310208439826965,
      "learning_rate": 9.148438308489167e-07,
      "loss": 0.0011,
      "step": 40450
    },
    {
      "epoch": 7356.363636363636,
      "grad_norm": 0.008197959512472153,
      "learning_rate": 9.147788668516395e-07,
      "loss": 0.001,
      "step": 40460
    },
    {
      "epoch": 7358.181818181818,
      "grad_norm": 0.317676842212677,
      "learning_rate": 9.147138803923416e-07,
      "loss": 0.0013,
      "step": 40470
    },
    {
      "epoch": 7360.0,
      "grad_norm": 0.28296589851379395,
      "learning_rate": 9.146488714745419e-07,
      "loss": 0.0011,
      "step": 40480
    },
    {
      "epoch": 7361.818181818182,
      "grad_norm": 0.2634846866130829,
      "learning_rate": 9.145838401017609e-07,
      "loss": 0.0012,
      "step": 40490
    },
    {
      "epoch": 7363.636363636364,
      "grad_norm": 0.030657168477773666,
      "learning_rate": 9.145187862775208e-07,
      "loss": 0.001,
      "step": 40500
    },
    {
      "epoch": 7363.636363636364,
      "eval_loss": 4.612434387207031,
      "eval_runtime": 0.9509,
      "eval_samples_per_second": 10.516,
      "eval_steps_per_second": 5.258,
      "step": 40500
    },
    {
      "epoch": 7365.454545454545,
      "grad_norm": 0.024695921689271927,
      "learning_rate": 9.144537100053442e-07,
      "loss": 0.0013,
      "step": 40510
    },
    {
      "epoch": 7367.272727272727,
      "grad_norm": 0.3289276361465454,
      "learning_rate": 9.143886112887552e-07,
      "loss": 0.001,
      "step": 40520
    },
    {
      "epoch": 7369.090909090909,
      "grad_norm": 0.22721797227859497,
      "learning_rate": 9.143234901312793e-07,
      "loss": 0.0011,
      "step": 40530
    },
    {
      "epoch": 7370.909090909091,
      "grad_norm": 0.28165102005004883,
      "learning_rate": 9.142583465364431e-07,
      "loss": 0.001,
      "step": 40540
    },
    {
      "epoch": 7372.727272727273,
      "grad_norm": 0.004439163953065872,
      "learning_rate": 9.141931805077742e-07,
      "loss": 0.0011,
      "step": 40550
    },
    {
      "epoch": 7374.545454545455,
      "grad_norm": 0.0033231761772185564,
      "learning_rate": 9.141279920488021e-07,
      "loss": 0.0013,
      "step": 40560
    },
    {
      "epoch": 7376.363636363636,
      "grad_norm": 0.23806579411029816,
      "learning_rate": 9.140627811630563e-07,
      "loss": 0.0012,
      "step": 40570
    },
    {
      "epoch": 7378.181818181818,
      "grad_norm": 0.001666997093707323,
      "learning_rate": 9.139975478540689e-07,
      "loss": 0.001,
      "step": 40580
    },
    {
      "epoch": 7380.0,
      "grad_norm": 0.2832827866077423,
      "learning_rate": 9.139322921253723e-07,
      "loss": 0.0012,
      "step": 40590
    },
    {
      "epoch": 7381.818181818182,
      "grad_norm": 0.0027599111199378967,
      "learning_rate": 9.138670139805003e-07,
      "loss": 0.0011,
      "step": 40600
    },
    {
      "epoch": 7383.636363636364,
      "grad_norm": 0.0052336822263896465,
      "learning_rate": 9.13801713422988e-07,
      "loss": 0.001,
      "step": 40610
    },
    {
      "epoch": 7385.454545454545,
      "grad_norm": 0.001875675399787724,
      "learning_rate": 9.137363904563718e-07,
      "loss": 0.0013,
      "step": 40620
    },
    {
      "epoch": 7387.272727272727,
      "grad_norm": 0.003239258425310254,
      "learning_rate": 9.136710450841893e-07,
      "loss": 0.0013,
      "step": 40630
    },
    {
      "epoch": 7389.090909090909,
      "grad_norm": 0.0029426252003759146,
      "learning_rate": 9.13605677309979e-07,
      "loss": 0.0011,
      "step": 40640
    },
    {
      "epoch": 7390.909090909091,
      "grad_norm": 0.26268529891967773,
      "learning_rate": 9.135402871372808e-07,
      "loss": 0.0011,
      "step": 40650
    },
    {
      "epoch": 7392.727272727273,
      "grad_norm": 0.24787035584449768,
      "learning_rate": 9.134748745696362e-07,
      "loss": 0.001,
      "step": 40660
    },
    {
      "epoch": 7394.545454545455,
      "grad_norm": 0.0024357647635042667,
      "learning_rate": 9.134094396105872e-07,
      "loss": 0.0014,
      "step": 40670
    },
    {
      "epoch": 7396.363636363636,
      "grad_norm": 0.004122949671000242,
      "learning_rate": 9.133439822636777e-07,
      "loss": 0.0008,
      "step": 40680
    },
    {
      "epoch": 7398.181818181818,
      "grad_norm": 0.28310486674308777,
      "learning_rate": 9.132785025324522e-07,
      "loss": 0.0014,
      "step": 40690
    },
    {
      "epoch": 7400.0,
      "grad_norm": 0.0021047876216471195,
      "learning_rate": 9.132130004204568e-07,
      "loss": 0.0011,
      "step": 40700
    },
    {
      "epoch": 7401.818181818182,
      "grad_norm": 0.3415592610836029,
      "learning_rate": 9.131474759312389e-07,
      "loss": 0.0012,
      "step": 40710
    },
    {
      "epoch": 7403.636363636364,
      "grad_norm": 0.20563821494579315,
      "learning_rate": 9.130819290683466e-07,
      "loss": 0.0009,
      "step": 40720
    },
    {
      "epoch": 7405.454545454545,
      "grad_norm": 0.011829162016510963,
      "learning_rate": 9.130163598353297e-07,
      "loss": 0.0013,
      "step": 40730
    },
    {
      "epoch": 7407.272727272727,
      "grad_norm": 0.28353115916252136,
      "learning_rate": 9.129507682357392e-07,
      "loss": 0.0012,
      "step": 40740
    },
    {
      "epoch": 7409.090909090909,
      "grad_norm": 0.3596044182777405,
      "learning_rate": 9.128851542731271e-07,
      "loss": 0.0009,
      "step": 40750
    },
    {
      "epoch": 7410.909090909091,
      "grad_norm": 0.2782319486141205,
      "learning_rate": 9.128195179510464e-07,
      "loss": 0.001,
      "step": 40760
    },
    {
      "epoch": 7412.727272727273,
      "grad_norm": 0.0036005345173180103,
      "learning_rate": 9.127538592730519e-07,
      "loss": 0.0013,
      "step": 40770
    },
    {
      "epoch": 7414.545454545455,
      "grad_norm": 0.00676347641274333,
      "learning_rate": 9.126881782426989e-07,
      "loss": 0.0011,
      "step": 40780
    },
    {
      "epoch": 7416.363636363636,
      "grad_norm": 0.0036288888659328222,
      "learning_rate": 9.12622474863545e-07,
      "loss": 0.0012,
      "step": 40790
    },
    {
      "epoch": 7418.181818181818,
      "grad_norm": 0.002982674865052104,
      "learning_rate": 9.125567491391475e-07,
      "loss": 0.0011,
      "step": 40800
    },
    {
      "epoch": 7420.0,
      "grad_norm": 0.29943642020225525,
      "learning_rate": 9.124910010730664e-07,
      "loss": 0.0012,
      "step": 40810
    },
    {
      "epoch": 7421.818181818182,
      "grad_norm": 0.2149246782064438,
      "learning_rate": 9.124252306688617e-07,
      "loss": 0.0012,
      "step": 40820
    },
    {
      "epoch": 7423.636363636364,
      "grad_norm": 0.2713630199432373,
      "learning_rate": 9.123594379300955e-07,
      "loss": 0.0009,
      "step": 40830
    },
    {
      "epoch": 7425.454545454545,
      "grad_norm": 0.27039992809295654,
      "learning_rate": 9.122936228603306e-07,
      "loss": 0.0013,
      "step": 40840
    },
    {
      "epoch": 7427.272727272727,
      "grad_norm": 0.2849881052970886,
      "learning_rate": 9.122277854631313e-07,
      "loss": 0.0011,
      "step": 40850
    },
    {
      "epoch": 7429.090909090909,
      "grad_norm": 0.0031625921837985516,
      "learning_rate": 9.121619257420629e-07,
      "loss": 0.0011,
      "step": 40860
    },
    {
      "epoch": 7430.909090909091,
      "grad_norm": 0.042722102254629135,
      "learning_rate": 9.12096043700692e-07,
      "loss": 0.0012,
      "step": 40870
    },
    {
      "epoch": 7432.727272727273,
      "grad_norm": 0.029092857614159584,
      "learning_rate": 9.120301393425862e-07,
      "loss": 0.0012,
      "step": 40880
    },
    {
      "epoch": 7434.545454545455,
      "grad_norm": 0.0018065163167193532,
      "learning_rate": 9.119642126713146e-07,
      "loss": 0.001,
      "step": 40890
    },
    {
      "epoch": 7436.363636363636,
      "grad_norm": 0.0022471665870398283,
      "learning_rate": 9.118982636904475e-07,
      "loss": 0.0009,
      "step": 40900
    },
    {
      "epoch": 7438.181818181818,
      "grad_norm": 0.003035501344129443,
      "learning_rate": 9.118322924035564e-07,
      "loss": 0.0012,
      "step": 40910
    },
    {
      "epoch": 7440.0,
      "grad_norm": 0.20943933725357056,
      "learning_rate": 9.117662988142136e-07,
      "loss": 0.0012,
      "step": 40920
    },
    {
      "epoch": 7441.818181818182,
      "grad_norm": 0.01204132940620184,
      "learning_rate": 9.117002829259933e-07,
      "loss": 0.0009,
      "step": 40930
    },
    {
      "epoch": 7443.636363636364,
      "grad_norm": 0.0026395830791443586,
      "learning_rate": 9.116342447424703e-07,
      "loss": 0.0012,
      "step": 40940
    },
    {
      "epoch": 7445.454545454545,
      "grad_norm": 0.20312874019145966,
      "learning_rate": 9.11568184267221e-07,
      "loss": 0.001,
      "step": 40950
    },
    {
      "epoch": 7447.272727272727,
      "grad_norm": 0.023304244503378868,
      "learning_rate": 9.115021015038226e-07,
      "loss": 0.0015,
      "step": 40960
    },
    {
      "epoch": 7449.090909090909,
      "grad_norm": 0.2549586892127991,
      "learning_rate": 9.11435996455854e-07,
      "loss": 0.0009,
      "step": 40970
    },
    {
      "epoch": 7450.909090909091,
      "grad_norm": 0.2566375136375427,
      "learning_rate": 9.11369869126895e-07,
      "loss": 0.0011,
      "step": 40980
    },
    {
      "epoch": 7452.727272727273,
      "grad_norm": 0.24015094339847565,
      "learning_rate": 9.113037195205265e-07,
      "loss": 0.0012,
      "step": 40990
    },
    {
      "epoch": 7454.545454545455,
      "grad_norm": 0.34054043889045715,
      "learning_rate": 9.112375476403311e-07,
      "loss": 0.0013,
      "step": 41000
    },
    {
      "epoch": 7454.545454545455,
      "eval_loss": 4.660868167877197,
      "eval_runtime": 0.9505,
      "eval_samples_per_second": 10.521,
      "eval_steps_per_second": 5.26,
      "step": 41000
    },
    {
      "epoch": 7456.363636363636,
      "grad_norm": 0.0027267627883702517,
      "learning_rate": 9.111713534898922e-07,
      "loss": 0.0009,
      "step": 41010
    },
    {
      "epoch": 7458.181818181818,
      "grad_norm": 0.19918619096279144,
      "learning_rate": 9.111051370727943e-07,
      "loss": 0.0013,
      "step": 41020
    },
    {
      "epoch": 7460.0,
      "grad_norm": 0.2220386564731598,
      "learning_rate": 9.110388983926233e-07,
      "loss": 0.0011,
      "step": 41030
    },
    {
      "epoch": 7461.818181818182,
      "grad_norm": 0.2875801920890808,
      "learning_rate": 9.109726374529665e-07,
      "loss": 0.0012,
      "step": 41040
    },
    {
      "epoch": 7463.636363636364,
      "grad_norm": 0.31301185488700867,
      "learning_rate": 9.109063542574123e-07,
      "loss": 0.0011,
      "step": 41050
    },
    {
      "epoch": 7465.454545454545,
      "grad_norm": 0.20898309350013733,
      "learning_rate": 9.108400488095498e-07,
      "loss": 0.0011,
      "step": 41060
    },
    {
      "epoch": 7467.272727272727,
      "grad_norm": 0.3398585915565491,
      "learning_rate": 9.1077372111297e-07,
      "loss": 0.0012,
      "step": 41070
    },
    {
      "epoch": 7469.090909090909,
      "grad_norm": 0.20943017303943634,
      "learning_rate": 9.107073711712648e-07,
      "loss": 0.0011,
      "step": 41080
    },
    {
      "epoch": 7470.909090909091,
      "grad_norm": 0.4117429554462433,
      "learning_rate": 9.106409989880274e-07,
      "loss": 0.0012,
      "step": 41090
    },
    {
      "epoch": 7472.727272727273,
      "grad_norm": 0.24820299446582794,
      "learning_rate": 9.10574604566852e-07,
      "loss": 0.0013,
      "step": 41100
    },
    {
      "epoch": 7474.545454545455,
      "grad_norm": 0.37951260805130005,
      "learning_rate": 9.105081879113341e-07,
      "loss": 0.0011,
      "step": 41110
    },
    {
      "epoch": 7476.363636363636,
      "grad_norm": 0.2784768044948578,
      "learning_rate": 9.104417490250706e-07,
      "loss": 0.0011,
      "step": 41120
    },
    {
      "epoch": 7478.181818181818,
      "grad_norm": 0.2618632912635803,
      "learning_rate": 9.103752879116594e-07,
      "loss": 0.0013,
      "step": 41130
    },
    {
      "epoch": 7480.0,
      "grad_norm": 0.003091176738962531,
      "learning_rate": 9.103088045746996e-07,
      "loss": 0.0011,
      "step": 41140
    },
    {
      "epoch": 7481.818181818182,
      "grad_norm": 0.2642664611339569,
      "learning_rate": 9.102422990177915e-07,
      "loss": 0.0012,
      "step": 41150
    },
    {
      "epoch": 7483.636363636364,
      "grad_norm": 0.0016027772799134254,
      "learning_rate": 9.101757712445368e-07,
      "loss": 0.0011,
      "step": 41160
    },
    {
      "epoch": 7485.454545454545,
      "grad_norm": 0.001718160230666399,
      "learning_rate": 9.101092212585381e-07,
      "loss": 0.0013,
      "step": 41170
    },
    {
      "epoch": 7487.272727272727,
      "grad_norm": 0.0027271180879324675,
      "learning_rate": 9.100426490633995e-07,
      "loss": 0.0008,
      "step": 41180
    },
    {
      "epoch": 7489.090909090909,
      "grad_norm": 0.18275685608386993,
      "learning_rate": 9.099760546627261e-07,
      "loss": 0.0013,
      "step": 41190
    },
    {
      "epoch": 7490.909090909091,
      "grad_norm": 0.0030665798112750053,
      "learning_rate": 9.099094380601243e-07,
      "loss": 0.0011,
      "step": 41200
    },
    {
      "epoch": 7492.727272727273,
      "grad_norm": 0.0027084830217063427,
      "learning_rate": 9.098427992592016e-07,
      "loss": 0.001,
      "step": 41210
    },
    {
      "epoch": 7494.545454545455,
      "grad_norm": 0.0020430749282240868,
      "learning_rate": 9.097761382635669e-07,
      "loss": 0.0008,
      "step": 41220
    },
    {
      "epoch": 7496.363636363636,
      "grad_norm": 0.2382282167673111,
      "learning_rate": 9.097094550768301e-07,
      "loss": 0.0017,
      "step": 41230
    },
    {
      "epoch": 7498.181818181818,
      "grad_norm": 0.34011712670326233,
      "learning_rate": 9.096427497026023e-07,
      "loss": 0.0013,
      "step": 41240
    },
    {
      "epoch": 7500.0,
      "grad_norm": 0.002285238355398178,
      "learning_rate": 9.095760221444959e-07,
      "loss": 0.0009,
      "step": 41250
    },
    {
      "epoch": 7501.818181818182,
      "grad_norm": 0.04618261381983757,
      "learning_rate": 9.095092724061247e-07,
      "loss": 0.0012,
      "step": 41260
    },
    {
      "epoch": 7503.636363636364,
      "grad_norm": 0.23449821770191193,
      "learning_rate": 9.094425004911031e-07,
      "loss": 0.0012,
      "step": 41270
    },
    {
      "epoch": 7505.454545454545,
      "grad_norm": 0.27855315804481506,
      "learning_rate": 9.093757064030472e-07,
      "loss": 0.0009,
      "step": 41280
    },
    {
      "epoch": 7507.272727272727,
      "grad_norm": 0.24509742856025696,
      "learning_rate": 9.093088901455744e-07,
      "loss": 0.0011,
      "step": 41290
    },
    {
      "epoch": 7509.090909090909,
      "grad_norm": 0.0019822404719889164,
      "learning_rate": 9.09242051722303e-07,
      "loss": 0.0011,
      "step": 41300
    },
    {
      "epoch": 7510.909090909091,
      "grad_norm": 0.4202369153499603,
      "learning_rate": 9.091751911368524e-07,
      "loss": 0.0012,
      "step": 41310
    },
    {
      "epoch": 7512.727272727273,
      "grad_norm": 0.006283302791416645,
      "learning_rate": 9.091083083928437e-07,
      "loss": 0.0011,
      "step": 41320
    },
    {
      "epoch": 7514.545454545455,
      "grad_norm": 0.2394091933965683,
      "learning_rate": 9.090414034938985e-07,
      "loss": 0.0009,
      "step": 41330
    },
    {
      "epoch": 7516.363636363636,
      "grad_norm": 0.24572499096393585,
      "learning_rate": 9.089744764436402e-07,
      "loss": 0.0014,
      "step": 41340
    },
    {
      "epoch": 7518.181818181818,
      "grad_norm": 0.029236745089292526,
      "learning_rate": 9.089075272456932e-07,
      "loss": 0.0012,
      "step": 41350
    },
    {
      "epoch": 7520.0,
      "grad_norm": 0.29959115386009216,
      "learning_rate": 9.088405559036831e-07,
      "loss": 0.001,
      "step": 41360
    },
    {
      "epoch": 7521.818181818182,
      "grad_norm": 0.2344917356967926,
      "learning_rate": 9.087735624212365e-07,
      "loss": 0.0013,
      "step": 41370
    },
    {
      "epoch": 7523.636363636364,
      "grad_norm": 0.2426079362630844,
      "learning_rate": 9.087065468019815e-07,
      "loss": 0.0008,
      "step": 41380
    },
    {
      "epoch": 7525.454545454545,
      "grad_norm": 0.014026465825736523,
      "learning_rate": 9.086395090495473e-07,
      "loss": 0.0014,
      "step": 41390
    },
    {
      "epoch": 7527.272727272727,
      "grad_norm": 0.3031691014766693,
      "learning_rate": 9.085724491675642e-07,
      "loss": 0.0011,
      "step": 41400
    },
    {
      "epoch": 7529.090909090909,
      "grad_norm": 0.0022160359658300877,
      "learning_rate": 9.085053671596639e-07,
      "loss": 0.0011,
      "step": 41410
    },
    {
      "epoch": 7530.909090909091,
      "grad_norm": 0.0031399910803884268,
      "learning_rate": 9.08438263029479e-07,
      "loss": 0.0012,
      "step": 41420
    },
    {
      "epoch": 7532.727272727273,
      "grad_norm": 0.21206034719944,
      "learning_rate": 9.083711367806437e-07,
      "loss": 0.0012,
      "step": 41430
    },
    {
      "epoch": 7534.545454545455,
      "grad_norm": 0.0063436576165258884,
      "learning_rate": 9.083039884167928e-07,
      "loss": 0.001,
      "step": 41440
    },
    {
      "epoch": 7536.363636363636,
      "grad_norm": 0.0020421850495040417,
      "learning_rate": 9.082368179415632e-07,
      "loss": 0.0011,
      "step": 41450
    },
    {
      "epoch": 7538.181818181818,
      "grad_norm": 0.25159594416618347,
      "learning_rate": 9.08169625358592e-07,
      "loss": 0.0012,
      "step": 41460
    },
    {
      "epoch": 7540.0,
      "grad_norm": 0.18974071741104126,
      "learning_rate": 9.081024106715181e-07,
      "loss": 0.0012,
      "step": 41470
    },
    {
      "epoch": 7541.818181818182,
      "grad_norm": 0.0013570054434239864,
      "learning_rate": 9.080351738839815e-07,
      "loss": 0.0013,
      "step": 41480
    },
    {
      "epoch": 7543.636363636364,
      "grad_norm": 0.004150771535933018,
      "learning_rate": 9.079679149996233e-07,
      "loss": 0.0012,
      "step": 41490
    },
    {
      "epoch": 7545.454545454545,
      "grad_norm": 0.0023284938652068377,
      "learning_rate": 9.079006340220861e-07,
      "loss": 0.0008,
      "step": 41500
    },
    {
      "epoch": 7545.454545454545,
      "eval_loss": 4.692131996154785,
      "eval_runtime": 0.9504,
      "eval_samples_per_second": 10.522,
      "eval_steps_per_second": 5.261,
      "step": 41500
    },
    {
      "epoch": 7547.272727272727,
      "grad_norm": 0.017871545627713203,
      "learning_rate": 9.07833330955013e-07,
      "loss": 0.0013,
      "step": 41510
    },
    {
      "epoch": 7549.090909090909,
      "grad_norm": 0.00351396924816072,
      "learning_rate": 9.077660058020491e-07,
      "loss": 0.0009,
      "step": 41520
    },
    {
      "epoch": 7550.909090909091,
      "grad_norm": 0.002503175288438797,
      "learning_rate": 9.076986585668402e-07,
      "loss": 0.0012,
      "step": 41530
    },
    {
      "epoch": 7552.727272727273,
      "grad_norm": 0.0026059674564749002,
      "learning_rate": 9.076312892530333e-07,
      "loss": 0.0009,
      "step": 41540
    },
    {
      "epoch": 7554.545454545455,
      "grad_norm": 0.0019901408813893795,
      "learning_rate": 9.07563897864277e-07,
      "loss": 0.0011,
      "step": 41550
    },
    {
      "epoch": 7556.363636363636,
      "grad_norm": 0.20133128762245178,
      "learning_rate": 9.074964844042208e-07,
      "loss": 0.0014,
      "step": 41560
    },
    {
      "epoch": 7558.181818181818,
      "grad_norm": 0.0034412452951073647,
      "learning_rate": 9.074290488765153e-07,
      "loss": 0.0009,
      "step": 41570
    },
    {
      "epoch": 7560.0,
      "grad_norm": 0.31640493869781494,
      "learning_rate": 9.073615912848125e-07,
      "loss": 0.0013,
      "step": 41580
    },
    {
      "epoch": 7561.818181818182,
      "grad_norm": 0.0035170826595276594,
      "learning_rate": 9.072941116327654e-07,
      "loss": 0.0012,
      "step": 41590
    },
    {
      "epoch": 7563.636363636364,
      "grad_norm": 0.006429826375097036,
      "learning_rate": 9.072266099240285e-07,
      "loss": 0.0011,
      "step": 41600
    },
    {
      "epoch": 7565.454545454545,
      "grad_norm": 0.2272917628288269,
      "learning_rate": 9.071590861622569e-07,
      "loss": 0.0011,
      "step": 41610
    },
    {
      "epoch": 7567.272727272727,
      "grad_norm": 0.0056472462601959705,
      "learning_rate": 9.070915403511078e-07,
      "loss": 0.0009,
      "step": 41620
    },
    {
      "epoch": 7569.090909090909,
      "grad_norm": 0.38597846031188965,
      "learning_rate": 9.070239724942387e-07,
      "loss": 0.0013,
      "step": 41630
    },
    {
      "epoch": 7570.909090909091,
      "grad_norm": 0.002740567782893777,
      "learning_rate": 9.069563825953091e-07,
      "loss": 0.0011,
      "step": 41640
    },
    {
      "epoch": 7572.727272727273,
      "grad_norm": 0.21718139946460724,
      "learning_rate": 9.068887706579789e-07,
      "loss": 0.0014,
      "step": 41650
    },
    {
      "epoch": 7574.545454545455,
      "grad_norm": 0.0028765955939888954,
      "learning_rate": 9.068211366859096e-07,
      "loss": 0.0007,
      "step": 41660
    },
    {
      "epoch": 7576.363636363636,
      "grad_norm": 0.21374833583831787,
      "learning_rate": 9.067534806827639e-07,
      "loss": 0.0012,
      "step": 41670
    },
    {
      "epoch": 7578.181818181818,
      "grad_norm": 0.2652657628059387,
      "learning_rate": 9.066858026522059e-07,
      "loss": 0.0013,
      "step": 41680
    },
    {
      "epoch": 7580.0,
      "grad_norm": 0.3579629361629486,
      "learning_rate": 9.066181025979005e-07,
      "loss": 0.0011,
      "step": 41690
    },
    {
      "epoch": 7581.818181818182,
      "grad_norm": 0.22443364560604095,
      "learning_rate": 9.065503805235137e-07,
      "loss": 0.0012,
      "step": 41700
    },
    {
      "epoch": 7583.636363636364,
      "grad_norm": 0.034111250191926956,
      "learning_rate": 9.064826364327134e-07,
      "loss": 0.0011,
      "step": 41710
    },
    {
      "epoch": 7585.454545454545,
      "grad_norm": 0.004012828692793846,
      "learning_rate": 9.064148703291679e-07,
      "loss": 0.0014,
      "step": 41720
    },
    {
      "epoch": 7587.272727272727,
      "grad_norm": 0.0029394228477030993,
      "learning_rate": 9.063470822165468e-07,
      "loss": 0.0006,
      "step": 41730
    },
    {
      "epoch": 7589.090909090909,
      "grad_norm": 0.23808683454990387,
      "learning_rate": 9.062792720985218e-07,
      "loss": 0.0013,
      "step": 41740
    },
    {
      "epoch": 7590.909090909091,
      "grad_norm": 0.003756827674806118,
      "learning_rate": 9.062114399787646e-07,
      "loss": 0.0011,
      "step": 41750
    },
    {
      "epoch": 7592.727272727273,
      "grad_norm": 0.0025466580409556627,
      "learning_rate": 9.061435858609486e-07,
      "loss": 0.0014,
      "step": 41760
    },
    {
      "epoch": 7594.545454545455,
      "grad_norm": 0.0027929567731916904,
      "learning_rate": 9.060757097487485e-07,
      "loss": 0.0007,
      "step": 41770
    },
    {
      "epoch": 7596.363636363636,
      "grad_norm": 0.003960906527936459,
      "learning_rate": 9.0600781164584e-07,
      "loss": 0.0016,
      "step": 41780
    },
    {
      "epoch": 7598.181818181818,
      "grad_norm": 0.008503096178174019,
      "learning_rate": 9.059398915559005e-07,
      "loss": 0.0008,
      "step": 41790
    },
    {
      "epoch": 7600.0,
      "grad_norm": 0.0034147915430366993,
      "learning_rate": 9.058719494826074e-07,
      "loss": 0.0013,
      "step": 41800
    },
    {
      "epoch": 7601.818181818182,
      "grad_norm": 0.22770468890666962,
      "learning_rate": 9.058039854296407e-07,
      "loss": 0.001,
      "step": 41810
    },
    {
      "epoch": 7603.636363636364,
      "grad_norm": 0.29999062418937683,
      "learning_rate": 9.057359994006805e-07,
      "loss": 0.0011,
      "step": 41820
    },
    {
      "epoch": 7605.454545454545,
      "grad_norm": 0.2661283612251282,
      "learning_rate": 9.056679913994088e-07,
      "loss": 0.0014,
      "step": 41830
    },
    {
      "epoch": 7607.272727272727,
      "grad_norm": 0.24558740854263306,
      "learning_rate": 9.055999614295082e-07,
      "loss": 0.0011,
      "step": 41840
    },
    {
      "epoch": 7609.090909090909,
      "grad_norm": 0.0024223567452281713,
      "learning_rate": 9.055319094946633e-07,
      "loss": 0.0009,
      "step": 41850
    },
    {
      "epoch": 7610.909090909091,
      "grad_norm": 0.2852294147014618,
      "learning_rate": 9.054638355985591e-07,
      "loss": 0.0013,
      "step": 41860
    },
    {
      "epoch": 7612.727272727273,
      "grad_norm": 0.20069120824337006,
      "learning_rate": 9.05395739744882e-07,
      "loss": 0.0012,
      "step": 41870
    },
    {
      "epoch": 7614.545454545455,
      "grad_norm": 0.00575177650898695,
      "learning_rate": 9.0532762193732e-07,
      "loss": 0.0009,
      "step": 41880
    },
    {
      "epoch": 7616.363636363636,
      "grad_norm": 0.30215924978256226,
      "learning_rate": 9.052594821795616e-07,
      "loss": 0.0013,
      "step": 41890
    },
    {
      "epoch": 7618.181818181818,
      "grad_norm": 0.2163335233926773,
      "learning_rate": 9.051913204752971e-07,
      "loss": 0.0012,
      "step": 41900
    },
    {
      "epoch": 7620.0,
      "grad_norm": 0.2671792507171631,
      "learning_rate": 9.051231368282176e-07,
      "loss": 0.0011,
      "step": 41910
    },
    {
      "epoch": 7621.818181818182,
      "grad_norm": 0.28324389457702637,
      "learning_rate": 9.050549312420158e-07,
      "loss": 0.0013,
      "step": 41920
    },
    {
      "epoch": 7623.636363636364,
      "grad_norm": 0.0019545017275959253,
      "learning_rate": 9.049867037203849e-07,
      "loss": 0.0011,
      "step": 41930
    },
    {
      "epoch": 7625.454545454545,
      "grad_norm": 0.31720972061157227,
      "learning_rate": 9.049184542670199e-07,
      "loss": 0.0011,
      "step": 41940
    },
    {
      "epoch": 7627.272727272727,
      "grad_norm": 0.29537084698677063,
      "learning_rate": 9.04850182885617e-07,
      "loss": 0.0011,
      "step": 41950
    },
    {
      "epoch": 7629.090909090909,
      "grad_norm": 0.0017273875419050455,
      "learning_rate": 9.04781889579873e-07,
      "loss": 0.0011,
      "step": 41960
    },
    {
      "epoch": 7630.909090909091,
      "grad_norm": 0.35520225763320923,
      "learning_rate": 9.047135743534866e-07,
      "loss": 0.001,
      "step": 41970
    },
    {
      "epoch": 7632.727272727273,
      "grad_norm": 0.2174411565065384,
      "learning_rate": 9.046452372101572e-07,
      "loss": 0.0012,
      "step": 41980
    },
    {
      "epoch": 7634.545454545455,
      "grad_norm": 0.26697349548339844,
      "learning_rate": 9.045768781535857e-07,
      "loss": 0.0014,
      "step": 41990
    },
    {
      "epoch": 7636.363636363636,
      "grad_norm": 0.2652929425239563,
      "learning_rate": 9.045084971874737e-07,
      "loss": 0.001,
      "step": 42000
    },
    {
      "epoch": 7636.363636363636,
      "eval_loss": 4.675431251525879,
      "eval_runtime": 0.9496,
      "eval_samples_per_second": 10.53,
      "eval_steps_per_second": 5.265,
      "step": 42000
    },
    {
      "epoch": 7638.181818181818,
      "grad_norm": 0.21125681698322296,
      "learning_rate": 9.044400943155246e-07,
      "loss": 0.0011,
      "step": 42010
    },
    {
      "epoch": 7640.0,
      "grad_norm": 0.34705236554145813,
      "learning_rate": 9.043716695414426e-07,
      "loss": 0.0011,
      "step": 42020
    },
    {
      "epoch": 7641.818181818182,
      "grad_norm": 0.2117771953344345,
      "learning_rate": 9.043032228689332e-07,
      "loss": 0.0011,
      "step": 42030
    },
    {
      "epoch": 7643.636363636364,
      "grad_norm": 0.1970592439174652,
      "learning_rate": 9.042347543017031e-07,
      "loss": 0.0012,
      "step": 42040
    },
    {
      "epoch": 7645.454545454545,
      "grad_norm": 0.21092216670513153,
      "learning_rate": 9.041662638434601e-07,
      "loss": 0.0011,
      "step": 42050
    },
    {
      "epoch": 7647.272727272727,
      "grad_norm": 0.002555722836405039,
      "learning_rate": 9.040977514979135e-07,
      "loss": 0.0009,
      "step": 42060
    },
    {
      "epoch": 7649.090909090909,
      "grad_norm": 0.004214670974761248,
      "learning_rate": 9.040292172687731e-07,
      "loss": 0.0012,
      "step": 42070
    },
    {
      "epoch": 7650.909090909091,
      "grad_norm": 0.27900588512420654,
      "learning_rate": 9.039606611597508e-07,
      "loss": 0.0013,
      "step": 42080
    },
    {
      "epoch": 7652.727272727273,
      "grad_norm": 0.2314973771572113,
      "learning_rate": 9.038920831745587e-07,
      "loss": 0.001,
      "step": 42090
    },
    {
      "epoch": 7654.545454545455,
      "grad_norm": 0.0021051145158708096,
      "learning_rate": 9.038234833169109e-07,
      "loss": 0.0012,
      "step": 42100
    },
    {
      "epoch": 7656.363636363636,
      "grad_norm": 0.22831086814403534,
      "learning_rate": 9.037548615905223e-07,
      "loss": 0.0011,
      "step": 42110
    },
    {
      "epoch": 7658.181818181818,
      "grad_norm": 0.002565436763688922,
      "learning_rate": 9.036862179991091e-07,
      "loss": 0.0011,
      "step": 42120
    },
    {
      "epoch": 7660.0,
      "grad_norm": 0.002111458219587803,
      "learning_rate": 9.036175525463886e-07,
      "loss": 0.0013,
      "step": 42130
    },
    {
      "epoch": 7661.818181818182,
      "grad_norm": 0.0016216712538152933,
      "learning_rate": 9.035488652360793e-07,
      "loss": 0.0013,
      "step": 42140
    },
    {
      "epoch": 7663.636363636364,
      "grad_norm": 0.00784205924719572,
      "learning_rate": 9.034801560719009e-07,
      "loss": 0.0013,
      "step": 42150
    },
    {
      "epoch": 7665.454545454545,
      "grad_norm": 0.31744900345802307,
      "learning_rate": 9.034114250575744e-07,
      "loss": 0.0012,
      "step": 42160
    },
    {
      "epoch": 7667.272727272727,
      "grad_norm": 0.22419624030590057,
      "learning_rate": 9.033426721968217e-07,
      "loss": 0.0013,
      "step": 42170
    },
    {
      "epoch": 7669.090909090909,
      "grad_norm": 0.001813374925404787,
      "learning_rate": 9.032738974933663e-07,
      "loss": 0.0009,
      "step": 42180
    },
    {
      "epoch": 7670.909090909091,
      "grad_norm": 0.0016928170807659626,
      "learning_rate": 9.032051009509323e-07,
      "loss": 0.001,
      "step": 42190
    },
    {
      "epoch": 7672.727272727273,
      "grad_norm": 0.0029002544470131397,
      "learning_rate": 9.031362825732456e-07,
      "loss": 0.0014,
      "step": 42200
    },
    {
      "epoch": 7674.545454545455,
      "grad_norm": 0.2980504631996155,
      "learning_rate": 9.030674423640329e-07,
      "loss": 0.0009,
      "step": 42210
    },
    {
      "epoch": 7676.363636363636,
      "grad_norm": 0.3591575622558594,
      "learning_rate": 9.029985803270222e-07,
      "loss": 0.0012,
      "step": 42220
    },
    {
      "epoch": 7678.181818181818,
      "grad_norm": 0.35813355445861816,
      "learning_rate": 9.029296964659427e-07,
      "loss": 0.0013,
      "step": 42230
    },
    {
      "epoch": 7680.0,
      "grad_norm": 0.002388377208262682,
      "learning_rate": 9.028607907845246e-07,
      "loss": 0.0009,
      "step": 42240
    },
    {
      "epoch": 7681.818181818182,
      "grad_norm": 0.003549546003341675,
      "learning_rate": 9.027918632864997e-07,
      "loss": 0.0009,
      "step": 42250
    },
    {
      "epoch": 7683.636363636364,
      "grad_norm": 0.29630112648010254,
      "learning_rate": 9.027229139756005e-07,
      "loss": 0.0016,
      "step": 42260
    },
    {
      "epoch": 7685.454545454545,
      "grad_norm": 0.0024051349610090256,
      "learning_rate": 9.026539428555608e-07,
      "loss": 0.0008,
      "step": 42270
    },
    {
      "epoch": 7687.272727272727,
      "grad_norm": 0.365255743265152,
      "learning_rate": 9.025849499301162e-07,
      "loss": 0.0014,
      "step": 42280
    },
    {
      "epoch": 7689.090909090909,
      "grad_norm": 0.0025654106866568327,
      "learning_rate": 9.025159352030023e-07,
      "loss": 0.0009,
      "step": 42290
    },
    {
      "epoch": 7690.909090909091,
      "grad_norm": 0.36921149492263794,
      "learning_rate": 9.02446898677957e-07,
      "loss": 0.0012,
      "step": 42300
    },
    {
      "epoch": 7692.727272727273,
      "grad_norm": 0.0015913266688585281,
      "learning_rate": 9.023778403587188e-07,
      "loss": 0.0011,
      "step": 42310
    },
    {
      "epoch": 7694.545454545455,
      "grad_norm": 0.22633159160614014,
      "learning_rate": 9.023087602490274e-07,
      "loss": 0.0009,
      "step": 42320
    },
    {
      "epoch": 7696.363636363636,
      "grad_norm": 0.003561202436685562,
      "learning_rate": 9.022396583526237e-07,
      "loss": 0.0014,
      "step": 42330
    },
    {
      "epoch": 7698.181818181818,
      "grad_norm": 0.3998261094093323,
      "learning_rate": 9.0217053467325e-07,
      "loss": 0.0012,
      "step": 42340
    },
    {
      "epoch": 7700.0,
      "grad_norm": 0.22981415688991547,
      "learning_rate": 9.021013892146499e-07,
      "loss": 0.001,
      "step": 42350
    },
    {
      "epoch": 7701.818181818182,
      "grad_norm": 0.001968676457181573,
      "learning_rate": 9.020322219805673e-07,
      "loss": 0.0012,
      "step": 42360
    },
    {
      "epoch": 7703.636363636364,
      "grad_norm": 0.19446143507957458,
      "learning_rate": 9.019630329747484e-07,
      "loss": 0.0011,
      "step": 42370
    },
    {
      "epoch": 7705.454545454545,
      "grad_norm": 0.004065609537065029,
      "learning_rate": 9.0189382220094e-07,
      "loss": 0.0009,
      "step": 42380
    },
    {
      "epoch": 7707.272727272727,
      "grad_norm": 0.2104371190071106,
      "learning_rate": 9.018245896628899e-07,
      "loss": 0.0012,
      "step": 42390
    },
    {
      "epoch": 7709.090909090909,
      "grad_norm": 0.37383997440338135,
      "learning_rate": 9.017553353643477e-07,
      "loss": 0.0011,
      "step": 42400
    },
    {
      "epoch": 7710.909090909091,
      "grad_norm": 0.2217031866312027,
      "learning_rate": 9.016860593090634e-07,
      "loss": 0.0012,
      "step": 42410
    },
    {
      "epoch": 7712.727272727273,
      "grad_norm": 0.2065092772245407,
      "learning_rate": 9.01616761500789e-07,
      "loss": 0.0011,
      "step": 42420
    },
    {
      "epoch": 7714.545454545455,
      "grad_norm": 0.0036172005347907543,
      "learning_rate": 9.01547441943277e-07,
      "loss": 0.0009,
      "step": 42430
    },
    {
      "epoch": 7716.363636363636,
      "grad_norm": 0.34807923436164856,
      "learning_rate": 9.014781006402812e-07,
      "loss": 0.0017,
      "step": 42440
    },
    {
      "epoch": 7718.181818181818,
      "grad_norm": 0.004949977155774832,
      "learning_rate": 9.014087375955572e-07,
      "loss": 0.0006,
      "step": 42450
    },
    {
      "epoch": 7720.0,
      "grad_norm": 0.007016065530478954,
      "learning_rate": 9.01339352812861e-07,
      "loss": 0.0012,
      "step": 42460
    },
    {
      "epoch": 7721.818181818182,
      "grad_norm": 0.0024230408016592264,
      "learning_rate": 9.012699462959501e-07,
      "loss": 0.0011,
      "step": 42470
    },
    {
      "epoch": 7723.636363636364,
      "grad_norm": 0.25769683718681335,
      "learning_rate": 9.012005180485833e-07,
      "loss": 0.001,
      "step": 42480
    },
    {
      "epoch": 7725.454545454545,
      "grad_norm": 0.20556406676769257,
      "learning_rate": 9.011310680745201e-07,
      "loss": 0.0011,
      "step": 42490
    },
    {
      "epoch": 7727.272727272727,
      "grad_norm": 0.26973238587379456,
      "learning_rate": 9.010615963775219e-07,
      "loss": 0.0012,
      "step": 42500
    },
    {
      "epoch": 7727.272727272727,
      "eval_loss": 4.630393981933594,
      "eval_runtime": 0.9512,
      "eval_samples_per_second": 10.513,
      "eval_steps_per_second": 5.257,
      "step": 42500
    },
    {
      "epoch": 7729.090909090909,
      "grad_norm": 0.27317649126052856,
      "learning_rate": 9.009921029613505e-07,
      "loss": 0.0013,
      "step": 42510
    },
    {
      "epoch": 7730.909090909091,
      "grad_norm": 0.21315990388393402,
      "learning_rate": 9.009225878297697e-07,
      "loss": 0.0011,
      "step": 42520
    },
    {
      "epoch": 7732.727272727273,
      "grad_norm": 0.02220740169286728,
      "learning_rate": 9.008530509865436e-07,
      "loss": 0.0012,
      "step": 42530
    },
    {
      "epoch": 7734.545454545455,
      "grad_norm": 0.0043081557378172874,
      "learning_rate": 9.007834924354383e-07,
      "loss": 0.0006,
      "step": 42540
    },
    {
      "epoch": 7736.363636363636,
      "grad_norm": 0.003639115020632744,
      "learning_rate": 9.007139121802203e-07,
      "loss": 0.0012,
      "step": 42550
    },
    {
      "epoch": 7738.181818181818,
      "grad_norm": 0.277445524930954,
      "learning_rate": 9.00644310224658e-07,
      "loss": 0.0013,
      "step": 42560
    },
    {
      "epoch": 7740.0,
      "grad_norm": 0.29513248801231384,
      "learning_rate": 9.005746865725206e-07,
      "loss": 0.0011,
      "step": 42570
    },
    {
      "epoch": 7741.818181818182,
      "grad_norm": 0.24653591215610504,
      "learning_rate": 9.005050412275781e-07,
      "loss": 0.0011,
      "step": 42580
    },
    {
      "epoch": 7743.636363636364,
      "grad_norm": 0.0018129232339560986,
      "learning_rate": 9.004353741936026e-07,
      "loss": 0.0014,
      "step": 42590
    },
    {
      "epoch": 7745.454545454545,
      "grad_norm": 0.24237652122974396,
      "learning_rate": 9.003656854743666e-07,
      "loss": 0.0012,
      "step": 42600
    },
    {
      "epoch": 7747.272727272727,
      "grad_norm": 0.004166395403444767,
      "learning_rate": 9.002959750736443e-07,
      "loss": 0.0009,
      "step": 42610
    },
    {
      "epoch": 7749.090909090909,
      "grad_norm": 0.0024015125818550587,
      "learning_rate": 9.002262429952104e-07,
      "loss": 0.0013,
      "step": 42620
    },
    {
      "epoch": 7750.909090909091,
      "grad_norm": 0.0025380714796483517,
      "learning_rate": 9.001564892428415e-07,
      "loss": 0.0012,
      "step": 42630
    },
    {
      "epoch": 7752.727272727273,
      "grad_norm": 0.30007100105285645,
      "learning_rate": 9.000867138203149e-07,
      "loss": 0.0011,
      "step": 42640
    },
    {
      "epoch": 7754.545454545455,
      "grad_norm": 0.2717249393463135,
      "learning_rate": 9.000169167314095e-07,
      "loss": 0.0011,
      "step": 42650
    },
    {
      "epoch": 7756.363636363636,
      "grad_norm": 0.25977617502212524,
      "learning_rate": 8.999470979799047e-07,
      "loss": 0.0011,
      "step": 42660
    },
    {
      "epoch": 7758.181818181818,
      "grad_norm": 0.2756069004535675,
      "learning_rate": 8.998772575695816e-07,
      "loss": 0.0013,
      "step": 42670
    },
    {
      "epoch": 7760.0,
      "grad_norm": 0.22920271754264832,
      "learning_rate": 8.998073955042227e-07,
      "loss": 0.0011,
      "step": 42680
    },
    {
      "epoch": 7761.818181818182,
      "grad_norm": 0.21348965167999268,
      "learning_rate": 8.997375117876109e-07,
      "loss": 0.0012,
      "step": 42690
    },
    {
      "epoch": 7763.636363636364,
      "grad_norm": 0.004802038427442312,
      "learning_rate": 8.996676064235308e-07,
      "loss": 0.0006,
      "step": 42700
    },
    {
      "epoch": 7765.454545454545,
      "grad_norm": 0.006512217689305544,
      "learning_rate": 8.995976794157682e-07,
      "loss": 0.0013,
      "step": 42710
    },
    {
      "epoch": 7767.272727272727,
      "grad_norm": 0.03908548876643181,
      "learning_rate": 8.995277307681098e-07,
      "loss": 0.0014,
      "step": 42720
    },
    {
      "epoch": 7769.090909090909,
      "grad_norm": 0.0025227945297956467,
      "learning_rate": 8.994577604843438e-07,
      "loss": 0.0009,
      "step": 42730
    },
    {
      "epoch": 7770.909090909091,
      "grad_norm": 0.0021010655909776688,
      "learning_rate": 8.993877685682592e-07,
      "loss": 0.0012,
      "step": 42740
    },
    {
      "epoch": 7772.727272727273,
      "grad_norm": 0.24923311173915863,
      "learning_rate": 8.993177550236463e-07,
      "loss": 0.0011,
      "step": 42750
    },
    {
      "epoch": 7774.545454545455,
      "grad_norm": 0.003987159579992294,
      "learning_rate": 8.992477198542969e-07,
      "loss": 0.0012,
      "step": 42760
    },
    {
      "epoch": 7776.363636363636,
      "grad_norm": 0.22505144774913788,
      "learning_rate": 8.991776630640036e-07,
      "loss": 0.0009,
      "step": 42770
    },
    {
      "epoch": 7778.181818181818,
      "grad_norm": 0.002382003702223301,
      "learning_rate": 8.991075846565601e-07,
      "loss": 0.001,
      "step": 42780
    },
    {
      "epoch": 7780.0,
      "grad_norm": 0.0018880947027355433,
      "learning_rate": 8.990374846357616e-07,
      "loss": 0.0012,
      "step": 42790
    },
    {
      "epoch": 7781.818181818182,
      "grad_norm": 0.22993525862693787,
      "learning_rate": 8.989673630054043e-07,
      "loss": 0.0011,
      "step": 42800
    },
    {
      "epoch": 7783.636363636364,
      "grad_norm": 0.2599135935306549,
      "learning_rate": 8.988972197692854e-07,
      "loss": 0.0012,
      "step": 42810
    },
    {
      "epoch": 7785.454545454545,
      "grad_norm": 0.0022767349146306515,
      "learning_rate": 8.988270549312038e-07,
      "loss": 0.0012,
      "step": 42820
    },
    {
      "epoch": 7787.272727272727,
      "grad_norm": 0.2749485671520233,
      "learning_rate": 8.987568684949589e-07,
      "loss": 0.0012,
      "step": 42830
    },
    {
      "epoch": 7789.090909090909,
      "grad_norm": 0.1767662763595581,
      "learning_rate": 8.986866604643517e-07,
      "loss": 0.0011,
      "step": 42840
    },
    {
      "epoch": 7790.909090909091,
      "grad_norm": 0.25401806831359863,
      "learning_rate": 8.986164308431844e-07,
      "loss": 0.0012,
      "step": 42850
    },
    {
      "epoch": 7792.727272727273,
      "grad_norm": 0.30417197942733765,
      "learning_rate": 8.985461796352601e-07,
      "loss": 0.0011,
      "step": 42860
    },
    {
      "epoch": 7794.545454545455,
      "grad_norm": 0.19883045554161072,
      "learning_rate": 8.98475906844383e-07,
      "loss": 0.0013,
      "step": 42870
    },
    {
      "epoch": 7796.363636363636,
      "grad_norm": 0.3378385603427887,
      "learning_rate": 8.984056124743591e-07,
      "loss": 0.0011,
      "step": 42880
    },
    {
      "epoch": 7798.181818181818,
      "grad_norm": 0.19901560246944427,
      "learning_rate": 8.983352965289948e-07,
      "loss": 0.001,
      "step": 42890
    },
    {
      "epoch": 7800.0,
      "grad_norm": 0.29184749722480774,
      "learning_rate": 8.982649590120981e-07,
      "loss": 0.0011,
      "step": 42900
    },
    {
      "epoch": 7801.818181818182,
      "grad_norm": 0.2861197590827942,
      "learning_rate": 8.98194599927478e-07,
      "loss": 0.0014,
      "step": 42910
    },
    {
      "epoch": 7803.636363636364,
      "grad_norm": 0.00467382837086916,
      "learning_rate": 8.981242192789451e-07,
      "loss": 0.0007,
      "step": 42920
    },
    {
      "epoch": 7805.454545454545,
      "grad_norm": 0.20833562314510345,
      "learning_rate": 8.980538170703103e-07,
      "loss": 0.0014,
      "step": 42930
    },
    {
      "epoch": 7807.272727272727,
      "grad_norm": 0.2321704775094986,
      "learning_rate": 8.979833933053864e-07,
      "loss": 0.0012,
      "step": 42940
    },
    {
      "epoch": 7809.090909090909,
      "grad_norm": 0.18971610069274902,
      "learning_rate": 8.979129479879872e-07,
      "loss": 0.0011,
      "step": 42950
    },
    {
      "epoch": 7810.909090909091,
      "grad_norm": 0.0075736804865300655,
      "learning_rate": 8.978424811219276e-07,
      "loss": 0.0011,
      "step": 42960
    },
    {
      "epoch": 7812.727272727273,
      "grad_norm": 0.28185099363327026,
      "learning_rate": 8.977719927110236e-07,
      "loss": 0.0014,
      "step": 42970
    },
    {
      "epoch": 7814.545454545455,
      "grad_norm": 0.0023176786489784718,
      "learning_rate": 8.977014827590926e-07,
      "loss": 0.0006,
      "step": 42980
    },
    {
      "epoch": 7816.363636363636,
      "grad_norm": 0.0012102306354790926,
      "learning_rate": 8.976309512699528e-07,
      "loss": 0.0013,
      "step": 42990
    },
    {
      "epoch": 7818.181818181818,
      "grad_norm": 0.0025532813742756844,
      "learning_rate": 8.975603982474238e-07,
      "loss": 0.0011,
      "step": 43000
    },
    {
      "epoch": 7818.181818181818,
      "eval_loss": 4.694399356842041,
      "eval_runtime": 0.9518,
      "eval_samples_per_second": 10.506,
      "eval_steps_per_second": 5.253,
      "step": 43000
    },
    {
      "epoch": 7820.0,
      "grad_norm": 0.3251073658466339,
      "learning_rate": 8.974898236953265e-07,
      "loss": 0.0013,
      "step": 43010
    },
    {
      "epoch": 7821.818181818182,
      "grad_norm": 0.015351127833127975,
      "learning_rate": 8.974192276174828e-07,
      "loss": 0.001,
      "step": 43020
    },
    {
      "epoch": 7823.636363636364,
      "grad_norm": 0.003757569007575512,
      "learning_rate": 8.973486100177155e-07,
      "loss": 0.0009,
      "step": 43030
    },
    {
      "epoch": 7825.454545454545,
      "grad_norm": 0.17283299565315247,
      "learning_rate": 8.972779708998492e-07,
      "loss": 0.0014,
      "step": 43040
    },
    {
      "epoch": 7827.272727272727,
      "grad_norm": 0.0018010279163718224,
      "learning_rate": 8.97207310267709e-07,
      "loss": 0.001,
      "step": 43050
    },
    {
      "epoch": 7829.090909090909,
      "grad_norm": 0.007295264396816492,
      "learning_rate": 8.971366281251217e-07,
      "loss": 0.0012,
      "step": 43060
    },
    {
      "epoch": 7830.909090909091,
      "grad_norm": 0.28952881693840027,
      "learning_rate": 8.970659244759149e-07,
      "loss": 0.0012,
      "step": 43070
    },
    {
      "epoch": 7832.727272727273,
      "grad_norm": 0.2440965622663498,
      "learning_rate": 8.969951993239177e-07,
      "loss": 0.0011,
      "step": 43080
    },
    {
      "epoch": 7834.545454545455,
      "grad_norm": 0.20620372891426086,
      "learning_rate": 8.969244526729598e-07,
      "loss": 0.0012,
      "step": 43090
    },
    {
      "epoch": 7836.363636363636,
      "grad_norm": 0.270399808883667,
      "learning_rate": 8.968536845268729e-07,
      "loss": 0.001,
      "step": 43100
    },
    {
      "epoch": 7838.181818181818,
      "grad_norm": 0.33642151951789856,
      "learning_rate": 8.967828948894889e-07,
      "loss": 0.0013,
      "step": 43110
    },
    {
      "epoch": 7840.0,
      "grad_norm": 0.013897891156375408,
      "learning_rate": 8.967120837646415e-07,
      "loss": 0.0009,
      "step": 43120
    },
    {
      "epoch": 7841.818181818182,
      "grad_norm": 0.2735242545604706,
      "learning_rate": 8.966412511561658e-07,
      "loss": 0.0011,
      "step": 43130
    },
    {
      "epoch": 7843.636363636364,
      "grad_norm": 0.16803298890590668,
      "learning_rate": 8.965703970678973e-07,
      "loss": 0.0012,
      "step": 43140
    },
    {
      "epoch": 7845.454545454545,
      "grad_norm": 0.22100289165973663,
      "learning_rate": 8.964995215036731e-07,
      "loss": 0.0014,
      "step": 43150
    },
    {
      "epoch": 7847.272727272727,
      "grad_norm": 0.1926807165145874,
      "learning_rate": 8.964286244673314e-07,
      "loss": 0.0009,
      "step": 43160
    },
    {
      "epoch": 7849.090909090909,
      "grad_norm": 0.28770002722740173,
      "learning_rate": 8.963577059627116e-07,
      "loss": 0.0014,
      "step": 43170
    },
    {
      "epoch": 7850.909090909091,
      "grad_norm": 0.004087094683200121,
      "learning_rate": 8.962867659936545e-07,
      "loss": 0.0011,
      "step": 43180
    },
    {
      "epoch": 7852.727272727273,
      "grad_norm": 0.34893733263015747,
      "learning_rate": 8.962158045640014e-07,
      "loss": 0.0011,
      "step": 43190
    },
    {
      "epoch": 7854.545454545455,
      "grad_norm": 0.35076192021369934,
      "learning_rate": 8.961448216775953e-07,
      "loss": 0.0012,
      "step": 43200
    },
    {
      "epoch": 7856.363636363636,
      "grad_norm": 0.1889277547597885,
      "learning_rate": 8.960738173382803e-07,
      "loss": 0.0011,
      "step": 43210
    },
    {
      "epoch": 7858.181818181818,
      "grad_norm": 0.002782524796202779,
      "learning_rate": 8.960027915499016e-07,
      "loss": 0.0011,
      "step": 43220
    },
    {
      "epoch": 7860.0,
      "grad_norm": 0.2006664127111435,
      "learning_rate": 8.959317443163052e-07,
      "loss": 0.0012,
      "step": 43230
    },
    {
      "epoch": 7861.818181818182,
      "grad_norm": 0.0018804316641762853,
      "learning_rate": 8.958606756413391e-07,
      "loss": 0.0011,
      "step": 43240
    },
    {
      "epoch": 7863.636363636364,
      "grad_norm": 0.00649346923455596,
      "learning_rate": 8.957895855288517e-07,
      "loss": 0.0012,
      "step": 43250
    },
    {
      "epoch": 7865.454545454545,
      "grad_norm": 0.005130964796990156,
      "learning_rate": 8.957184739826928e-07,
      "loss": 0.0011,
      "step": 43260
    },
    {
      "epoch": 7867.272727272727,
      "grad_norm": 0.0017374654999002814,
      "learning_rate": 8.956473410067135e-07,
      "loss": 0.0009,
      "step": 43270
    },
    {
      "epoch": 7869.090909090909,
      "grad_norm": 0.1862964928150177,
      "learning_rate": 8.955761866047659e-07,
      "loss": 0.0013,
      "step": 43280
    },
    {
      "epoch": 7870.909090909091,
      "grad_norm": 0.29214248061180115,
      "learning_rate": 8.955050107807034e-07,
      "loss": 0.0011,
      "step": 43290
    },
    {
      "epoch": 7872.727272727273,
      "grad_norm": 0.0029315685387700796,
      "learning_rate": 8.954338135383804e-07,
      "loss": 0.0011,
      "step": 43300
    },
    {
      "epoch": 7874.545454545455,
      "grad_norm": 0.01377250999212265,
      "learning_rate": 8.953625948816524e-07,
      "loss": 0.0012,
      "step": 43310
    },
    {
      "epoch": 7876.363636363636,
      "grad_norm": 0.2509787678718567,
      "learning_rate": 8.952913548143764e-07,
      "loss": 0.0014,
      "step": 43320
    },
    {
      "epoch": 7878.181818181818,
      "grad_norm": 0.0024994313716888428,
      "learning_rate": 8.952200933404103e-07,
      "loss": 0.001,
      "step": 43330
    },
    {
      "epoch": 7880.0,
      "grad_norm": 0.21083620190620422,
      "learning_rate": 8.951488104636132e-07,
      "loss": 0.0012,
      "step": 43340
    },
    {
      "epoch": 7881.818181818182,
      "grad_norm": 0.18850137293338776,
      "learning_rate": 8.950775061878452e-07,
      "loss": 0.0011,
      "step": 43350
    },
    {
      "epoch": 7883.636363636364,
      "grad_norm": 0.2393214851617813,
      "learning_rate": 8.950061805169679e-07,
      "loss": 0.0014,
      "step": 43360
    },
    {
      "epoch": 7885.454545454545,
      "grad_norm": 0.2644284665584564,
      "learning_rate": 8.949348334548439e-07,
      "loss": 0.0012,
      "step": 43370
    },
    {
      "epoch": 7887.272727272727,
      "grad_norm": 0.2695779502391815,
      "learning_rate": 8.948634650053369e-07,
      "loss": 0.0012,
      "step": 43380
    },
    {
      "epoch": 7889.090909090909,
      "grad_norm": 0.2835373878479004,
      "learning_rate": 8.947920751723117e-07,
      "loss": 0.0008,
      "step": 43390
    },
    {
      "epoch": 7890.909090909091,
      "grad_norm": 0.3280150890350342,
      "learning_rate": 8.947206639596345e-07,
      "loss": 0.001,
      "step": 43400
    },
    {
      "epoch": 7892.727272727273,
      "grad_norm": 0.0021770510356873274,
      "learning_rate": 8.946492313711724e-07,
      "loss": 0.0013,
      "step": 43410
    },
    {
      "epoch": 7894.545454545455,
      "grad_norm": 0.0026207878254354,
      "learning_rate": 8.94577777410794e-07,
      "loss": 0.0012,
      "step": 43420
    },
    {
      "epoch": 7896.363636363636,
      "grad_norm": 0.0022473896387964487,
      "learning_rate": 8.945063020823686e-07,
      "loss": 0.0012,
      "step": 43430
    },
    {
      "epoch": 7898.181818181818,
      "grad_norm": 0.2541779577732086,
      "learning_rate": 8.94434805389767e-07,
      "loss": 0.0011,
      "step": 43440
    },
    {
      "epoch": 7900.0,
      "grad_norm": 0.0020378429908305407,
      "learning_rate": 8.943632873368611e-07,
      "loss": 0.0011,
      "step": 43450
    },
    {
      "epoch": 7901.818181818182,
      "grad_norm": 0.0030338363721966743,
      "learning_rate": 8.942917479275236e-07,
      "loss": 0.001,
      "step": 43460
    },
    {
      "epoch": 7903.636363636364,
      "grad_norm": 0.0023638734128326178,
      "learning_rate": 8.94220187165629e-07,
      "loss": 0.0011,
      "step": 43470
    },
    {
      "epoch": 7905.454545454545,
      "grad_norm": 0.2882973253726959,
      "learning_rate": 8.941486050550527e-07,
      "loss": 0.0012,
      "step": 43480
    },
    {
      "epoch": 7907.272727272727,
      "grad_norm": 0.2013804018497467,
      "learning_rate": 8.940770015996706e-07,
      "loss": 0.0012,
      "step": 43490
    },
    {
      "epoch": 7909.090909090909,
      "grad_norm": 0.002377596450969577,
      "learning_rate": 8.940053768033608e-07,
      "loss": 0.0011,
      "step": 43500
    },
    {
      "epoch": 7909.090909090909,
      "eval_loss": 4.814007759094238,
      "eval_runtime": 0.9499,
      "eval_samples_per_second": 10.527,
      "eval_steps_per_second": 5.263,
      "step": 43500
    },
    {
      "epoch": 7910.909090909091,
      "grad_norm": 0.004146629478782415,
      "learning_rate": 8.939337306700022e-07,
      "loss": 0.0013,
      "step": 43510
    },
    {
      "epoch": 7912.727272727273,
      "grad_norm": 0.0027064024470746517,
      "learning_rate": 8.938620632034743e-07,
      "loss": 0.0014,
      "step": 43520
    },
    {
      "epoch": 7914.545454545455,
      "grad_norm": 0.2525292634963989,
      "learning_rate": 8.937903744076586e-07,
      "loss": 0.0011,
      "step": 43530
    },
    {
      "epoch": 7916.363636363636,
      "grad_norm": 0.0020145138259977102,
      "learning_rate": 8.937186642864371e-07,
      "loss": 0.0011,
      "step": 43540
    },
    {
      "epoch": 7918.181818181818,
      "grad_norm": 0.001627843827009201,
      "learning_rate": 8.936469328436932e-07,
      "loss": 0.0009,
      "step": 43550
    },
    {
      "epoch": 7920.0,
      "grad_norm": 0.0028530191630125046,
      "learning_rate": 8.935751800833116e-07,
      "loss": 0.0012,
      "step": 43560
    },
    {
      "epoch": 7921.818181818182,
      "grad_norm": 0.002120701130479574,
      "learning_rate": 8.93503406009178e-07,
      "loss": 0.0013,
      "step": 43570
    },
    {
      "epoch": 7923.636363636364,
      "grad_norm": 0.0024778051301836967,
      "learning_rate": 8.93431610625179e-07,
      "loss": 0.0011,
      "step": 43580
    },
    {
      "epoch": 7925.454545454545,
      "grad_norm": 0.23993313312530518,
      "learning_rate": 8.93359793935203e-07,
      "loss": 0.0009,
      "step": 43590
    },
    {
      "epoch": 7927.272727272727,
      "grad_norm": 0.2695104777812958,
      "learning_rate": 8.93287955943139e-07,
      "loss": 0.0012,
      "step": 43600
    },
    {
      "epoch": 7929.090909090909,
      "grad_norm": 0.002543615410104394,
      "learning_rate": 8.932160966528772e-07,
      "loss": 0.001,
      "step": 43610
    },
    {
      "epoch": 7930.909090909091,
      "grad_norm": 0.20923566818237305,
      "learning_rate": 8.931442160683094e-07,
      "loss": 0.0011,
      "step": 43620
    },
    {
      "epoch": 7932.727272727273,
      "grad_norm": 0.1728316694498062,
      "learning_rate": 8.930723141933281e-07,
      "loss": 0.0015,
      "step": 43630
    },
    {
      "epoch": 7934.545454545455,
      "grad_norm": 0.001868512248620391,
      "learning_rate": 8.930003910318268e-07,
      "loss": 0.0009,
      "step": 43640
    },
    {
      "epoch": 7936.363636363636,
      "grad_norm": 0.002391239395365119,
      "learning_rate": 8.929284465877009e-07,
      "loss": 0.0009,
      "step": 43650
    },
    {
      "epoch": 7938.181818181818,
      "grad_norm": 0.19559092819690704,
      "learning_rate": 8.928564808648462e-07,
      "loss": 0.0015,
      "step": 43660
    },
    {
      "epoch": 7940.0,
      "grad_norm": 0.0018789262976497412,
      "learning_rate": 8.927844938671601e-07,
      "loss": 0.0011,
      "step": 43670
    },
    {
      "epoch": 7941.818181818182,
      "grad_norm": 0.2697027623653412,
      "learning_rate": 8.927124855985408e-07,
      "loss": 0.0009,
      "step": 43680
    },
    {
      "epoch": 7943.636363636364,
      "grad_norm": 0.0026723891496658325,
      "learning_rate": 8.926404560628882e-07,
      "loss": 0.001,
      "step": 43690
    },
    {
      "epoch": 7945.454545454545,
      "grad_norm": 0.002544278744608164,
      "learning_rate": 8.925684052641026e-07,
      "loss": 0.0014,
      "step": 43700
    },
    {
      "epoch": 7947.272727272727,
      "grad_norm": 0.0023528358433395624,
      "learning_rate": 8.924963332060862e-07,
      "loss": 0.0009,
      "step": 43710
    },
    {
      "epoch": 7949.090909090909,
      "grad_norm": 0.5455058813095093,
      "learning_rate": 8.924242398927418e-07,
      "loss": 0.0014,
      "step": 43720
    },
    {
      "epoch": 7950.909090909091,
      "grad_norm": 0.003909841645509005,
      "learning_rate": 8.923521253279733e-07,
      "loss": 0.0008,
      "step": 43730
    },
    {
      "epoch": 7952.727272727273,
      "grad_norm": 0.20471839606761932,
      "learning_rate": 8.922799895156866e-07,
      "loss": 0.0014,
      "step": 43740
    },
    {
      "epoch": 7954.545454545455,
      "grad_norm": 0.23745611310005188,
      "learning_rate": 8.922078324597878e-07,
      "loss": 0.001,
      "step": 43750
    },
    {
      "epoch": 7956.363636363636,
      "grad_norm": 0.2689523994922638,
      "learning_rate": 8.921356541641847e-07,
      "loss": 0.0014,
      "step": 43760
    },
    {
      "epoch": 7958.181818181818,
      "grad_norm": 0.0021270927973091602,
      "learning_rate": 8.920634546327857e-07,
      "loss": 0.0009,
      "step": 43770
    },
    {
      "epoch": 7960.0,
      "grad_norm": 0.2596442401409149,
      "learning_rate": 8.919912338695011e-07,
      "loss": 0.0012,
      "step": 43780
    },
    {
      "epoch": 7961.818181818182,
      "grad_norm": 0.00691859470680356,
      "learning_rate": 8.919189918782417e-07,
      "loss": 0.0012,
      "step": 43790
    },
    {
      "epoch": 7963.636363636364,
      "grad_norm": 0.20509123802185059,
      "learning_rate": 8.918467286629198e-07,
      "loss": 0.001,
      "step": 43800
    },
    {
      "epoch": 7965.454545454545,
      "grad_norm": 0.3199978768825531,
      "learning_rate": 8.917744442274488e-07,
      "loss": 0.0014,
      "step": 43810
    },
    {
      "epoch": 7967.272727272727,
      "grad_norm": 0.24732865393161774,
      "learning_rate": 8.917021385757433e-07,
      "loss": 0.0012,
      "step": 43820
    },
    {
      "epoch": 7969.090909090909,
      "grad_norm": 0.002168067032471299,
      "learning_rate": 8.916298117117187e-07,
      "loss": 0.0008,
      "step": 43830
    },
    {
      "epoch": 7970.909090909091,
      "grad_norm": 0.2562556266784668,
      "learning_rate": 8.91557463639292e-07,
      "loss": 0.0012,
      "step": 43840
    },
    {
      "epoch": 7972.727272727273,
      "grad_norm": 0.29018715023994446,
      "learning_rate": 8.914850943623808e-07,
      "loss": 0.001,
      "step": 43850
    },
    {
      "epoch": 7974.545454545455,
      "grad_norm": 0.0019127613632008433,
      "learning_rate": 8.914127038849048e-07,
      "loss": 0.0009,
      "step": 43860
    },
    {
      "epoch": 7976.363636363636,
      "grad_norm": 0.002303522313013673,
      "learning_rate": 8.913402922107838e-07,
      "loss": 0.0012,
      "step": 43870
    },
    {
      "epoch": 7978.181818181818,
      "grad_norm": 0.0027608263771981,
      "learning_rate": 8.912678593439393e-07,
      "loss": 0.0011,
      "step": 43880
    },
    {
      "epoch": 7980.0,
      "grad_norm": 0.004624067805707455,
      "learning_rate": 8.911954052882939e-07,
      "loss": 0.0012,
      "step": 43890
    },
    {
      "epoch": 7981.818181818182,
      "grad_norm": 0.28891468048095703,
      "learning_rate": 8.911229300477714e-07,
      "loss": 0.0012,
      "step": 43900
    },
    {
      "epoch": 7983.636363636364,
      "grad_norm": 0.23502181470394135,
      "learning_rate": 8.910504336262965e-07,
      "loss": 0.0012,
      "step": 43910
    },
    {
      "epoch": 7985.454545454545,
      "grad_norm": 0.02268272265791893,
      "learning_rate": 8.90977916027795e-07,
      "loss": 0.0011,
      "step": 43920
    },
    {
      "epoch": 7987.272727272727,
      "grad_norm": 0.002105571096763015,
      "learning_rate": 8.909053772561943e-07,
      "loss": 0.0009,
      "step": 43930
    },
    {
      "epoch": 7989.090909090909,
      "grad_norm": 0.005324520170688629,
      "learning_rate": 8.908328173154226e-07,
      "loss": 0.0012,
      "step": 43940
    },
    {
      "epoch": 7990.909090909091,
      "grad_norm": 0.3184064030647278,
      "learning_rate": 8.907602362094093e-07,
      "loss": 0.0012,
      "step": 43950
    },
    {
      "epoch": 7992.727272727273,
      "grad_norm": 0.22428585588932037,
      "learning_rate": 8.90687633942085e-07,
      "loss": 0.0011,
      "step": 43960
    },
    {
      "epoch": 7994.545454545455,
      "grad_norm": 0.010522552765905857,
      "learning_rate": 8.906150105173815e-07,
      "loss": 0.0012,
      "step": 43970
    },
    {
      "epoch": 7996.363636363636,
      "grad_norm": 0.0019670070614665747,
      "learning_rate": 8.905423659392315e-07,
      "loss": 0.0008,
      "step": 43980
    },
    {
      "epoch": 7998.181818181818,
      "grad_norm": 0.21057943999767303,
      "learning_rate": 8.904697002115692e-07,
      "loss": 0.0014,
      "step": 43990
    },
    {
      "epoch": 8000.0,
      "grad_norm": 0.37865912914276123,
      "learning_rate": 8.903970133383296e-07,
      "loss": 0.0011,
      "step": 44000
    },
    {
      "epoch": 8000.0,
      "eval_loss": 4.760375499725342,
      "eval_runtime": 0.9497,
      "eval_samples_per_second": 10.529,
      "eval_steps_per_second": 5.265,
      "step": 44000
    },
    {
      "epoch": 8001.818181818182,
      "grad_norm": 0.2680910527706146,
      "learning_rate": 8.90324305323449e-07,
      "loss": 0.0014,
      "step": 44010
    },
    {
      "epoch": 8003.636363636364,
      "grad_norm": 0.17479169368743896,
      "learning_rate": 8.902515761708648e-07,
      "loss": 0.0011,
      "step": 44020
    },
    {
      "epoch": 8005.454545454545,
      "grad_norm": 0.21273888647556305,
      "learning_rate": 8.90178825884516e-07,
      "loss": 0.0011,
      "step": 44030
    },
    {
      "epoch": 8007.272727272727,
      "grad_norm": 0.054264117032289505,
      "learning_rate": 8.901060544683418e-07,
      "loss": 0.0012,
      "step": 44040
    },
    {
      "epoch": 8009.090909090909,
      "grad_norm": 0.2288637012243271,
      "learning_rate": 8.900332619262833e-07,
      "loss": 0.0011,
      "step": 44050
    },
    {
      "epoch": 8010.909090909091,
      "grad_norm": 0.2692417502403259,
      "learning_rate": 8.899604482622824e-07,
      "loss": 0.0011,
      "step": 44060
    },
    {
      "epoch": 8012.727272727273,
      "grad_norm": 0.003958985675126314,
      "learning_rate": 8.898876134802826e-07,
      "loss": 0.0012,
      "step": 44070
    },
    {
      "epoch": 8014.545454545455,
      "grad_norm": 0.2161739021539688,
      "learning_rate": 8.898147575842278e-07,
      "loss": 0.0013,
      "step": 44080
    },
    {
      "epoch": 8016.363636363636,
      "grad_norm": 0.19297592341899872,
      "learning_rate": 8.897418805780639e-07,
      "loss": 0.001,
      "step": 44090
    },
    {
      "epoch": 8018.181818181818,
      "grad_norm": 0.00451680226251483,
      "learning_rate": 8.896689824657371e-07,
      "loss": 0.0011,
      "step": 44100
    },
    {
      "epoch": 8020.0,
      "grad_norm": 0.001471231458708644,
      "learning_rate": 8.895960632511952e-07,
      "loss": 0.0012,
      "step": 44110
    },
    {
      "epoch": 8021.818181818182,
      "grad_norm": 0.2842068672180176,
      "learning_rate": 8.895231229383873e-07,
      "loss": 0.0012,
      "step": 44120
    },
    {
      "epoch": 8023.636363636364,
      "grad_norm": 0.3472215533256531,
      "learning_rate": 8.894501615312632e-07,
      "loss": 0.0011,
      "step": 44130
    },
    {
      "epoch": 8025.454545454545,
      "grad_norm": 0.24016989767551422,
      "learning_rate": 8.893771790337742e-07,
      "loss": 0.0009,
      "step": 44140
    },
    {
      "epoch": 8027.272727272727,
      "grad_norm": 0.002880050567910075,
      "learning_rate": 8.893041754498724e-07,
      "loss": 0.0012,
      "step": 44150
    },
    {
      "epoch": 8029.090909090909,
      "grad_norm": 0.1931028515100479,
      "learning_rate": 8.892311507835117e-07,
      "loss": 0.0012,
      "step": 44160
    },
    {
      "epoch": 8030.909090909091,
      "grad_norm": 0.19314157962799072,
      "learning_rate": 8.891581050386463e-07,
      "loss": 0.0011,
      "step": 44170
    },
    {
      "epoch": 8032.727272727273,
      "grad_norm": 0.0023363465443253517,
      "learning_rate": 8.89085038219232e-07,
      "loss": 0.0009,
      "step": 44180
    },
    {
      "epoch": 8034.545454545455,
      "grad_norm": 0.19284379482269287,
      "learning_rate": 8.890119503292257e-07,
      "loss": 0.0011,
      "step": 44190
    },
    {
      "epoch": 8036.363636363636,
      "grad_norm": 0.26545095443725586,
      "learning_rate": 8.889388413725856e-07,
      "loss": 0.0013,
      "step": 44200
    },
    {
      "epoch": 8038.181818181818,
      "grad_norm": 0.002434871857985854,
      "learning_rate": 8.888657113532704e-07,
      "loss": 0.0009,
      "step": 44210
    },
    {
      "epoch": 8040.0,
      "grad_norm": 0.2627621293067932,
      "learning_rate": 8.88792560275241e-07,
      "loss": 0.0012,
      "step": 44220
    },
    {
      "epoch": 8041.818181818182,
      "grad_norm": 0.20039260387420654,
      "learning_rate": 8.887193881424584e-07,
      "loss": 0.0012,
      "step": 44230
    },
    {
      "epoch": 8043.636363636364,
      "grad_norm": 0.0057823839597404,
      "learning_rate": 8.886461949588853e-07,
      "loss": 0.001,
      "step": 44240
    },
    {
      "epoch": 8045.454545454545,
      "grad_norm": 0.006610704120248556,
      "learning_rate": 8.885729807284854e-07,
      "loss": 0.0011,
      "step": 44250
    },
    {
      "epoch": 8047.272727272727,
      "grad_norm": 0.2680187225341797,
      "learning_rate": 8.884997454552236e-07,
      "loss": 0.001,
      "step": 44260
    },
    {
      "epoch": 8049.090909090909,
      "grad_norm": 0.0032620239071547985,
      "learning_rate": 8.88426489143066e-07,
      "loss": 0.0011,
      "step": 44270
    },
    {
      "epoch": 8050.909090909091,
      "grad_norm": 0.0030864947475492954,
      "learning_rate": 8.883532117959796e-07,
      "loss": 0.0012,
      "step": 44280
    },
    {
      "epoch": 8052.727272727273,
      "grad_norm": 0.25136619806289673,
      "learning_rate": 8.882799134179325e-07,
      "loss": 0.0009,
      "step": 44290
    },
    {
      "epoch": 8054.545454545455,
      "grad_norm": 0.21020427346229553,
      "learning_rate": 8.882065940128944e-07,
      "loss": 0.0013,
      "step": 44300
    },
    {
      "epoch": 8056.363636363636,
      "grad_norm": 0.3390445411205292,
      "learning_rate": 8.88133253584836e-07,
      "loss": 0.0011,
      "step": 44310
    },
    {
      "epoch": 8058.181818181818,
      "grad_norm": 0.30845049023628235,
      "learning_rate": 8.880598921377285e-07,
      "loss": 0.0014,
      "step": 44320
    },
    {
      "epoch": 8060.0,
      "grad_norm": 0.24199800193309784,
      "learning_rate": 8.879865096755451e-07,
      "loss": 0.0011,
      "step": 44330
    },
    {
      "epoch": 8061.818181818182,
      "grad_norm": 0.24793463945388794,
      "learning_rate": 8.879131062022597e-07,
      "loss": 0.0012,
      "step": 44340
    },
    {
      "epoch": 8063.636363636364,
      "grad_norm": 0.1926131695508957,
      "learning_rate": 8.878396817218472e-07,
      "loss": 0.0011,
      "step": 44350
    },
    {
      "epoch": 8065.454545454545,
      "grad_norm": 0.003149326890707016,
      "learning_rate": 8.877662362382843e-07,
      "loss": 0.001,
      "step": 44360
    },
    {
      "epoch": 8067.272727272727,
      "grad_norm": 0.10919781029224396,
      "learning_rate": 8.876927697555479e-07,
      "loss": 0.0023,
      "step": 44370
    },
    {
      "epoch": 8069.090909090909,
      "grad_norm": 16.32132339477539,
      "learning_rate": 8.876192822776167e-07,
      "loss": 0.0032,
      "step": 44380
    },
    {
      "epoch": 8070.909090909091,
      "grad_norm": 8.594602584838867,
      "learning_rate": 8.875457738084704e-07,
      "loss": 0.0023,
      "step": 44390
    },
    {
      "epoch": 8072.727272727273,
      "grad_norm": 0.3403245210647583,
      "learning_rate": 8.874722443520898e-07,
      "loss": 0.0012,
      "step": 44400
    },
    {
      "epoch": 8074.545454545455,
      "grad_norm": 0.3335624039173126,
      "learning_rate": 8.873986939124567e-07,
      "loss": 0.0017,
      "step": 44410
    },
    {
      "epoch": 8076.363636363636,
      "grad_norm": 0.3578447103500366,
      "learning_rate": 8.873251224935543e-07,
      "loss": 0.001,
      "step": 44420
    },
    {
      "epoch": 8078.181818181818,
      "grad_norm": 0.05955669656395912,
      "learning_rate": 8.872515300993668e-07,
      "loss": 0.0011,
      "step": 44430
    },
    {
      "epoch": 8080.0,
      "grad_norm": 0.013856575824320316,
      "learning_rate": 8.871779167338795e-07,
      "loss": 0.0012,
      "step": 44440
    },
    {
      "epoch": 8081.818181818182,
      "grad_norm": 0.0099927494302392,
      "learning_rate": 8.871042824010789e-07,
      "loss": 0.001,
      "step": 44450
    },
    {
      "epoch": 8083.636363636364,
      "grad_norm": 0.2293037325143814,
      "learning_rate": 8.870306271049525e-07,
      "loss": 0.0011,
      "step": 44460
    },
    {
      "epoch": 8085.454545454545,
      "grad_norm": 0.08293507993221283,
      "learning_rate": 8.869569508494893e-07,
      "loss": 0.0013,
      "step": 44470
    },
    {
      "epoch": 8087.272727272727,
      "grad_norm": 0.06773392111063004,
      "learning_rate": 8.868832536386789e-07,
      "loss": 0.0011,
      "step": 44480
    },
    {
      "epoch": 8089.090909090909,
      "grad_norm": 0.408272385597229,
      "learning_rate": 8.868095354765124e-07,
      "loss": 0.0014,
      "step": 44490
    },
    {
      "epoch": 8090.909090909091,
      "grad_norm": 0.2020689994096756,
      "learning_rate": 8.86735796366982e-07,
      "loss": 0.0011,
      "step": 44500
    },
    {
      "epoch": 8090.909090909091,
      "eval_loss": 4.620859146118164,
      "eval_runtime": 0.9575,
      "eval_samples_per_second": 10.444,
      "eval_steps_per_second": 5.222,
      "step": 44500
    },
    {
      "epoch": 8092.727272727273,
      "grad_norm": 0.1989104449748993,
      "learning_rate": 8.866620363140808e-07,
      "loss": 0.0011,
      "step": 44510
    },
    {
      "epoch": 8094.545454545455,
      "grad_norm": 0.2827717363834381,
      "learning_rate": 8.865882553218036e-07,
      "loss": 0.0011,
      "step": 44520
    },
    {
      "epoch": 8096.363636363636,
      "grad_norm": 0.003036320675164461,
      "learning_rate": 8.865144533941455e-07,
      "loss": 0.0009,
      "step": 44530
    },
    {
      "epoch": 8098.181818181818,
      "grad_norm": 0.01644224114716053,
      "learning_rate": 8.864406305351036e-07,
      "loss": 0.0015,
      "step": 44540
    },
    {
      "epoch": 8100.0,
      "grad_norm": 0.2001255452632904,
      "learning_rate": 8.863667867486755e-07,
      "loss": 0.0009,
      "step": 44550
    },
    {
      "epoch": 8101.818181818182,
      "grad_norm": 0.20480002462863922,
      "learning_rate": 8.862929220388601e-07,
      "loss": 0.0011,
      "step": 44560
    },
    {
      "epoch": 8103.636363636364,
      "grad_norm": 0.3036559224128723,
      "learning_rate": 8.862190364096575e-07,
      "loss": 0.0012,
      "step": 44570
    },
    {
      "epoch": 8105.454545454545,
      "grad_norm": 0.2148772031068802,
      "learning_rate": 8.86145129865069e-07,
      "loss": 0.0011,
      "step": 44580
    },
    {
      "epoch": 8107.272727272727,
      "grad_norm": 0.24238210916519165,
      "learning_rate": 8.86071202409097e-07,
      "loss": 0.001,
      "step": 44590
    },
    {
      "epoch": 8109.090909090909,
      "grad_norm": 0.003574336413294077,
      "learning_rate": 8.85997254045745e-07,
      "loss": 0.0011,
      "step": 44600
    },
    {
      "epoch": 8110.909090909091,
      "grad_norm": 0.002275449223816395,
      "learning_rate": 8.859232847790174e-07,
      "loss": 0.0012,
      "step": 44610
    },
    {
      "epoch": 8112.727272727273,
      "grad_norm": 0.23636958003044128,
      "learning_rate": 8.858492946129202e-07,
      "loss": 0.0012,
      "step": 44620
    },
    {
      "epoch": 8114.545454545455,
      "grad_norm": 0.005737715400755405,
      "learning_rate": 8.8577528355146e-07,
      "loss": 0.001,
      "step": 44630
    },
    {
      "epoch": 8116.363636363636,
      "grad_norm": 0.004569001030176878,
      "learning_rate": 8.85701251598645e-07,
      "loss": 0.0011,
      "step": 44640
    },
    {
      "epoch": 8118.181818181818,
      "grad_norm": 0.17587408423423767,
      "learning_rate": 8.856271987584843e-07,
      "loss": 0.0012,
      "step": 44650
    },
    {
      "epoch": 8120.0,
      "grad_norm": 0.0029058854561299086,
      "learning_rate": 8.855531250349882e-07,
      "loss": 0.0011,
      "step": 44660
    },
    {
      "epoch": 8121.818181818182,
      "grad_norm": 0.1975797712802887,
      "learning_rate": 8.854790304321681e-07,
      "loss": 0.0011,
      "step": 44670
    },
    {
      "epoch": 8123.636363636364,
      "grad_norm": 0.21012264490127563,
      "learning_rate": 8.854049149540365e-07,
      "loss": 0.0012,
      "step": 44680
    },
    {
      "epoch": 8125.454545454545,
      "grad_norm": 0.004252460319548845,
      "learning_rate": 8.853307786046071e-07,
      "loss": 0.0014,
      "step": 44690
    },
    {
      "epoch": 8127.272727272727,
      "grad_norm": 0.1959076225757599,
      "learning_rate": 8.852566213878946e-07,
      "loss": 0.0009,
      "step": 44700
    },
    {
      "epoch": 8129.090909090909,
      "grad_norm": 0.22814041376113892,
      "learning_rate": 8.851824433079151e-07,
      "loss": 0.0012,
      "step": 44710
    },
    {
      "epoch": 8130.909090909091,
      "grad_norm": 0.20466248691082,
      "learning_rate": 8.851082443686854e-07,
      "loss": 0.0011,
      "step": 44720
    },
    {
      "epoch": 8132.727272727273,
      "grad_norm": 0.004072414245456457,
      "learning_rate": 8.850340245742239e-07,
      "loss": 0.0011,
      "step": 44730
    },
    {
      "epoch": 8134.545454545455,
      "grad_norm": 0.003436887403950095,
      "learning_rate": 8.849597839285499e-07,
      "loss": 0.001,
      "step": 44740
    },
    {
      "epoch": 8136.363636363636,
      "grad_norm": 0.004437977913767099,
      "learning_rate": 8.848855224356838e-07,
      "loss": 0.0011,
      "step": 44750
    },
    {
      "epoch": 8138.181818181818,
      "grad_norm": 0.0031171964947134256,
      "learning_rate": 8.848112400996473e-07,
      "loss": 0.0012,
      "step": 44760
    },
    {
      "epoch": 8140.0,
      "grad_norm": 0.00307622947730124,
      "learning_rate": 8.847369369244627e-07,
      "loss": 0.0012,
      "step": 44770
    },
    {
      "epoch": 8141.818181818182,
      "grad_norm": 0.3180294632911682,
      "learning_rate": 8.846626129141544e-07,
      "loss": 0.0012,
      "step": 44780
    },
    {
      "epoch": 8143.636363636364,
      "grad_norm": 0.003533022478222847,
      "learning_rate": 8.845882680727468e-07,
      "loss": 0.0008,
      "step": 44790
    },
    {
      "epoch": 8145.454545454545,
      "grad_norm": 0.003144647693261504,
      "learning_rate": 8.845139024042663e-07,
      "loss": 0.0014,
      "step": 44800
    },
    {
      "epoch": 8147.272727272727,
      "grad_norm": 0.0027957228012382984,
      "learning_rate": 8.844395159127401e-07,
      "loss": 0.0011,
      "step": 44810
    },
    {
      "epoch": 8149.090909090909,
      "grad_norm": 0.003802041755989194,
      "learning_rate": 8.843651086021966e-07,
      "loss": 0.0011,
      "step": 44820
    },
    {
      "epoch": 8150.909090909091,
      "grad_norm": 0.2688870429992676,
      "learning_rate": 8.84290680476665e-07,
      "loss": 0.0013,
      "step": 44830
    },
    {
      "epoch": 8152.727272727273,
      "grad_norm": 0.20716407895088196,
      "learning_rate": 8.842162315401761e-07,
      "loss": 0.0009,
      "step": 44840
    },
    {
      "epoch": 8154.545454545455,
      "grad_norm": 0.0026715451385825872,
      "learning_rate": 8.841417617967617e-07,
      "loss": 0.0013,
      "step": 44850
    },
    {
      "epoch": 8156.363636363636,
      "grad_norm": 0.23559649288654327,
      "learning_rate": 8.840672712504544e-07,
      "loss": 0.0011,
      "step": 44860
    },
    {
      "epoch": 8158.181818181818,
      "grad_norm": 0.004096287302672863,
      "learning_rate": 8.839927599052883e-07,
      "loss": 0.001,
      "step": 44870
    },
    {
      "epoch": 8160.0,
      "grad_norm": 0.34235090017318726,
      "learning_rate": 8.839182277652988e-07,
      "loss": 0.0012,
      "step": 44880
    },
    {
      "epoch": 8161.818181818182,
      "grad_norm": 0.004799776244908571,
      "learning_rate": 8.838436748345215e-07,
      "loss": 0.0011,
      "step": 44890
    },
    {
      "epoch": 8163.636363636364,
      "grad_norm": 0.1994006186723709,
      "learning_rate": 8.837691011169944e-07,
      "loss": 0.0011,
      "step": 44900
    },
    {
      "epoch": 8165.454545454545,
      "grad_norm": 0.18111014366149902,
      "learning_rate": 8.836945066167554e-07,
      "loss": 0.0015,
      "step": 44910
    },
    {
      "epoch": 8167.272727272727,
      "grad_norm": 0.19652776420116425,
      "learning_rate": 8.836198913378445e-07,
      "loss": 0.0008,
      "step": 44920
    },
    {
      "epoch": 8169.090909090909,
      "grad_norm": 0.012455799616873264,
      "learning_rate": 8.835452552843023e-07,
      "loss": 0.0012,
      "step": 44930
    },
    {
      "epoch": 8170.909090909091,
      "grad_norm": 0.004310780204832554,
      "learning_rate": 8.834705984601708e-07,
      "loss": 0.0011,
      "step": 44940
    },
    {
      "epoch": 8172.727272727273,
      "grad_norm": 0.23309288918972015,
      "learning_rate": 8.833959208694928e-07,
      "loss": 0.0009,
      "step": 44950
    },
    {
      "epoch": 8174.545454545455,
      "grad_norm": 0.1957472413778305,
      "learning_rate": 8.833212225163124e-07,
      "loss": 0.001,
      "step": 44960
    },
    {
      "epoch": 8176.363636363636,
      "grad_norm": 0.003614868503063917,
      "learning_rate": 8.832465034046749e-07,
      "loss": 0.0013,
      "step": 44970
    },
    {
      "epoch": 8178.181818181818,
      "grad_norm": 0.21487630903720856,
      "learning_rate": 8.831717635386267e-07,
      "loss": 0.0011,
      "step": 44980
    },
    {
      "epoch": 8180.0,
      "grad_norm": 0.013407490216195583,
      "learning_rate": 8.830970029222152e-07,
      "loss": 0.001,
      "step": 44990
    },
    {
      "epoch": 8181.818181818182,
      "grad_norm": 0.34131819009780884,
      "learning_rate": 8.83022221559489e-07,
      "loss": 0.0012,
      "step": 45000
    },
    {
      "epoch": 8181.818181818182,
      "eval_loss": 4.698338508605957,
      "eval_runtime": 0.9532,
      "eval_samples_per_second": 10.491,
      "eval_steps_per_second": 5.245,
      "step": 45000
    },
    {
      "epoch": 8183.636363636364,
      "grad_norm": 0.20896513760089874,
      "learning_rate": 8.829474194544979e-07,
      "loss": 0.0007,
      "step": 45010
    },
    {
      "epoch": 8185.454545454545,
      "grad_norm": 0.19674766063690186,
      "learning_rate": 8.828725966112926e-07,
      "loss": 0.0014,
      "step": 45020
    },
    {
      "epoch": 8187.272727272727,
      "grad_norm": 0.04351349174976349,
      "learning_rate": 8.827977530339252e-07,
      "loss": 0.0012,
      "step": 45030
    },
    {
      "epoch": 8189.090909090909,
      "grad_norm": 0.2377372533082962,
      "learning_rate": 8.827228887264489e-07,
      "loss": 0.001,
      "step": 45040
    },
    {
      "epoch": 8190.909090909091,
      "grad_norm": 0.198426753282547,
      "learning_rate": 8.826480036929178e-07,
      "loss": 0.0012,
      "step": 45050
    },
    {
      "epoch": 8192.727272727272,
      "grad_norm": 0.19528348743915558,
      "learning_rate": 8.825730979373871e-07,
      "loss": 0.0012,
      "step": 45060
    },
    {
      "epoch": 8194.545454545454,
      "grad_norm": 0.20731297135353088,
      "learning_rate": 8.824981714639134e-07,
      "loss": 0.001,
      "step": 45070
    },
    {
      "epoch": 8196.363636363636,
      "grad_norm": 0.0021463956218212843,
      "learning_rate": 8.824232242765542e-07,
      "loss": 0.0009,
      "step": 45080
    },
    {
      "epoch": 8198.181818181818,
      "grad_norm": 0.003096706001088023,
      "learning_rate": 8.823482563793685e-07,
      "loss": 0.0012,
      "step": 45090
    },
    {
      "epoch": 8200.0,
      "grad_norm": 0.0015634567243978381,
      "learning_rate": 8.822732677764157e-07,
      "loss": 0.0012,
      "step": 45100
    },
    {
      "epoch": 8201.818181818182,
      "grad_norm": 0.0021499572321772575,
      "learning_rate": 8.821982584717571e-07,
      "loss": 0.0009,
      "step": 45110
    },
    {
      "epoch": 8203.636363636364,
      "grad_norm": 0.2023916095495224,
      "learning_rate": 8.821232284694544e-07,
      "loss": 0.0015,
      "step": 45120
    },
    {
      "epoch": 8205.454545454546,
      "grad_norm": 0.0021275498438626528,
      "learning_rate": 8.820481777735713e-07,
      "loss": 0.0009,
      "step": 45130
    },
    {
      "epoch": 8207.272727272728,
      "grad_norm": 0.20864753425121307,
      "learning_rate": 8.819731063881716e-07,
      "loss": 0.0011,
      "step": 45140
    },
    {
      "epoch": 8209.09090909091,
      "grad_norm": 0.024809405207633972,
      "learning_rate": 8.818980143173212e-07,
      "loss": 0.0012,
      "step": 45150
    },
    {
      "epoch": 8210.90909090909,
      "grad_norm": 0.2816670835018158,
      "learning_rate": 8.818229015650861e-07,
      "loss": 0.0011,
      "step": 45160
    },
    {
      "epoch": 8212.727272727272,
      "grad_norm": 0.2622528076171875,
      "learning_rate": 8.817477681355344e-07,
      "loss": 0.001,
      "step": 45170
    },
    {
      "epoch": 8214.545454545454,
      "grad_norm": 0.2026357799768448,
      "learning_rate": 8.816726140327349e-07,
      "loss": 0.0011,
      "step": 45180
    },
    {
      "epoch": 8216.363636363636,
      "grad_norm": 0.0019175399793311954,
      "learning_rate": 8.815974392607572e-07,
      "loss": 0.0011,
      "step": 45190
    },
    {
      "epoch": 8218.181818181818,
      "grad_norm": 0.19369281828403473,
      "learning_rate": 8.815222438236725e-07,
      "loss": 0.0012,
      "step": 45200
    },
    {
      "epoch": 8220.0,
      "grad_norm": 0.0023649593349546194,
      "learning_rate": 8.81447027725553e-07,
      "loss": 0.0011,
      "step": 45210
    },
    {
      "epoch": 8221.818181818182,
      "grad_norm": 0.003052162704989314,
      "learning_rate": 8.813717909704721e-07,
      "loss": 0.0012,
      "step": 45220
    },
    {
      "epoch": 8223.636363636364,
      "grad_norm": 0.005358295980840921,
      "learning_rate": 8.812965335625037e-07,
      "loss": 0.0008,
      "step": 45230
    },
    {
      "epoch": 8225.454545454546,
      "grad_norm": 0.011438059620559216,
      "learning_rate": 8.812212555057239e-07,
      "loss": 0.0016,
      "step": 45240
    },
    {
      "epoch": 8227.272727272728,
      "grad_norm": 0.2448389083147049,
      "learning_rate": 8.81145956804209e-07,
      "loss": 0.0011,
      "step": 45250
    },
    {
      "epoch": 8229.09090909091,
      "grad_norm": 0.002820462454110384,
      "learning_rate": 8.810706374620367e-07,
      "loss": 0.0011,
      "step": 45260
    },
    {
      "epoch": 8230.90909090909,
      "grad_norm": 0.0038809983525425196,
      "learning_rate": 8.80995297483286e-07,
      "loss": 0.0012,
      "step": 45270
    },
    {
      "epoch": 8232.727272727272,
      "grad_norm": 0.1927432417869568,
      "learning_rate": 8.809199368720367e-07,
      "loss": 0.0012,
      "step": 45280
    },
    {
      "epoch": 8234.545454545454,
      "grad_norm": 0.2541736364364624,
      "learning_rate": 8.808445556323702e-07,
      "loss": 0.0008,
      "step": 45290
    },
    {
      "epoch": 8236.363636363636,
      "grad_norm": 0.0025638306979089975,
      "learning_rate": 8.807691537683684e-07,
      "loss": 0.001,
      "step": 45300
    },
    {
      "epoch": 8238.181818181818,
      "grad_norm": 0.2507443428039551,
      "learning_rate": 8.806937312841148e-07,
      "loss": 0.0014,
      "step": 45310
    },
    {
      "epoch": 8240.0,
      "grad_norm": 0.3275635540485382,
      "learning_rate": 8.806182881836936e-07,
      "loss": 0.0011,
      "step": 45320
    },
    {
      "epoch": 8241.818181818182,
      "grad_norm": 0.005509957205504179,
      "learning_rate": 8.805428244711909e-07,
      "loss": 0.0012,
      "step": 45330
    },
    {
      "epoch": 8243.636363636364,
      "grad_norm": 0.003654375672340393,
      "learning_rate": 8.804673401506928e-07,
      "loss": 0.0009,
      "step": 45340
    },
    {
      "epoch": 8245.454545454546,
      "grad_norm": 0.0017708012601360679,
      "learning_rate": 8.803918352262874e-07,
      "loss": 0.0011,
      "step": 45350
    },
    {
      "epoch": 8247.272727272728,
      "grad_norm": 0.2589191794395447,
      "learning_rate": 8.803163097020636e-07,
      "loss": 0.0015,
      "step": 45360
    },
    {
      "epoch": 8249.09090909091,
      "grad_norm": 0.0023473084438592196,
      "learning_rate": 8.802407635821113e-07,
      "loss": 0.0009,
      "step": 45370
    },
    {
      "epoch": 8250.90909090909,
      "grad_norm": 0.2252076417207718,
      "learning_rate": 8.801651968705217e-07,
      "loss": 0.0013,
      "step": 45380
    },
    {
      "epoch": 8252.727272727272,
      "grad_norm": 0.3207490146160126,
      "learning_rate": 8.80089609571387e-07,
      "loss": 0.0011,
      "step": 45390
    },
    {
      "epoch": 8254.545454545454,
      "grad_norm": 0.23745512962341309,
      "learning_rate": 8.800140016888009e-07,
      "loss": 0.0009,
      "step": 45400
    },
    {
      "epoch": 8256.363636363636,
      "grad_norm": 0.3473556935787201,
      "learning_rate": 8.799383732268573e-07,
      "loss": 0.0015,
      "step": 45410
    },
    {
      "epoch": 8258.181818181818,
      "grad_norm": 0.005436550825834274,
      "learning_rate": 8.798627241896524e-07,
      "loss": 0.0008,
      "step": 45420
    },
    {
      "epoch": 8260.0,
      "grad_norm": 0.20202460885047913,
      "learning_rate": 8.797870545812825e-07,
      "loss": 0.0012,
      "step": 45430
    },
    {
      "epoch": 8261.818181818182,
      "grad_norm": 0.32694804668426514,
      "learning_rate": 8.797113644058455e-07,
      "loss": 0.0012,
      "step": 45440
    },
    {
      "epoch": 8263.636363636364,
      "grad_norm": 0.18002425134181976,
      "learning_rate": 8.796356536674403e-07,
      "loss": 0.0009,
      "step": 45450
    },
    {
      "epoch": 8265.454545454546,
      "grad_norm": 0.01690477505326271,
      "learning_rate": 8.795599223701673e-07,
      "loss": 0.0015,
      "step": 45460
    },
    {
      "epoch": 8267.272727272728,
      "grad_norm": 0.1983034759759903,
      "learning_rate": 8.794841705181273e-07,
      "loss": 0.0008,
      "step": 45470
    },
    {
      "epoch": 8269.09090909091,
      "grad_norm": 0.2638850808143616,
      "learning_rate": 8.794083981154228e-07,
      "loss": 0.001,
      "step": 45480
    },
    {
      "epoch": 8270.90909090909,
      "grad_norm": 0.0018533356487751007,
      "learning_rate": 8.79332605166157e-07,
      "loss": 0.0012,
      "step": 45490
    },
    {
      "epoch": 8272.727272727272,
      "grad_norm": 0.0024343361146748066,
      "learning_rate": 8.792567916744345e-07,
      "loss": 0.0009,
      "step": 45500
    },
    {
      "epoch": 8272.727272727272,
      "eval_loss": 4.6505937576293945,
      "eval_runtime": 0.9559,
      "eval_samples_per_second": 10.461,
      "eval_steps_per_second": 5.231,
      "step": 45500
    },
    {
      "epoch": 8274.545454545454,
      "grad_norm": 0.26811084151268005,
      "learning_rate": 8.79180957644361e-07,
      "loss": 0.0017,
      "step": 45510
    },
    {
      "epoch": 8276.363636363636,
      "grad_norm": 0.2843434810638428,
      "learning_rate": 8.791051030800429e-07,
      "loss": 0.0008,
      "step": 45520
    },
    {
      "epoch": 8278.181818181818,
      "grad_norm": 0.001944183255545795,
      "learning_rate": 8.790292279855887e-07,
      "loss": 0.001,
      "step": 45530
    },
    {
      "epoch": 8280.0,
      "grad_norm": 0.0032934064511209726,
      "learning_rate": 8.789533323651067e-07,
      "loss": 0.0013,
      "step": 45540
    },
    {
      "epoch": 8281.818181818182,
      "grad_norm": 0.0023390338756144047,
      "learning_rate": 8.788774162227073e-07,
      "loss": 0.0011,
      "step": 45550
    },
    {
      "epoch": 8283.636363636364,
      "grad_norm": 0.20352566242218018,
      "learning_rate": 8.788014795625016e-07,
      "loss": 0.0011,
      "step": 45560
    },
    {
      "epoch": 8285.454545454546,
      "grad_norm": 0.012063114903867245,
      "learning_rate": 8.787255223886019e-07,
      "loss": 0.0012,
      "step": 45570
    },
    {
      "epoch": 8287.272727272728,
      "grad_norm": 0.002057882957160473,
      "learning_rate": 8.786495447051215e-07,
      "loss": 0.0009,
      "step": 45580
    },
    {
      "epoch": 8289.09090909091,
      "grad_norm": 0.02258327044546604,
      "learning_rate": 8.78573546516175e-07,
      "loss": 0.0015,
      "step": 45590
    },
    {
      "epoch": 8290.90909090909,
      "grad_norm": 0.0018512674141675234,
      "learning_rate": 8.784975278258782e-07,
      "loss": 0.0009,
      "step": 45600
    },
    {
      "epoch": 8292.727272727272,
      "grad_norm": 0.003943695221096277,
      "learning_rate": 8.784214886383475e-07,
      "loss": 0.001,
      "step": 45610
    },
    {
      "epoch": 8294.545454545454,
      "grad_norm": 0.0014250550884753466,
      "learning_rate": 8.78345428957701e-07,
      "loss": 0.0013,
      "step": 45620
    },
    {
      "epoch": 8296.363636363636,
      "grad_norm": 0.002648988040164113,
      "learning_rate": 8.782693487880574e-07,
      "loss": 0.0012,
      "step": 45630
    },
    {
      "epoch": 8298.181818181818,
      "grad_norm": 0.2686432898044586,
      "learning_rate": 8.781932481335373e-07,
      "loss": 0.0011,
      "step": 45640
    },
    {
      "epoch": 8300.0,
      "grad_norm": 0.0015441939467564225,
      "learning_rate": 8.78117126998261e-07,
      "loss": 0.0012,
      "step": 45650
    },
    {
      "epoch": 8301.818181818182,
      "grad_norm": 0.0018175783334299922,
      "learning_rate": 8.780409853863515e-07,
      "loss": 0.0009,
      "step": 45660
    },
    {
      "epoch": 8303.636363636364,
      "grad_norm": 0.18263481557369232,
      "learning_rate": 8.779648233019319e-07,
      "loss": 0.0012,
      "step": 45670
    },
    {
      "epoch": 8305.454545454546,
      "grad_norm": 0.00403933459892869,
      "learning_rate": 8.778886407491268e-07,
      "loss": 0.0012,
      "step": 45680
    },
    {
      "epoch": 8307.272727272728,
      "grad_norm": 0.2700030207633972,
      "learning_rate": 8.778124377320617e-07,
      "loss": 0.0012,
      "step": 45690
    },
    {
      "epoch": 8309.09090909091,
      "grad_norm": 0.33498379588127136,
      "learning_rate": 8.777362142548635e-07,
      "loss": 0.0012,
      "step": 45700
    },
    {
      "epoch": 8310.90909090909,
      "grad_norm": 0.28798964619636536,
      "learning_rate": 8.776599703216597e-07,
      "loss": 0.0011,
      "step": 45710
    },
    {
      "epoch": 8312.727272727272,
      "grad_norm": 0.005433795042335987,
      "learning_rate": 8.775837059365795e-07,
      "loss": 0.0008,
      "step": 45720
    },
    {
      "epoch": 8314.545454545454,
      "grad_norm": 0.0031372965313494205,
      "learning_rate": 8.775074211037528e-07,
      "loss": 0.0011,
      "step": 45730
    },
    {
      "epoch": 8316.363636363636,
      "grad_norm": 0.0019834842532873154,
      "learning_rate": 8.774311158273109e-07,
      "loss": 0.0012,
      "step": 45740
    },
    {
      "epoch": 8318.181818181818,
      "grad_norm": 0.00267048180103302,
      "learning_rate": 8.773547901113861e-07,
      "loss": 0.0012,
      "step": 45750
    },
    {
      "epoch": 8320.0,
      "grad_norm": 0.0027733033057302237,
      "learning_rate": 8.772784439601113e-07,
      "loss": 0.0012,
      "step": 45760
    },
    {
      "epoch": 8321.818181818182,
      "grad_norm": 0.0019782681483775377,
      "learning_rate": 8.772020773776214e-07,
      "loss": 0.0012,
      "step": 45770
    },
    {
      "epoch": 8323.636363636364,
      "grad_norm": 0.0013947958359494805,
      "learning_rate": 8.771256903680519e-07,
      "loss": 0.0008,
      "step": 45780
    },
    {
      "epoch": 8325.454545454546,
      "grad_norm": 0.2617763578891754,
      "learning_rate": 8.770492829355394e-07,
      "loss": 0.0014,
      "step": 45790
    },
    {
      "epoch": 8327.272727272728,
      "grad_norm": 0.21840469539165497,
      "learning_rate": 8.769728550842217e-07,
      "loss": 0.001,
      "step": 45800
    },
    {
      "epoch": 8329.09090909091,
      "grad_norm": 0.0011735419975593686,
      "learning_rate": 8.768964068182376e-07,
      "loss": 0.0011,
      "step": 45810
    },
    {
      "epoch": 8330.90909090909,
      "grad_norm": 0.20108763873577118,
      "learning_rate": 8.768199381417274e-07,
      "loss": 0.0012,
      "step": 45820
    },
    {
      "epoch": 8332.727272727272,
      "grad_norm": 0.24987219274044037,
      "learning_rate": 8.767434490588319e-07,
      "loss": 0.0011,
      "step": 45830
    },
    {
      "epoch": 8334.545454545454,
      "grad_norm": 0.25910520553588867,
      "learning_rate": 8.766669395736934e-07,
      "loss": 0.0011,
      "step": 45840
    },
    {
      "epoch": 8336.363636363636,
      "grad_norm": 0.19925987720489502,
      "learning_rate": 8.765904096904552e-07,
      "loss": 0.0017,
      "step": 45850
    },
    {
      "epoch": 8338.181818181818,
      "grad_norm": 0.3051336705684662,
      "learning_rate": 8.765138594132617e-07,
      "loss": 0.0008,
      "step": 45860
    },
    {
      "epoch": 8340.0,
      "grad_norm": 0.0031792963854968548,
      "learning_rate": 8.764372887462585e-07,
      "loss": 0.0009,
      "step": 45870
    },
    {
      "epoch": 8341.818181818182,
      "grad_norm": 0.27634716033935547,
      "learning_rate": 8.763606976935922e-07,
      "loss": 0.0012,
      "step": 45880
    },
    {
      "epoch": 8343.636363636364,
      "grad_norm": 0.2633899450302124,
      "learning_rate": 8.762840862594106e-07,
      "loss": 0.0011,
      "step": 45890
    },
    {
      "epoch": 8345.454545454546,
      "grad_norm": 0.32744720578193665,
      "learning_rate": 8.762074544478621e-07,
      "loss": 0.0011,
      "step": 45900
    },
    {
      "epoch": 8347.272727272728,
      "grad_norm": 0.0017324637155979872,
      "learning_rate": 8.761308022630972e-07,
      "loss": 0.0009,
      "step": 45910
    },
    {
      "epoch": 8349.09090909091,
      "grad_norm": 0.20385271310806274,
      "learning_rate": 8.760541297092668e-07,
      "loss": 0.0012,
      "step": 45920
    },
    {
      "epoch": 8350.90909090909,
      "grad_norm": 0.0025875107385218143,
      "learning_rate": 8.759774367905227e-07,
      "loss": 0.0012,
      "step": 45930
    },
    {
      "epoch": 8352.727272727272,
      "grad_norm": 0.004434104077517986,
      "learning_rate": 8.759007235110185e-07,
      "loss": 0.001,
      "step": 45940
    },
    {
      "epoch": 8354.545454545454,
      "grad_norm": 0.001766290282830596,
      "learning_rate": 8.758239898749084e-07,
      "loss": 0.0011,
      "step": 45950
    },
    {
      "epoch": 8356.363636363636,
      "grad_norm": 0.2500974237918854,
      "learning_rate": 8.757472358863481e-07,
      "loss": 0.0013,
      "step": 45960
    },
    {
      "epoch": 8358.181818181818,
      "grad_norm": 0.0029903650283813477,
      "learning_rate": 8.756704615494936e-07,
      "loss": 0.001,
      "step": 45970
    },
    {
      "epoch": 8360.0,
      "grad_norm": 0.24102210998535156,
      "learning_rate": 8.755936668685031e-07,
      "loss": 0.0013,
      "step": 45980
    },
    {
      "epoch": 8361.818181818182,
      "grad_norm": 0.27721303701400757,
      "learning_rate": 8.755168518475351e-07,
      "loss": 0.0012,
      "step": 45990
    },
    {
      "epoch": 8363.636363636364,
      "grad_norm": 0.3289535343647003,
      "learning_rate": 8.754400164907496e-07,
      "loss": 0.0012,
      "step": 46000
    },
    {
      "epoch": 8363.636363636364,
      "eval_loss": 4.703945159912109,
      "eval_runtime": 0.9495,
      "eval_samples_per_second": 10.531,
      "eval_steps_per_second": 5.266,
      "step": 46000
    },
    {
      "epoch": 8365.454545454546,
      "grad_norm": 0.3045842945575714,
      "learning_rate": 8.753631608023073e-07,
      "loss": 0.0009,
      "step": 46010
    },
    {
      "epoch": 8367.272727272728,
      "grad_norm": 0.023892274126410484,
      "learning_rate": 8.752862847863706e-07,
      "loss": 0.0012,
      "step": 46020
    },
    {
      "epoch": 8369.09090909091,
      "grad_norm": 0.19320131838321686,
      "learning_rate": 8.752093884471024e-07,
      "loss": 0.0009,
      "step": 46030
    },
    {
      "epoch": 8370.90909090909,
      "grad_norm": 0.20487962663173676,
      "learning_rate": 8.75132471788667e-07,
      "loss": 0.0012,
      "step": 46040
    },
    {
      "epoch": 8372.727272727272,
      "grad_norm": 0.19140496850013733,
      "learning_rate": 8.750555348152298e-07,
      "loss": 0.0011,
      "step": 46050
    },
    {
      "epoch": 8374.545454545454,
      "grad_norm": 0.0017150616040453315,
      "learning_rate": 8.749785775309573e-07,
      "loss": 0.0012,
      "step": 46060
    },
    {
      "epoch": 8376.363636363636,
      "grad_norm": 0.28331103920936584,
      "learning_rate": 8.749015999400169e-07,
      "loss": 0.0011,
      "step": 46070
    },
    {
      "epoch": 8378.181818181818,
      "grad_norm": 0.24701091647148132,
      "learning_rate": 8.748246020465775e-07,
      "loss": 0.001,
      "step": 46080
    },
    {
      "epoch": 8380.0,
      "grad_norm": 0.004417839460074902,
      "learning_rate": 8.747475838548087e-07,
      "loss": 0.0011,
      "step": 46090
    },
    {
      "epoch": 8381.818181818182,
      "grad_norm": 0.2727690637111664,
      "learning_rate": 8.746705453688814e-07,
      "loss": 0.0012,
      "step": 46100
    },
    {
      "epoch": 8383.636363636364,
      "grad_norm": 0.2581556737422943,
      "learning_rate": 8.745934865929676e-07,
      "loss": 0.0009,
      "step": 46110
    },
    {
      "epoch": 8385.454545454546,
      "grad_norm": 0.24187184870243073,
      "learning_rate": 8.745164075312404e-07,
      "loss": 0.0014,
      "step": 46120
    },
    {
      "epoch": 8387.272727272728,
      "grad_norm": 0.24621203541755676,
      "learning_rate": 8.744393081878737e-07,
      "loss": 0.0011,
      "step": 46130
    },
    {
      "epoch": 8389.09090909091,
      "grad_norm": 0.005616624373942614,
      "learning_rate": 8.74362188567043e-07,
      "loss": 0.0009,
      "step": 46140
    },
    {
      "epoch": 8390.90909090909,
      "grad_norm": 0.00247904728166759,
      "learning_rate": 8.742850486729246e-07,
      "loss": 0.0012,
      "step": 46150
    },
    {
      "epoch": 8392.727272727272,
      "grad_norm": 0.24653169512748718,
      "learning_rate": 8.74207888509696e-07,
      "loss": 0.001,
      "step": 46160
    },
    {
      "epoch": 8394.545454545454,
      "grad_norm": 0.0030087463092058897,
      "learning_rate": 8.741307080815355e-07,
      "loss": 0.0011,
      "step": 46170
    },
    {
      "epoch": 8396.363636363636,
      "grad_norm": 0.2617381513118744,
      "learning_rate": 8.740535073926231e-07,
      "loss": 0.001,
      "step": 46180
    },
    {
      "epoch": 8398.181818181818,
      "grad_norm": 0.19507457315921783,
      "learning_rate": 8.739762864471391e-07,
      "loss": 0.0012,
      "step": 46190
    },
    {
      "epoch": 8400.0,
      "grad_norm": 0.003289284650236368,
      "learning_rate": 8.73899045249266e-07,
      "loss": 0.001,
      "step": 46200
    },
    {
      "epoch": 8401.818181818182,
      "grad_norm": 0.2625592350959778,
      "learning_rate": 8.73821783803186e-07,
      "loss": 0.0012,
      "step": 46210
    },
    {
      "epoch": 8403.636363636364,
      "grad_norm": 0.0017856683116406202,
      "learning_rate": 8.737445021130836e-07,
      "loss": 0.0013,
      "step": 46220
    },
    {
      "epoch": 8405.454545454546,
      "grad_norm": 0.26186999678611755,
      "learning_rate": 8.736672001831438e-07,
      "loss": 0.0011,
      "step": 46230
    },
    {
      "epoch": 8407.272727272728,
      "grad_norm": 0.002860651584342122,
      "learning_rate": 8.735898780175528e-07,
      "loss": 0.0011,
      "step": 46240
    },
    {
      "epoch": 8409.09090909091,
      "grad_norm": 0.07093077152967453,
      "learning_rate": 8.735125356204979e-07,
      "loss": 0.0012,
      "step": 46250
    },
    {
      "epoch": 8410.90909090909,
      "grad_norm": 0.002606314606964588,
      "learning_rate": 8.734351729961678e-07,
      "loss": 0.0013,
      "step": 46260
    },
    {
      "epoch": 8412.727272727272,
      "grad_norm": 0.2222486287355423,
      "learning_rate": 8.733577901487514e-07,
      "loss": 0.001,
      "step": 46270
    },
    {
      "epoch": 8414.545454545454,
      "grad_norm": 0.001587270642630756,
      "learning_rate": 8.732803870824399e-07,
      "loss": 0.0011,
      "step": 46280
    },
    {
      "epoch": 8416.363636363636,
      "grad_norm": 0.19303350150585175,
      "learning_rate": 8.732029638014248e-07,
      "loss": 0.0012,
      "step": 46290
    },
    {
      "epoch": 8418.181818181818,
      "grad_norm": 0.18210987746715546,
      "learning_rate": 8.731255203098988e-07,
      "loss": 0.0012,
      "step": 46300
    },
    {
      "epoch": 8420.0,
      "grad_norm": 0.21901488304138184,
      "learning_rate": 8.730480566120559e-07,
      "loss": 0.0012,
      "step": 46310
    },
    {
      "epoch": 8421.818181818182,
      "grad_norm": 0.2973090708255768,
      "learning_rate": 8.729705727120911e-07,
      "loss": 0.0014,
      "step": 46320
    },
    {
      "epoch": 8423.636363636364,
      "grad_norm": 0.001781557803042233,
      "learning_rate": 8.728930686142003e-07,
      "loss": 0.0011,
      "step": 46330
    },
    {
      "epoch": 8425.454545454546,
      "grad_norm": 0.3309658467769623,
      "learning_rate": 8.728155443225808e-07,
      "loss": 0.0012,
      "step": 46340
    },
    {
      "epoch": 8427.272727272728,
      "grad_norm": 0.33078762888908386,
      "learning_rate": 8.72737999841431e-07,
      "loss": 0.0013,
      "step": 46350
    },
    {
      "epoch": 8429.09090909091,
      "grad_norm": 0.04329795017838478,
      "learning_rate": 8.726604351749502e-07,
      "loss": 0.0009,
      "step": 46360
    },
    {
      "epoch": 8430.90909090909,
      "grad_norm": 0.2002364844083786,
      "learning_rate": 8.725828503273387e-07,
      "loss": 0.0011,
      "step": 46370
    },
    {
      "epoch": 8432.727272727272,
      "grad_norm": 0.001594673260115087,
      "learning_rate": 8.725052453027981e-07,
      "loss": 0.0013,
      "step": 46380
    },
    {
      "epoch": 8434.545454545454,
      "grad_norm": 0.25798383355140686,
      "learning_rate": 8.724276201055309e-07,
      "loss": 0.001,
      "step": 46390
    },
    {
      "epoch": 8436.363636363636,
      "grad_norm": 0.17543438076972961,
      "learning_rate": 8.723499747397414e-07,
      "loss": 0.0012,
      "step": 46400
    },
    {
      "epoch": 8438.181818181818,
      "grad_norm": 0.0019224543357267976,
      "learning_rate": 8.722723092096337e-07,
      "loss": 0.0009,
      "step": 46410
    },
    {
      "epoch": 8440.0,
      "grad_norm": 0.0021701513323932886,
      "learning_rate": 8.721946235194142e-07,
      "loss": 0.0012,
      "step": 46420
    },
    {
      "epoch": 8441.818181818182,
      "grad_norm": 0.1870986968278885,
      "learning_rate": 8.721169176732897e-07,
      "loss": 0.0012,
      "step": 46430
    },
    {
      "epoch": 8443.636363636364,
      "grad_norm": 0.18095700442790985,
      "learning_rate": 8.720391916754683e-07,
      "loss": 0.0009,
      "step": 46440
    },
    {
      "epoch": 8445.454545454546,
      "grad_norm": 0.0015410856576636434,
      "learning_rate": 8.719614455301592e-07,
      "loss": 0.0012,
      "step": 46450
    },
    {
      "epoch": 8447.272727272728,
      "grad_norm": 0.20832276344299316,
      "learning_rate": 8.718836792415728e-07,
      "loss": 0.001,
      "step": 46460
    },
    {
      "epoch": 8449.09090909091,
      "grad_norm": 0.006226901896297932,
      "learning_rate": 8.718058928139204e-07,
      "loss": 0.001,
      "step": 46470
    },
    {
      "epoch": 8450.90909090909,
      "grad_norm": 0.2735622525215149,
      "learning_rate": 8.717280862514144e-07,
      "loss": 0.0012,
      "step": 46480
    },
    {
      "epoch": 8452.727272727272,
      "grad_norm": 0.1805354356765747,
      "learning_rate": 8.716502595582684e-07,
      "loss": 0.001,
      "step": 46490
    },
    {
      "epoch": 8454.545454545454,
      "grad_norm": 0.0026715800631791353,
      "learning_rate": 8.71572412738697e-07,
      "loss": 0.0008,
      "step": 46500
    },
    {
      "epoch": 8454.545454545454,
      "eval_loss": 4.722527027130127,
      "eval_runtime": 0.9517,
      "eval_samples_per_second": 10.507,
      "eval_steps_per_second": 5.254,
      "step": 46500
    },
    {
      "epoch": 8456.363636363636,
      "grad_norm": 0.3480706214904785,
      "learning_rate": 8.714945457969161e-07,
      "loss": 0.0015,
      "step": 46510
    },
    {
      "epoch": 8458.181818181818,
      "grad_norm": 0.003001544624567032,
      "learning_rate": 8.714166587371423e-07,
      "loss": 0.0009,
      "step": 46520
    },
    {
      "epoch": 8460.0,
      "grad_norm": 0.002071134513244033,
      "learning_rate": 8.713387515635937e-07,
      "loss": 0.0012,
      "step": 46530
    },
    {
      "epoch": 8461.818181818182,
      "grad_norm": 0.001647575874812901,
      "learning_rate": 8.712608242804891e-07,
      "loss": 0.0012,
      "step": 46540
    },
    {
      "epoch": 8463.636363636364,
      "grad_norm": 0.18933147192001343,
      "learning_rate": 8.711828768920489e-07,
      "loss": 0.0008,
      "step": 46550
    },
    {
      "epoch": 8465.454545454546,
      "grad_norm": 0.273510217666626,
      "learning_rate": 8.71104909402494e-07,
      "loss": 0.0016,
      "step": 46560
    },
    {
      "epoch": 8467.272727272728,
      "grad_norm": 0.3207581043243408,
      "learning_rate": 8.710269218160466e-07,
      "loss": 0.0009,
      "step": 46570
    },
    {
      "epoch": 8469.09090909091,
      "grad_norm": 0.20102429389953613,
      "learning_rate": 8.709489141369304e-07,
      "loss": 0.0011,
      "step": 46580
    },
    {
      "epoch": 8470.90909090909,
      "grad_norm": 0.0011232977267354727,
      "learning_rate": 8.708708863693695e-07,
      "loss": 0.0011,
      "step": 46590
    },
    {
      "epoch": 8472.727272727272,
      "grad_norm": 0.0017196096014231443,
      "learning_rate": 8.707928385175898e-07,
      "loss": 0.0011,
      "step": 46600
    },
    {
      "epoch": 8474.545454545454,
      "grad_norm": 0.27719148993492126,
      "learning_rate": 8.707147705858175e-07,
      "loss": 0.001,
      "step": 46610
    },
    {
      "epoch": 8476.363636363636,
      "grad_norm": 0.24630829691886902,
      "learning_rate": 8.706366825782805e-07,
      "loss": 0.0014,
      "step": 46620
    },
    {
      "epoch": 8478.181818181818,
      "grad_norm": 0.00230311113409698,
      "learning_rate": 8.705585744992077e-07,
      "loss": 0.0009,
      "step": 46630
    },
    {
      "epoch": 8480.0,
      "grad_norm": 0.0021057703997939825,
      "learning_rate": 8.704804463528289e-07,
      "loss": 0.0012,
      "step": 46640
    },
    {
      "epoch": 8481.818181818182,
      "grad_norm": 0.009877469390630722,
      "learning_rate": 8.70402298143375e-07,
      "loss": 0.0011,
      "step": 46650
    },
    {
      "epoch": 8483.636363636364,
      "grad_norm": 0.0021586359944194555,
      "learning_rate": 8.703241298750781e-07,
      "loss": 0.0012,
      "step": 46660
    },
    {
      "epoch": 8485.454545454546,
      "grad_norm": 0.21424028277397156,
      "learning_rate": 8.702459415521713e-07,
      "loss": 0.001,
      "step": 46670
    },
    {
      "epoch": 8487.272727272728,
      "grad_norm": 0.0019669667817652225,
      "learning_rate": 8.70167733178889e-07,
      "loss": 0.0015,
      "step": 46680
    },
    {
      "epoch": 8489.09090909091,
      "grad_norm": 0.18905547261238098,
      "learning_rate": 8.700895047594663e-07,
      "loss": 0.0011,
      "step": 46690
    },
    {
      "epoch": 8490.90909090909,
      "grad_norm": 0.26331275701522827,
      "learning_rate": 8.700112562981397e-07,
      "loss": 0.0009,
      "step": 46700
    },
    {
      "epoch": 8492.727272727272,
      "grad_norm": 0.0025262448471039534,
      "learning_rate": 8.699329877991467e-07,
      "loss": 0.0012,
      "step": 46710
    },
    {
      "epoch": 8494.545454545454,
      "grad_norm": 0.3285536468029022,
      "learning_rate": 8.698546992667258e-07,
      "loss": 0.0014,
      "step": 46720
    },
    {
      "epoch": 8496.363636363636,
      "grad_norm": 0.3399118483066559,
      "learning_rate": 8.697763907051167e-07,
      "loss": 0.0011,
      "step": 46730
    },
    {
      "epoch": 8498.181818181818,
      "grad_norm": 0.26482585072517395,
      "learning_rate": 8.696980621185601e-07,
      "loss": 0.0009,
      "step": 46740
    },
    {
      "epoch": 8500.0,
      "grad_norm": 0.0022752827499061823,
      "learning_rate": 8.696197135112979e-07,
      "loss": 0.0011,
      "step": 46750
    },
    {
      "epoch": 8501.818181818182,
      "grad_norm": 0.0037017641589045525,
      "learning_rate": 8.69541344887573e-07,
      "loss": 0.0011,
      "step": 46760
    },
    {
      "epoch": 8503.636363636364,
      "grad_norm": 0.0012447218177840114,
      "learning_rate": 8.694629562516293e-07,
      "loss": 0.0012,
      "step": 46770
    },
    {
      "epoch": 8505.454545454546,
      "grad_norm": 0.19139859080314636,
      "learning_rate": 8.69384547607712e-07,
      "loss": 0.0009,
      "step": 46780
    },
    {
      "epoch": 8507.272727272728,
      "grad_norm": 0.0020813799928873777,
      "learning_rate": 8.69306118960067e-07,
      "loss": 0.001,
      "step": 46790
    },
    {
      "epoch": 8509.09090909091,
      "grad_norm": 0.0026889382861554623,
      "learning_rate": 8.69227670312942e-07,
      "loss": 0.0012,
      "step": 46800
    },
    {
      "epoch": 8510.90909090909,
      "grad_norm": 0.0029872353188693523,
      "learning_rate": 8.691492016705848e-07,
      "loss": 0.0012,
      "step": 46810
    },
    {
      "epoch": 8512.727272727272,
      "grad_norm": 0.0018470140639692545,
      "learning_rate": 8.690707130372451e-07,
      "loss": 0.0011,
      "step": 46820
    },
    {
      "epoch": 8514.545454545454,
      "grad_norm": 0.35880669951438904,
      "learning_rate": 8.689922044171733e-07,
      "loss": 0.0012,
      "step": 46830
    },
    {
      "epoch": 8516.363636363636,
      "grad_norm": 0.002306561451405287,
      "learning_rate": 8.689136758146211e-07,
      "loss": 0.0011,
      "step": 46840
    },
    {
      "epoch": 8518.181818181818,
      "grad_norm": 0.3443601429462433,
      "learning_rate": 8.68835127233841e-07,
      "loss": 0.0013,
      "step": 46850
    },
    {
      "epoch": 8520.0,
      "grad_norm": 0.002154948888346553,
      "learning_rate": 8.68756558679087e-07,
      "loss": 0.0009,
      "step": 46860
    },
    {
      "epoch": 8521.818181818182,
      "grad_norm": 0.0031071046832948923,
      "learning_rate": 8.686779701546134e-07,
      "loss": 0.001,
      "step": 46870
    },
    {
      "epoch": 8523.636363636364,
      "grad_norm": 0.19385619461536407,
      "learning_rate": 8.685993616646768e-07,
      "loss": 0.0012,
      "step": 46880
    },
    {
      "epoch": 8525.454545454546,
      "grad_norm": 0.19187219440937042,
      "learning_rate": 8.685207332135336e-07,
      "loss": 0.0009,
      "step": 46890
    },
    {
      "epoch": 8527.272727272728,
      "grad_norm": 0.25740933418273926,
      "learning_rate": 8.684420848054419e-07,
      "loss": 0.0014,
      "step": 46900
    },
    {
      "epoch": 8529.09090909091,
      "grad_norm": 0.2267729490995407,
      "learning_rate": 8.683634164446613e-07,
      "loss": 0.001,
      "step": 46910
    },
    {
      "epoch": 8530.90909090909,
      "grad_norm": 0.20391623675823212,
      "learning_rate": 8.682847281354516e-07,
      "loss": 0.0009,
      "step": 46920
    },
    {
      "epoch": 8532.727272727272,
      "grad_norm": 0.3166050910949707,
      "learning_rate": 8.682060198820742e-07,
      "loss": 0.0013,
      "step": 46930
    },
    {
      "epoch": 8534.545454545454,
      "grad_norm": 0.25953933596611023,
      "learning_rate": 8.681272916887916e-07,
      "loss": 0.0007,
      "step": 46940
    },
    {
      "epoch": 8536.363636363636,
      "grad_norm": 0.3087429106235504,
      "learning_rate": 8.680485435598672e-07,
      "loss": 0.0017,
      "step": 46950
    },
    {
      "epoch": 8538.181818181818,
      "grad_norm": 0.002331931609660387,
      "learning_rate": 8.679697754995654e-07,
      "loss": 0.0008,
      "step": 46960
    },
    {
      "epoch": 8540.0,
      "grad_norm": 0.18918044865131378,
      "learning_rate": 8.67890987512152e-07,
      "loss": 0.0012,
      "step": 46970
    },
    {
      "epoch": 8541.818181818182,
      "grad_norm": 0.001995907397940755,
      "learning_rate": 8.678121796018937e-07,
      "loss": 0.0012,
      "step": 46980
    },
    {
      "epoch": 8543.636363636364,
      "grad_norm": 0.003258822485804558,
      "learning_rate": 8.677333517730581e-07,
      "loss": 0.0009,
      "step": 46990
    },
    {
      "epoch": 8545.454545454546,
      "grad_norm": 0.20158545672893524,
      "learning_rate": 8.676545040299143e-07,
      "loss": 0.0011,
      "step": 47000
    },
    {
      "epoch": 8545.454545454546,
      "eval_loss": 4.751254081726074,
      "eval_runtime": 0.9533,
      "eval_samples_per_second": 10.49,
      "eval_steps_per_second": 5.245,
      "step": 47000
    },
    {
      "epoch": 8547.272727272728,
      "grad_norm": 0.2604960501194,
      "learning_rate": 8.675756363767322e-07,
      "loss": 0.0013,
      "step": 47010
    },
    {
      "epoch": 8549.09090909091,
      "grad_norm": 0.23725029826164246,
      "learning_rate": 8.674967488177825e-07,
      "loss": 0.0012,
      "step": 47020
    },
    {
      "epoch": 8550.90909090909,
      "grad_norm": 0.0018689996795728803,
      "learning_rate": 8.674178413573377e-07,
      "loss": 0.001,
      "step": 47030
    },
    {
      "epoch": 8552.727272727272,
      "grad_norm": 0.0031266401056200266,
      "learning_rate": 8.673389139996708e-07,
      "loss": 0.0012,
      "step": 47040
    },
    {
      "epoch": 8554.545454545454,
      "grad_norm": 0.19314974546432495,
      "learning_rate": 8.672599667490559e-07,
      "loss": 0.0013,
      "step": 47050
    },
    {
      "epoch": 8556.363636363636,
      "grad_norm": 0.3455931544303894,
      "learning_rate": 8.671809996097686e-07,
      "loss": 0.0011,
      "step": 47060
    },
    {
      "epoch": 8558.181818181818,
      "grad_norm": 0.0017718760063871741,
      "learning_rate": 8.671020125860851e-07,
      "loss": 0.0009,
      "step": 47070
    },
    {
      "epoch": 8560.0,
      "grad_norm": 0.0025321829598397017,
      "learning_rate": 8.67023005682283e-07,
      "loss": 0.0012,
      "step": 47080
    },
    {
      "epoch": 8561.818181818182,
      "grad_norm": 0.25597983598709106,
      "learning_rate": 8.669439789026409e-07,
      "loss": 0.0012,
      "step": 47090
    },
    {
      "epoch": 8563.636363636364,
      "grad_norm": 0.002964245155453682,
      "learning_rate": 8.668649322514381e-07,
      "loss": 0.0009,
      "step": 47100
    },
    {
      "epoch": 8565.454545454546,
      "grad_norm": 0.0020373561419546604,
      "learning_rate": 8.667858657329558e-07,
      "loss": 0.0014,
      "step": 47110
    },
    {
      "epoch": 8567.272727272728,
      "grad_norm": 0.0019962915685027838,
      "learning_rate": 8.667067793514754e-07,
      "loss": 0.001,
      "step": 47120
    },
    {
      "epoch": 8569.09090909091,
      "grad_norm": 0.005638815928250551,
      "learning_rate": 8.6662767311128e-07,
      "loss": 0.0012,
      "step": 47130
    },
    {
      "epoch": 8570.90909090909,
      "grad_norm": 0.20335820317268372,
      "learning_rate": 8.665485470166533e-07,
      "loss": 0.0011,
      "step": 47140
    },
    {
      "epoch": 8572.727272727272,
      "grad_norm": 0.004036801401525736,
      "learning_rate": 8.664694010718808e-07,
      "loss": 0.0009,
      "step": 47150
    },
    {
      "epoch": 8574.545454545454,
      "grad_norm": 0.0027957891579717398,
      "learning_rate": 8.663902352812478e-07,
      "loss": 0.0014,
      "step": 47160
    },
    {
      "epoch": 8576.363636363636,
      "grad_norm": 0.0026515282224863768,
      "learning_rate": 8.66311049649042e-07,
      "loss": 0.0012,
      "step": 47170
    },
    {
      "epoch": 8578.181818181818,
      "grad_norm": 0.23183681070804596,
      "learning_rate": 8.662318441795517e-07,
      "loss": 0.0011,
      "step": 47180
    },
    {
      "epoch": 8580.0,
      "grad_norm": 0.00649773795157671,
      "learning_rate": 8.661526188770658e-07,
      "loss": 0.0011,
      "step": 47190
    },
    {
      "epoch": 8581.818181818182,
      "grad_norm": 0.3299068808555603,
      "learning_rate": 8.660733737458751e-07,
      "loss": 0.0012,
      "step": 47200
    },
    {
      "epoch": 8583.636363636364,
      "grad_norm": 0.0025133155286312103,
      "learning_rate": 8.659941087902708e-07,
      "loss": 0.0009,
      "step": 47210
    },
    {
      "epoch": 8585.454545454546,
      "grad_norm": 0.1715032458305359,
      "learning_rate": 8.659148240145455e-07,
      "loss": 0.0013,
      "step": 47220
    },
    {
      "epoch": 8587.272727272728,
      "grad_norm": 0.0017457444919273257,
      "learning_rate": 8.658355194229929e-07,
      "loss": 0.0011,
      "step": 47230
    },
    {
      "epoch": 8589.09090909091,
      "grad_norm": 0.3239321708679199,
      "learning_rate": 8.657561950199075e-07,
      "loss": 0.0011,
      "step": 47240
    },
    {
      "epoch": 8590.90909090909,
      "grad_norm": 0.015546480193734169,
      "learning_rate": 8.656768508095852e-07,
      "loss": 0.0012,
      "step": 47250
    },
    {
      "epoch": 8592.727272727272,
      "grad_norm": 0.1937250792980194,
      "learning_rate": 8.655974867963227e-07,
      "loss": 0.0012,
      "step": 47260
    },
    {
      "epoch": 8594.545454545454,
      "grad_norm": 0.0019481092458590865,
      "learning_rate": 8.655181029844181e-07,
      "loss": 0.0013,
      "step": 47270
    },
    {
      "epoch": 8596.363636363636,
      "grad_norm": 0.22099316120147705,
      "learning_rate": 8.654386993781701e-07,
      "loss": 0.0012,
      "step": 47280
    },
    {
      "epoch": 8598.181818181818,
      "grad_norm": 0.0015468867495656013,
      "learning_rate": 8.653592759818789e-07,
      "loss": 0.0006,
      "step": 47290
    },
    {
      "epoch": 8600.0,
      "grad_norm": 0.1863093376159668,
      "learning_rate": 8.652798327998456e-07,
      "loss": 0.0012,
      "step": 47300
    },
    {
      "epoch": 8601.818181818182,
      "grad_norm": 0.2734794020652771,
      "learning_rate": 8.652003698363724e-07,
      "loss": 0.001,
      "step": 47310
    },
    {
      "epoch": 8603.636363636364,
      "grad_norm": 0.19329312443733215,
      "learning_rate": 8.651208870957622e-07,
      "loss": 0.0011,
      "step": 47320
    },
    {
      "epoch": 8605.454545454546,
      "grad_norm": 0.014767940156161785,
      "learning_rate": 8.6504138458232e-07,
      "loss": 0.0013,
      "step": 47330
    },
    {
      "epoch": 8607.272727272728,
      "grad_norm": 0.0027963172178715467,
      "learning_rate": 8.649618623003507e-07,
      "loss": 0.0009,
      "step": 47340
    },
    {
      "epoch": 8609.09090909091,
      "grad_norm": 0.2622692883014679,
      "learning_rate": 8.648823202541608e-07,
      "loss": 0.0014,
      "step": 47350
    },
    {
      "epoch": 8610.90909090909,
      "grad_norm": 0.22506731748580933,
      "learning_rate": 8.648027584480582e-07,
      "loss": 0.0009,
      "step": 47360
    },
    {
      "epoch": 8612.727272727272,
      "grad_norm": 0.19639280438423157,
      "learning_rate": 8.647231768863511e-07,
      "loss": 0.001,
      "step": 47370
    },
    {
      "epoch": 8614.545454545454,
      "grad_norm": 0.30275270342826843,
      "learning_rate": 8.646435755733493e-07,
      "loss": 0.0016,
      "step": 47380
    },
    {
      "epoch": 8616.363636363636,
      "grad_norm": 0.32760024070739746,
      "learning_rate": 8.645639545133636e-07,
      "loss": 0.0011,
      "step": 47390
    },
    {
      "epoch": 8618.181818181818,
      "grad_norm": 0.2593950033187866,
      "learning_rate": 8.644843137107057e-07,
      "loss": 0.0011,
      "step": 47400
    },
    {
      "epoch": 8620.0,
      "grad_norm": 0.30220016837120056,
      "learning_rate": 8.644046531696887e-07,
      "loss": 0.001,
      "step": 47410
    },
    {
      "epoch": 8621.818181818182,
      "grad_norm": 0.17293183505535126,
      "learning_rate": 8.643249728946262e-07,
      "loss": 0.0011,
      "step": 47420
    },
    {
      "epoch": 8623.636363636364,
      "grad_norm": 0.3079076111316681,
      "learning_rate": 8.642452728898338e-07,
      "loss": 0.0012,
      "step": 47430
    },
    {
      "epoch": 8625.454545454546,
      "grad_norm": 0.2711491882801056,
      "learning_rate": 8.64165553159627e-07,
      "loss": 0.0012,
      "step": 47440
    },
    {
      "epoch": 8627.272727272728,
      "grad_norm": 0.004611824173480272,
      "learning_rate": 8.640858137083232e-07,
      "loss": 0.0008,
      "step": 47450
    },
    {
      "epoch": 8629.09090909091,
      "grad_norm": 0.0016822197940200567,
      "learning_rate": 8.640060545402405e-07,
      "loss": 0.0012,
      "step": 47460
    },
    {
      "epoch": 8630.90909090909,
      "grad_norm": 0.2565365433692932,
      "learning_rate": 8.639262756596984e-07,
      "loss": 0.0012,
      "step": 47470
    },
    {
      "epoch": 8632.727272727272,
      "grad_norm": 0.0014039864763617516,
      "learning_rate": 8.638464770710173e-07,
      "loss": 0.0009,
      "step": 47480
    },
    {
      "epoch": 8634.545454545454,
      "grad_norm": 0.001800184603780508,
      "learning_rate": 8.637666587785184e-07,
      "loss": 0.0014,
      "step": 47490
    },
    {
      "epoch": 8636.363636363636,
      "grad_norm": 0.002457232214510441,
      "learning_rate": 8.636868207865243e-07,
      "loss": 0.0009,
      "step": 47500
    },
    {
      "epoch": 8636.363636363636,
      "eval_loss": 4.755159854888916,
      "eval_runtime": 0.9494,
      "eval_samples_per_second": 10.533,
      "eval_steps_per_second": 5.266,
      "step": 47500
    },
    {
      "epoch": 8638.181818181818,
      "grad_norm": 0.003146311268210411,
      "learning_rate": 8.636069630993586e-07,
      "loss": 0.001,
      "step": 47510
    },
    {
      "epoch": 8640.0,
      "grad_norm": 0.04221002012491226,
      "learning_rate": 8.635270857213459e-07,
      "loss": 0.0014,
      "step": 47520
    },
    {
      "epoch": 8641.818181818182,
      "grad_norm": 0.18878956139087677,
      "learning_rate": 8.634471886568117e-07,
      "loss": 0.0012,
      "step": 47530
    },
    {
      "epoch": 8643.636363636364,
      "grad_norm": 0.17902898788452148,
      "learning_rate": 8.633672719100833e-07,
      "loss": 0.0009,
      "step": 47540
    },
    {
      "epoch": 8645.454545454546,
      "grad_norm": 0.1827201545238495,
      "learning_rate": 8.632873354854879e-07,
      "loss": 0.0014,
      "step": 47550
    },
    {
      "epoch": 8647.272727272728,
      "grad_norm": 0.003157400293275714,
      "learning_rate": 8.632073793873548e-07,
      "loss": 0.0008,
      "step": 47560
    },
    {
      "epoch": 8649.09090909091,
      "grad_norm": 0.27365270256996155,
      "learning_rate": 8.631274036200137e-07,
      "loss": 0.0012,
      "step": 47570
    },
    {
      "epoch": 8650.90909090909,
      "grad_norm": 0.0019483575597405434,
      "learning_rate": 8.630474081877958e-07,
      "loss": 0.0012,
      "step": 47580
    },
    {
      "epoch": 8652.727272727272,
      "grad_norm": 0.16745023429393768,
      "learning_rate": 8.629673930950334e-07,
      "loss": 0.0012,
      "step": 47590
    },
    {
      "epoch": 8654.545454545454,
      "grad_norm": 0.3144018054008484,
      "learning_rate": 8.628873583460591e-07,
      "loss": 0.001,
      "step": 47600
    },
    {
      "epoch": 8656.363636363636,
      "grad_norm": 0.18454550206661224,
      "learning_rate": 8.628073039452075e-07,
      "loss": 0.0011,
      "step": 47610
    },
    {
      "epoch": 8658.181818181818,
      "grad_norm": 0.026612553745508194,
      "learning_rate": 8.627272298968138e-07,
      "loss": 0.0012,
      "step": 47620
    },
    {
      "epoch": 8660.0,
      "grad_norm": 0.003220924874767661,
      "learning_rate": 8.626471362052144e-07,
      "loss": 0.001,
      "step": 47630
    },
    {
      "epoch": 8661.818181818182,
      "grad_norm": 0.2705078125,
      "learning_rate": 8.625670228747466e-07,
      "loss": 0.0012,
      "step": 47640
    },
    {
      "epoch": 8663.636363636364,
      "grad_norm": 0.34195688366889954,
      "learning_rate": 8.62486889909749e-07,
      "loss": 0.0012,
      "step": 47650
    },
    {
      "epoch": 8665.454545454546,
      "grad_norm": 0.012953383848071098,
      "learning_rate": 8.624067373145609e-07,
      "loss": 0.001,
      "step": 47660
    },
    {
      "epoch": 8667.272727272728,
      "grad_norm": 0.2602846324443817,
      "learning_rate": 8.623265650935233e-07,
      "loss": 0.001,
      "step": 47670
    },
    {
      "epoch": 8669.09090909091,
      "grad_norm": 0.24658899009227753,
      "learning_rate": 8.622463732509775e-07,
      "loss": 0.0012,
      "step": 47680
    },
    {
      "epoch": 8670.90909090909,
      "grad_norm": 0.3322558104991913,
      "learning_rate": 8.621661617912664e-07,
      "loss": 0.001,
      "step": 47690
    },
    {
      "epoch": 8672.727272727272,
      "grad_norm": 0.34653449058532715,
      "learning_rate": 8.620859307187338e-07,
      "loss": 0.0012,
      "step": 47700
    },
    {
      "epoch": 8674.545454545454,
      "grad_norm": 0.31504616141319275,
      "learning_rate": 8.620056800377243e-07,
      "loss": 0.0011,
      "step": 47710
    },
    {
      "epoch": 8676.363636363636,
      "grad_norm": 0.34936031699180603,
      "learning_rate": 8.619254097525842e-07,
      "loss": 0.0011,
      "step": 47720
    },
    {
      "epoch": 8678.181818181818,
      "grad_norm": 0.031580809503793716,
      "learning_rate": 8.618451198676601e-07,
      "loss": 0.0012,
      "step": 47730
    },
    {
      "epoch": 8680.0,
      "grad_norm": 0.2719089090824127,
      "learning_rate": 8.617648103873004e-07,
      "loss": 0.0009,
      "step": 47740
    },
    {
      "epoch": 8681.818181818182,
      "grad_norm": 0.004241043236106634,
      "learning_rate": 8.616844813158539e-07,
      "loss": 0.0012,
      "step": 47750
    },
    {
      "epoch": 8683.636363636364,
      "grad_norm": 0.009542852640151978,
      "learning_rate": 8.61604132657671e-07,
      "loss": 0.001,
      "step": 47760
    },
    {
      "epoch": 8685.454545454546,
      "grad_norm": 0.23315304517745972,
      "learning_rate": 8.615237644171027e-07,
      "loss": 0.0014,
      "step": 47770
    },
    {
      "epoch": 8687.272727272728,
      "grad_norm": 0.001503791194409132,
      "learning_rate": 8.614433765985014e-07,
      "loss": 0.0008,
      "step": 47780
    },
    {
      "epoch": 8689.09090909091,
      "grad_norm": 0.0013094788882881403,
      "learning_rate": 8.613629692062203e-07,
      "loss": 0.0012,
      "step": 47790
    },
    {
      "epoch": 8690.90909090909,
      "grad_norm": 0.27059078216552734,
      "learning_rate": 8.612825422446139e-07,
      "loss": 0.0012,
      "step": 47800
    },
    {
      "epoch": 8692.727272727272,
      "grad_norm": 0.24316419661045074,
      "learning_rate": 8.61202095718038e-07,
      "loss": 0.0011,
      "step": 47810
    },
    {
      "epoch": 8694.545454545454,
      "grad_norm": 0.0017694627167657018,
      "learning_rate": 8.611216296308483e-07,
      "loss": 0.0014,
      "step": 47820
    },
    {
      "epoch": 8696.363636363636,
      "grad_norm": 0.3192707896232605,
      "learning_rate": 8.610411439874031e-07,
      "loss": 0.001,
      "step": 47830
    },
    {
      "epoch": 8698.181818181818,
      "grad_norm": 0.002265030052512884,
      "learning_rate": 8.609606387920609e-07,
      "loss": 0.0008,
      "step": 47840
    },
    {
      "epoch": 8700.0,
      "grad_norm": 0.22766081988811493,
      "learning_rate": 8.60880114049181e-07,
      "loss": 0.0012,
      "step": 47850
    },
    {
      "epoch": 8701.818181818182,
      "grad_norm": 0.3007209300994873,
      "learning_rate": 8.607995697631247e-07,
      "loss": 0.0013,
      "step": 47860
    },
    {
      "epoch": 8703.636363636364,
      "grad_norm": 0.20056471228599548,
      "learning_rate": 8.607190059382534e-07,
      "loss": 0.0012,
      "step": 47870
    },
    {
      "epoch": 8705.454545454546,
      "grad_norm": 0.0036971585359424353,
      "learning_rate": 8.606384225789303e-07,
      "loss": 0.001,
      "step": 47880
    },
    {
      "epoch": 8707.272727272728,
      "grad_norm": 0.005231999326497316,
      "learning_rate": 8.605578196895189e-07,
      "loss": 0.0008,
      "step": 47890
    },
    {
      "epoch": 8709.09090909091,
      "grad_norm": 0.0024114621337503195,
      "learning_rate": 8.604771972743847e-07,
      "loss": 0.0012,
      "step": 47900
    },
    {
      "epoch": 8710.90909090909,
      "grad_norm": 0.2047780305147171,
      "learning_rate": 8.603965553378934e-07,
      "loss": 0.0011,
      "step": 47910
    },
    {
      "epoch": 8712.727272727272,
      "grad_norm": 0.20309525728225708,
      "learning_rate": 8.603158938844121e-07,
      "loss": 0.0012,
      "step": 47920
    },
    {
      "epoch": 8714.545454545454,
      "grad_norm": 0.001606361591257155,
      "learning_rate": 8.60235212918309e-07,
      "loss": 0.0012,
      "step": 47930
    },
    {
      "epoch": 8716.363636363636,
      "grad_norm": 0.27234843373298645,
      "learning_rate": 8.601545124439535e-07,
      "loss": 0.0011,
      "step": 47940
    },
    {
      "epoch": 8718.181818181818,
      "grad_norm": 0.34490370750427246,
      "learning_rate": 8.600737924657156e-07,
      "loss": 0.0014,
      "step": 47950
    },
    {
      "epoch": 8720.0,
      "grad_norm": 0.26258018612861633,
      "learning_rate": 8.599930529879668e-07,
      "loss": 0.001,
      "step": 47960
    },
    {
      "epoch": 8721.818181818182,
      "grad_norm": 0.2515794336795807,
      "learning_rate": 8.599122940150793e-07,
      "loss": 0.0014,
      "step": 47970
    },
    {
      "epoch": 8723.636363636364,
      "grad_norm": 0.00335687049664557,
      "learning_rate": 8.598315155514269e-07,
      "loss": 0.0009,
      "step": 47980
    },
    {
      "epoch": 8725.454545454546,
      "grad_norm": 0.021525466814637184,
      "learning_rate": 8.597507176013837e-07,
      "loss": 0.0012,
      "step": 47990
    },
    {
      "epoch": 8727.272727272728,
      "grad_norm": 0.0019113973248749971,
      "learning_rate": 8.596699001693255e-07,
      "loss": 0.001,
      "step": 48000
    },
    {
      "epoch": 8727.272727272728,
      "eval_loss": 4.817364692687988,
      "eval_runtime": 0.9536,
      "eval_samples_per_second": 10.487,
      "eval_steps_per_second": 5.244,
      "step": 48000
    },
    {
      "epoch": 8729.09090909091,
      "grad_norm": 0.2998619079589844,
      "learning_rate": 8.595890632596288e-07,
      "loss": 0.0011,
      "step": 48010
    },
    {
      "epoch": 8730.90909090909,
      "grad_norm": 0.0018245193641632795,
      "learning_rate": 8.595082068766713e-07,
      "loss": 0.0012,
      "step": 48020
    },
    {
      "epoch": 8732.727272727272,
      "grad_norm": 0.004259623121470213,
      "learning_rate": 8.594273310248316e-07,
      "loss": 0.001,
      "step": 48030
    },
    {
      "epoch": 8734.545454545454,
      "grad_norm": 0.20802254974842072,
      "learning_rate": 8.593464357084897e-07,
      "loss": 0.0013,
      "step": 48040
    },
    {
      "epoch": 8736.363636363636,
      "grad_norm": 0.004004479385912418,
      "learning_rate": 8.592655209320261e-07,
      "loss": 0.0008,
      "step": 48050
    },
    {
      "epoch": 8738.181818181818,
      "grad_norm": 0.0032856559846550226,
      "learning_rate": 8.591845866998231e-07,
      "loss": 0.0012,
      "step": 48060
    },
    {
      "epoch": 8740.0,
      "grad_norm": 0.0018964523915201426,
      "learning_rate": 8.591036330162631e-07,
      "loss": 0.0012,
      "step": 48070
    },
    {
      "epoch": 8741.818181818182,
      "grad_norm": 0.20114882290363312,
      "learning_rate": 8.590226598857305e-07,
      "loss": 0.0011,
      "step": 48080
    },
    {
      "epoch": 8743.636363636364,
      "grad_norm": 0.0026636940892785788,
      "learning_rate": 8.589416673126102e-07,
      "loss": 0.001,
      "step": 48090
    },
    {
      "epoch": 8745.454545454546,
      "grad_norm": 0.20990800857543945,
      "learning_rate": 8.588606553012883e-07,
      "loss": 0.0013,
      "step": 48100
    },
    {
      "epoch": 8747.272727272728,
      "grad_norm": 0.22691881656646729,
      "learning_rate": 8.58779623856152e-07,
      "loss": 0.0011,
      "step": 48110
    },
    {
      "epoch": 8749.09090909091,
      "grad_norm": 0.1951119303703308,
      "learning_rate": 8.586985729815894e-07,
      "loss": 0.0012,
      "step": 48120
    },
    {
      "epoch": 8750.90909090909,
      "grad_norm": 0.25117772817611694,
      "learning_rate": 8.586175026819895e-07,
      "loss": 0.0012,
      "step": 48130
    },
    {
      "epoch": 8752.727272727272,
      "grad_norm": 0.002005687216296792,
      "learning_rate": 8.585364129617432e-07,
      "loss": 0.0009,
      "step": 48140
    },
    {
      "epoch": 8754.545454545454,
      "grad_norm": 0.21769054234027863,
      "learning_rate": 8.584553038252413e-07,
      "loss": 0.0014,
      "step": 48150
    },
    {
      "epoch": 8756.363636363636,
      "grad_norm": 0.0016063872026279569,
      "learning_rate": 8.583741752768765e-07,
      "loss": 0.0009,
      "step": 48160
    },
    {
      "epoch": 8758.181818181818,
      "grad_norm": 0.001699937623925507,
      "learning_rate": 8.582930273210422e-07,
      "loss": 0.0011,
      "step": 48170
    },
    {
      "epoch": 8760.0,
      "grad_norm": 0.2158273160457611,
      "learning_rate": 8.582118599621329e-07,
      "loss": 0.0012,
      "step": 48180
    },
    {
      "epoch": 8761.818181818182,
      "grad_norm": 0.2297736406326294,
      "learning_rate": 8.581306732045441e-07,
      "loss": 0.0012,
      "step": 48190
    },
    {
      "epoch": 8763.636363636364,
      "grad_norm": 0.009220394305884838,
      "learning_rate": 8.580494670526724e-07,
      "loss": 0.001,
      "step": 48200
    },
    {
      "epoch": 8765.454545454546,
      "grad_norm": 0.2537544369697571,
      "learning_rate": 8.579682415109155e-07,
      "loss": 0.0011,
      "step": 48210
    },
    {
      "epoch": 8767.272727272728,
      "grad_norm": 0.2177354395389557,
      "learning_rate": 8.578869965836721e-07,
      "loss": 0.0011,
      "step": 48220
    },
    {
      "epoch": 8769.09090909091,
      "grad_norm": 0.002051420509815216,
      "learning_rate": 8.57805732275342e-07,
      "loss": 0.0011,
      "step": 48230
    },
    {
      "epoch": 8770.90909090909,
      "grad_norm": 0.0016688969917595387,
      "learning_rate": 8.577244485903259e-07,
      "loss": 0.0012,
      "step": 48240
    },
    {
      "epoch": 8772.727272727272,
      "grad_norm": 0.21626289188861847,
      "learning_rate": 8.576431455330257e-07,
      "loss": 0.0012,
      "step": 48250
    },
    {
      "epoch": 8774.545454545454,
      "grad_norm": 0.20714591443538666,
      "learning_rate": 8.575618231078443e-07,
      "loss": 0.001,
      "step": 48260
    },
    {
      "epoch": 8776.363636363636,
      "grad_norm": 0.018859485164284706,
      "learning_rate": 8.574804813191857e-07,
      "loss": 0.0013,
      "step": 48270
    },
    {
      "epoch": 8778.181818181818,
      "grad_norm": 0.0021117678843438625,
      "learning_rate": 8.573991201714548e-07,
      "loss": 0.0006,
      "step": 48280
    },
    {
      "epoch": 8780.0,
      "grad_norm": 0.0013603746192529798,
      "learning_rate": 8.573177396690579e-07,
      "loss": 0.0012,
      "step": 48290
    },
    {
      "epoch": 8781.818181818182,
      "grad_norm": 0.004041794687509537,
      "learning_rate": 8.572363398164016e-07,
      "loss": 0.0009,
      "step": 48300
    },
    {
      "epoch": 8783.636363636364,
      "grad_norm": 0.004090797621756792,
      "learning_rate": 8.571549206178945e-07,
      "loss": 0.0011,
      "step": 48310
    },
    {
      "epoch": 8785.454545454546,
      "grad_norm": 0.19269320368766785,
      "learning_rate": 8.570734820779457e-07,
      "loss": 0.0015,
      "step": 48320
    },
    {
      "epoch": 8787.272727272728,
      "grad_norm": 0.00209981482475996,
      "learning_rate": 8.569920242009654e-07,
      "loss": 0.0007,
      "step": 48330
    },
    {
      "epoch": 8789.09090909091,
      "grad_norm": 0.002849203534424305,
      "learning_rate": 8.569105469913646e-07,
      "loss": 0.0013,
      "step": 48340
    },
    {
      "epoch": 8790.90909090909,
      "grad_norm": 0.002726753242313862,
      "learning_rate": 8.568290504535562e-07,
      "loss": 0.0011,
      "step": 48350
    },
    {
      "epoch": 8792.727272727272,
      "grad_norm": 0.0015879292041063309,
      "learning_rate": 8.567475345919532e-07,
      "loss": 0.0012,
      "step": 48360
    },
    {
      "epoch": 8794.545454545454,
      "grad_norm": 0.18815940618515015,
      "learning_rate": 8.566659994109701e-07,
      "loss": 0.0011,
      "step": 48370
    },
    {
      "epoch": 8796.363636363636,
      "grad_norm": 0.25350421667099,
      "learning_rate": 8.565844449150224e-07,
      "loss": 0.0013,
      "step": 48380
    },
    {
      "epoch": 8798.181818181818,
      "grad_norm": 0.2367592304944992,
      "learning_rate": 8.565028711085265e-07,
      "loss": 0.0009,
      "step": 48390
    },
    {
      "epoch": 8800.0,
      "grad_norm": 0.0020343686919659376,
      "learning_rate": 8.564212779959001e-07,
      "loss": 0.0011,
      "step": 48400
    },
    {
      "epoch": 8801.818181818182,
      "grad_norm": 0.2545528709888458,
      "learning_rate": 8.563396655815618e-07,
      "loss": 0.0013,
      "step": 48410
    },
    {
      "epoch": 8803.636363636364,
      "grad_norm": 0.003819886827841401,
      "learning_rate": 8.562580338699312e-07,
      "loss": 0.0008,
      "step": 48420
    },
    {
      "epoch": 8805.454545454546,
      "grad_norm": 0.001481685321778059,
      "learning_rate": 8.561763828654292e-07,
      "loss": 0.0015,
      "step": 48430
    },
    {
      "epoch": 8807.272727272728,
      "grad_norm": 0.3073149025440216,
      "learning_rate": 8.560947125724771e-07,
      "loss": 0.0012,
      "step": 48440
    },
    {
      "epoch": 8809.09090909091,
      "grad_norm": 0.3228764235973358,
      "learning_rate": 8.560130229954983e-07,
      "loss": 0.0011,
      "step": 48450
    },
    {
      "epoch": 8810.90909090909,
      "grad_norm": 0.26810067892074585,
      "learning_rate": 8.55931314138916e-07,
      "loss": 0.0011,
      "step": 48460
    },
    {
      "epoch": 8812.727272727272,
      "grad_norm": 0.2064485102891922,
      "learning_rate": 8.558495860071555e-07,
      "loss": 0.001,
      "step": 48470
    },
    {
      "epoch": 8814.545454545454,
      "grad_norm": 0.004195918329060078,
      "learning_rate": 8.557678386046428e-07,
      "loss": 0.0011,
      "step": 48480
    },
    {
      "epoch": 8816.363636363636,
      "grad_norm": 0.19473131000995636,
      "learning_rate": 8.556860719358043e-07,
      "loss": 0.0013,
      "step": 48490
    },
    {
      "epoch": 8818.181818181818,
      "grad_norm": 0.00380103662610054,
      "learning_rate": 8.556042860050685e-07,
      "loss": 0.001,
      "step": 48500
    },
    {
      "epoch": 8818.181818181818,
      "eval_loss": 4.876538276672363,
      "eval_runtime": 0.9515,
      "eval_samples_per_second": 10.51,
      "eval_steps_per_second": 5.255,
      "step": 48500
    },
    {
      "epoch": 8820.0,
      "grad_norm": 0.20852962136268616,
      "learning_rate": 8.555224808168644e-07,
      "loss": 0.0012,
      "step": 48510
    },
    {
      "epoch": 8821.818181818182,
      "grad_norm": 0.20937086641788483,
      "learning_rate": 8.55440656375622e-07,
      "loss": 0.0012,
      "step": 48520
    },
    {
      "epoch": 8823.636363636364,
      "grad_norm": 0.001358266919851303,
      "learning_rate": 8.553588126857725e-07,
      "loss": 0.0011,
      "step": 48530
    },
    {
      "epoch": 8825.454545454546,
      "grad_norm": 0.18993453681468964,
      "learning_rate": 8.55276949751748e-07,
      "loss": 0.0013,
      "step": 48540
    },
    {
      "epoch": 8827.272727272728,
      "grad_norm": 0.24314025044441223,
      "learning_rate": 8.551950675779818e-07,
      "loss": 0.0009,
      "step": 48550
    },
    {
      "epoch": 8829.09090909091,
      "grad_norm": 0.0019836477003991604,
      "learning_rate": 8.551131661689082e-07,
      "loss": 0.001,
      "step": 48560
    },
    {
      "epoch": 8830.90909090909,
      "grad_norm": 0.2557210922241211,
      "learning_rate": 8.550312455289624e-07,
      "loss": 0.0012,
      "step": 48570
    },
    {
      "epoch": 8832.727272727272,
      "grad_norm": 0.0013861742336302996,
      "learning_rate": 8.549493056625806e-07,
      "loss": 0.0011,
      "step": 48580
    },
    {
      "epoch": 8834.545454545454,
      "grad_norm": 0.003978309221565723,
      "learning_rate": 8.548673465742004e-07,
      "loss": 0.0009,
      "step": 48590
    },
    {
      "epoch": 8836.363636363636,
      "grad_norm": 0.2372898906469345,
      "learning_rate": 8.547853682682604e-07,
      "loss": 0.0014,
      "step": 48600
    },
    {
      "epoch": 8838.181818181818,
      "grad_norm": 0.0025907917879521847,
      "learning_rate": 8.547033707491998e-07,
      "loss": 0.0011,
      "step": 48610
    },
    {
      "epoch": 8840.0,
      "grad_norm": 0.2554493546485901,
      "learning_rate": 8.546213540214592e-07,
      "loss": 0.0012,
      "step": 48620
    },
    {
      "epoch": 8841.818181818182,
      "grad_norm": 0.2583813965320587,
      "learning_rate": 8.5453931808948e-07,
      "loss": 0.001,
      "step": 48630
    },
    {
      "epoch": 8843.636363636364,
      "grad_norm": 0.23022916913032532,
      "learning_rate": 8.54457262957705e-07,
      "loss": 0.001,
      "step": 48640
    },
    {
      "epoch": 8845.454545454546,
      "grad_norm": 0.2560129165649414,
      "learning_rate": 8.543751886305779e-07,
      "loss": 0.0012,
      "step": 48650
    },
    {
      "epoch": 8847.272727272728,
      "grad_norm": 0.001324547571130097,
      "learning_rate": 8.542930951125431e-07,
      "loss": 0.0011,
      "step": 48660
    },
    {
      "epoch": 8849.09090909091,
      "grad_norm": 0.0015704638790339231,
      "learning_rate": 8.542109824080463e-07,
      "loss": 0.0011,
      "step": 48670
    },
    {
      "epoch": 8850.90909090909,
      "grad_norm": 0.2148764729499817,
      "learning_rate": 8.541288505215346e-07,
      "loss": 0.0012,
      "step": 48680
    },
    {
      "epoch": 8852.727272727272,
      "grad_norm": 0.004253466613590717,
      "learning_rate": 8.540466994574555e-07,
      "loss": 0.001,
      "step": 48690
    },
    {
      "epoch": 8854.545454545454,
      "grad_norm": 0.004104373976588249,
      "learning_rate": 8.539645292202579e-07,
      "loss": 0.0011,
      "step": 48700
    },
    {
      "epoch": 8856.363636363636,
      "grad_norm": 0.0013893708819523454,
      "learning_rate": 8.538823398143915e-07,
      "loss": 0.001,
      "step": 48710
    },
    {
      "epoch": 8858.181818181818,
      "grad_norm": 0.004238773602992296,
      "learning_rate": 8.538001312443076e-07,
      "loss": 0.0011,
      "step": 48720
    },
    {
      "epoch": 8860.0,
      "grad_norm": 0.0014385004760697484,
      "learning_rate": 8.537179035144578e-07,
      "loss": 0.0012,
      "step": 48730
    },
    {
      "epoch": 8861.818181818182,
      "grad_norm": 0.21438397467136383,
      "learning_rate": 8.536356566292951e-07,
      "loss": 0.0012,
      "step": 48740
    },
    {
      "epoch": 8863.636363636364,
      "grad_norm": 0.00236904202029109,
      "learning_rate": 8.535533905932737e-07,
      "loss": 0.0008,
      "step": 48750
    },
    {
      "epoch": 8865.454545454546,
      "grad_norm": 0.25709930062294006,
      "learning_rate": 8.534711054108485e-07,
      "loss": 0.0014,
      "step": 48760
    },
    {
      "epoch": 8867.272727272728,
      "grad_norm": 0.18642480671405792,
      "learning_rate": 8.533888010864756e-07,
      "loss": 0.0011,
      "step": 48770
    },
    {
      "epoch": 8869.09090909091,
      "grad_norm": 0.0021585007198154926,
      "learning_rate": 8.533064776246124e-07,
      "loss": 0.001,
      "step": 48780
    },
    {
      "epoch": 8870.90909090909,
      "grad_norm": 0.20089876651763916,
      "learning_rate": 8.532241350297166e-07,
      "loss": 0.001,
      "step": 48790
    },
    {
      "epoch": 8872.727272727272,
      "grad_norm": 0.0017286255024373531,
      "learning_rate": 8.531417733062476e-07,
      "loss": 0.0012,
      "step": 48800
    },
    {
      "epoch": 8874.545454545454,
      "grad_norm": 0.002426536288112402,
      "learning_rate": 8.530593924586658e-07,
      "loss": 0.0008,
      "step": 48810
    },
    {
      "epoch": 8876.363636363636,
      "grad_norm": 0.002938249846920371,
      "learning_rate": 8.529769924914322e-07,
      "loss": 0.0014,
      "step": 48820
    },
    {
      "epoch": 8878.181818181818,
      "grad_norm": 0.26568710803985596,
      "learning_rate": 8.528945734090093e-07,
      "loss": 0.0014,
      "step": 48830
    },
    {
      "epoch": 8880.0,
      "grad_norm": 0.22775067389011383,
      "learning_rate": 8.528121352158604e-07,
      "loss": 0.0011,
      "step": 48840
    },
    {
      "epoch": 8881.818181818182,
      "grad_norm": 0.013387286104261875,
      "learning_rate": 8.527296779164497e-07,
      "loss": 0.0012,
      "step": 48850
    },
    {
      "epoch": 8883.636363636364,
      "grad_norm": 0.3323231041431427,
      "learning_rate": 8.526472015152428e-07,
      "loss": 0.0009,
      "step": 48860
    },
    {
      "epoch": 8885.454545454546,
      "grad_norm": 0.19908854365348816,
      "learning_rate": 8.525647060167061e-07,
      "loss": 0.0016,
      "step": 48870
    },
    {
      "epoch": 8887.272727272728,
      "grad_norm": 0.25306230783462524,
      "learning_rate": 8.524821914253072e-07,
      "loss": 0.0008,
      "step": 48880
    },
    {
      "epoch": 8889.09090909091,
      "grad_norm": 0.0024380867835134268,
      "learning_rate": 8.523996577455144e-07,
      "loss": 0.0011,
      "step": 48890
    },
    {
      "epoch": 8890.90909090909,
      "grad_norm": 0.0033802511170506477,
      "learning_rate": 8.523171049817973e-07,
      "loss": 0.0012,
      "step": 48900
    },
    {
      "epoch": 8892.727272727272,
      "grad_norm": 0.0024462174624204636,
      "learning_rate": 8.522345331386266e-07,
      "loss": 0.001,
      "step": 48910
    },
    {
      "epoch": 8894.545454545454,
      "grad_norm": 0.008997263386845589,
      "learning_rate": 8.521519422204737e-07,
      "loss": 0.0014,
      "step": 48920
    },
    {
      "epoch": 8896.363636363636,
      "grad_norm": 0.2812426686286926,
      "learning_rate": 8.520693322318114e-07,
      "loss": 0.0012,
      "step": 48930
    },
    {
      "epoch": 8898.181818181818,
      "grad_norm": 0.18690070509910583,
      "learning_rate": 8.519867031771135e-07,
      "loss": 0.0011,
      "step": 48940
    },
    {
      "epoch": 8900.0,
      "grad_norm": 0.0034898347221314907,
      "learning_rate": 8.519040550608545e-07,
      "loss": 0.0011,
      "step": 48950
    },
    {
      "epoch": 8901.818181818182,
      "grad_norm": 0.25538763403892517,
      "learning_rate": 8.518213878875101e-07,
      "loss": 0.0011,
      "step": 48960
    },
    {
      "epoch": 8903.636363636364,
      "grad_norm": 0.0014579507987946272,
      "learning_rate": 8.517387016615572e-07,
      "loss": 0.001,
      "step": 48970
    },
    {
      "epoch": 8905.454545454546,
      "grad_norm": 0.2381449043750763,
      "learning_rate": 8.516559963874736e-07,
      "loss": 0.0012,
      "step": 48980
    },
    {
      "epoch": 8907.272727272728,
      "grad_norm": 0.2131797820329666,
      "learning_rate": 8.515732720697383e-07,
      "loss": 0.0012,
      "step": 48990
    },
    {
      "epoch": 8909.09090909091,
      "grad_norm": 0.360132098197937,
      "learning_rate": 8.514905287128309e-07,
      "loss": 0.0012,
      "step": 49000
    },
    {
      "epoch": 8909.09090909091,
      "eval_loss": 4.78195858001709,
      "eval_runtime": 0.9512,
      "eval_samples_per_second": 10.513,
      "eval_steps_per_second": 5.256,
      "step": 49000
    },
    {
      "epoch": 8910.90909090909,
      "grad_norm": 0.0015617902390658855,
      "learning_rate": 8.514077663212322e-07,
      "loss": 0.0011,
      "step": 49010
    },
    {
      "epoch": 8912.727272727272,
      "grad_norm": 0.001462469925172627,
      "learning_rate": 8.513249848994246e-07,
      "loss": 0.0012,
      "step": 49020
    },
    {
      "epoch": 8914.545454545454,
      "grad_norm": 0.18177130818367004,
      "learning_rate": 8.512421844518906e-07,
      "loss": 0.0011,
      "step": 49030
    },
    {
      "epoch": 8916.363636363636,
      "grad_norm": 0.0035496188793331385,
      "learning_rate": 8.511593649831146e-07,
      "loss": 0.0008,
      "step": 49040
    },
    {
      "epoch": 8918.181818181818,
      "grad_norm": 0.2559599280357361,
      "learning_rate": 8.510765264975812e-07,
      "loss": 0.0014,
      "step": 49050
    },
    {
      "epoch": 8920.0,
      "grad_norm": 0.2507103383541107,
      "learning_rate": 8.509936689997768e-07,
      "loss": 0.001,
      "step": 49060
    },
    {
      "epoch": 8921.818181818182,
      "grad_norm": 0.3231510818004608,
      "learning_rate": 8.509107924941883e-07,
      "loss": 0.0012,
      "step": 49070
    },
    {
      "epoch": 8923.636363636364,
      "grad_norm": 0.0025301917921751738,
      "learning_rate": 8.508278969853037e-07,
      "loss": 0.001,
      "step": 49080
    },
    {
      "epoch": 8925.454545454546,
      "grad_norm": 0.0019075574818998575,
      "learning_rate": 8.507449824776124e-07,
      "loss": 0.0008,
      "step": 49090
    },
    {
      "epoch": 8927.272727272728,
      "grad_norm": 0.31812864542007446,
      "learning_rate": 8.506620489756044e-07,
      "loss": 0.0016,
      "step": 49100
    },
    {
      "epoch": 8929.09090909091,
      "grad_norm": 0.2427567094564438,
      "learning_rate": 8.505790964837712e-07,
      "loss": 0.0009,
      "step": 49110
    },
    {
      "epoch": 8930.90909090909,
      "grad_norm": 0.1966971904039383,
      "learning_rate": 8.504961250066045e-07,
      "loss": 0.0012,
      "step": 49120
    },
    {
      "epoch": 8932.727272727272,
      "grad_norm": 0.0019164831610396504,
      "learning_rate": 8.504131345485981e-07,
      "loss": 0.0012,
      "step": 49130
    },
    {
      "epoch": 8934.545454545454,
      "grad_norm": 0.18469537794589996,
      "learning_rate": 8.503301251142458e-07,
      "loss": 0.001,
      "step": 49140
    },
    {
      "epoch": 8936.363636363636,
      "grad_norm": 0.0012975436402484775,
      "learning_rate": 8.502470967080433e-07,
      "loss": 0.0009,
      "step": 49150
    },
    {
      "epoch": 8938.181818181818,
      "grad_norm": 0.049591392278671265,
      "learning_rate": 8.501640493344866e-07,
      "loss": 0.0015,
      "step": 49160
    },
    {
      "epoch": 8940.0,
      "grad_norm": 0.32912707328796387,
      "learning_rate": 8.500809829980733e-07,
      "loss": 0.0009,
      "step": 49170
    },
    {
      "epoch": 8941.818181818182,
      "grad_norm": 0.2594718337059021,
      "learning_rate": 8.499978977033018e-07,
      "loss": 0.001,
      "step": 49180
    },
    {
      "epoch": 8943.636363636364,
      "grad_norm": 0.0017554929945617914,
      "learning_rate": 8.499147934546715e-07,
      "loss": 0.0009,
      "step": 49190
    },
    {
      "epoch": 8945.454545454546,
      "grad_norm": 0.0015722339740023017,
      "learning_rate": 8.498316702566826e-07,
      "loss": 0.0014,
      "step": 49200
    },
    {
      "epoch": 8947.272727272728,
      "grad_norm": 0.25514352321624756,
      "learning_rate": 8.497485281138369e-07,
      "loss": 0.001,
      "step": 49210
    },
    {
      "epoch": 8949.09090909091,
      "grad_norm": 0.18644165992736816,
      "learning_rate": 8.496653670306368e-07,
      "loss": 0.001,
      "step": 49220
    },
    {
      "epoch": 8950.90909090909,
      "grad_norm": 0.024762269109487534,
      "learning_rate": 8.495821870115857e-07,
      "loss": 0.0012,
      "step": 49230
    },
    {
      "epoch": 8952.727272727272,
      "grad_norm": 0.012621971778571606,
      "learning_rate": 8.494989880611883e-07,
      "loss": 0.001,
      "step": 49240
    },
    {
      "epoch": 8954.545454545454,
      "grad_norm": 0.0024950597435235977,
      "learning_rate": 8.494157701839499e-07,
      "loss": 0.0012,
      "step": 49250
    },
    {
      "epoch": 8956.363636363636,
      "grad_norm": 0.25163695216178894,
      "learning_rate": 8.493325333843775e-07,
      "loss": 0.0011,
      "step": 49260
    },
    {
      "epoch": 8958.181818181818,
      "grad_norm": 0.001506060129031539,
      "learning_rate": 8.492492776669785e-07,
      "loss": 0.0009,
      "step": 49270
    },
    {
      "epoch": 8960.0,
      "grad_norm": 0.2666659355163574,
      "learning_rate": 8.491660030362616e-07,
      "loss": 0.0012,
      "step": 49280
    },
    {
      "epoch": 8961.818181818182,
      "grad_norm": 0.00837587658315897,
      "learning_rate": 8.490827094967363e-07,
      "loss": 0.001,
      "step": 49290
    },
    {
      "epoch": 8963.636363636364,
      "grad_norm": 0.17926105856895447,
      "learning_rate": 8.489993970529136e-07,
      "loss": 0.0011,
      "step": 49300
    },
    {
      "epoch": 8965.454545454546,
      "grad_norm": 0.0017008883878588676,
      "learning_rate": 8.489160657093049e-07,
      "loss": 0.0014,
      "step": 49310
    },
    {
      "epoch": 8967.272727272728,
      "grad_norm": 0.19667072594165802,
      "learning_rate": 8.488327154704232e-07,
      "loss": 0.0011,
      "step": 49320
    },
    {
      "epoch": 8969.09090909091,
      "grad_norm": 0.0023638217244297266,
      "learning_rate": 8.487493463407822e-07,
      "loss": 0.001,
      "step": 49330
    },
    {
      "epoch": 8970.90909090909,
      "grad_norm": 0.2061302661895752,
      "learning_rate": 8.486659583248965e-07,
      "loss": 0.001,
      "step": 49340
    },
    {
      "epoch": 8972.727272727272,
      "grad_norm": 0.19100165367126465,
      "learning_rate": 8.485825514272823e-07,
      "loss": 0.0009,
      "step": 49350
    },
    {
      "epoch": 8974.545454545454,
      "grad_norm": 0.2011292725801468,
      "learning_rate": 8.48499125652456e-07,
      "loss": 0.0013,
      "step": 49360
    },
    {
      "epoch": 8976.363636363636,
      "grad_norm": 0.0017473112093284726,
      "learning_rate": 8.484156810049358e-07,
      "loss": 0.0012,
      "step": 49370
    },
    {
      "epoch": 8978.181818181818,
      "grad_norm": 0.003315223380923271,
      "learning_rate": 8.483322174892404e-07,
      "loss": 0.001,
      "step": 49380
    },
    {
      "epoch": 8980.0,
      "grad_norm": 0.0031582466326653957,
      "learning_rate": 8.482487351098897e-07,
      "loss": 0.0012,
      "step": 49390
    },
    {
      "epoch": 8981.818181818182,
      "grad_norm": 0.2088780403137207,
      "learning_rate": 8.481652338714047e-07,
      "loss": 0.0011,
      "step": 49400
    },
    {
      "epoch": 8983.636363636364,
      "grad_norm": 0.013355827890336514,
      "learning_rate": 8.480817137783072e-07,
      "loss": 0.0013,
      "step": 49410
    },
    {
      "epoch": 8985.454545454546,
      "grad_norm": 0.2499459981918335,
      "learning_rate": 8.479981748351204e-07,
      "loss": 0.0009,
      "step": 49420
    },
    {
      "epoch": 8987.272727272728,
      "grad_norm": 0.18828071653842926,
      "learning_rate": 8.47914617046368e-07,
      "loss": 0.0012,
      "step": 49430
    },
    {
      "epoch": 8989.09090909091,
      "grad_norm": 0.0017536327941343188,
      "learning_rate": 8.478310404165754e-07,
      "loss": 0.001,
      "step": 49440
    },
    {
      "epoch": 8990.90909090909,
      "grad_norm": 0.0016477273311465979,
      "learning_rate": 8.477474449502682e-07,
      "loss": 0.0012,
      "step": 49450
    },
    {
      "epoch": 8992.727272727272,
      "grad_norm": 0.22381402552127838,
      "learning_rate": 8.476638306519738e-07,
      "loss": 0.0011,
      "step": 49460
    },
    {
      "epoch": 8994.545454545454,
      "grad_norm": 0.016403349116444588,
      "learning_rate": 8.475801975262199e-07,
      "loss": 0.0012,
      "step": 49470
    },
    {
      "epoch": 8996.363636363636,
      "grad_norm": 0.0020579365082085133,
      "learning_rate": 8.474965455775358e-07,
      "loss": 0.0009,
      "step": 49480
    },
    {
      "epoch": 8998.181818181818,
      "grad_norm": 0.24986949563026428,
      "learning_rate": 8.474128748104518e-07,
      "loss": 0.0012,
      "step": 49490
    },
    {
      "epoch": 9000.0,
      "grad_norm": 0.28092384338378906,
      "learning_rate": 8.473291852294986e-07,
      "loss": 0.0011,
      "step": 49500
    },
    {
      "epoch": 9000.0,
      "eval_loss": 4.861410617828369,
      "eval_runtime": 0.9495,
      "eval_samples_per_second": 10.531,
      "eval_steps_per_second": 5.266,
      "step": 49500
    },
    {
      "epoch": 9001.818181818182,
      "grad_norm": 0.2500166893005371,
      "learning_rate": 8.472454768392085e-07,
      "loss": 0.0012,
      "step": 49510
    },
    {
      "epoch": 9003.636363636364,
      "grad_norm": 0.0020347293466329575,
      "learning_rate": 8.47161749644115e-07,
      "loss": 0.001,
      "step": 49520
    },
    {
      "epoch": 9005.454545454546,
      "grad_norm": 0.22261270880699158,
      "learning_rate": 8.470780036487519e-07,
      "loss": 0.0012,
      "step": 49530
    },
    {
      "epoch": 9007.272727272728,
      "grad_norm": 0.2501576840877533,
      "learning_rate": 8.469942388576544e-07,
      "loss": 0.0011,
      "step": 49540
    },
    {
      "epoch": 9009.09090909091,
      "grad_norm": 0.0020407584961503744,
      "learning_rate": 8.469104552753588e-07,
      "loss": 0.0011,
      "step": 49550
    },
    {
      "epoch": 9010.90909090909,
      "grad_norm": 0.004276846535503864,
      "learning_rate": 8.468266529064024e-07,
      "loss": 0.0012,
      "step": 49560
    },
    {
      "epoch": 9012.727272727272,
      "grad_norm": 0.0015704904217272997,
      "learning_rate": 8.467428317553234e-07,
      "loss": 0.001,
      "step": 49570
    },
    {
      "epoch": 9014.545454545454,
      "grad_norm": 0.24016067385673523,
      "learning_rate": 8.466589918266611e-07,
      "loss": 0.0012,
      "step": 49580
    },
    {
      "epoch": 9016.363636363636,
      "grad_norm": 0.33129817247390747,
      "learning_rate": 8.465751331249558e-07,
      "loss": 0.0012,
      "step": 49590
    },
    {
      "epoch": 9018.181818181818,
      "grad_norm": 0.21963730454444885,
      "learning_rate": 8.464912556547485e-07,
      "loss": 0.0011,
      "step": 49600
    },
    {
      "epoch": 9020.0,
      "grad_norm": 0.001203857478685677,
      "learning_rate": 8.464073594205821e-07,
      "loss": 0.0011,
      "step": 49610
    },
    {
      "epoch": 9021.818181818182,
      "grad_norm": 0.18995481729507446,
      "learning_rate": 8.463234444269993e-07,
      "loss": 0.0011,
      "step": 49620
    },
    {
      "epoch": 9023.636363636364,
      "grad_norm": 0.16869767010211945,
      "learning_rate": 8.46239510678545e-07,
      "loss": 0.0012,
      "step": 49630
    },
    {
      "epoch": 9025.454545454546,
      "grad_norm": 0.28345805406570435,
      "learning_rate": 8.461555581797641e-07,
      "loss": 0.0014,
      "step": 49640
    },
    {
      "epoch": 9027.272727272728,
      "grad_norm": 0.005056122317910194,
      "learning_rate": 8.460715869352035e-07,
      "loss": 0.0008,
      "step": 49650
    },
    {
      "epoch": 9029.09090909091,
      "grad_norm": 0.178626149892807,
      "learning_rate": 8.4598759694941e-07,
      "loss": 0.0014,
      "step": 49660
    },
    {
      "epoch": 9030.90909090909,
      "grad_norm": 0.22553645074367523,
      "learning_rate": 8.459035882269323e-07,
      "loss": 0.0009,
      "step": 49670
    },
    {
      "epoch": 9032.727272727272,
      "grad_norm": 0.17782148718833923,
      "learning_rate": 8.4581956077232e-07,
      "loss": 0.0012,
      "step": 49680
    },
    {
      "epoch": 9034.545454545454,
      "grad_norm": 0.2187141627073288,
      "learning_rate": 8.457355145901234e-07,
      "loss": 0.0012,
      "step": 49690
    },
    {
      "epoch": 9036.363636363636,
      "grad_norm": 0.006796449422836304,
      "learning_rate": 8.456514496848938e-07,
      "loss": 0.0008,
      "step": 49700
    },
    {
      "epoch": 9038.181818181818,
      "grad_norm": 0.19169963896274567,
      "learning_rate": 8.455673660611838e-07,
      "loss": 0.0013,
      "step": 49710
    },
    {
      "epoch": 9040.0,
      "grad_norm": 0.32171326875686646,
      "learning_rate": 8.454832637235469e-07,
      "loss": 0.0011,
      "step": 49720
    },
    {
      "epoch": 9041.818181818182,
      "grad_norm": 0.002998868003487587,
      "learning_rate": 8.453991426765377e-07,
      "loss": 0.001,
      "step": 49730
    },
    {
      "epoch": 9043.636363636364,
      "grad_norm": 0.1830979436635971,
      "learning_rate": 8.453150029247114e-07,
      "loss": 0.0011,
      "step": 49740
    },
    {
      "epoch": 9045.454545454546,
      "grad_norm": 0.0017754659056663513,
      "learning_rate": 8.452308444726248e-07,
      "loss": 0.0012,
      "step": 49750
    },
    {
      "epoch": 9047.272727272728,
      "grad_norm": 0.01354143489152193,
      "learning_rate": 8.451466673248353e-07,
      "loss": 0.0012,
      "step": 49760
    },
    {
      "epoch": 9049.09090909091,
      "grad_norm": 0.0038742090109735727,
      "learning_rate": 8.450624714859016e-07,
      "loss": 0.0009,
      "step": 49770
    },
    {
      "epoch": 9050.90909090909,
      "grad_norm": 0.012986104935407639,
      "learning_rate": 8.44978256960383e-07,
      "loss": 0.0012,
      "step": 49780
    },
    {
      "epoch": 9052.727272727272,
      "grad_norm": 0.20297743380069733,
      "learning_rate": 8.448940237528402e-07,
      "loss": 0.0009,
      "step": 49790
    },
    {
      "epoch": 9054.545454545454,
      "grad_norm": 0.0023240880109369755,
      "learning_rate": 8.448097718678348e-07,
      "loss": 0.0012,
      "step": 49800
    },
    {
      "epoch": 9056.363636363636,
      "grad_norm": 0.003925157245248556,
      "learning_rate": 8.447255013099295e-07,
      "loss": 0.0009,
      "step": 49810
    },
    {
      "epoch": 9058.181818181818,
      "grad_norm": 0.014333534054458141,
      "learning_rate": 8.446412120836876e-07,
      "loss": 0.0015,
      "step": 49820
    },
    {
      "epoch": 9060.0,
      "grad_norm": 0.0063232434913516045,
      "learning_rate": 8.445569041936742e-07,
      "loss": 0.0009,
      "step": 49830
    },
    {
      "epoch": 9061.818181818182,
      "grad_norm": 0.19423624873161316,
      "learning_rate": 8.444725776444546e-07,
      "loss": 0.0009,
      "step": 49840
    },
    {
      "epoch": 9063.636363636364,
      "grad_norm": 0.005078133661299944,
      "learning_rate": 8.443882324405953e-07,
      "loss": 0.001,
      "step": 49850
    },
    {
      "epoch": 9065.454545454546,
      "grad_norm": 0.3418750464916229,
      "learning_rate": 8.443038685866641e-07,
      "loss": 0.0015,
      "step": 49860
    },
    {
      "epoch": 9067.272727272728,
      "grad_norm": 0.0036861554253846407,
      "learning_rate": 8.442194860872298e-07,
      "loss": 0.0009,
      "step": 49870
    },
    {
      "epoch": 9069.09090909091,
      "grad_norm": 0.22526386380195618,
      "learning_rate": 8.44135084946862e-07,
      "loss": 0.0012,
      "step": 49880
    },
    {
      "epoch": 9070.90909090909,
      "grad_norm": 0.1613009124994278,
      "learning_rate": 8.440506651701313e-07,
      "loss": 0.0012,
      "step": 49890
    },
    {
      "epoch": 9072.727272727272,
      "grad_norm": 0.002435231115669012,
      "learning_rate": 8.439662267616092e-07,
      "loss": 0.0012,
      "step": 49900
    },
    {
      "epoch": 9074.545454545454,
      "grad_norm": 0.19160343706607819,
      "learning_rate": 8.438817697258689e-07,
      "loss": 0.0009,
      "step": 49910
    },
    {
      "epoch": 9076.363636363636,
      "grad_norm": 0.18596340715885162,
      "learning_rate": 8.437972940674836e-07,
      "loss": 0.0014,
      "step": 49920
    },
    {
      "epoch": 9078.181818181818,
      "grad_norm": 0.26414725184440613,
      "learning_rate": 8.437127997910285e-07,
      "loss": 0.0009,
      "step": 49930
    },
    {
      "epoch": 9080.0,
      "grad_norm": 0.0026667716447263956,
      "learning_rate": 8.436282869010787e-07,
      "loss": 0.001,
      "step": 49940
    },
    {
      "epoch": 9081.818181818182,
      "grad_norm": 0.23193292319774628,
      "learning_rate": 8.435437554022114e-07,
      "loss": 0.0012,
      "step": 49950
    },
    {
      "epoch": 9083.636363636364,
      "grad_norm": 0.18508677184581757,
      "learning_rate": 8.434592052990044e-07,
      "loss": 0.0012,
      "step": 49960
    },
    {
      "epoch": 9085.454545454546,
      "grad_norm": 0.0019207856385037303,
      "learning_rate": 8.433746365960361e-07,
      "loss": 0.0013,
      "step": 49970
    },
    {
      "epoch": 9087.272727272728,
      "grad_norm": 0.26165807247161865,
      "learning_rate": 8.432900492978863e-07,
      "loss": 0.0009,
      "step": 49980
    },
    {
      "epoch": 9089.09090909091,
      "grad_norm": 0.18680457770824432,
      "learning_rate": 8.432054434091361e-07,
      "loss": 0.001,
      "step": 49990
    },
    {
      "epoch": 9090.90909090909,
      "grad_norm": 0.0017756172455847263,
      "learning_rate": 8.431208189343669e-07,
      "loss": 0.0011,
      "step": 50000
    },
    {
      "epoch": 9090.90909090909,
      "eval_loss": 4.748805046081543,
      "eval_runtime": 0.9511,
      "eval_samples_per_second": 10.514,
      "eval_steps_per_second": 5.257,
      "step": 50000
    },
    {
      "epoch": 9092.727272727272,
      "grad_norm": 0.0020990127231925726,
      "learning_rate": 8.430361758781615e-07,
      "loss": 0.001,
      "step": 50010
    },
    {
      "epoch": 9094.545454545454,
      "grad_norm": 0.24903683364391327,
      "learning_rate": 8.429515142451038e-07,
      "loss": 0.0016,
      "step": 50020
    },
    {
      "epoch": 9096.363636363636,
      "grad_norm": 0.0022110475692898035,
      "learning_rate": 8.428668340397787e-07,
      "loss": 0.0007,
      "step": 50030
    },
    {
      "epoch": 9098.181818181818,
      "grad_norm": 0.18327197432518005,
      "learning_rate": 8.427821352667718e-07,
      "loss": 0.0013,
      "step": 50040
    },
    {
      "epoch": 9100.0,
      "grad_norm": 0.20334652066230774,
      "learning_rate": 8.426974179306699e-07,
      "loss": 0.0011,
      "step": 50050
    },
    {
      "epoch": 9101.818181818182,
      "grad_norm": 0.0013215281069278717,
      "learning_rate": 8.426126820360609e-07,
      "loss": 0.0011,
      "step": 50060
    },
    {
      "epoch": 9103.636363636364,
      "grad_norm": 0.0019068877445533872,
      "learning_rate": 8.425279275875335e-07,
      "loss": 0.0011,
      "step": 50070
    },
    {
      "epoch": 9105.454545454546,
      "grad_norm": 0.2558423578739166,
      "learning_rate": 8.424431545896775e-07,
      "loss": 0.0012,
      "step": 50080
    },
    {
      "epoch": 9107.272727272728,
      "grad_norm": 0.00122774054761976,
      "learning_rate": 8.423583630470838e-07,
      "loss": 0.001,
      "step": 50090
    },
    {
      "epoch": 9109.09090909091,
      "grad_norm": 0.24749243259429932,
      "learning_rate": 8.422735529643443e-07,
      "loss": 0.0012,
      "step": 50100
    },
    {
      "epoch": 9110.90909090909,
      "grad_norm": 0.0027617644518613815,
      "learning_rate": 8.421887243460517e-07,
      "loss": 0.0011,
      "step": 50110
    },
    {
      "epoch": 9112.727272727272,
      "grad_norm": 0.0014119049301370978,
      "learning_rate": 8.421038771967999e-07,
      "loss": 0.0012,
      "step": 50120
    },
    {
      "epoch": 9114.545454545454,
      "grad_norm": 0.006035934668034315,
      "learning_rate": 8.420190115211834e-07,
      "loss": 0.001,
      "step": 50130
    },
    {
      "epoch": 9116.363636363636,
      "grad_norm": 0.2921580374240875,
      "learning_rate": 8.419341273237985e-07,
      "loss": 0.0013,
      "step": 50140
    },
    {
      "epoch": 9118.181818181818,
      "grad_norm": 0.3132806420326233,
      "learning_rate": 8.418492246092419e-07,
      "loss": 0.0011,
      "step": 50150
    },
    {
      "epoch": 9120.0,
      "grad_norm": 0.002637899946421385,
      "learning_rate": 8.417643033821113e-07,
      "loss": 0.0009,
      "step": 50160
    },
    {
      "epoch": 9121.818181818182,
      "grad_norm": 0.0012898091226816177,
      "learning_rate": 8.416793636470056e-07,
      "loss": 0.0012,
      "step": 50170
    },
    {
      "epoch": 9123.636363636364,
      "grad_norm": 0.19962769746780396,
      "learning_rate": 8.415944054085247e-07,
      "loss": 0.0012,
      "step": 50180
    },
    {
      "epoch": 9125.454545454546,
      "grad_norm": 0.22486068308353424,
      "learning_rate": 8.415094286712694e-07,
      "loss": 0.0009,
      "step": 50190
    },
    {
      "epoch": 9127.272727272728,
      "grad_norm": 0.1919933706521988,
      "learning_rate": 8.414244334398417e-07,
      "loss": 0.0011,
      "step": 50200
    },
    {
      "epoch": 9129.09090909091,
      "grad_norm": 0.0016881905030459166,
      "learning_rate": 8.413394197188442e-07,
      "loss": 0.0011,
      "step": 50210
    },
    {
      "epoch": 9130.90909090909,
      "grad_norm": 0.20533621311187744,
      "learning_rate": 8.412543875128808e-07,
      "loss": 0.0012,
      "step": 50220
    },
    {
      "epoch": 9132.727272727272,
      "grad_norm": 0.00206477427855134,
      "learning_rate": 8.411693368265565e-07,
      "loss": 0.001,
      "step": 50230
    },
    {
      "epoch": 9134.545454545454,
      "grad_norm": 0.0028206713031977415,
      "learning_rate": 8.410842676644769e-07,
      "loss": 0.0009,
      "step": 50240
    },
    {
      "epoch": 9136.363636363636,
      "grad_norm": 0.0014371451688930392,
      "learning_rate": 8.409991800312492e-07,
      "loss": 0.0013,
      "step": 50250
    },
    {
      "epoch": 9138.181818181818,
      "grad_norm": 0.00194441934581846,
      "learning_rate": 8.409140739314811e-07,
      "loss": 0.0011,
      "step": 50260
    },
    {
      "epoch": 9140.0,
      "grad_norm": 0.26203659176826477,
      "learning_rate": 8.408289493697813e-07,
      "loss": 0.0012,
      "step": 50270
    },
    {
      "epoch": 9141.818181818182,
      "grad_norm": 0.002133575500920415,
      "learning_rate": 8.407438063507599e-07,
      "loss": 0.0012,
      "step": 50280
    },
    {
      "epoch": 9143.636363636364,
      "grad_norm": 0.0017324292566627264,
      "learning_rate": 8.406586448790275e-07,
      "loss": 0.0009,
      "step": 50290
    },
    {
      "epoch": 9145.454545454546,
      "grad_norm": 0.0018552615074440837,
      "learning_rate": 8.405734649591962e-07,
      "loss": 0.0012,
      "step": 50300
    },
    {
      "epoch": 9147.272727272728,
      "grad_norm": 0.20023630559444427,
      "learning_rate": 8.404882665958787e-07,
      "loss": 0.0012,
      "step": 50310
    },
    {
      "epoch": 9149.09090909091,
      "grad_norm": 0.00239542149938643,
      "learning_rate": 8.404030497936887e-07,
      "loss": 0.0011,
      "step": 50320
    },
    {
      "epoch": 9150.90909090909,
      "grad_norm": 0.04884766787290573,
      "learning_rate": 8.403178145572414e-07,
      "loss": 0.0012,
      "step": 50330
    },
    {
      "epoch": 9152.727272727272,
      "grad_norm": 0.0019146406557410955,
      "learning_rate": 8.402325608911525e-07,
      "loss": 0.001,
      "step": 50340
    },
    {
      "epoch": 9154.545454545454,
      "grad_norm": 0.0016885045915842056,
      "learning_rate": 8.40147288800039e-07,
      "loss": 0.0008,
      "step": 50350
    },
    {
      "epoch": 9156.363636363636,
      "grad_norm": 0.31831902265548706,
      "learning_rate": 8.400619982885182e-07,
      "loss": 0.0015,
      "step": 50360
    },
    {
      "epoch": 9158.181818181818,
      "grad_norm": 0.244925856590271,
      "learning_rate": 8.399766893612096e-07,
      "loss": 0.0011,
      "step": 50370
    },
    {
      "epoch": 9160.0,
      "grad_norm": 0.001444379216991365,
      "learning_rate": 8.398913620227326e-07,
      "loss": 0.0011,
      "step": 50380
    },
    {
      "epoch": 9161.818181818182,
      "grad_norm": 0.309180349111557,
      "learning_rate": 8.398060162777082e-07,
      "loss": 0.0011,
      "step": 50390
    },
    {
      "epoch": 9163.636363636364,
      "grad_norm": 0.1861451417207718,
      "learning_rate": 8.397206521307583e-07,
      "loss": 0.0012,
      "step": 50400
    },
    {
      "epoch": 9165.454545454546,
      "grad_norm": 0.001297563430853188,
      "learning_rate": 8.396352695865054e-07,
      "loss": 0.0009,
      "step": 50410
    },
    {
      "epoch": 9167.272727272728,
      "grad_norm": 0.001771757728420198,
      "learning_rate": 8.395498686495739e-07,
      "loss": 0.001,
      "step": 50420
    },
    {
      "epoch": 9169.09090909091,
      "grad_norm": 0.0016610244056209922,
      "learning_rate": 8.394644493245881e-07,
      "loss": 0.0012,
      "step": 50430
    },
    {
      "epoch": 9170.90909090909,
      "grad_norm": 0.0014125732704997063,
      "learning_rate": 8.393790116161739e-07,
      "loss": 0.0011,
      "step": 50440
    },
    {
      "epoch": 9172.727272727272,
      "grad_norm": 0.2692045569419861,
      "learning_rate": 8.392935555289583e-07,
      "loss": 0.0011,
      "step": 50450
    },
    {
      "epoch": 9174.545454545454,
      "grad_norm": 0.002085721120238304,
      "learning_rate": 8.39208081067569e-07,
      "loss": 0.0011,
      "step": 50460
    },
    {
      "epoch": 9176.363636363636,
      "grad_norm": 0.19715023040771484,
      "learning_rate": 8.39122588236635e-07,
      "loss": 0.0011,
      "step": 50470
    },
    {
      "epoch": 9178.181818181818,
      "grad_norm": 0.0024566722568124533,
      "learning_rate": 8.390370770407857e-07,
      "loss": 0.0011,
      "step": 50480
    },
    {
      "epoch": 9180.0,
      "grad_norm": 0.29711368680000305,
      "learning_rate": 8.389515474846522e-07,
      "loss": 0.0013,
      "step": 50490
    },
    {
      "epoch": 9181.818181818182,
      "grad_norm": 0.22632203996181488,
      "learning_rate": 8.388659995728662e-07,
      "loss": 0.0012,
      "step": 50500
    },
    {
      "epoch": 9181.818181818182,
      "eval_loss": 4.801348686218262,
      "eval_runtime": 0.9524,
      "eval_samples_per_second": 10.5,
      "eval_steps_per_second": 5.25,
      "step": 50500
    },
    {
      "epoch": 9183.636363636364,
      "grad_norm": 0.16790655255317688,
      "learning_rate": 8.387804333100603e-07,
      "loss": 0.0009,
      "step": 50510
    },
    {
      "epoch": 9185.454545454546,
      "grad_norm": 0.2424793690443039,
      "learning_rate": 8.386948487008686e-07,
      "loss": 0.0012,
      "step": 50520
    },
    {
      "epoch": 9187.272727272728,
      "grad_norm": 0.1861192137002945,
      "learning_rate": 8.386092457499257e-07,
      "loss": 0.0012,
      "step": 50530
    },
    {
      "epoch": 9189.09090909091,
      "grad_norm": 0.3961586654186249,
      "learning_rate": 8.385236244618674e-07,
      "loss": 0.0011,
      "step": 50540
    },
    {
      "epoch": 9190.90909090909,
      "grad_norm": 0.04784984886646271,
      "learning_rate": 8.384379848413303e-07,
      "loss": 0.0009,
      "step": 50550
    },
    {
      "epoch": 9192.727272727272,
      "grad_norm": 0.1851024478673935,
      "learning_rate": 8.383523268929525e-07,
      "loss": 0.0012,
      "step": 50560
    },
    {
      "epoch": 9194.545454545454,
      "grad_norm": 0.25066259503364563,
      "learning_rate": 8.382666506213723e-07,
      "loss": 0.0009,
      "step": 50570
    },
    {
      "epoch": 9196.363636363636,
      "grad_norm": 0.0019977521151304245,
      "learning_rate": 8.381809560312296e-07,
      "loss": 0.0012,
      "step": 50580
    },
    {
      "epoch": 9198.181818181818,
      "grad_norm": 0.0019040004117414355,
      "learning_rate": 8.380952431271652e-07,
      "loss": 0.0011,
      "step": 50590
    },
    {
      "epoch": 9200.0,
      "grad_norm": 0.0020545346196740866,
      "learning_rate": 8.380095119138208e-07,
      "loss": 0.0012,
      "step": 50600
    },
    {
      "epoch": 9201.818181818182,
      "grad_norm": 0.2732495367527008,
      "learning_rate": 8.379237623958393e-07,
      "loss": 0.001,
      "step": 50610
    },
    {
      "epoch": 9203.636363636364,
      "grad_norm": 0.004215361550450325,
      "learning_rate": 8.37837994577864e-07,
      "loss": 0.001,
      "step": 50620
    },
    {
      "epoch": 9205.454545454546,
      "grad_norm": 0.181744784116745,
      "learning_rate": 8.377522084645397e-07,
      "loss": 0.0012,
      "step": 50630
    },
    {
      "epoch": 9207.272727272728,
      "grad_norm": 0.1793292909860611,
      "learning_rate": 8.376664040605122e-07,
      "loss": 0.0012,
      "step": 50640
    },
    {
      "epoch": 9209.09090909091,
      "grad_norm": 0.21134765446186066,
      "learning_rate": 8.375805813704282e-07,
      "loss": 0.0011,
      "step": 50650
    },
    {
      "epoch": 9210.90909090909,
      "grad_norm": 0.0014649551594629884,
      "learning_rate": 8.374947403989352e-07,
      "loss": 0.0012,
      "step": 50660
    },
    {
      "epoch": 9212.727272727272,
      "grad_norm": 0.22285595536231995,
      "learning_rate": 8.374088811506819e-07,
      "loss": 0.0012,
      "step": 50670
    },
    {
      "epoch": 9214.545454545454,
      "grad_norm": 0.0011656691785901785,
      "learning_rate": 8.373230036303178e-07,
      "loss": 0.0007,
      "step": 50680
    },
    {
      "epoch": 9216.363636363636,
      "grad_norm": 0.21064482629299164,
      "learning_rate": 8.372371078424941e-07,
      "loss": 0.0014,
      "step": 50690
    },
    {
      "epoch": 9218.181818181818,
      "grad_norm": 0.0011413382599130273,
      "learning_rate": 8.371511937918617e-07,
      "loss": 0.0009,
      "step": 50700
    },
    {
      "epoch": 9220.0,
      "grad_norm": 0.25081127882003784,
      "learning_rate": 8.370652614830736e-07,
      "loss": 0.0012,
      "step": 50710
    },
    {
      "epoch": 9221.818181818182,
      "grad_norm": 0.0013138956855982542,
      "learning_rate": 8.369793109207833e-07,
      "loss": 0.0011,
      "step": 50720
    },
    {
      "epoch": 9223.636363636364,
      "grad_norm": 0.19945648312568665,
      "learning_rate": 8.368933421096453e-07,
      "loss": 0.0013,
      "step": 50730
    },
    {
      "epoch": 9225.454545454546,
      "grad_norm": 0.003608024911954999,
      "learning_rate": 8.368073550543155e-07,
      "loss": 0.001,
      "step": 50740
    },
    {
      "epoch": 9227.272727272728,
      "grad_norm": 0.3158261775970459,
      "learning_rate": 8.367213497594501e-07,
      "loss": 0.0013,
      "step": 50750
    },
    {
      "epoch": 9229.09090909091,
      "grad_norm": 0.20046383142471313,
      "learning_rate": 8.366353262297068e-07,
      "loss": 0.001,
      "step": 50760
    },
    {
      "epoch": 9230.90909090909,
      "grad_norm": 0.23423083126544952,
      "learning_rate": 8.365492844697442e-07,
      "loss": 0.0011,
      "step": 50770
    },
    {
      "epoch": 9232.727272727272,
      "grad_norm": 0.0031284429132938385,
      "learning_rate": 8.364632244842217e-07,
      "loss": 0.0013,
      "step": 50780
    },
    {
      "epoch": 9234.545454545454,
      "grad_norm": 0.23551392555236816,
      "learning_rate": 8.363771462777998e-07,
      "loss": 0.001,
      "step": 50790
    },
    {
      "epoch": 9236.363636363636,
      "grad_norm": 0.020792314782738686,
      "learning_rate": 8.362910498551401e-07,
      "loss": 0.0011,
      "step": 50800
    },
    {
      "epoch": 9238.181818181818,
      "grad_norm": 0.0022042468190193176,
      "learning_rate": 8.36204935220905e-07,
      "loss": 0.001,
      "step": 50810
    },
    {
      "epoch": 9240.0,
      "grad_norm": 0.20496416091918945,
      "learning_rate": 8.361188023797581e-07,
      "loss": 0.0012,
      "step": 50820
    },
    {
      "epoch": 9241.818181818182,
      "grad_norm": 0.19387830793857574,
      "learning_rate": 8.360326513363635e-07,
      "loss": 0.0011,
      "step": 50830
    },
    {
      "epoch": 9243.636363636364,
      "grad_norm": 0.001799311488866806,
      "learning_rate": 8.359464820953871e-07,
      "loss": 0.0009,
      "step": 50840
    },
    {
      "epoch": 9245.454545454546,
      "grad_norm": 0.23493410646915436,
      "learning_rate": 8.358602946614951e-07,
      "loss": 0.0014,
      "step": 50850
    },
    {
      "epoch": 9247.272727272728,
      "grad_norm": 0.23719456791877747,
      "learning_rate": 8.35774089039355e-07,
      "loss": 0.001,
      "step": 50860
    },
    {
      "epoch": 9249.09090909091,
      "grad_norm": 0.0015616773162037134,
      "learning_rate": 8.35687865233635e-07,
      "loss": 0.001,
      "step": 50870
    },
    {
      "epoch": 9250.90909090909,
      "grad_norm": 0.1803639531135559,
      "learning_rate": 8.356016232490047e-07,
      "loss": 0.0012,
      "step": 50880
    },
    {
      "epoch": 9252.727272727272,
      "grad_norm": 0.2656213939189911,
      "learning_rate": 8.355153630901344e-07,
      "loss": 0.001,
      "step": 50890
    },
    {
      "epoch": 9254.545454545454,
      "grad_norm": 0.19936411082744598,
      "learning_rate": 8.354290847616954e-07,
      "loss": 0.0009,
      "step": 50900
    },
    {
      "epoch": 9256.363636363636,
      "grad_norm": 0.2496967762708664,
      "learning_rate": 8.353427882683599e-07,
      "loss": 0.0014,
      "step": 50910
    },
    {
      "epoch": 9258.181818181818,
      "grad_norm": 0.18590731918811798,
      "learning_rate": 8.352564736148015e-07,
      "loss": 0.001,
      "step": 50920
    },
    {
      "epoch": 9260.0,
      "grad_norm": 0.020203087478876114,
      "learning_rate": 8.351701408056946e-07,
      "loss": 0.0011,
      "step": 50930
    },
    {
      "epoch": 9261.818181818182,
      "grad_norm": 0.18644393980503082,
      "learning_rate": 8.350837898457141e-07,
      "loss": 0.0011,
      "step": 50940
    },
    {
      "epoch": 9263.636363636364,
      "grad_norm": 0.001164202461950481,
      "learning_rate": 8.349974207395365e-07,
      "loss": 0.0013,
      "step": 50950
    },
    {
      "epoch": 9265.454545454546,
      "grad_norm": 0.006570270285010338,
      "learning_rate": 8.34911033491839e-07,
      "loss": 0.0006,
      "step": 50960
    },
    {
      "epoch": 9267.272727272728,
      "grad_norm": 0.2504191994667053,
      "learning_rate": 8.348246281072997e-07,
      "loss": 0.0014,
      "step": 50970
    },
    {
      "epoch": 9269.09090909091,
      "grad_norm": 0.16279958188533783,
      "learning_rate": 8.34738204590598e-07,
      "loss": 0.0012,
      "step": 50980
    },
    {
      "epoch": 9270.90909090909,
      "grad_norm": 0.1568785011768341,
      "learning_rate": 8.34651762946414e-07,
      "loss": 0.0009,
      "step": 50990
    },
    {
      "epoch": 9272.727272727272,
      "grad_norm": 0.001718832179903984,
      "learning_rate": 8.34565303179429e-07,
      "loss": 0.0011,
      "step": 51000
    },
    {
      "epoch": 9272.727272727272,
      "eval_loss": 4.737442970275879,
      "eval_runtime": 0.9551,
      "eval_samples_per_second": 10.47,
      "eval_steps_per_second": 5.235,
      "step": 51000
    },
    {
      "epoch": 9274.545454545454,
      "grad_norm": 0.00425688223913312,
      "learning_rate": 8.344788252943251e-07,
      "loss": 0.0012,
      "step": 51010
    },
    {
      "epoch": 9276.363636363636,
      "grad_norm": 0.1920114904642105,
      "learning_rate": 8.343923292957853e-07,
      "loss": 0.0013,
      "step": 51020
    },
    {
      "epoch": 9278.181818181818,
      "grad_norm": 0.3258833885192871,
      "learning_rate": 8.343058151884941e-07,
      "loss": 0.0011,
      "step": 51030
    },
    {
      "epoch": 9280.0,
      "grad_norm": 0.23730358481407166,
      "learning_rate": 8.342192829771361e-07,
      "loss": 0.0009,
      "step": 51040
    },
    {
      "epoch": 9281.818181818182,
      "grad_norm": 0.21156997978687286,
      "learning_rate": 8.341327326663976e-07,
      "loss": 0.0012,
      "step": 51050
    },
    {
      "epoch": 9283.636363636364,
      "grad_norm": 0.3115725815296173,
      "learning_rate": 8.340461642609657e-07,
      "loss": 0.001,
      "step": 51060
    },
    {
      "epoch": 9285.454545454546,
      "grad_norm": 0.1805974394083023,
      "learning_rate": 8.339595777655286e-07,
      "loss": 0.0012,
      "step": 51070
    },
    {
      "epoch": 9287.272727272728,
      "grad_norm": 0.0016131505835801363,
      "learning_rate": 8.338729731847751e-07,
      "loss": 0.0009,
      "step": 51080
    },
    {
      "epoch": 9289.09090909091,
      "grad_norm": 0.0019287244649603963,
      "learning_rate": 8.337863505233953e-07,
      "loss": 0.0012,
      "step": 51090
    },
    {
      "epoch": 9290.90909090909,
      "grad_norm": 0.002344924956560135,
      "learning_rate": 8.336997097860799e-07,
      "loss": 0.0012,
      "step": 51100
    },
    {
      "epoch": 9292.727272727272,
      "grad_norm": 0.0017196914413943887,
      "learning_rate": 8.336130509775212e-07,
      "loss": 0.0012,
      "step": 51110
    },
    {
      "epoch": 9294.545454545454,
      "grad_norm": 0.00247512711212039,
      "learning_rate": 8.335263741024122e-07,
      "loss": 0.0007,
      "step": 51120
    },
    {
      "epoch": 9296.363636363636,
      "grad_norm": 0.0016924123046919703,
      "learning_rate": 8.334396791654466e-07,
      "loss": 0.0011,
      "step": 51130
    },
    {
      "epoch": 9298.181818181818,
      "grad_norm": 0.23999644815921783,
      "learning_rate": 8.333529661713192e-07,
      "loss": 0.0014,
      "step": 51140
    },
    {
      "epoch": 9300.0,
      "grad_norm": 0.02017187885940075,
      "learning_rate": 8.332662351247262e-07,
      "loss": 0.0011,
      "step": 51150
    },
    {
      "epoch": 9301.818181818182,
      "grad_norm": 0.17814506590366364,
      "learning_rate": 8.331794860303643e-07,
      "loss": 0.001,
      "step": 51160
    },
    {
      "epoch": 9303.636363636364,
      "grad_norm": 0.18499206006526947,
      "learning_rate": 8.330927188929311e-07,
      "loss": 0.0011,
      "step": 51170
    },
    {
      "epoch": 9305.454545454546,
      "grad_norm": 0.18152257800102234,
      "learning_rate": 8.330059337171258e-07,
      "loss": 0.0013,
      "step": 51180
    },
    {
      "epoch": 9307.272727272728,
      "grad_norm": 0.003252155613154173,
      "learning_rate": 8.32919130507648e-07,
      "loss": 0.0011,
      "step": 51190
    },
    {
      "epoch": 9309.09090909091,
      "grad_norm": 0.16800975799560547,
      "learning_rate": 8.328323092691984e-07,
      "loss": 0.0013,
      "step": 51200
    },
    {
      "epoch": 9310.90909090909,
      "grad_norm": 0.27786174416542053,
      "learning_rate": 8.327454700064786e-07,
      "loss": 0.001,
      "step": 51210
    },
    {
      "epoch": 9312.727272727272,
      "grad_norm": 0.2178102284669876,
      "learning_rate": 8.326586127241917e-07,
      "loss": 0.0011,
      "step": 51220
    },
    {
      "epoch": 9314.545454545454,
      "grad_norm": 0.2049838900566101,
      "learning_rate": 8.325717374270411e-07,
      "loss": 0.0012,
      "step": 51230
    },
    {
      "epoch": 9316.363636363636,
      "grad_norm": 0.002287871204316616,
      "learning_rate": 8.324848441197317e-07,
      "loss": 0.0011,
      "step": 51240
    },
    {
      "epoch": 9318.181818181818,
      "grad_norm": 0.23702126741409302,
      "learning_rate": 8.323979328069688e-07,
      "loss": 0.0012,
      "step": 51250
    },
    {
      "epoch": 9320.0,
      "grad_norm": 0.0012061403831467032,
      "learning_rate": 8.323110034934593e-07,
      "loss": 0.0011,
      "step": 51260
    },
    {
      "epoch": 9321.818181818182,
      "grad_norm": 0.23354315757751465,
      "learning_rate": 8.322240561839108e-07,
      "loss": 0.0012,
      "step": 51270
    },
    {
      "epoch": 9323.636363636364,
      "grad_norm": 0.0018778624944388866,
      "learning_rate": 8.321370908830317e-07,
      "loss": 0.0009,
      "step": 51280
    },
    {
      "epoch": 9325.454545454546,
      "grad_norm": 0.0017804999370127916,
      "learning_rate": 8.320501075955315e-07,
      "loss": 0.0011,
      "step": 51290
    },
    {
      "epoch": 9327.272727272728,
      "grad_norm": 0.18852367997169495,
      "learning_rate": 8.319631063261207e-07,
      "loss": 0.0012,
      "step": 51300
    },
    {
      "epoch": 9329.09090909091,
      "grad_norm": 0.2203889936208725,
      "learning_rate": 8.318760870795111e-07,
      "loss": 0.0012,
      "step": 51310
    },
    {
      "epoch": 9330.90909090909,
      "grad_norm": 0.2217666208744049,
      "learning_rate": 8.317890498604149e-07,
      "loss": 0.001,
      "step": 51320
    },
    {
      "epoch": 9332.727272727272,
      "grad_norm": 0.22005440294742584,
      "learning_rate": 8.317019946735455e-07,
      "loss": 0.0012,
      "step": 51330
    },
    {
      "epoch": 9334.545454545454,
      "grad_norm": 0.16373418271541595,
      "learning_rate": 8.316149215236175e-07,
      "loss": 0.0014,
      "step": 51340
    },
    {
      "epoch": 9336.363636363636,
      "grad_norm": 0.23391093313694,
      "learning_rate": 8.315278304153459e-07,
      "loss": 0.0009,
      "step": 51350
    },
    {
      "epoch": 9338.181818181818,
      "grad_norm": 0.001873894827440381,
      "learning_rate": 8.314407213534476e-07,
      "loss": 0.0009,
      "step": 51360
    },
    {
      "epoch": 9340.0,
      "grad_norm": 0.0016732064541429281,
      "learning_rate": 8.313535943426394e-07,
      "loss": 0.0012,
      "step": 51370
    },
    {
      "epoch": 9341.818181818182,
      "grad_norm": 0.2359766960144043,
      "learning_rate": 8.3126644938764e-07,
      "loss": 0.0012,
      "step": 51380
    },
    {
      "epoch": 9343.636363636364,
      "grad_norm": 0.02429601550102234,
      "learning_rate": 8.311792864931683e-07,
      "loss": 0.0012,
      "step": 51390
    },
    {
      "epoch": 9345.454545454546,
      "grad_norm": 0.2295566201210022,
      "learning_rate": 8.31092105663945e-07,
      "loss": 0.0009,
      "step": 51400
    },
    {
      "epoch": 9347.272727272728,
      "grad_norm": 0.0013326964108273387,
      "learning_rate": 8.310049069046909e-07,
      "loss": 0.0009,
      "step": 51410
    },
    {
      "epoch": 9349.09090909091,
      "grad_norm": 0.001906228601001203,
      "learning_rate": 8.309176902201283e-07,
      "loss": 0.0012,
      "step": 51420
    },
    {
      "epoch": 9350.90909090909,
      "grad_norm": 0.1796066015958786,
      "learning_rate": 8.308304556149803e-07,
      "loss": 0.0012,
      "step": 51430
    },
    {
      "epoch": 9352.727272727272,
      "grad_norm": 0.0015640881611034274,
      "learning_rate": 8.307432030939712e-07,
      "loss": 0.0012,
      "step": 51440
    },
    {
      "epoch": 9354.545454545454,
      "grad_norm": 0.30991360545158386,
      "learning_rate": 8.306559326618259e-07,
      "loss": 0.0011,
      "step": 51450
    },
    {
      "epoch": 9356.363636363636,
      "grad_norm": 0.0016709694173187017,
      "learning_rate": 8.305686443232706e-07,
      "loss": 0.0009,
      "step": 51460
    },
    {
      "epoch": 9358.181818181818,
      "grad_norm": 0.0015860074199736118,
      "learning_rate": 8.304813380830322e-07,
      "loss": 0.0011,
      "step": 51470
    },
    {
      "epoch": 9360.0,
      "grad_norm": 0.28467586636543274,
      "learning_rate": 8.303940139458388e-07,
      "loss": 0.0012,
      "step": 51480
    },
    {
      "epoch": 9361.818181818182,
      "grad_norm": 0.18039453029632568,
      "learning_rate": 8.303066719164193e-07,
      "loss": 0.001,
      "step": 51490
    },
    {
      "epoch": 9363.636363636364,
      "grad_norm": 0.3059920370578766,
      "learning_rate": 8.302193119995038e-07,
      "loss": 0.0014,
      "step": 51500
    },
    {
      "epoch": 9363.636363636364,
      "eval_loss": 4.756705284118652,
      "eval_runtime": 0.9529,
      "eval_samples_per_second": 10.494,
      "eval_steps_per_second": 5.247,
      "step": 51500
    },
    {
      "epoch": 9365.454545454546,
      "grad_norm": 0.0013637065421789885,
      "learning_rate": 8.301319341998231e-07,
      "loss": 0.0009,
      "step": 51510
    },
    {
      "epoch": 9367.272727272728,
      "grad_norm": 0.002299543935805559,
      "learning_rate": 8.300445385221089e-07,
      "loss": 0.001,
      "step": 51520
    },
    {
      "epoch": 9369.09090909091,
      "grad_norm": 0.19746769964694977,
      "learning_rate": 8.299571249710944e-07,
      "loss": 0.0013,
      "step": 51530
    },
    {
      "epoch": 9370.90909090909,
      "grad_norm": 0.0024250640999525785,
      "learning_rate": 8.298696935515131e-07,
      "loss": 0.0011,
      "step": 51540
    },
    {
      "epoch": 9372.727272727272,
      "grad_norm": 0.0020114548970013857,
      "learning_rate": 8.297822442680998e-07,
      "loss": 0.0009,
      "step": 51550
    },
    {
      "epoch": 9374.545454545454,
      "grad_norm": 0.23414379358291626,
      "learning_rate": 8.296947771255904e-07,
      "loss": 0.0014,
      "step": 51560
    },
    {
      "epoch": 9376.363636363636,
      "grad_norm": 0.01566597819328308,
      "learning_rate": 8.296072921287216e-07,
      "loss": 0.0011,
      "step": 51570
    },
    {
      "epoch": 9378.181818181818,
      "grad_norm": 0.22771161794662476,
      "learning_rate": 8.295197892822311e-07,
      "loss": 0.001,
      "step": 51580
    },
    {
      "epoch": 9380.0,
      "grad_norm": 0.2456316500902176,
      "learning_rate": 8.294322685908575e-07,
      "loss": 0.0011,
      "step": 51590
    },
    {
      "epoch": 9381.818181818182,
      "grad_norm": 0.3231452703475952,
      "learning_rate": 8.293447300593402e-07,
      "loss": 0.0011,
      "step": 51600
    },
    {
      "epoch": 9383.636363636364,
      "grad_norm": 0.0014566350728273392,
      "learning_rate": 8.292571736924201e-07,
      "loss": 0.0011,
      "step": 51610
    },
    {
      "epoch": 9385.454545454546,
      "grad_norm": 0.001595764420926571,
      "learning_rate": 8.291695994948386e-07,
      "loss": 0.0009,
      "step": 51620
    },
    {
      "epoch": 9387.272727272728,
      "grad_norm": 0.002395190531387925,
      "learning_rate": 8.290820074713382e-07,
      "loss": 0.0012,
      "step": 51630
    },
    {
      "epoch": 9389.09090909091,
      "grad_norm": 0.0016622624825686216,
      "learning_rate": 8.289943976266624e-07,
      "loss": 0.0012,
      "step": 51640
    },
    {
      "epoch": 9390.90909090909,
      "grad_norm": 0.24963167309761047,
      "learning_rate": 8.289067699655558e-07,
      "loss": 0.0012,
      "step": 51650
    },
    {
      "epoch": 9392.727272727272,
      "grad_norm": 0.22940027713775635,
      "learning_rate": 8.288191244927636e-07,
      "loss": 0.0012,
      "step": 51660
    },
    {
      "epoch": 9394.545454545454,
      "grad_norm": 0.1677175760269165,
      "learning_rate": 8.287314612130323e-07,
      "loss": 0.0009,
      "step": 51670
    },
    {
      "epoch": 9396.363636363636,
      "grad_norm": 0.23504668474197388,
      "learning_rate": 8.28643780131109e-07,
      "loss": 0.0013,
      "step": 51680
    },
    {
      "epoch": 9398.181818181818,
      "grad_norm": 0.304539293050766,
      "learning_rate": 8.285560812517422e-07,
      "loss": 0.0012,
      "step": 51690
    },
    {
      "epoch": 9400.0,
      "grad_norm": 0.002187205944210291,
      "learning_rate": 8.284683645796813e-07,
      "loss": 0.0009,
      "step": 51700
    },
    {
      "epoch": 9401.818181818182,
      "grad_norm": 0.21045005321502686,
      "learning_rate": 8.28380630119676e-07,
      "loss": 0.0012,
      "step": 51710
    },
    {
      "epoch": 9403.636363636364,
      "grad_norm": 0.003213584190234542,
      "learning_rate": 8.282928778764782e-07,
      "loss": 0.0011,
      "step": 51720
    },
    {
      "epoch": 9405.454545454546,
      "grad_norm": 0.002036012476310134,
      "learning_rate": 8.282051078548396e-07,
      "loss": 0.0007,
      "step": 51730
    },
    {
      "epoch": 9407.272727272728,
      "grad_norm": 0.001489768037572503,
      "learning_rate": 8.281173200595133e-07,
      "loss": 0.0012,
      "step": 51740
    },
    {
      "epoch": 9409.09090909091,
      "grad_norm": 0.0012808723840862513,
      "learning_rate": 8.280295144952536e-07,
      "loss": 0.0012,
      "step": 51750
    },
    {
      "epoch": 9410.90909090909,
      "grad_norm": 0.0026965828146785498,
      "learning_rate": 8.279416911668153e-07,
      "loss": 0.0012,
      "step": 51760
    },
    {
      "epoch": 9412.727272727272,
      "grad_norm": 0.001985427923500538,
      "learning_rate": 8.278538500789547e-07,
      "loss": 0.0011,
      "step": 51770
    },
    {
      "epoch": 9414.545454545454,
      "grad_norm": 0.18217243254184723,
      "learning_rate": 8.277659912364286e-07,
      "loss": 0.001,
      "step": 51780
    },
    {
      "epoch": 9416.363636363636,
      "grad_norm": 0.001674748957157135,
      "learning_rate": 8.27678114643995e-07,
      "loss": 0.0009,
      "step": 51790
    },
    {
      "epoch": 9418.181818181818,
      "grad_norm": 0.18695569038391113,
      "learning_rate": 8.275902203064125e-07,
      "loss": 0.0013,
      "step": 51800
    },
    {
      "epoch": 9420.0,
      "grad_norm": 0.18994054198265076,
      "learning_rate": 8.275023082284412e-07,
      "loss": 0.0011,
      "step": 51810
    },
    {
      "epoch": 9421.818181818182,
      "grad_norm": 0.0014822522643953562,
      "learning_rate": 8.27414378414842e-07,
      "loss": 0.001,
      "step": 51820
    },
    {
      "epoch": 9423.636363636364,
      "grad_norm": 0.2311030775308609,
      "learning_rate": 8.273264308703765e-07,
      "loss": 0.0012,
      "step": 51830
    },
    {
      "epoch": 9425.454545454546,
      "grad_norm": 0.2000349462032318,
      "learning_rate": 8.272384655998074e-07,
      "loss": 0.0011,
      "step": 51840
    },
    {
      "epoch": 9427.272727272728,
      "grad_norm": 0.24648188054561615,
      "learning_rate": 8.271504826078985e-07,
      "loss": 0.001,
      "step": 51850
    },
    {
      "epoch": 9429.09090909091,
      "grad_norm": 0.001977585256099701,
      "learning_rate": 8.270624818994144e-07,
      "loss": 0.0011,
      "step": 51860
    },
    {
      "epoch": 9430.90909090909,
      "grad_norm": 0.0009141002665273845,
      "learning_rate": 8.269744634791207e-07,
      "loss": 0.0012,
      "step": 51870
    },
    {
      "epoch": 9432.727272727272,
      "grad_norm": 0.22228877246379852,
      "learning_rate": 8.268864273517841e-07,
      "loss": 0.001,
      "step": 51880
    },
    {
      "epoch": 9434.545454545454,
      "grad_norm": 0.01704281009733677,
      "learning_rate": 8.267983735221719e-07,
      "loss": 0.0015,
      "step": 51890
    },
    {
      "epoch": 9436.363636363636,
      "grad_norm": 0.18698029220104218,
      "learning_rate": 8.267103019950528e-07,
      "loss": 0.0008,
      "step": 51900
    },
    {
      "epoch": 9438.181818181818,
      "grad_norm": 0.0016126225236803293,
      "learning_rate": 8.266222127751959e-07,
      "loss": 0.001,
      "step": 51910
    },
    {
      "epoch": 9440.0,
      "grad_norm": 0.25263169407844543,
      "learning_rate": 8.26534105867372e-07,
      "loss": 0.0012,
      "step": 51920
    },
    {
      "epoch": 9441.818181818182,
      "grad_norm": 0.026351869106292725,
      "learning_rate": 8.264459812763524e-07,
      "loss": 0.0012,
      "step": 51930
    },
    {
      "epoch": 9443.636363636364,
      "grad_norm": 0.17817813158035278,
      "learning_rate": 8.263578390069092e-07,
      "loss": 0.0012,
      "step": 51940
    },
    {
      "epoch": 9445.454545454546,
      "grad_norm": 0.23112046718597412,
      "learning_rate": 8.262696790638158e-07,
      "loss": 0.0009,
      "step": 51950
    },
    {
      "epoch": 9447.272727272728,
      "grad_norm": 0.0014732896815985441,
      "learning_rate": 8.261815014518465e-07,
      "loss": 0.001,
      "step": 51960
    },
    {
      "epoch": 9449.09090909091,
      "grad_norm": 0.001850512227974832,
      "learning_rate": 8.260933061757765e-07,
      "loss": 0.0011,
      "step": 51970
    },
    {
      "epoch": 9450.90909090909,
      "grad_norm": 0.3115573823451996,
      "learning_rate": 8.260050932403817e-07,
      "loss": 0.0012,
      "step": 51980
    },
    {
      "epoch": 9452.727272727272,
      "grad_norm": 0.0013224303256720304,
      "learning_rate": 8.259168626504393e-07,
      "loss": 0.0009,
      "step": 51990
    },
    {
      "epoch": 9454.545454545454,
      "grad_norm": 0.2365824431180954,
      "learning_rate": 8.258286144107276e-07,
      "loss": 0.0015,
      "step": 52000
    },
    {
      "epoch": 9454.545454545454,
      "eval_loss": 4.794813632965088,
      "eval_runtime": 0.9516,
      "eval_samples_per_second": 10.509,
      "eval_steps_per_second": 5.254,
      "step": 52000
    },
    {
      "epoch": 9456.363636363636,
      "grad_norm": 0.0012931067030876875,
      "learning_rate": 8.257403485260254e-07,
      "loss": 0.0011,
      "step": 52010
    },
    {
      "epoch": 9458.181818181818,
      "grad_norm": 0.0016447642119601369,
      "learning_rate": 8.256520650011126e-07,
      "loss": 0.0009,
      "step": 52020
    },
    {
      "epoch": 9460.0,
      "grad_norm": 0.0016573442844673991,
      "learning_rate": 8.255637638407702e-07,
      "loss": 0.0012,
      "step": 52030
    },
    {
      "epoch": 9461.818181818182,
      "grad_norm": 0.0017252558609470725,
      "learning_rate": 8.254754450497803e-07,
      "loss": 0.0012,
      "step": 52040
    },
    {
      "epoch": 9463.636363636364,
      "grad_norm": 0.0021041075233370066,
      "learning_rate": 8.253871086329254e-07,
      "loss": 0.0011,
      "step": 52050
    },
    {
      "epoch": 9465.454545454546,
      "grad_norm": 0.004071880131959915,
      "learning_rate": 8.252987545949895e-07,
      "loss": 0.0012,
      "step": 52060
    },
    {
      "epoch": 9467.272727272728,
      "grad_norm": 0.1894625872373581,
      "learning_rate": 8.252103829407573e-07,
      "loss": 0.0012,
      "step": 52070
    },
    {
      "epoch": 9469.09090909091,
      "grad_norm": 0.0014847314450889826,
      "learning_rate": 8.251219936750143e-07,
      "loss": 0.0011,
      "step": 52080
    },
    {
      "epoch": 9470.90909090909,
      "grad_norm": 0.20359081029891968,
      "learning_rate": 8.250335868025475e-07,
      "loss": 0.001,
      "step": 52090
    },
    {
      "epoch": 9472.727272727272,
      "grad_norm": 0.0023183219600468874,
      "learning_rate": 8.249451623281442e-07,
      "loss": 0.0011,
      "step": 52100
    },
    {
      "epoch": 9474.545454545454,
      "grad_norm": 0.3059215843677521,
      "learning_rate": 8.248567202565932e-07,
      "loss": 0.0015,
      "step": 52110
    },
    {
      "epoch": 9476.363636363636,
      "grad_norm": 0.248724564909935,
      "learning_rate": 8.247682605926839e-07,
      "loss": 0.0009,
      "step": 52120
    },
    {
      "epoch": 9478.181818181818,
      "grad_norm": 0.0015398506075143814,
      "learning_rate": 8.246797833412067e-07,
      "loss": 0.001,
      "step": 52130
    },
    {
      "epoch": 9480.0,
      "grad_norm": 0.0024808719754219055,
      "learning_rate": 8.24591288506953e-07,
      "loss": 0.0012,
      "step": 52140
    },
    {
      "epoch": 9481.818181818182,
      "grad_norm": 0.002387173706665635,
      "learning_rate": 8.245027760947153e-07,
      "loss": 0.0011,
      "step": 52150
    },
    {
      "epoch": 9483.636363636364,
      "grad_norm": 0.2894355356693268,
      "learning_rate": 8.244142461092868e-07,
      "loss": 0.0013,
      "step": 52160
    },
    {
      "epoch": 9485.454545454546,
      "grad_norm": 0.18584048748016357,
      "learning_rate": 8.243256985554621e-07,
      "loss": 0.0007,
      "step": 52170
    },
    {
      "epoch": 9487.272727272728,
      "grad_norm": 0.2457374483346939,
      "learning_rate": 8.242371334380358e-07,
      "loss": 0.0014,
      "step": 52180
    },
    {
      "epoch": 9489.09090909091,
      "grad_norm": 0.18869031965732574,
      "learning_rate": 8.241485507618046e-07,
      "loss": 0.001,
      "step": 52190
    },
    {
      "epoch": 9490.90909090909,
      "grad_norm": 0.2606287896633148,
      "learning_rate": 8.240599505315654e-07,
      "loss": 0.0011,
      "step": 52200
    },
    {
      "epoch": 9492.727272727272,
      "grad_norm": 0.001545643899589777,
      "learning_rate": 8.239713327521164e-07,
      "loss": 0.0011,
      "step": 52210
    },
    {
      "epoch": 9494.545454545454,
      "grad_norm": 0.24513059854507446,
      "learning_rate": 8.238826974282564e-07,
      "loss": 0.0011,
      "step": 52220
    },
    {
      "epoch": 9496.363636363636,
      "grad_norm": 0.001614731620065868,
      "learning_rate": 8.237940445647857e-07,
      "loss": 0.0011,
      "step": 52230
    },
    {
      "epoch": 9498.181818181818,
      "grad_norm": 0.16909024119377136,
      "learning_rate": 8.23705374166505e-07,
      "loss": 0.0012,
      "step": 52240
    },
    {
      "epoch": 9500.0,
      "grad_norm": 0.005344733130186796,
      "learning_rate": 8.236166862382162e-07,
      "loss": 0.001,
      "step": 52250
    },
    {
      "epoch": 9501.818181818182,
      "grad_norm": 0.0017766987439244986,
      "learning_rate": 8.235279807847222e-07,
      "loss": 0.0011,
      "step": 52260
    },
    {
      "epoch": 9503.636363636364,
      "grad_norm": 0.21208816766738892,
      "learning_rate": 8.234392578108267e-07,
      "loss": 0.0014,
      "step": 52270
    },
    {
      "epoch": 9505.454545454546,
      "grad_norm": 0.27506306767463684,
      "learning_rate": 8.233505173213343e-07,
      "loss": 0.001,
      "step": 52280
    },
    {
      "epoch": 9507.272727272728,
      "grad_norm": 0.002805291209369898,
      "learning_rate": 8.23261759321051e-07,
      "loss": 0.0011,
      "step": 52290
    },
    {
      "epoch": 9509.09090909091,
      "grad_norm": 0.2932572662830353,
      "learning_rate": 8.231729838147832e-07,
      "loss": 0.0012,
      "step": 52300
    },
    {
      "epoch": 9510.90909090909,
      "grad_norm": 0.0029672100208699703,
      "learning_rate": 8.230841908073385e-07,
      "loss": 0.0009,
      "step": 52310
    },
    {
      "epoch": 9512.727272727272,
      "grad_norm": 0.23526224493980408,
      "learning_rate": 8.229953803035254e-07,
      "loss": 0.0014,
      "step": 52320
    },
    {
      "epoch": 9514.545454545454,
      "grad_norm": 0.26164278388023376,
      "learning_rate": 8.229065523081535e-07,
      "loss": 0.001,
      "step": 52330
    },
    {
      "epoch": 9516.363636363636,
      "grad_norm": 0.24218237400054932,
      "learning_rate": 8.228177068260329e-07,
      "loss": 0.001,
      "step": 52340
    },
    {
      "epoch": 9518.181818181818,
      "grad_norm": 0.24638983607292175,
      "learning_rate": 8.227288438619752e-07,
      "loss": 0.0015,
      "step": 52350
    },
    {
      "epoch": 9520.0,
      "grad_norm": 0.17248442769050598,
      "learning_rate": 8.226399634207928e-07,
      "loss": 0.0008,
      "step": 52360
    },
    {
      "epoch": 9521.818181818182,
      "grad_norm": 0.2333521693944931,
      "learning_rate": 8.225510655072987e-07,
      "loss": 0.0012,
      "step": 52370
    },
    {
      "epoch": 9523.636363636364,
      "grad_norm": 0.2447700798511505,
      "learning_rate": 8.224621501263071e-07,
      "loss": 0.0012,
      "step": 52380
    },
    {
      "epoch": 9525.454545454546,
      "grad_norm": 0.1879367232322693,
      "learning_rate": 8.223732172826334e-07,
      "loss": 0.0011,
      "step": 52390
    },
    {
      "epoch": 9527.272727272728,
      "grad_norm": 0.0012237566988915205,
      "learning_rate": 8.222842669810934e-07,
      "loss": 0.0008,
      "step": 52400
    },
    {
      "epoch": 9529.09090909091,
      "grad_norm": 0.3073420822620392,
      "learning_rate": 8.221952992265045e-07,
      "loss": 0.0013,
      "step": 52410
    },
    {
      "epoch": 9530.90909090909,
      "grad_norm": 0.0012060191947966814,
      "learning_rate": 8.221063140236842e-07,
      "loss": 0.0009,
      "step": 52420
    },
    {
      "epoch": 9532.727272727272,
      "grad_norm": 0.2328588366508484,
      "learning_rate": 8.220173113774517e-07,
      "loss": 0.0012,
      "step": 52430
    },
    {
      "epoch": 9534.545454545454,
      "grad_norm": 0.003125977935269475,
      "learning_rate": 8.219282912926269e-07,
      "loss": 0.001,
      "step": 52440
    },
    {
      "epoch": 9536.363636363636,
      "grad_norm": 0.22139619290828705,
      "learning_rate": 8.218392537740305e-07,
      "loss": 0.0013,
      "step": 52450
    },
    {
      "epoch": 9538.181818181818,
      "grad_norm": 0.0013590174494311213,
      "learning_rate": 8.217501988264843e-07,
      "loss": 0.0011,
      "step": 52460
    },
    {
      "epoch": 9540.0,
      "grad_norm": 0.025979138910770416,
      "learning_rate": 8.21661126454811e-07,
      "loss": 0.0012,
      "step": 52470
    },
    {
      "epoch": 9541.818181818182,
      "grad_norm": 0.18632976710796356,
      "learning_rate": 8.215720366638342e-07,
      "loss": 0.0009,
      "step": 52480
    },
    {
      "epoch": 9543.636363636364,
      "grad_norm": 0.19772475957870483,
      "learning_rate": 8.214829294583784e-07,
      "loss": 0.0013,
      "step": 52490
    },
    {
      "epoch": 9545.454545454546,
      "grad_norm": 0.27289608120918274,
      "learning_rate": 8.213938048432696e-07,
      "loss": 0.0013,
      "step": 52500
    },
    {
      "epoch": 9545.454545454546,
      "eval_loss": 4.858697891235352,
      "eval_runtime": 0.9496,
      "eval_samples_per_second": 10.531,
      "eval_steps_per_second": 5.266,
      "step": 52500
    },
    {
      "epoch": 9547.272727272728,
      "grad_norm": 0.19984284043312073,
      "learning_rate": 8.213046628233338e-07,
      "loss": 0.001,
      "step": 52510
    },
    {
      "epoch": 9549.09090909091,
      "grad_norm": 0.0019083942752331495,
      "learning_rate": 8.212155034033985e-07,
      "loss": 0.001,
      "step": 52520
    },
    {
      "epoch": 9550.90909090909,
      "grad_norm": 0.002760553965345025,
      "learning_rate": 8.211263265882922e-07,
      "loss": 0.0012,
      "step": 52530
    },
    {
      "epoch": 9552.727272727272,
      "grad_norm": 0.19639833271503448,
      "learning_rate": 8.21037132382844e-07,
      "loss": 0.001,
      "step": 52540
    },
    {
      "epoch": 9554.545454545454,
      "grad_norm": 0.0012794840149581432,
      "learning_rate": 8.209479207918844e-07,
      "loss": 0.0014,
      "step": 52550
    },
    {
      "epoch": 9556.363636363636,
      "grad_norm": 0.0015010818606242537,
      "learning_rate": 8.208586918202443e-07,
      "loss": 0.0006,
      "step": 52560
    },
    {
      "epoch": 9558.181818181818,
      "grad_norm": 0.0040086256340146065,
      "learning_rate": 8.207694454727558e-07,
      "loss": 0.0012,
      "step": 52570
    },
    {
      "epoch": 9560.0,
      "grad_norm": 0.21202731132507324,
      "learning_rate": 8.206801817542524e-07,
      "loss": 0.0012,
      "step": 52580
    },
    {
      "epoch": 9561.818181818182,
      "grad_norm": 0.0017308802343904972,
      "learning_rate": 8.205909006695677e-07,
      "loss": 0.001,
      "step": 52590
    },
    {
      "epoch": 9563.636363636364,
      "grad_norm": 0.1736816167831421,
      "learning_rate": 8.205016022235367e-07,
      "loss": 0.0012,
      "step": 52600
    },
    {
      "epoch": 9565.454545454546,
      "grad_norm": 0.19491714239120483,
      "learning_rate": 8.204122864209955e-07,
      "loss": 0.001,
      "step": 52610
    },
    {
      "epoch": 9567.272727272728,
      "grad_norm": 0.30606746673583984,
      "learning_rate": 8.203229532667807e-07,
      "loss": 0.0012,
      "step": 52620
    },
    {
      "epoch": 9569.09090909091,
      "grad_norm": 0.003129123942926526,
      "learning_rate": 8.202336027657302e-07,
      "loss": 0.0009,
      "step": 52630
    },
    {
      "epoch": 9570.90909090909,
      "grad_norm": 0.29817652702331543,
      "learning_rate": 8.201442349226827e-07,
      "loss": 0.0012,
      "step": 52640
    },
    {
      "epoch": 9572.727272727272,
      "grad_norm": 0.012460428290069103,
      "learning_rate": 8.200548497424777e-07,
      "loss": 0.001,
      "step": 52650
    },
    {
      "epoch": 9574.545454545454,
      "grad_norm": 0.009110206738114357,
      "learning_rate": 8.19965447229956e-07,
      "loss": 0.0011,
      "step": 52660
    },
    {
      "epoch": 9576.363636363636,
      "grad_norm": 0.2768672704696655,
      "learning_rate": 8.198760273899589e-07,
      "loss": 0.0014,
      "step": 52670
    },
    {
      "epoch": 9578.181818181818,
      "grad_norm": 1.2717885971069336,
      "learning_rate": 8.19786590227329e-07,
      "loss": 0.001,
      "step": 52680
    },
    {
      "epoch": 9580.0,
      "grad_norm": 0.02414579875767231,
      "learning_rate": 8.196971357469097e-07,
      "loss": 0.0031,
      "step": 52690
    },
    {
      "epoch": 9581.818181818182,
      "grad_norm": 1.7789463996887207,
      "learning_rate": 8.196076639535452e-07,
      "loss": 0.0012,
      "step": 52700
    },
    {
      "epoch": 9583.636363636364,
      "grad_norm": 0.15978918969631195,
      "learning_rate": 8.19518174852081e-07,
      "loss": 0.0012,
      "step": 52710
    },
    {
      "epoch": 9585.454545454546,
      "grad_norm": 0.19481056928634644,
      "learning_rate": 8.194286684473633e-07,
      "loss": 0.0015,
      "step": 52720
    },
    {
      "epoch": 9587.272727272728,
      "grad_norm": 0.2552371919155121,
      "learning_rate": 8.193391447442388e-07,
      "loss": 0.0009,
      "step": 52730
    },
    {
      "epoch": 9589.09090909091,
      "grad_norm": 0.18309885263442993,
      "learning_rate": 8.192496037475561e-07,
      "loss": 0.0013,
      "step": 52740
    },
    {
      "epoch": 9590.90909090909,
      "grad_norm": 0.2321346253156662,
      "learning_rate": 8.191600454621641e-07,
      "loss": 0.0012,
      "step": 52750
    },
    {
      "epoch": 9592.727272727272,
      "grad_norm": 0.2476392388343811,
      "learning_rate": 8.190704698929127e-07,
      "loss": 0.0009,
      "step": 52760
    },
    {
      "epoch": 9594.545454545454,
      "grad_norm": 0.024722285568714142,
      "learning_rate": 8.189808770446527e-07,
      "loss": 0.0012,
      "step": 52770
    },
    {
      "epoch": 9596.363636363636,
      "grad_norm": 0.018092922866344452,
      "learning_rate": 8.188912669222361e-07,
      "loss": 0.0011,
      "step": 52780
    },
    {
      "epoch": 9598.181818181818,
      "grad_norm": 0.06532354652881622,
      "learning_rate": 8.188016395305155e-07,
      "loss": 0.0013,
      "step": 52790
    },
    {
      "epoch": 9600.0,
      "grad_norm": 0.04991065710783005,
      "learning_rate": 8.187119948743449e-07,
      "loss": 0.0013,
      "step": 52800
    },
    {
      "epoch": 9601.818181818182,
      "grad_norm": 0.02488407865166664,
      "learning_rate": 8.186223329585784e-07,
      "loss": 0.0013,
      "step": 52810
    },
    {
      "epoch": 9603.636363636364,
      "grad_norm": 0.20178787410259247,
      "learning_rate": 8.185326537880721e-07,
      "loss": 0.0009,
      "step": 52820
    },
    {
      "epoch": 9605.454545454546,
      "grad_norm": 0.005361584480851889,
      "learning_rate": 8.184429573676824e-07,
      "loss": 0.0012,
      "step": 52830
    },
    {
      "epoch": 9607.272727272728,
      "grad_norm": 0.004454436711966991,
      "learning_rate": 8.183532437022664e-07,
      "loss": 0.0009,
      "step": 52840
    },
    {
      "epoch": 9609.09090909091,
      "grad_norm": 0.005811360664665699,
      "learning_rate": 8.18263512796683e-07,
      "loss": 0.0013,
      "step": 52850
    },
    {
      "epoch": 9610.90909090909,
      "grad_norm": 0.002447709208354354,
      "learning_rate": 8.181737646557911e-07,
      "loss": 0.0012,
      "step": 52860
    },
    {
      "epoch": 9612.727272727272,
      "grad_norm": 0.20943637192249298,
      "learning_rate": 8.180839992844511e-07,
      "loss": 0.0012,
      "step": 52870
    },
    {
      "epoch": 9614.545454545454,
      "grad_norm": 0.17716053128242493,
      "learning_rate": 8.179942166875239e-07,
      "loss": 0.0008,
      "step": 52880
    },
    {
      "epoch": 9616.363636363636,
      "grad_norm": 0.0036892842035740614,
      "learning_rate": 8.17904416869872e-07,
      "loss": 0.0013,
      "step": 52890
    },
    {
      "epoch": 9618.181818181818,
      "grad_norm": 0.2449275702238083,
      "learning_rate": 8.178145998363584e-07,
      "loss": 0.0012,
      "step": 52900
    },
    {
      "epoch": 9620.0,
      "grad_norm": 0.22439152002334595,
      "learning_rate": 8.177247655918467e-07,
      "loss": 0.001,
      "step": 52910
    },
    {
      "epoch": 9621.818181818182,
      "grad_norm": 0.0047278692945837975,
      "learning_rate": 8.176349141412021e-07,
      "loss": 0.0007,
      "step": 52920
    },
    {
      "epoch": 9623.636363636364,
      "grad_norm": 0.23136378824710846,
      "learning_rate": 8.175450454892905e-07,
      "loss": 0.0017,
      "step": 52930
    },
    {
      "epoch": 9625.454545454546,
      "grad_norm": 0.005987291224300861,
      "learning_rate": 8.174551596409784e-07,
      "loss": 0.0008,
      "step": 52940
    },
    {
      "epoch": 9627.272727272728,
      "grad_norm": 0.0031906391959637403,
      "learning_rate": 8.173652566011337e-07,
      "loss": 0.0012,
      "step": 52950
    },
    {
      "epoch": 9629.09090909091,
      "grad_norm": 0.24345950782299042,
      "learning_rate": 8.17275336374625e-07,
      "loss": 0.0012,
      "step": 52960
    },
    {
      "epoch": 9630.90909090909,
      "grad_norm": 0.19989413022994995,
      "learning_rate": 8.171853989663219e-07,
      "loss": 0.0011,
      "step": 52970
    },
    {
      "epoch": 9632.727272727272,
      "grad_norm": 0.003963305614888668,
      "learning_rate": 8.170954443810948e-07,
      "loss": 0.0011,
      "step": 52980
    },
    {
      "epoch": 9634.545454545454,
      "grad_norm": 0.040529824793338776,
      "learning_rate": 8.170054726238151e-07,
      "loss": 0.0012,
      "step": 52990
    },
    {
      "epoch": 9636.363636363636,
      "grad_norm": 0.0020191033836454153,
      "learning_rate": 8.16915483699355e-07,
      "loss": 0.0009,
      "step": 53000
    },
    {
      "epoch": 9636.363636363636,
      "eval_loss": 4.802298069000244,
      "eval_runtime": 0.958,
      "eval_samples_per_second": 10.439,
      "eval_steps_per_second": 5.219,
      "step": 53000
    },
    {
      "epoch": 9638.181818181818,
      "grad_norm": 0.0016941135982051492,
      "learning_rate": 8.168254776125882e-07,
      "loss": 0.0011,
      "step": 53010
    },
    {
      "epoch": 9640.0,
      "grad_norm": 0.22458414733409882,
      "learning_rate": 8.167354543683886e-07,
      "loss": 0.0012,
      "step": 53020
    },
    {
      "epoch": 9641.818181818182,
      "grad_norm": 0.006259217392653227,
      "learning_rate": 8.166454139716314e-07,
      "loss": 0.0012,
      "step": 53030
    },
    {
      "epoch": 9643.636363636364,
      "grad_norm": 0.1868303418159485,
      "learning_rate": 8.165553564271928e-07,
      "loss": 0.0011,
      "step": 53040
    },
    {
      "epoch": 9645.454545454546,
      "grad_norm": 0.0015645480016246438,
      "learning_rate": 8.164652817399495e-07,
      "loss": 0.0011,
      "step": 53050
    },
    {
      "epoch": 9647.272727272728,
      "grad_norm": 0.18650208413600922,
      "learning_rate": 8.163751899147798e-07,
      "loss": 0.001,
      "step": 53060
    },
    {
      "epoch": 9649.09090909091,
      "grad_norm": 0.20128677785396576,
      "learning_rate": 8.162850809565623e-07,
      "loss": 0.0011,
      "step": 53070
    },
    {
      "epoch": 9650.90909090909,
      "grad_norm": 0.005761187523603439,
      "learning_rate": 8.161949548701767e-07,
      "loss": 0.0012,
      "step": 53080
    },
    {
      "epoch": 9652.727272727272,
      "grad_norm": 0.00438713887706399,
      "learning_rate": 8.161048116605038e-07,
      "loss": 0.0012,
      "step": 53090
    },
    {
      "epoch": 9654.545454545454,
      "grad_norm": 0.21354776620864868,
      "learning_rate": 8.160146513324254e-07,
      "loss": 0.0009,
      "step": 53100
    },
    {
      "epoch": 9656.363636363636,
      "grad_norm": 0.2295878827571869,
      "learning_rate": 8.159244738908239e-07,
      "loss": 0.0011,
      "step": 53110
    },
    {
      "epoch": 9658.181818181818,
      "grad_norm": 0.005105506628751755,
      "learning_rate": 8.158342793405828e-07,
      "loss": 0.0011,
      "step": 53120
    },
    {
      "epoch": 9660.0,
      "grad_norm": 0.0024717063643038273,
      "learning_rate": 8.157440676865865e-07,
      "loss": 0.0012,
      "step": 53130
    },
    {
      "epoch": 9661.818181818182,
      "grad_norm": 0.0025491444393992424,
      "learning_rate": 8.156538389337204e-07,
      "loss": 0.0012,
      "step": 53140
    },
    {
      "epoch": 9663.636363636364,
      "grad_norm": 0.24464207887649536,
      "learning_rate": 8.155635930868707e-07,
      "loss": 0.0015,
      "step": 53150
    },
    {
      "epoch": 9665.454545454546,
      "grad_norm": 0.24626046419143677,
      "learning_rate": 8.154733301509247e-07,
      "loss": 0.0008,
      "step": 53160
    },
    {
      "epoch": 9667.272727272728,
      "grad_norm": 0.00207275478169322,
      "learning_rate": 8.153830501307705e-07,
      "loss": 0.0014,
      "step": 53170
    },
    {
      "epoch": 9669.09090909091,
      "grad_norm": 0.002539656823500991,
      "learning_rate": 8.15292753031297e-07,
      "loss": 0.0009,
      "step": 53180
    },
    {
      "epoch": 9670.90909090909,
      "grad_norm": 0.0017779279733076692,
      "learning_rate": 8.152024388573944e-07,
      "loss": 0.0012,
      "step": 53190
    },
    {
      "epoch": 9672.727272727272,
      "grad_norm": 0.0019239771645516157,
      "learning_rate": 8.151121076139533e-07,
      "loss": 0.0012,
      "step": 53200
    },
    {
      "epoch": 9674.545454545454,
      "grad_norm": 0.07108408957719803,
      "learning_rate": 8.150217593058657e-07,
      "loss": 0.0011,
      "step": 53210
    },
    {
      "epoch": 9676.363636363636,
      "grad_norm": 0.2400057315826416,
      "learning_rate": 8.149313939380244e-07,
      "loss": 0.0011,
      "step": 53220
    },
    {
      "epoch": 9678.181818181818,
      "grad_norm": 0.23987923562526703,
      "learning_rate": 8.148410115153228e-07,
      "loss": 0.0012,
      "step": 53230
    },
    {
      "epoch": 9680.0,
      "grad_norm": 0.002004066715016961,
      "learning_rate": 8.147506120426557e-07,
      "loss": 0.0011,
      "step": 53240
    },
    {
      "epoch": 9681.818181818182,
      "grad_norm": 0.22942429780960083,
      "learning_rate": 8.146601955249187e-07,
      "loss": 0.0011,
      "step": 53250
    },
    {
      "epoch": 9683.636363636364,
      "grad_norm": 0.002629240043461323,
      "learning_rate": 8.14569761967008e-07,
      "loss": 0.001,
      "step": 53260
    },
    {
      "epoch": 9685.454545454546,
      "grad_norm": 0.2286238819360733,
      "learning_rate": 8.144793113738211e-07,
      "loss": 0.0011,
      "step": 53270
    },
    {
      "epoch": 9687.272727272728,
      "grad_norm": 0.25494396686553955,
      "learning_rate": 8.143888437502564e-07,
      "loss": 0.0012,
      "step": 53280
    },
    {
      "epoch": 9689.09090909091,
      "grad_norm": 0.001856788876466453,
      "learning_rate": 8.142983591012127e-07,
      "loss": 0.0011,
      "step": 53290
    },
    {
      "epoch": 9690.90909090909,
      "grad_norm": 0.22435379028320312,
      "learning_rate": 8.142078574315906e-07,
      "loss": 0.0012,
      "step": 53300
    },
    {
      "epoch": 9692.727272727272,
      "grad_norm": 0.22448141872882843,
      "learning_rate": 8.141173387462907e-07,
      "loss": 0.0012,
      "step": 53310
    },
    {
      "epoch": 9694.545454545454,
      "grad_norm": 0.003390178782865405,
      "learning_rate": 8.140268030502153e-07,
      "loss": 0.0009,
      "step": 53320
    },
    {
      "epoch": 9696.363636363636,
      "grad_norm": 0.002008663723245263,
      "learning_rate": 8.139362503482672e-07,
      "loss": 0.001,
      "step": 53330
    },
    {
      "epoch": 9698.181818181818,
      "grad_norm": 0.18447189033031464,
      "learning_rate": 8.138456806453502e-07,
      "loss": 0.0012,
      "step": 53340
    },
    {
      "epoch": 9700.0,
      "grad_norm": 0.0017544992733746767,
      "learning_rate": 8.137550939463689e-07,
      "loss": 0.0011,
      "step": 53350
    },
    {
      "epoch": 9701.818181818182,
      "grad_norm": 0.21520768105983734,
      "learning_rate": 8.136644902562293e-07,
      "loss": 0.0012,
      "step": 53360
    },
    {
      "epoch": 9703.636363636364,
      "grad_norm": 0.0028217481449246407,
      "learning_rate": 8.135738695798376e-07,
      "loss": 0.0011,
      "step": 53370
    },
    {
      "epoch": 9705.454545454546,
      "grad_norm": 0.003044131910428405,
      "learning_rate": 8.134832319221014e-07,
      "loss": 0.0009,
      "step": 53380
    },
    {
      "epoch": 9707.272727272728,
      "grad_norm": 0.002293413272127509,
      "learning_rate": 8.133925772879291e-07,
      "loss": 0.0011,
      "step": 53390
    },
    {
      "epoch": 9709.09090909091,
      "grad_norm": 0.0020756602752953768,
      "learning_rate": 8.133019056822302e-07,
      "loss": 0.0012,
      "step": 53400
    },
    {
      "epoch": 9710.90909090909,
      "grad_norm": 0.002024985384196043,
      "learning_rate": 8.132112171099148e-07,
      "loss": 0.0012,
      "step": 53410
    },
    {
      "epoch": 9712.727272727272,
      "grad_norm": 0.2098260372877121,
      "learning_rate": 8.131205115758941e-07,
      "loss": 0.0013,
      "step": 53420
    },
    {
      "epoch": 9714.545454545454,
      "grad_norm": 0.09086424112319946,
      "learning_rate": 8.130297890850802e-07,
      "loss": 0.0012,
      "step": 53430
    },
    {
      "epoch": 9716.363636363636,
      "grad_norm": 0.2528007924556732,
      "learning_rate": 8.129390496423859e-07,
      "loss": 0.0011,
      "step": 53440
    },
    {
      "epoch": 9718.181818181818,
      "grad_norm": 0.21082662045955658,
      "learning_rate": 8.128482932527254e-07,
      "loss": 0.0009,
      "step": 53450
    },
    {
      "epoch": 9720.0,
      "grad_norm": 0.3088703751564026,
      "learning_rate": 8.127575199210135e-07,
      "loss": 0.0012,
      "step": 53460
    },
    {
      "epoch": 9721.818181818182,
      "grad_norm": 0.013766534626483917,
      "learning_rate": 8.126667296521659e-07,
      "loss": 0.0009,
      "step": 53470
    },
    {
      "epoch": 9723.636363636364,
      "grad_norm": 0.0028304678853601217,
      "learning_rate": 8.125759224510992e-07,
      "loss": 0.0012,
      "step": 53480
    },
    {
      "epoch": 9725.454545454546,
      "grad_norm": 0.01941242814064026,
      "learning_rate": 8.124850983227312e-07,
      "loss": 0.0014,
      "step": 53490
    },
    {
      "epoch": 9727.272727272728,
      "grad_norm": 0.29000088572502136,
      "learning_rate": 8.123942572719799e-07,
      "loss": 0.0011,
      "step": 53500
    },
    {
      "epoch": 9727.272727272728,
      "eval_loss": 4.854465484619141,
      "eval_runtime": 0.9527,
      "eval_samples_per_second": 10.496,
      "eval_steps_per_second": 5.248,
      "step": 53500
    },
    {
      "epoch": 9729.09090909091,
      "grad_norm": 0.24107405543327332,
      "learning_rate": 8.123033993037656e-07,
      "loss": 0.001,
      "step": 53510
    },
    {
      "epoch": 9730.90909090909,
      "grad_norm": 0.24408194422721863,
      "learning_rate": 8.122125244230078e-07,
      "loss": 0.0012,
      "step": 53520
    },
    {
      "epoch": 9732.727272727272,
      "grad_norm": 0.001942462520673871,
      "learning_rate": 8.121216326346283e-07,
      "loss": 0.0012,
      "step": 53530
    },
    {
      "epoch": 9734.545454545454,
      "grad_norm": 0.3219949007034302,
      "learning_rate": 8.120307239435489e-07,
      "loss": 0.0011,
      "step": 53540
    },
    {
      "epoch": 9736.363636363636,
      "grad_norm": 0.24246035516262054,
      "learning_rate": 8.119397983546931e-07,
      "loss": 0.0009,
      "step": 53550
    },
    {
      "epoch": 9738.181818181818,
      "grad_norm": 0.20082373917102814,
      "learning_rate": 8.118488558729845e-07,
      "loss": 0.0013,
      "step": 53560
    },
    {
      "epoch": 9740.0,
      "grad_norm": 0.002426423830911517,
      "learning_rate": 8.117578965033482e-07,
      "loss": 0.0011,
      "step": 53570
    },
    {
      "epoch": 9741.818181818182,
      "grad_norm": 0.060160599648952484,
      "learning_rate": 8.116669202507101e-07,
      "loss": 0.0012,
      "step": 53580
    },
    {
      "epoch": 9743.636363636364,
      "grad_norm": 0.22510486841201782,
      "learning_rate": 8.115759271199966e-07,
      "loss": 0.0007,
      "step": 53590
    },
    {
      "epoch": 9745.454545454546,
      "grad_norm": 0.00194265553727746,
      "learning_rate": 8.11484917116136e-07,
      "loss": 0.0012,
      "step": 53600
    },
    {
      "epoch": 9747.272727272728,
      "grad_norm": 0.20940810441970825,
      "learning_rate": 8.113938902440563e-07,
      "loss": 0.0015,
      "step": 53610
    },
    {
      "epoch": 9749.09090909091,
      "grad_norm": 0.20640330016613007,
      "learning_rate": 8.11302846508687e-07,
      "loss": 0.0007,
      "step": 53620
    },
    {
      "epoch": 9750.90909090909,
      "grad_norm": 0.002072670264169574,
      "learning_rate": 8.11211785914959e-07,
      "loss": 0.0012,
      "step": 53630
    },
    {
      "epoch": 9752.727272727272,
      "grad_norm": 0.0019560435321182013,
      "learning_rate": 8.111207084678033e-07,
      "loss": 0.001,
      "step": 53640
    },
    {
      "epoch": 9754.545454545454,
      "grad_norm": 0.0015349104069173336,
      "learning_rate": 8.110296141721519e-07,
      "loss": 0.0013,
      "step": 53650
    },
    {
      "epoch": 9756.363636363636,
      "grad_norm": 0.3128187954425812,
      "learning_rate": 8.109385030329383e-07,
      "loss": 0.0011,
      "step": 53660
    },
    {
      "epoch": 9758.181818181818,
      "grad_norm": 0.2662435472011566,
      "learning_rate": 8.108473750550964e-07,
      "loss": 0.0011,
      "step": 53670
    },
    {
      "epoch": 9760.0,
      "grad_norm": 0.17584936320781708,
      "learning_rate": 8.107562302435611e-07,
      "loss": 0.001,
      "step": 53680
    },
    {
      "epoch": 9761.818181818182,
      "grad_norm": 0.002065136795863509,
      "learning_rate": 8.106650686032686e-07,
      "loss": 0.0012,
      "step": 53690
    },
    {
      "epoch": 9763.636363636364,
      "grad_norm": 0.25288763642311096,
      "learning_rate": 8.105738901391551e-07,
      "loss": 0.0013,
      "step": 53700
    },
    {
      "epoch": 9765.454545454546,
      "grad_norm": 0.0015600392362102866,
      "learning_rate": 8.104826948561589e-07,
      "loss": 0.001,
      "step": 53710
    },
    {
      "epoch": 9767.272727272728,
      "grad_norm": 0.1965576410293579,
      "learning_rate": 8.103914827592182e-07,
      "loss": 0.0012,
      "step": 53720
    },
    {
      "epoch": 9769.09090909091,
      "grad_norm": 0.0015084199840202928,
      "learning_rate": 8.103002538532728e-07,
      "loss": 0.0011,
      "step": 53730
    },
    {
      "epoch": 9770.90909090909,
      "grad_norm": 0.25558823347091675,
      "learning_rate": 8.102090081432629e-07,
      "loss": 0.0012,
      "step": 53740
    },
    {
      "epoch": 9772.727272727272,
      "grad_norm": 0.001963329268619418,
      "learning_rate": 8.101177456341299e-07,
      "loss": 0.0012,
      "step": 53750
    },
    {
      "epoch": 9774.545454545454,
      "grad_norm": 0.0009385459707118571,
      "learning_rate": 8.100264663308163e-07,
      "loss": 0.001,
      "step": 53760
    },
    {
      "epoch": 9776.363636363636,
      "grad_norm": 0.19790565967559814,
      "learning_rate": 8.099351702382648e-07,
      "loss": 0.0011,
      "step": 53770
    },
    {
      "epoch": 9778.181818181818,
      "grad_norm": 0.0014333139406517148,
      "learning_rate": 8.098438573614199e-07,
      "loss": 0.0011,
      "step": 53780
    },
    {
      "epoch": 9780.0,
      "grad_norm": 0.23754097521305084,
      "learning_rate": 8.097525277052263e-07,
      "loss": 0.0012,
      "step": 53790
    },
    {
      "epoch": 9781.818181818182,
      "grad_norm": 0.20626552402973175,
      "learning_rate": 8.096611812746301e-07,
      "loss": 0.0009,
      "step": 53800
    },
    {
      "epoch": 9783.636363636364,
      "grad_norm": 0.0015277046477422118,
      "learning_rate": 8.095698180745777e-07,
      "loss": 0.0013,
      "step": 53810
    },
    {
      "epoch": 9785.454545454546,
      "grad_norm": 0.0015368435997515917,
      "learning_rate": 8.094784381100173e-07,
      "loss": 0.001,
      "step": 53820
    },
    {
      "epoch": 9787.272727272728,
      "grad_norm": 0.0011970418272539973,
      "learning_rate": 8.093870413858972e-07,
      "loss": 0.0011,
      "step": 53830
    },
    {
      "epoch": 9789.09090909091,
      "grad_norm": 0.0031201383098959923,
      "learning_rate": 8.09295627907167e-07,
      "loss": 0.0012,
      "step": 53840
    },
    {
      "epoch": 9790.90909090909,
      "grad_norm": 0.0017471372848376632,
      "learning_rate": 8.09204197678777e-07,
      "loss": 0.001,
      "step": 53850
    },
    {
      "epoch": 9792.727272727272,
      "grad_norm": 0.187431201338768,
      "learning_rate": 8.091127507056787e-07,
      "loss": 0.0014,
      "step": 53860
    },
    {
      "epoch": 9794.545454545454,
      "grad_norm": 0.2921791970729828,
      "learning_rate": 8.090212869928245e-07,
      "loss": 0.0009,
      "step": 53870
    },
    {
      "epoch": 9796.363636363636,
      "grad_norm": 0.0015084726037457585,
      "learning_rate": 8.089298065451671e-07,
      "loss": 0.001,
      "step": 53880
    },
    {
      "epoch": 9798.181818181818,
      "grad_norm": 0.2425500899553299,
      "learning_rate": 8.088383093676609e-07,
      "loss": 0.0012,
      "step": 53890
    },
    {
      "epoch": 9800.0,
      "grad_norm": 0.0014734062133356929,
      "learning_rate": 8.087467954652607e-07,
      "loss": 0.0012,
      "step": 53900
    },
    {
      "epoch": 9801.818181818182,
      "grad_norm": 0.0013404741184785962,
      "learning_rate": 8.086552648429224e-07,
      "loss": 0.0011,
      "step": 53910
    },
    {
      "epoch": 9803.636363636364,
      "grad_norm": 0.18235675990581512,
      "learning_rate": 8.085637175056028e-07,
      "loss": 0.001,
      "step": 53920
    },
    {
      "epoch": 9805.454545454546,
      "grad_norm": 0.3212660253047943,
      "learning_rate": 8.084721534582596e-07,
      "loss": 0.0012,
      "step": 53930
    },
    {
      "epoch": 9807.272727272728,
      "grad_norm": 0.22623053193092346,
      "learning_rate": 8.083805727058513e-07,
      "loss": 0.001,
      "step": 53940
    },
    {
      "epoch": 9809.09090909091,
      "grad_norm": 0.25312721729278564,
      "learning_rate": 8.082889752533374e-07,
      "loss": 0.0013,
      "step": 53950
    },
    {
      "epoch": 9810.90909090909,
      "grad_norm": 0.0013201877009123564,
      "learning_rate": 8.081973611056783e-07,
      "loss": 0.001,
      "step": 53960
    },
    {
      "epoch": 9812.727272727272,
      "grad_norm": 0.0017042828258126974,
      "learning_rate": 8.081057302678352e-07,
      "loss": 0.0009,
      "step": 53970
    },
    {
      "epoch": 9814.545454545454,
      "grad_norm": 0.23058663308620453,
      "learning_rate": 8.080140827447704e-07,
      "loss": 0.0012,
      "step": 53980
    },
    {
      "epoch": 9816.363636363636,
      "grad_norm": 0.2864227592945099,
      "learning_rate": 8.07922418541447e-07,
      "loss": 0.0012,
      "step": 53990
    },
    {
      "epoch": 9818.181818181818,
      "grad_norm": 0.0023369614500552416,
      "learning_rate": 8.07830737662829e-07,
      "loss": 0.0009,
      "step": 54000
    },
    {
      "epoch": 9818.181818181818,
      "eval_loss": 4.840240478515625,
      "eval_runtime": 0.9516,
      "eval_samples_per_second": 10.508,
      "eval_steps_per_second": 5.254,
      "step": 54000
    },
    {
      "epoch": 9820.0,
      "grad_norm": 0.31466910243034363,
      "learning_rate": 8.077390401138815e-07,
      "loss": 0.0012,
      "step": 54010
    },
    {
      "epoch": 9821.818181818182,
      "grad_norm": 0.25596845149993896,
      "learning_rate": 8.076473258995699e-07,
      "loss": 0.0009,
      "step": 54020
    },
    {
      "epoch": 9823.636363636364,
      "grad_norm": 0.2927181124687195,
      "learning_rate": 8.075555950248611e-07,
      "loss": 0.0013,
      "step": 54030
    },
    {
      "epoch": 9825.454545454546,
      "grad_norm": 0.0033983325120061636,
      "learning_rate": 8.074638474947227e-07,
      "loss": 0.0009,
      "step": 54040
    },
    {
      "epoch": 9827.272727272728,
      "grad_norm": 0.22763891518115997,
      "learning_rate": 8.073720833141233e-07,
      "loss": 0.0012,
      "step": 54050
    },
    {
      "epoch": 9829.09090909091,
      "grad_norm": 0.002064411761239171,
      "learning_rate": 8.072803024880321e-07,
      "loss": 0.0011,
      "step": 54060
    },
    {
      "epoch": 9830.90909090909,
      "grad_norm": 0.23884136974811554,
      "learning_rate": 8.071885050214198e-07,
      "loss": 0.0012,
      "step": 54070
    },
    {
      "epoch": 9832.727272727272,
      "grad_norm": 0.002063765423372388,
      "learning_rate": 8.070966909192573e-07,
      "loss": 0.0011,
      "step": 54080
    },
    {
      "epoch": 9834.545454545454,
      "grad_norm": 0.001363916671834886,
      "learning_rate": 8.070048601865168e-07,
      "loss": 0.001,
      "step": 54090
    },
    {
      "epoch": 9836.363636363636,
      "grad_norm": 0.20610985159873962,
      "learning_rate": 8.069130128281714e-07,
      "loss": 0.0011,
      "step": 54100
    },
    {
      "epoch": 9838.181818181818,
      "grad_norm": 0.0191382747143507,
      "learning_rate": 8.068211488491948e-07,
      "loss": 0.0014,
      "step": 54110
    },
    {
      "epoch": 9840.0,
      "grad_norm": 0.17784719169139862,
      "learning_rate": 8.067292682545621e-07,
      "loss": 0.001,
      "step": 54120
    },
    {
      "epoch": 9841.818181818182,
      "grad_norm": 0.24061809480190277,
      "learning_rate": 8.066373710492489e-07,
      "loss": 0.0012,
      "step": 54130
    },
    {
      "epoch": 9843.636363636364,
      "grad_norm": 0.0019239417742937803,
      "learning_rate": 8.065454572382318e-07,
      "loss": 0.0009,
      "step": 54140
    },
    {
      "epoch": 9845.454545454546,
      "grad_norm": 0.3057050406932831,
      "learning_rate": 8.064535268264883e-07,
      "loss": 0.0013,
      "step": 54150
    },
    {
      "epoch": 9847.272727272728,
      "grad_norm": 0.18547981977462769,
      "learning_rate": 8.063615798189968e-07,
      "loss": 0.001,
      "step": 54160
    },
    {
      "epoch": 9849.09090909091,
      "grad_norm": 0.22372467815876007,
      "learning_rate": 8.062696162207367e-07,
      "loss": 0.001,
      "step": 54170
    },
    {
      "epoch": 9850.90909090909,
      "grad_norm": 0.18988317251205444,
      "learning_rate": 8.061776360366882e-07,
      "loss": 0.0011,
      "step": 54180
    },
    {
      "epoch": 9852.727272727272,
      "grad_norm": 0.1714092195034027,
      "learning_rate": 8.060856392718325e-07,
      "loss": 0.001,
      "step": 54190
    },
    {
      "epoch": 9854.545454545454,
      "grad_norm": 0.00499134324491024,
      "learning_rate": 8.059936259311513e-07,
      "loss": 0.0008,
      "step": 54200
    },
    {
      "epoch": 9856.363636363636,
      "grad_norm": 0.2362699657678604,
      "learning_rate": 8.059015960196278e-07,
      "loss": 0.0016,
      "step": 54210
    },
    {
      "epoch": 9858.181818181818,
      "grad_norm": 0.253169983625412,
      "learning_rate": 8.058095495422459e-07,
      "loss": 0.001,
      "step": 54220
    },
    {
      "epoch": 9860.0,
      "grad_norm": 0.2390400916337967,
      "learning_rate": 8.0571748650399e-07,
      "loss": 0.0011,
      "step": 54230
    },
    {
      "epoch": 9861.818181818182,
      "grad_norm": 0.002651988295838237,
      "learning_rate": 8.056254069098458e-07,
      "loss": 0.001,
      "step": 54240
    },
    {
      "epoch": 9863.636363636364,
      "grad_norm": 0.0019132617162540555,
      "learning_rate": 8.055333107647999e-07,
      "loss": 0.0012,
      "step": 54250
    },
    {
      "epoch": 9865.454545454546,
      "grad_norm": 0.0010685299057513475,
      "learning_rate": 8.054411980738396e-07,
      "loss": 0.0012,
      "step": 54260
    },
    {
      "epoch": 9867.272727272728,
      "grad_norm": 0.18917736411094666,
      "learning_rate": 8.053490688419532e-07,
      "loss": 0.0011,
      "step": 54270
    },
    {
      "epoch": 9869.09090909091,
      "grad_norm": 0.23813481628894806,
      "learning_rate": 8.052569230741298e-07,
      "loss": 0.0009,
      "step": 54280
    },
    {
      "epoch": 9870.90909090909,
      "grad_norm": 0.16741031408309937,
      "learning_rate": 8.051647607753598e-07,
      "loss": 0.0011,
      "step": 54290
    },
    {
      "epoch": 9872.727272727272,
      "grad_norm": 0.19953188300132751,
      "learning_rate": 8.050725819506339e-07,
      "loss": 0.0011,
      "step": 54300
    },
    {
      "epoch": 9874.545454545454,
      "grad_norm": 0.18923866748809814,
      "learning_rate": 8.049803866049439e-07,
      "loss": 0.001,
      "step": 54310
    },
    {
      "epoch": 9876.363636363636,
      "grad_norm": 0.002318411599844694,
      "learning_rate": 8.048881747432827e-07,
      "loss": 0.0011,
      "step": 54320
    },
    {
      "epoch": 9878.181818181818,
      "grad_norm": 0.3178233504295349,
      "learning_rate": 8.04795946370644e-07,
      "loss": 0.0017,
      "step": 54330
    },
    {
      "epoch": 9880.0,
      "grad_norm": 0.1014207974076271,
      "learning_rate": 8.047037014920224e-07,
      "loss": 0.0008,
      "step": 54340
    },
    {
      "epoch": 9881.818181818182,
      "grad_norm": 0.02939850464463234,
      "learning_rate": 8.046114401124131e-07,
      "loss": 0.0011,
      "step": 54350
    },
    {
      "epoch": 9883.636363636364,
      "grad_norm": 0.25163811445236206,
      "learning_rate": 8.045191622368127e-07,
      "loss": 0.0013,
      "step": 54360
    },
    {
      "epoch": 9885.454545454546,
      "grad_norm": 0.28991633653640747,
      "learning_rate": 8.044268678702184e-07,
      "loss": 0.0011,
      "step": 54370
    },
    {
      "epoch": 9887.272727272728,
      "grad_norm": 0.011204644106328487,
      "learning_rate": 8.043345570176281e-07,
      "loss": 0.001,
      "step": 54380
    },
    {
      "epoch": 9889.09090909091,
      "grad_norm": 0.17658790946006775,
      "learning_rate": 8.04242229684041e-07,
      "loss": 0.0009,
      "step": 54390
    },
    {
      "epoch": 9890.90909090909,
      "grad_norm": 0.19980229437351227,
      "learning_rate": 8.041498858744571e-07,
      "loss": 0.0013,
      "step": 54400
    },
    {
      "epoch": 9892.727272727272,
      "grad_norm": 0.1788313388824463,
      "learning_rate": 8.040575255938771e-07,
      "loss": 0.0011,
      "step": 54410
    },
    {
      "epoch": 9894.545454545454,
      "grad_norm": 0.2402198612689972,
      "learning_rate": 8.039651488473027e-07,
      "loss": 0.0011,
      "step": 54420
    },
    {
      "epoch": 9896.363636363636,
      "grad_norm": 0.0023348992690443993,
      "learning_rate": 8.038727556397365e-07,
      "loss": 0.001,
      "step": 54430
    },
    {
      "epoch": 9898.181818181818,
      "grad_norm": 0.0020409012213349342,
      "learning_rate": 8.037803459761819e-07,
      "loss": 0.0012,
      "step": 54440
    },
    {
      "epoch": 9900.0,
      "grad_norm": 0.0023972492199391127,
      "learning_rate": 8.036879198616434e-07,
      "loss": 0.0012,
      "step": 54450
    },
    {
      "epoch": 9901.818181818182,
      "grad_norm": 0.002580324886366725,
      "learning_rate": 8.035954773011262e-07,
      "loss": 0.0009,
      "step": 54460
    },
    {
      "epoch": 9903.636363636364,
      "grad_norm": 0.0012423424050211906,
      "learning_rate": 8.035030182996363e-07,
      "loss": 0.0014,
      "step": 54470
    },
    {
      "epoch": 9905.454545454546,
      "grad_norm": 0.17613926529884338,
      "learning_rate": 8.034105428621811e-07,
      "loss": 0.0012,
      "step": 54480
    },
    {
      "epoch": 9907.272727272728,
      "grad_norm": 0.2424112856388092,
      "learning_rate": 8.033180509937682e-07,
      "loss": 0.0009,
      "step": 54490
    },
    {
      "epoch": 9909.09090909091,
      "grad_norm": 0.286503404378891,
      "learning_rate": 8.032255426994068e-07,
      "loss": 0.0012,
      "step": 54500
    },
    {
      "epoch": 9909.09090909091,
      "eval_loss": 4.839046478271484,
      "eval_runtime": 0.9504,
      "eval_samples_per_second": 10.522,
      "eval_steps_per_second": 5.261,
      "step": 54500
    },
    {
      "epoch": 9910.90909090909,
      "grad_norm": 0.17913110554218292,
      "learning_rate": 8.031330179841061e-07,
      "loss": 0.0011,
      "step": 54510
    },
    {
      "epoch": 9912.727272727272,
      "grad_norm": 0.0018903010059148073,
      "learning_rate": 8.030404768528771e-07,
      "loss": 0.0012,
      "step": 54520
    },
    {
      "epoch": 9914.545454545454,
      "grad_norm": 0.004997474607080221,
      "learning_rate": 8.029479193107311e-07,
      "loss": 0.0009,
      "step": 54530
    },
    {
      "epoch": 9916.363636363636,
      "grad_norm": 0.002843414433300495,
      "learning_rate": 8.028553453626807e-07,
      "loss": 0.0009,
      "step": 54540
    },
    {
      "epoch": 9918.181818181818,
      "grad_norm": 0.22221851348876953,
      "learning_rate": 8.027627550137389e-07,
      "loss": 0.0013,
      "step": 54550
    },
    {
      "epoch": 9920.0,
      "grad_norm": 0.0020908662118017673,
      "learning_rate": 8.0267014826892e-07,
      "loss": 0.0011,
      "step": 54560
    },
    {
      "epoch": 9921.818181818182,
      "grad_norm": 0.25123971700668335,
      "learning_rate": 8.02577525133239e-07,
      "loss": 0.0011,
      "step": 54570
    },
    {
      "epoch": 9923.636363636364,
      "grad_norm": 0.22468537092208862,
      "learning_rate": 8.024848856117119e-07,
      "loss": 0.0014,
      "step": 54580
    },
    {
      "epoch": 9925.454545454546,
      "grad_norm": 0.0016774731921032071,
      "learning_rate": 8.023922297093555e-07,
      "loss": 0.001,
      "step": 54590
    },
    {
      "epoch": 9927.272727272728,
      "grad_norm": 0.0027708113193511963,
      "learning_rate": 8.022995574311875e-07,
      "loss": 0.0009,
      "step": 54600
    },
    {
      "epoch": 9929.09090909091,
      "grad_norm": 0.25361281633377075,
      "learning_rate": 8.022068687822263e-07,
      "loss": 0.0012,
      "step": 54610
    },
    {
      "epoch": 9930.90909090909,
      "grad_norm": 0.22356753051280975,
      "learning_rate": 8.021141637674917e-07,
      "loss": 0.001,
      "step": 54620
    },
    {
      "epoch": 9932.727272727272,
      "grad_norm": 0.25468969345092773,
      "learning_rate": 8.020214423920039e-07,
      "loss": 0.0013,
      "step": 54630
    },
    {
      "epoch": 9934.545454545454,
      "grad_norm": 0.003895447589457035,
      "learning_rate": 8.019287046607841e-07,
      "loss": 0.0008,
      "step": 54640
    },
    {
      "epoch": 9936.363636363636,
      "grad_norm": 0.2942923307418823,
      "learning_rate": 8.018359505788545e-07,
      "loss": 0.0014,
      "step": 54650
    },
    {
      "epoch": 9938.181818181818,
      "grad_norm": 0.0012237548362463713,
      "learning_rate": 8.017431801512382e-07,
      "loss": 0.0009,
      "step": 54660
    },
    {
      "epoch": 9940.0,
      "grad_norm": 0.3024836778640747,
      "learning_rate": 8.016503933829592e-07,
      "loss": 0.0012,
      "step": 54670
    },
    {
      "epoch": 9941.818181818182,
      "grad_norm": 0.2544915974140167,
      "learning_rate": 8.015575902790419e-07,
      "loss": 0.0011,
      "step": 54680
    },
    {
      "epoch": 9943.636363636364,
      "grad_norm": 0.22517822682857513,
      "learning_rate": 8.014647708445124e-07,
      "loss": 0.0012,
      "step": 54690
    },
    {
      "epoch": 9945.454545454546,
      "grad_norm": 0.004254146944731474,
      "learning_rate": 8.013719350843968e-07,
      "loss": 0.0009,
      "step": 54700
    },
    {
      "epoch": 9947.272727272728,
      "grad_norm": 0.17954622209072113,
      "learning_rate": 8.01279083003723e-07,
      "loss": 0.0013,
      "step": 54710
    },
    {
      "epoch": 9949.09090909091,
      "grad_norm": 0.2072499543428421,
      "learning_rate": 8.011862146075193e-07,
      "loss": 0.0009,
      "step": 54720
    },
    {
      "epoch": 9950.90909090909,
      "grad_norm": 0.1831192672252655,
      "learning_rate": 8.010933299008147e-07,
      "loss": 0.0012,
      "step": 54730
    },
    {
      "epoch": 9952.727272727272,
      "grad_norm": 0.18485364317893982,
      "learning_rate": 8.010004288886392e-07,
      "loss": 0.0013,
      "step": 54740
    },
    {
      "epoch": 9954.545454545454,
      "grad_norm": 0.0015227932017296553,
      "learning_rate": 8.009075115760242e-07,
      "loss": 0.0007,
      "step": 54750
    },
    {
      "epoch": 9956.363636363636,
      "grad_norm": 0.19079475104808807,
      "learning_rate": 8.008145779680011e-07,
      "loss": 0.0012,
      "step": 54760
    },
    {
      "epoch": 9958.181818181818,
      "grad_norm": 0.19510170817375183,
      "learning_rate": 8.007216280696029e-07,
      "loss": 0.0013,
      "step": 54770
    },
    {
      "epoch": 9960.0,
      "grad_norm": 0.23949046432971954,
      "learning_rate": 8.006286618858633e-07,
      "loss": 0.001,
      "step": 54780
    },
    {
      "epoch": 9961.818181818182,
      "grad_norm": 0.0014536326052621007,
      "learning_rate": 8.005356794218165e-07,
      "loss": 0.0009,
      "step": 54790
    },
    {
      "epoch": 9963.636363636364,
      "grad_norm": 0.16923052072525024,
      "learning_rate": 8.004426806824983e-07,
      "loss": 0.0015,
      "step": 54800
    },
    {
      "epoch": 9965.454545454546,
      "grad_norm": 0.295465350151062,
      "learning_rate": 8.003496656729447e-07,
      "loss": 0.0012,
      "step": 54810
    },
    {
      "epoch": 9967.272727272728,
      "grad_norm": 0.18470050394535065,
      "learning_rate": 8.00256634398193e-07,
      "loss": 0.0011,
      "step": 54820
    },
    {
      "epoch": 9969.09090909091,
      "grad_norm": 0.0016654301434755325,
      "learning_rate": 8.001635868632809e-07,
      "loss": 0.0011,
      "step": 54830
    },
    {
      "epoch": 9970.90909090909,
      "grad_norm": 0.0014990156050771475,
      "learning_rate": 8.000705230732478e-07,
      "loss": 0.0009,
      "step": 54840
    },
    {
      "epoch": 9972.727272727272,
      "grad_norm": 0.19103063642978668,
      "learning_rate": 7.999774430331332e-07,
      "loss": 0.0013,
      "step": 54850
    },
    {
      "epoch": 9974.545454545454,
      "grad_norm": 0.0021093592513352633,
      "learning_rate": 7.998843467479777e-07,
      "loss": 0.001,
      "step": 54860
    },
    {
      "epoch": 9976.363636363636,
      "grad_norm": 0.22132505476474762,
      "learning_rate": 7.997912342228232e-07,
      "loss": 0.0016,
      "step": 54870
    },
    {
      "epoch": 9978.181818181818,
      "grad_norm": 0.34149613976478577,
      "learning_rate": 7.996981054627119e-07,
      "loss": 0.001,
      "step": 54880
    },
    {
      "epoch": 9980.0,
      "grad_norm": 0.0028143860399723053,
      "learning_rate": 7.99604960472687e-07,
      "loss": 0.0009,
      "step": 54890
    },
    {
      "epoch": 9981.818181818182,
      "grad_norm": 0.3461131453514099,
      "learning_rate": 7.995117992577928e-07,
      "loss": 0.0011,
      "step": 54900
    },
    {
      "epoch": 9983.636363636364,
      "grad_norm": 0.001041086157783866,
      "learning_rate": 7.994186218230746e-07,
      "loss": 0.001,
      "step": 54910
    },
    {
      "epoch": 9985.454545454546,
      "grad_norm": 0.22240325808525085,
      "learning_rate": 7.993254281735782e-07,
      "loss": 0.0012,
      "step": 54920
    },
    {
      "epoch": 9987.272727272728,
      "grad_norm": 0.015941090881824493,
      "learning_rate": 7.992322183143503e-07,
      "loss": 0.0013,
      "step": 54930
    },
    {
      "epoch": 9989.09090909091,
      "grad_norm": 0.23695062100887299,
      "learning_rate": 7.991389922504386e-07,
      "loss": 0.0009,
      "step": 54940
    },
    {
      "epoch": 9990.90909090909,
      "grad_norm": 0.0013138483045622706,
      "learning_rate": 7.990457499868919e-07,
      "loss": 0.0011,
      "step": 54950
    },
    {
      "epoch": 9992.727272727272,
      "grad_norm": 0.020345468074083328,
      "learning_rate": 7.989524915287594e-07,
      "loss": 0.0013,
      "step": 54960
    },
    {
      "epoch": 9994.545454545454,
      "grad_norm": 0.0012699116487056017,
      "learning_rate": 7.988592168810917e-07,
      "loss": 0.0008,
      "step": 54970
    },
    {
      "epoch": 9996.363636363636,
      "grad_norm": 0.001619303715415299,
      "learning_rate": 7.987659260489399e-07,
      "loss": 0.0011,
      "step": 54980
    },
    {
      "epoch": 9998.181818181818,
      "grad_norm": 0.0013639244716614485,
      "learning_rate": 7.98672619037356e-07,
      "loss": 0.0014,
      "step": 54990
    },
    {
      "epoch": 10000.0,
      "grad_norm": 0.2290460169315338,
      "learning_rate": 7.985792958513931e-07,
      "loss": 0.0012,
      "step": 55000
    },
    {
      "epoch": 10000.0,
      "eval_loss": 4.852643013000488,
      "eval_runtime": 0.9516,
      "eval_samples_per_second": 10.508,
      "eval_steps_per_second": 5.254,
      "step": 55000
    },
    {
      "epoch": 10001.818181818182,
      "grad_norm": 0.23270705342292786,
      "learning_rate": 7.984859564961048e-07,
      "loss": 0.001,
      "step": 55010
    },
    {
      "epoch": 10003.636363636364,
      "grad_norm": 0.001388360047712922,
      "learning_rate": 7.983926009765463e-07,
      "loss": 0.0009,
      "step": 55020
    },
    {
      "epoch": 10005.454545454546,
      "grad_norm": 0.1952233910560608,
      "learning_rate": 7.982992292977728e-07,
      "loss": 0.0013,
      "step": 55030
    },
    {
      "epoch": 10007.272727272728,
      "grad_norm": 0.22143758833408356,
      "learning_rate": 7.982058414648409e-07,
      "loss": 0.0012,
      "step": 55040
    },
    {
      "epoch": 10009.09090909091,
      "grad_norm": 0.001292837900109589,
      "learning_rate": 7.981124374828079e-07,
      "loss": 0.0009,
      "step": 55050
    },
    {
      "epoch": 10010.90909090909,
      "grad_norm": 0.19795624911785126,
      "learning_rate": 7.98019017356732e-07,
      "loss": 0.0012,
      "step": 55060
    },
    {
      "epoch": 10012.727272727272,
      "grad_norm": 0.008970120921730995,
      "learning_rate": 7.979255810916723e-07,
      "loss": 0.001,
      "step": 55070
    },
    {
      "epoch": 10014.545454545454,
      "grad_norm": 0.23876269161701202,
      "learning_rate": 7.97832128692689e-07,
      "loss": 0.001,
      "step": 55080
    },
    {
      "epoch": 10016.363636363636,
      "grad_norm": 0.21528252959251404,
      "learning_rate": 7.977386601648425e-07,
      "loss": 0.001,
      "step": 55090
    },
    {
      "epoch": 10018.181818181818,
      "grad_norm": 0.0017240492161363363,
      "learning_rate": 7.976451755131948e-07,
      "loss": 0.0011,
      "step": 55100
    },
    {
      "epoch": 10020.0,
      "grad_norm": 0.2485746592283249,
      "learning_rate": 7.975516747428086e-07,
      "loss": 0.0012,
      "step": 55110
    },
    {
      "epoch": 10021.818181818182,
      "grad_norm": 0.25201332569122314,
      "learning_rate": 7.97458157858747e-07,
      "loss": 0.0011,
      "step": 55120
    },
    {
      "epoch": 10023.636363636364,
      "grad_norm": 0.24869439005851746,
      "learning_rate": 7.973646248660745e-07,
      "loss": 0.0013,
      "step": 55130
    },
    {
      "epoch": 10025.454545454546,
      "grad_norm": 0.18735572695732117,
      "learning_rate": 7.972710757698567e-07,
      "loss": 0.0009,
      "step": 55140
    },
    {
      "epoch": 10027.272727272728,
      "grad_norm": 0.30749353766441345,
      "learning_rate": 7.971775105751589e-07,
      "loss": 0.0014,
      "step": 55150
    },
    {
      "epoch": 10029.09090909091,
      "grad_norm": 0.2753981649875641,
      "learning_rate": 7.970839292870487e-07,
      "loss": 0.0012,
      "step": 55160
    },
    {
      "epoch": 10030.90909090909,
      "grad_norm": 0.22236615419387817,
      "learning_rate": 7.969903319105935e-07,
      "loss": 0.0011,
      "step": 55170
    },
    {
      "epoch": 10032.727272727272,
      "grad_norm": 0.2045794129371643,
      "learning_rate": 7.968967184508624e-07,
      "loss": 0.0011,
      "step": 55180
    },
    {
      "epoch": 10034.545454545454,
      "grad_norm": 0.0027389044407755136,
      "learning_rate": 7.968030889129247e-07,
      "loss": 0.001,
      "step": 55190
    },
    {
      "epoch": 10036.363636363636,
      "grad_norm": 0.001689813332632184,
      "learning_rate": 7.967094433018508e-07,
      "loss": 0.0013,
      "step": 55200
    },
    {
      "epoch": 10038.181818181818,
      "grad_norm": 0.0017800795612856746,
      "learning_rate": 7.966157816227119e-07,
      "loss": 0.0009,
      "step": 55210
    },
    {
      "epoch": 10040.0,
      "grad_norm": 0.3114210069179535,
      "learning_rate": 7.965221038805806e-07,
      "loss": 0.0014,
      "step": 55220
    },
    {
      "epoch": 10041.818181818182,
      "grad_norm": 0.0017371316207572818,
      "learning_rate": 7.964284100805296e-07,
      "loss": 0.0009,
      "step": 55230
    },
    {
      "epoch": 10043.636363636364,
      "grad_norm": 0.001614206936210394,
      "learning_rate": 7.96334700227633e-07,
      "loss": 0.0013,
      "step": 55240
    },
    {
      "epoch": 10045.454545454546,
      "grad_norm": 0.23481999337673187,
      "learning_rate": 7.962409743269654e-07,
      "loss": 0.0011,
      "step": 55250
    },
    {
      "epoch": 10047.272727272728,
      "grad_norm": 0.21042729914188385,
      "learning_rate": 7.961472323836025e-07,
      "loss": 0.0011,
      "step": 55260
    },
    {
      "epoch": 10049.09090909091,
      "grad_norm": 0.0017201153095811605,
      "learning_rate": 7.960534744026209e-07,
      "loss": 0.0011,
      "step": 55270
    },
    {
      "epoch": 10050.90909090909,
      "grad_norm": 0.18044640123844147,
      "learning_rate": 7.959597003890978e-07,
      "loss": 0.0012,
      "step": 55280
    },
    {
      "epoch": 10052.727272727272,
      "grad_norm": 0.0013724121963605285,
      "learning_rate": 7.958659103481119e-07,
      "loss": 0.001,
      "step": 55290
    },
    {
      "epoch": 10054.545454545454,
      "grad_norm": 0.25014370679855347,
      "learning_rate": 7.95772104284742e-07,
      "loss": 0.0013,
      "step": 55300
    },
    {
      "epoch": 10056.363636363636,
      "grad_norm": 0.22277146577835083,
      "learning_rate": 7.95678282204068e-07,
      "loss": 0.0009,
      "step": 55310
    },
    {
      "epoch": 10058.181818181818,
      "grad_norm": 0.0013294110540300608,
      "learning_rate": 7.95584444111171e-07,
      "loss": 0.001,
      "step": 55320
    },
    {
      "epoch": 10060.0,
      "grad_norm": 0.23704755306243896,
      "learning_rate": 7.954905900111325e-07,
      "loss": 0.0012,
      "step": 55330
    },
    {
      "epoch": 10061.818181818182,
      "grad_norm": 0.0020736963488161564,
      "learning_rate": 7.953967199090352e-07,
      "loss": 0.001,
      "step": 55340
    },
    {
      "epoch": 10063.636363636364,
      "grad_norm": 0.0022838159929960966,
      "learning_rate": 7.953028338099627e-07,
      "loss": 0.001,
      "step": 55350
    },
    {
      "epoch": 10065.454545454546,
      "grad_norm": 0.0025118584744632244,
      "learning_rate": 7.952089317189991e-07,
      "loss": 0.0011,
      "step": 55360
    },
    {
      "epoch": 10067.272727272728,
      "grad_norm": 0.28408434987068176,
      "learning_rate": 7.951150136412297e-07,
      "loss": 0.0014,
      "step": 55370
    },
    {
      "epoch": 10069.09090909091,
      "grad_norm": 0.0017652608221396804,
      "learning_rate": 7.950210795817406e-07,
      "loss": 0.0009,
      "step": 55380
    },
    {
      "epoch": 10070.90909090909,
      "grad_norm": 0.18729111552238464,
      "learning_rate": 7.949271295456185e-07,
      "loss": 0.0012,
      "step": 55390
    },
    {
      "epoch": 10072.727272727272,
      "grad_norm": 0.0016315917018800974,
      "learning_rate": 7.948331635379516e-07,
      "loss": 0.0011,
      "step": 55400
    },
    {
      "epoch": 10074.545454545454,
      "grad_norm": 0.18401992321014404,
      "learning_rate": 7.947391815638283e-07,
      "loss": 0.0011,
      "step": 55410
    },
    {
      "epoch": 10076.363636363636,
      "grad_norm": 0.0010305262403562665,
      "learning_rate": 7.94645183628338e-07,
      "loss": 0.0009,
      "step": 55420
    },
    {
      "epoch": 10078.181818181818,
      "grad_norm": 0.0012993243290111423,
      "learning_rate": 7.945511697365714e-07,
      "loss": 0.0011,
      "step": 55430
    },
    {
      "epoch": 10080.0,
      "grad_norm": 0.18908372521400452,
      "learning_rate": 7.944571398936193e-07,
      "loss": 0.0012,
      "step": 55440
    },
    {
      "epoch": 10081.818181818182,
      "grad_norm": 0.2927537262439728,
      "learning_rate": 7.943630941045743e-07,
      "loss": 0.0012,
      "step": 55450
    },
    {
      "epoch": 10083.636363636364,
      "grad_norm": 0.0015143988421186805,
      "learning_rate": 7.942690323745291e-07,
      "loss": 0.0012,
      "step": 55460
    },
    {
      "epoch": 10085.454545454546,
      "grad_norm": 0.002121181460097432,
      "learning_rate": 7.941749547085776e-07,
      "loss": 0.0009,
      "step": 55470
    },
    {
      "epoch": 10087.272727272728,
      "grad_norm": 0.19440029561519623,
      "learning_rate": 7.940808611118144e-07,
      "loss": 0.0011,
      "step": 55480
    },
    {
      "epoch": 10089.09090909091,
      "grad_norm": 0.1923815906047821,
      "learning_rate": 7.939867515893353e-07,
      "loss": 0.0013,
      "step": 55490
    },
    {
      "epoch": 10090.90909090909,
      "grad_norm": 0.20024563372135162,
      "learning_rate": 7.938926261462365e-07,
      "loss": 0.001,
      "step": 55500
    },
    {
      "epoch": 10090.90909090909,
      "eval_loss": 4.830256462097168,
      "eval_runtime": 0.9506,
      "eval_samples_per_second": 10.519,
      "eval_steps_per_second": 5.26,
      "step": 55500
    },
    {
      "epoch": 10092.727272727272,
      "grad_norm": 0.2374037653207779,
      "learning_rate": 7.937984847876154e-07,
      "loss": 0.0012,
      "step": 55510
    },
    {
      "epoch": 10094.545454545454,
      "grad_norm": 0.0011843238025903702,
      "learning_rate": 7.937043275185702e-07,
      "loss": 0.0011,
      "step": 55520
    },
    {
      "epoch": 10096.363636363636,
      "grad_norm": 0.22591930627822876,
      "learning_rate": 7.936101543441998e-07,
      "loss": 0.0011,
      "step": 55530
    },
    {
      "epoch": 10098.181818181818,
      "grad_norm": 0.0011687740916386247,
      "learning_rate": 7.935159652696041e-07,
      "loss": 0.0009,
      "step": 55540
    },
    {
      "epoch": 10100.0,
      "grad_norm": 0.0021443390287458897,
      "learning_rate": 7.934217602998838e-07,
      "loss": 0.0012,
      "step": 55550
    },
    {
      "epoch": 10101.818181818182,
      "grad_norm": 0.0017568389885127544,
      "learning_rate": 7.933275394401406e-07,
      "loss": 0.0012,
      "step": 55560
    },
    {
      "epoch": 10103.636363636364,
      "grad_norm": 0.22456882894039154,
      "learning_rate": 7.932333026954769e-07,
      "loss": 0.0011,
      "step": 55570
    },
    {
      "epoch": 10105.454545454546,
      "grad_norm": 0.0019989581778645515,
      "learning_rate": 7.93139050070996e-07,
      "loss": 0.0011,
      "step": 55580
    },
    {
      "epoch": 10107.272727272728,
      "grad_norm": 0.21670618653297424,
      "learning_rate": 7.930447815718021e-07,
      "loss": 0.0012,
      "step": 55590
    },
    {
      "epoch": 10109.09090909091,
      "grad_norm": 0.001682204194366932,
      "learning_rate": 7.929504972030002e-07,
      "loss": 0.001,
      "step": 55600
    },
    {
      "epoch": 10110.90909090909,
      "grad_norm": 0.25074025988578796,
      "learning_rate": 7.928561969696963e-07,
      "loss": 0.0012,
      "step": 55610
    },
    {
      "epoch": 10112.727272727272,
      "grad_norm": 0.2386222630739212,
      "learning_rate": 7.92761880876997e-07,
      "loss": 0.0009,
      "step": 55620
    },
    {
      "epoch": 10114.545454545454,
      "grad_norm": 0.17564953863620758,
      "learning_rate": 7.9266754893001e-07,
      "loss": 0.0012,
      "step": 55630
    },
    {
      "epoch": 10116.363636363636,
      "grad_norm": 0.002402089536190033,
      "learning_rate": 7.925732011338437e-07,
      "loss": 0.0011,
      "step": 55640
    },
    {
      "epoch": 10118.181818181818,
      "grad_norm": 0.0011828381102532148,
      "learning_rate": 7.924788374936077e-07,
      "loss": 0.001,
      "step": 55650
    },
    {
      "epoch": 10120.0,
      "grad_norm": 0.0015974415000528097,
      "learning_rate": 7.923844580144119e-07,
      "loss": 0.0012,
      "step": 55660
    },
    {
      "epoch": 10121.818181818182,
      "grad_norm": 0.18675589561462402,
      "learning_rate": 7.922900627013673e-07,
      "loss": 0.0012,
      "step": 55670
    },
    {
      "epoch": 10123.636363636364,
      "grad_norm": 0.004033285193145275,
      "learning_rate": 7.92195651559586e-07,
      "loss": 0.0011,
      "step": 55680
    },
    {
      "epoch": 10125.454545454546,
      "grad_norm": 0.24208782613277435,
      "learning_rate": 7.921012245941807e-07,
      "loss": 0.0011,
      "step": 55690
    },
    {
      "epoch": 10127.272727272728,
      "grad_norm": 0.030338959768414497,
      "learning_rate": 7.920067818102652e-07,
      "loss": 0.0011,
      "step": 55700
    },
    {
      "epoch": 10129.09090909091,
      "grad_norm": 0.18225149810314178,
      "learning_rate": 7.919123232129534e-07,
      "loss": 0.0011,
      "step": 55710
    },
    {
      "epoch": 10130.90909090909,
      "grad_norm": 0.002569532487541437,
      "learning_rate": 7.918178488073612e-07,
      "loss": 0.0012,
      "step": 55720
    },
    {
      "epoch": 10132.727272727272,
      "grad_norm": 0.008152307942509651,
      "learning_rate": 7.917233585986048e-07,
      "loss": 0.0014,
      "step": 55730
    },
    {
      "epoch": 10134.545454545454,
      "grad_norm": 0.1779356300830841,
      "learning_rate": 7.916288525918007e-07,
      "loss": 0.0009,
      "step": 55740
    },
    {
      "epoch": 10136.363636363636,
      "grad_norm": 0.18899650871753693,
      "learning_rate": 7.915343307920674e-07,
      "loss": 0.0014,
      "step": 55750
    },
    {
      "epoch": 10138.181818181818,
      "grad_norm": 0.20573441684246063,
      "learning_rate": 7.914397932045232e-07,
      "loss": 0.0011,
      "step": 55760
    },
    {
      "epoch": 10140.0,
      "grad_norm": 0.2196379005908966,
      "learning_rate": 7.91345239834288e-07,
      "loss": 0.0012,
      "step": 55770
    },
    {
      "epoch": 10141.818181818182,
      "grad_norm": 0.17471884191036224,
      "learning_rate": 7.912506706864822e-07,
      "loss": 0.0012,
      "step": 55780
    },
    {
      "epoch": 10143.636363636364,
      "grad_norm": 0.003162663197144866,
      "learning_rate": 7.911560857662269e-07,
      "loss": 0.001,
      "step": 55790
    },
    {
      "epoch": 10145.454545454546,
      "grad_norm": 0.0021200834307819605,
      "learning_rate": 7.910614850786447e-07,
      "loss": 0.0012,
      "step": 55800
    },
    {
      "epoch": 10147.272727272728,
      "grad_norm": 0.0013071463909000158,
      "learning_rate": 7.909668686288582e-07,
      "loss": 0.001,
      "step": 55810
    },
    {
      "epoch": 10149.09090909091,
      "grad_norm": 0.003708016127347946,
      "learning_rate": 7.908722364219916e-07,
      "loss": 0.0011,
      "step": 55820
    },
    {
      "epoch": 10150.90909090909,
      "grad_norm": 0.2883654534816742,
      "learning_rate": 7.907775884631693e-07,
      "loss": 0.0012,
      "step": 55830
    },
    {
      "epoch": 10152.727272727272,
      "grad_norm": 0.1734692007303238,
      "learning_rate": 7.906829247575173e-07,
      "loss": 0.0011,
      "step": 55840
    },
    {
      "epoch": 10154.545454545454,
      "grad_norm": 0.28740742802619934,
      "learning_rate": 7.905882453101617e-07,
      "loss": 0.0014,
      "step": 55850
    },
    {
      "epoch": 10156.363636363636,
      "grad_norm": 0.0036759646609425545,
      "learning_rate": 7.9049355012623e-07,
      "loss": 0.0006,
      "step": 55860
    },
    {
      "epoch": 10158.181818181818,
      "grad_norm": 0.181428924202919,
      "learning_rate": 7.9039883921085e-07,
      "loss": 0.0013,
      "step": 55870
    },
    {
      "epoch": 10160.0,
      "grad_norm": 0.19589370489120483,
      "learning_rate": 7.90304112569151e-07,
      "loss": 0.001,
      "step": 55880
    },
    {
      "epoch": 10161.818181818182,
      "grad_norm": 0.0048574116080999374,
      "learning_rate": 7.90209370206263e-07,
      "loss": 0.0012,
      "step": 55890
    },
    {
      "epoch": 10163.636363636364,
      "grad_norm": 0.2617810368537903,
      "learning_rate": 7.901146121273164e-07,
      "loss": 0.001,
      "step": 55900
    },
    {
      "epoch": 10165.454545454546,
      "grad_norm": 0.003937102388590574,
      "learning_rate": 7.900198383374428e-07,
      "loss": 0.0012,
      "step": 55910
    },
    {
      "epoch": 10167.272727272728,
      "grad_norm": 0.30240046977996826,
      "learning_rate": 7.899250488417745e-07,
      "loss": 0.0014,
      "step": 55920
    },
    {
      "epoch": 10169.09090909091,
      "grad_norm": 0.22371311485767365,
      "learning_rate": 7.898302436454452e-07,
      "loss": 0.0008,
      "step": 55930
    },
    {
      "epoch": 10170.90909090909,
      "grad_norm": 0.0017978178802877665,
      "learning_rate": 7.897354227535884e-07,
      "loss": 0.0012,
      "step": 55940
    },
    {
      "epoch": 10172.727272727272,
      "grad_norm": 0.23807470500469208,
      "learning_rate": 7.896405861713393e-07,
      "loss": 0.0012,
      "step": 55950
    },
    {
      "epoch": 10174.545454545454,
      "grad_norm": 0.29338255524635315,
      "learning_rate": 7.895457339038339e-07,
      "loss": 0.001,
      "step": 55960
    },
    {
      "epoch": 10176.363636363636,
      "grad_norm": 0.002976811258122325,
      "learning_rate": 7.894508659562087e-07,
      "loss": 0.0009,
      "step": 55970
    },
    {
      "epoch": 10178.181818181818,
      "grad_norm": 0.18352535367012024,
      "learning_rate": 7.893559823336011e-07,
      "loss": 0.0015,
      "step": 55980
    },
    {
      "epoch": 10180.0,
      "grad_norm": 0.0010032608406618237,
      "learning_rate": 7.892610830411495e-07,
      "loss": 0.0008,
      "step": 55990
    },
    {
      "epoch": 10181.818181818182,
      "grad_norm": 0.30759578943252563,
      "learning_rate": 7.891661680839932e-07,
      "loss": 0.0011,
      "step": 56000
    },
    {
      "epoch": 10181.818181818182,
      "eval_loss": 4.8615827560424805,
      "eval_runtime": 0.9545,
      "eval_samples_per_second": 10.476,
      "eval_steps_per_second": 5.238,
      "step": 56000
    },
    {
      "epoch": 10183.636363636364,
      "grad_norm": 0.0019290366908535361,
      "learning_rate": 7.890712374672722e-07,
      "loss": 0.001,
      "step": 56010
    },
    {
      "epoch": 10185.454545454546,
      "grad_norm": 0.21536877751350403,
      "learning_rate": 7.889762911961273e-07,
      "loss": 0.0013,
      "step": 56020
    },
    {
      "epoch": 10187.272727272728,
      "grad_norm": 0.18187573552131653,
      "learning_rate": 7.888813292757002e-07,
      "loss": 0.0009,
      "step": 56030
    },
    {
      "epoch": 10189.09090909091,
      "grad_norm": 0.00316635612398386,
      "learning_rate": 7.887863517111337e-07,
      "loss": 0.0011,
      "step": 56040
    },
    {
      "epoch": 10190.90909090909,
      "grad_norm": 0.24891945719718933,
      "learning_rate": 7.886913585075712e-07,
      "loss": 0.0012,
      "step": 56050
    },
    {
      "epoch": 10192.727272727272,
      "grad_norm": 0.0020013207104057074,
      "learning_rate": 7.885963496701567e-07,
      "loss": 0.0011,
      "step": 56060
    },
    {
      "epoch": 10194.545454545454,
      "grad_norm": 0.23216256499290466,
      "learning_rate": 7.885013252040358e-07,
      "loss": 0.0013,
      "step": 56070
    },
    {
      "epoch": 10196.363636363636,
      "grad_norm": 0.22092600166797638,
      "learning_rate": 7.884062851143541e-07,
      "loss": 0.0009,
      "step": 56080
    },
    {
      "epoch": 10198.181818181818,
      "grad_norm": 0.0014148815535008907,
      "learning_rate": 7.883112294062584e-07,
      "loss": 0.0011,
      "step": 56090
    },
    {
      "epoch": 10200.0,
      "grad_norm": 0.001375339343212545,
      "learning_rate": 7.882161580848966e-07,
      "loss": 0.0012,
      "step": 56100
    },
    {
      "epoch": 10201.818181818182,
      "grad_norm": 0.300077348947525,
      "learning_rate": 7.881210711554171e-07,
      "loss": 0.0012,
      "step": 56110
    },
    {
      "epoch": 10203.636363636364,
      "grad_norm": 0.2306315153837204,
      "learning_rate": 7.880259686229693e-07,
      "loss": 0.0009,
      "step": 56120
    },
    {
      "epoch": 10205.454545454546,
      "grad_norm": 0.001233214046806097,
      "learning_rate": 7.879308504927034e-07,
      "loss": 0.0009,
      "step": 56130
    },
    {
      "epoch": 10207.272727272728,
      "grad_norm": 0.02141471952199936,
      "learning_rate": 7.878357167697703e-07,
      "loss": 0.0015,
      "step": 56140
    },
    {
      "epoch": 10209.09090909091,
      "grad_norm": 0.19566553831100464,
      "learning_rate": 7.87740567459322e-07,
      "loss": 0.001,
      "step": 56150
    },
    {
      "epoch": 10210.90909090909,
      "grad_norm": 0.1948477029800415,
      "learning_rate": 7.876454025665113e-07,
      "loss": 0.0008,
      "step": 56160
    },
    {
      "epoch": 10212.727272727272,
      "grad_norm": 0.0011506550945341587,
      "learning_rate": 7.875502220964917e-07,
      "loss": 0.0015,
      "step": 56170
    },
    {
      "epoch": 10214.545454545454,
      "grad_norm": 0.32954171299934387,
      "learning_rate": 7.874550260544176e-07,
      "loss": 0.001,
      "step": 56180
    },
    {
      "epoch": 10216.363636363636,
      "grad_norm": 0.3169145882129669,
      "learning_rate": 7.873598144454443e-07,
      "loss": 0.0011,
      "step": 56190
    },
    {
      "epoch": 10218.181818181818,
      "grad_norm": 0.1922331154346466,
      "learning_rate": 7.872645872747281e-07,
      "loss": 0.0011,
      "step": 56200
    },
    {
      "epoch": 10220.0,
      "grad_norm": 0.04613092541694641,
      "learning_rate": 7.871693445474256e-07,
      "loss": 0.0012,
      "step": 56210
    },
    {
      "epoch": 10221.818181818182,
      "grad_norm": 0.001959016779437661,
      "learning_rate": 7.870740862686949e-07,
      "loss": 0.0012,
      "step": 56220
    },
    {
      "epoch": 10223.636363636364,
      "grad_norm": 0.2286520153284073,
      "learning_rate": 7.869788124436943e-07,
      "loss": 0.0011,
      "step": 56230
    },
    {
      "epoch": 10225.454545454546,
      "grad_norm": 0.0027386450674384832,
      "learning_rate": 7.868835230775836e-07,
      "loss": 0.0012,
      "step": 56240
    },
    {
      "epoch": 10227.272727272728,
      "grad_norm": 0.2193538099527359,
      "learning_rate": 7.86788218175523e-07,
      "loss": 0.0009,
      "step": 56250
    },
    {
      "epoch": 10229.09090909091,
      "grad_norm": 0.21470104157924652,
      "learning_rate": 7.866928977426737e-07,
      "loss": 0.0012,
      "step": 56260
    },
    {
      "epoch": 10230.90909090909,
      "grad_norm": 0.020647253841161728,
      "learning_rate": 7.865975617841976e-07,
      "loss": 0.0011,
      "step": 56270
    },
    {
      "epoch": 10232.727272727272,
      "grad_norm": 0.0021673180162906647,
      "learning_rate": 7.865022103052576e-07,
      "loss": 0.0012,
      "step": 56280
    },
    {
      "epoch": 10234.545454545454,
      "grad_norm": 0.20867976546287537,
      "learning_rate": 7.864068433110174e-07,
      "loss": 0.0009,
      "step": 56290
    },
    {
      "epoch": 10236.363636363636,
      "grad_norm": 0.003185535781085491,
      "learning_rate": 7.863114608066417e-07,
      "loss": 0.0009,
      "step": 56300
    },
    {
      "epoch": 10238.181818181818,
      "grad_norm": 0.0020059156231582165,
      "learning_rate": 7.862160627972955e-07,
      "loss": 0.0012,
      "step": 56310
    },
    {
      "epoch": 10240.0,
      "grad_norm": 0.0013042251812294126,
      "learning_rate": 7.861206492881452e-07,
      "loss": 0.0012,
      "step": 56320
    },
    {
      "epoch": 10241.818181818182,
      "grad_norm": 0.0016657584346830845,
      "learning_rate": 7.860252202843578e-07,
      "loss": 0.0011,
      "step": 56330
    },
    {
      "epoch": 10243.636363636364,
      "grad_norm": 0.001468413625843823,
      "learning_rate": 7.859297757911012e-07,
      "loss": 0.0011,
      "step": 56340
    },
    {
      "epoch": 10245.454545454546,
      "grad_norm": 0.0028979822527617216,
      "learning_rate": 7.85834315813544e-07,
      "loss": 0.0011,
      "step": 56350
    },
    {
      "epoch": 10247.272727272728,
      "grad_norm": 0.23316660523414612,
      "learning_rate": 7.857388403568563e-07,
      "loss": 0.0012,
      "step": 56360
    },
    {
      "epoch": 10249.09090909091,
      "grad_norm": 0.0020144907757639885,
      "learning_rate": 7.856433494262077e-07,
      "loss": 0.0011,
      "step": 56370
    },
    {
      "epoch": 10250.90909090909,
      "grad_norm": 0.23491109907627106,
      "learning_rate": 7.8554784302677e-07,
      "loss": 0.0012,
      "step": 56380
    },
    {
      "epoch": 10252.727272727272,
      "grad_norm": 0.1968425214290619,
      "learning_rate": 7.85452321163715e-07,
      "loss": 0.0009,
      "step": 56390
    },
    {
      "epoch": 10254.545454545454,
      "grad_norm": 0.29858824610710144,
      "learning_rate": 7.853567838422159e-07,
      "loss": 0.0013,
      "step": 56400
    },
    {
      "epoch": 10256.363636363636,
      "grad_norm": 0.2913632094860077,
      "learning_rate": 7.852612310674461e-07,
      "loss": 0.0011,
      "step": 56410
    },
    {
      "epoch": 10258.181818181818,
      "grad_norm": 0.24533824622631073,
      "learning_rate": 7.851656628445803e-07,
      "loss": 0.0011,
      "step": 56420
    },
    {
      "epoch": 10260.0,
      "grad_norm": 0.3667879104614258,
      "learning_rate": 7.85070079178794e-07,
      "loss": 0.0011,
      "step": 56430
    },
    {
      "epoch": 10261.818181818182,
      "grad_norm": 0.225807324051857,
      "learning_rate": 7.849744800752635e-07,
      "loss": 0.0011,
      "step": 56440
    },
    {
      "epoch": 10263.636363636364,
      "grad_norm": 0.26956263184547424,
      "learning_rate": 7.848788655391657e-07,
      "loss": 0.0011,
      "step": 56450
    },
    {
      "epoch": 10265.454545454546,
      "grad_norm": 0.0013920579804107547,
      "learning_rate": 7.847832355756787e-07,
      "loss": 0.0011,
      "step": 56460
    },
    {
      "epoch": 10267.272727272728,
      "grad_norm": 0.20358020067214966,
      "learning_rate": 7.846875901899812e-07,
      "loss": 0.0013,
      "step": 56470
    },
    {
      "epoch": 10269.09090909091,
      "grad_norm": 0.22819216549396515,
      "learning_rate": 7.845919293872527e-07,
      "loss": 0.001,
      "step": 56480
    },
    {
      "epoch": 10270.90909090909,
      "grad_norm": 0.2361232191324234,
      "learning_rate": 7.844962531726739e-07,
      "loss": 0.0011,
      "step": 56490
    },
    {
      "epoch": 10272.727272727272,
      "grad_norm": 0.011106474325060844,
      "learning_rate": 7.844005615514258e-07,
      "loss": 0.0013,
      "step": 56500
    },
    {
      "epoch": 10272.727272727272,
      "eval_loss": 4.9037628173828125,
      "eval_runtime": 0.9522,
      "eval_samples_per_second": 10.502,
      "eval_steps_per_second": 5.251,
      "step": 56500
    },
    {
      "epoch": 10274.545454545454,
      "grad_norm": 0.0023973642382770777,
      "learning_rate": 7.843048545286907e-07,
      "loss": 0.0009,
      "step": 56510
    },
    {
      "epoch": 10276.363636363636,
      "grad_norm": 0.0010455427691340446,
      "learning_rate": 7.842091321096514e-07,
      "loss": 0.0013,
      "step": 56520
    },
    {
      "epoch": 10278.181818181818,
      "grad_norm": 0.001091061276383698,
      "learning_rate": 7.841133942994917e-07,
      "loss": 0.0009,
      "step": 56530
    },
    {
      "epoch": 10280.0,
      "grad_norm": 0.0011457885848358274,
      "learning_rate": 7.840176411033964e-07,
      "loss": 0.0012,
      "step": 56540
    },
    {
      "epoch": 10281.818181818182,
      "grad_norm": 0.22799977660179138,
      "learning_rate": 7.839218725265506e-07,
      "loss": 0.0012,
      "step": 56550
    },
    {
      "epoch": 10283.636363636364,
      "grad_norm": 0.0017212205566465855,
      "learning_rate": 7.838260885741407e-07,
      "loss": 0.0009,
      "step": 56560
    },
    {
      "epoch": 10285.454545454546,
      "grad_norm": 0.21694548428058624,
      "learning_rate": 7.837302892513539e-07,
      "loss": 0.0015,
      "step": 56570
    },
    {
      "epoch": 10287.272727272728,
      "grad_norm": 0.25038787722587585,
      "learning_rate": 7.836344745633782e-07,
      "loss": 0.0009,
      "step": 56580
    },
    {
      "epoch": 10289.09090909091,
      "grad_norm": 0.005655345506966114,
      "learning_rate": 7.835386445154022e-07,
      "loss": 0.0011,
      "step": 56590
    },
    {
      "epoch": 10290.90909090909,
      "grad_norm": 0.24485543370246887,
      "learning_rate": 7.834427991126155e-07,
      "loss": 0.0011,
      "step": 56600
    },
    {
      "epoch": 10292.727272727272,
      "grad_norm": 0.29278168082237244,
      "learning_rate": 7.833469383602085e-07,
      "loss": 0.0012,
      "step": 56610
    },
    {
      "epoch": 10294.545454545454,
      "grad_norm": 0.0011861870298162103,
      "learning_rate": 7.832510622633726e-07,
      "loss": 0.0011,
      "step": 56620
    },
    {
      "epoch": 10296.363636363636,
      "grad_norm": 0.0021563738118857145,
      "learning_rate": 7.831551708273e-07,
      "loss": 0.001,
      "step": 56630
    },
    {
      "epoch": 10298.181818181818,
      "grad_norm": 0.0021266364492475986,
      "learning_rate": 7.830592640571833e-07,
      "loss": 0.0011,
      "step": 56640
    },
    {
      "epoch": 10300.0,
      "grad_norm": 0.0010025381343439221,
      "learning_rate": 7.829633419582165e-07,
      "loss": 0.0012,
      "step": 56650
    },
    {
      "epoch": 10301.818181818182,
      "grad_norm": 0.31625068187713623,
      "learning_rate": 7.828674045355939e-07,
      "loss": 0.0012,
      "step": 56660
    },
    {
      "epoch": 10303.636363636364,
      "grad_norm": 0.0014703326160088181,
      "learning_rate": 7.827714517945114e-07,
      "loss": 0.0011,
      "step": 56670
    },
    {
      "epoch": 10305.454545454546,
      "grad_norm": 0.0011305147781968117,
      "learning_rate": 7.826754837401647e-07,
      "loss": 0.0009,
      "step": 56680
    },
    {
      "epoch": 10307.272727272728,
      "grad_norm": 0.0014787116087973118,
      "learning_rate": 7.825795003777514e-07,
      "loss": 0.0012,
      "step": 56690
    },
    {
      "epoch": 10309.09090909091,
      "grad_norm": 0.0014139359118416905,
      "learning_rate": 7.82483501712469e-07,
      "loss": 0.0012,
      "step": 56700
    },
    {
      "epoch": 10310.90909090909,
      "grad_norm": 0.0011961829150095582,
      "learning_rate": 7.823874877495164e-07,
      "loss": 0.0012,
      "step": 56710
    },
    {
      "epoch": 10312.727272727272,
      "grad_norm": 0.0009390945197083056,
      "learning_rate": 7.82291458494093e-07,
      "loss": 0.001,
      "step": 56720
    },
    {
      "epoch": 10314.545454545454,
      "grad_norm": 0.0025610937736928463,
      "learning_rate": 7.821954139513996e-07,
      "loss": 0.0013,
      "step": 56730
    },
    {
      "epoch": 10316.363636363636,
      "grad_norm": 0.0015348134329542518,
      "learning_rate": 7.820993541266368e-07,
      "loss": 0.0009,
      "step": 56740
    },
    {
      "epoch": 10318.181818181818,
      "grad_norm": 0.3155108392238617,
      "learning_rate": 7.820032790250073e-07,
      "loss": 0.0017,
      "step": 56750
    },
    {
      "epoch": 10320.0,
      "grad_norm": 0.001832440379075706,
      "learning_rate": 7.819071886517133e-07,
      "loss": 0.0011,
      "step": 56760
    },
    {
      "epoch": 10321.818181818182,
      "grad_norm": 0.0013235198566690087,
      "learning_rate": 7.818110830119591e-07,
      "loss": 0.001,
      "step": 56770
    },
    {
      "epoch": 10323.636363636364,
      "grad_norm": 0.004138844087719917,
      "learning_rate": 7.817149621109489e-07,
      "loss": 0.0012,
      "step": 56780
    },
    {
      "epoch": 10325.454545454546,
      "grad_norm": 0.00248443684540689,
      "learning_rate": 7.816188259538883e-07,
      "loss": 0.0008,
      "step": 56790
    },
    {
      "epoch": 10327.272727272728,
      "grad_norm": 0.2298544943332672,
      "learning_rate": 7.81522674545983e-07,
      "loss": 0.0013,
      "step": 56800
    },
    {
      "epoch": 10329.09090909091,
      "grad_norm": 0.0029250234365463257,
      "learning_rate": 7.814265078924404e-07,
      "loss": 0.0011,
      "step": 56810
    },
    {
      "epoch": 10330.90909090909,
      "grad_norm": 0.0037664843257516623,
      "learning_rate": 7.813303259984684e-07,
      "loss": 0.0012,
      "step": 56820
    },
    {
      "epoch": 10332.727272727272,
      "grad_norm": 0.002301351400092244,
      "learning_rate": 7.812341288692754e-07,
      "loss": 0.0011,
      "step": 56830
    },
    {
      "epoch": 10334.545454545454,
      "grad_norm": 0.0022348801139742136,
      "learning_rate": 7.811379165100709e-07,
      "loss": 0.0011,
      "step": 56840
    },
    {
      "epoch": 10336.363636363636,
      "grad_norm": 0.0020041619427502155,
      "learning_rate": 7.810416889260653e-07,
      "loss": 0.001,
      "step": 56850
    },
    {
      "epoch": 10338.181818181818,
      "grad_norm": 0.0010643817950040102,
      "learning_rate": 7.809454461224698e-07,
      "loss": 0.001,
      "step": 56860
    },
    {
      "epoch": 10340.0,
      "grad_norm": 0.005209698341786861,
      "learning_rate": 7.80849188104496e-07,
      "loss": 0.0012,
      "step": 56870
    },
    {
      "epoch": 10341.818181818182,
      "grad_norm": 0.18854552507400513,
      "learning_rate": 7.807529148773571e-07,
      "loss": 0.001,
      "step": 56880
    },
    {
      "epoch": 10343.636363636364,
      "grad_norm": 0.20794625580310822,
      "learning_rate": 7.806566264462666e-07,
      "loss": 0.0012,
      "step": 56890
    },
    {
      "epoch": 10345.454545454546,
      "grad_norm": 0.0011603414313867688,
      "learning_rate": 7.805603228164388e-07,
      "loss": 0.001,
      "step": 56900
    },
    {
      "epoch": 10347.272727272728,
      "grad_norm": 0.21221154928207397,
      "learning_rate": 7.80464003993089e-07,
      "loss": 0.0012,
      "step": 56910
    },
    {
      "epoch": 10349.09090909091,
      "grad_norm": 0.3007650375366211,
      "learning_rate": 7.803676699814333e-07,
      "loss": 0.0012,
      "step": 56920
    },
    {
      "epoch": 10350.90909090909,
      "grad_norm": 0.0013746338663622737,
      "learning_rate": 7.802713207866884e-07,
      "loss": 0.0009,
      "step": 56930
    },
    {
      "epoch": 10352.727272727272,
      "grad_norm": 0.3126124441623688,
      "learning_rate": 7.801749564140722e-07,
      "loss": 0.0015,
      "step": 56940
    },
    {
      "epoch": 10354.545454545454,
      "grad_norm": 0.002378813223913312,
      "learning_rate": 7.800785768688034e-07,
      "loss": 0.0009,
      "step": 56950
    },
    {
      "epoch": 10356.363636363636,
      "grad_norm": 0.003175794379785657,
      "learning_rate": 7.79982182156101e-07,
      "loss": 0.0011,
      "step": 56960
    },
    {
      "epoch": 10358.181818181818,
      "grad_norm": 0.30037638545036316,
      "learning_rate": 7.798857722811856e-07,
      "loss": 0.0014,
      "step": 56970
    },
    {
      "epoch": 10360.0,
      "grad_norm": 0.23508046567440033,
      "learning_rate": 7.797893472492777e-07,
      "loss": 0.0009,
      "step": 56980
    },
    {
      "epoch": 10361.818181818182,
      "grad_norm": 0.2170596718788147,
      "learning_rate": 7.796929070655993e-07,
      "loss": 0.0012,
      "step": 56990
    },
    {
      "epoch": 10363.636363636364,
      "grad_norm": 0.28811320662498474,
      "learning_rate": 7.795964517353733e-07,
      "loss": 0.001,
      "step": 57000
    },
    {
      "epoch": 10363.636363636364,
      "eval_loss": 4.9004225730896,
      "eval_runtime": 0.9535,
      "eval_samples_per_second": 10.488,
      "eval_steps_per_second": 5.244,
      "step": 57000
    },
    {
      "epoch": 10365.454545454546,
      "grad_norm": 0.002262239344418049,
      "learning_rate": 7.79499981263823e-07,
      "loss": 0.0009,
      "step": 57010
    },
    {
      "epoch": 10367.272727272728,
      "grad_norm": 0.2693619728088379,
      "learning_rate": 7.794034956561726e-07,
      "loss": 0.0013,
      "step": 57020
    },
    {
      "epoch": 10369.09090909091,
      "grad_norm": 0.16792339086532593,
      "learning_rate": 7.793069949176473e-07,
      "loss": 0.0011,
      "step": 57030
    },
    {
      "epoch": 10370.90909090909,
      "grad_norm": 0.23253437876701355,
      "learning_rate": 7.79210479053473e-07,
      "loss": 0.001,
      "step": 57040
    },
    {
      "epoch": 10372.727272727272,
      "grad_norm": 0.0225042924284935,
      "learning_rate": 7.791139480688762e-07,
      "loss": 0.0014,
      "step": 57050
    },
    {
      "epoch": 10374.545454545454,
      "grad_norm": 0.0011585421161726117,
      "learning_rate": 7.79017401969085e-07,
      "loss": 0.0009,
      "step": 57060
    },
    {
      "epoch": 10376.363636363636,
      "grad_norm": 0.0018850977066904306,
      "learning_rate": 7.789208407593272e-07,
      "loss": 0.001,
      "step": 57070
    },
    {
      "epoch": 10378.181818181818,
      "grad_norm": 0.0017327626701444387,
      "learning_rate": 7.788242644448324e-07,
      "loss": 0.0012,
      "step": 57080
    },
    {
      "epoch": 10380.0,
      "grad_norm": 0.0019306320464238524,
      "learning_rate": 7.787276730308303e-07,
      "loss": 0.0012,
      "step": 57090
    },
    {
      "epoch": 10381.818181818182,
      "grad_norm": 0.21378837525844574,
      "learning_rate": 7.786310665225522e-07,
      "loss": 0.001,
      "step": 57100
    },
    {
      "epoch": 10383.636363636364,
      "grad_norm": 0.0018934302497655153,
      "learning_rate": 7.785344449252292e-07,
      "loss": 0.001,
      "step": 57110
    },
    {
      "epoch": 10385.454545454546,
      "grad_norm": 0.001187954912893474,
      "learning_rate": 7.78437808244094e-07,
      "loss": 0.0012,
      "step": 57120
    },
    {
      "epoch": 10387.272727272728,
      "grad_norm": 0.000884222099557519,
      "learning_rate": 7.783411564843801e-07,
      "loss": 0.0011,
      "step": 57130
    },
    {
      "epoch": 10389.09090909091,
      "grad_norm": 0.2333066165447235,
      "learning_rate": 7.78244489651321e-07,
      "loss": 0.0012,
      "step": 57140
    },
    {
      "epoch": 10390.90909090909,
      "grad_norm": 0.0014670421369373798,
      "learning_rate": 7.781478077501524e-07,
      "loss": 0.0012,
      "step": 57150
    },
    {
      "epoch": 10392.727272727272,
      "grad_norm": 0.0012463341699913144,
      "learning_rate": 7.780511107861094e-07,
      "loss": 0.0012,
      "step": 57160
    },
    {
      "epoch": 10394.545454545454,
      "grad_norm": 0.002398095792159438,
      "learning_rate": 7.77954398764429e-07,
      "loss": 0.0009,
      "step": 57170
    },
    {
      "epoch": 10396.363636363636,
      "grad_norm": 0.0013505152892321348,
      "learning_rate": 7.778576716903482e-07,
      "loss": 0.0012,
      "step": 57180
    },
    {
      "epoch": 10398.181818181818,
      "grad_norm": 0.0012796116061508656,
      "learning_rate": 7.777609295691055e-07,
      "loss": 0.001,
      "step": 57190
    },
    {
      "epoch": 10400.0,
      "grad_norm": 0.0011542135616764426,
      "learning_rate": 7.776641724059396e-07,
      "loss": 0.0012,
      "step": 57200
    },
    {
      "epoch": 10401.818181818182,
      "grad_norm": 0.178924098610878,
      "learning_rate": 7.775674002060903e-07,
      "loss": 0.0011,
      "step": 57210
    },
    {
      "epoch": 10403.636363636364,
      "grad_norm": 0.0008917215163819492,
      "learning_rate": 7.774706129747985e-07,
      "loss": 0.0009,
      "step": 57220
    },
    {
      "epoch": 10405.454545454546,
      "grad_norm": 0.0016896097222343087,
      "learning_rate": 7.773738107173055e-07,
      "loss": 0.0012,
      "step": 57230
    },
    {
      "epoch": 10407.272727272728,
      "grad_norm": 0.2792893648147583,
      "learning_rate": 7.772769934388536e-07,
      "loss": 0.0015,
      "step": 57240
    },
    {
      "epoch": 10409.09090909091,
      "grad_norm": 0.1855003833770752,
      "learning_rate": 7.771801611446858e-07,
      "loss": 0.0007,
      "step": 57250
    },
    {
      "epoch": 10410.90909090909,
      "grad_norm": 0.0015295568155124784,
      "learning_rate": 7.770833138400458e-07,
      "loss": 0.0012,
      "step": 57260
    },
    {
      "epoch": 10412.727272727272,
      "grad_norm": 0.005481828935444355,
      "learning_rate": 7.769864515301786e-07,
      "loss": 0.0009,
      "step": 57270
    },
    {
      "epoch": 10414.545454545454,
      "grad_norm": 0.0012126897927373648,
      "learning_rate": 7.768895742203297e-07,
      "loss": 0.0015,
      "step": 57280
    },
    {
      "epoch": 10416.363636363636,
      "grad_norm": 0.22001796960830688,
      "learning_rate": 7.76792681915745e-07,
      "loss": 0.0011,
      "step": 57290
    },
    {
      "epoch": 10418.181818181818,
      "grad_norm": 0.013957520015537739,
      "learning_rate": 7.76695774621672e-07,
      "loss": 0.0011,
      "step": 57300
    },
    {
      "epoch": 10420.0,
      "grad_norm": 0.185416579246521,
      "learning_rate": 7.765988523433586e-07,
      "loss": 0.0009,
      "step": 57310
    },
    {
      "epoch": 10421.818181818182,
      "grad_norm": 0.0014447295106947422,
      "learning_rate": 7.765019150860534e-07,
      "loss": 0.0011,
      "step": 57320
    },
    {
      "epoch": 10423.636363636364,
      "grad_norm": 0.2890775203704834,
      "learning_rate": 7.764049628550063e-07,
      "loss": 0.0012,
      "step": 57330
    },
    {
      "epoch": 10425.454545454546,
      "grad_norm": 0.16330227255821228,
      "learning_rate": 7.763079956554671e-07,
      "loss": 0.001,
      "step": 57340
    },
    {
      "epoch": 10427.272727272728,
      "grad_norm": 0.00143836485221982,
      "learning_rate": 7.762110134926876e-07,
      "loss": 0.001,
      "step": 57350
    },
    {
      "epoch": 10429.09090909091,
      "grad_norm": 0.49225375056266785,
      "learning_rate": 7.761140163719193e-07,
      "loss": 0.0014,
      "step": 57360
    },
    {
      "epoch": 10430.90909090909,
      "grad_norm": 0.30010542273521423,
      "learning_rate": 7.760170042984153e-07,
      "loss": 0.0009,
      "step": 57370
    },
    {
      "epoch": 10432.727272727272,
      "grad_norm": 0.001978750806301832,
      "learning_rate": 7.75919977277429e-07,
      "loss": 0.0012,
      "step": 57380
    },
    {
      "epoch": 10434.545454545454,
      "grad_norm": 0.24729257822036743,
      "learning_rate": 7.758229353142152e-07,
      "loss": 0.0011,
      "step": 57390
    },
    {
      "epoch": 10436.363636363636,
      "grad_norm": 0.0011455590138211846,
      "learning_rate": 7.757258784140285e-07,
      "loss": 0.001,
      "step": 57400
    },
    {
      "epoch": 10438.181818181818,
      "grad_norm": 0.003455322002992034,
      "learning_rate": 7.756288065821257e-07,
      "loss": 0.0011,
      "step": 57410
    },
    {
      "epoch": 10440.0,
      "grad_norm": 0.26953673362731934,
      "learning_rate": 7.755317198237629e-07,
      "loss": 0.0012,
      "step": 57420
    },
    {
      "epoch": 10441.818181818182,
      "grad_norm": 0.1848517209291458,
      "learning_rate": 7.754346181441984e-07,
      "loss": 0.001,
      "step": 57430
    },
    {
      "epoch": 10443.636363636364,
      "grad_norm": 0.16760635375976562,
      "learning_rate": 7.753375015486901e-07,
      "loss": 0.0012,
      "step": 57440
    },
    {
      "epoch": 10445.454545454546,
      "grad_norm": 0.0035402264911681414,
      "learning_rate": 7.752403700424978e-07,
      "loss": 0.0011,
      "step": 57450
    },
    {
      "epoch": 10447.272727272728,
      "grad_norm": 0.2221916913986206,
      "learning_rate": 7.751432236308812e-07,
      "loss": 0.0012,
      "step": 57460
    },
    {
      "epoch": 10449.09090909091,
      "grad_norm": 0.0013069617561995983,
      "learning_rate": 7.750460623191013e-07,
      "loss": 0.0009,
      "step": 57470
    },
    {
      "epoch": 10450.90909090909,
      "grad_norm": 0.2218884378671646,
      "learning_rate": 7.7494888611242e-07,
      "loss": 0.0013,
      "step": 57480
    },
    {
      "epoch": 10452.727272727272,
      "grad_norm": 0.0022163153626024723,
      "learning_rate": 7.748516950160992e-07,
      "loss": 0.0011,
      "step": 57490
    },
    {
      "epoch": 10454.545454545454,
      "grad_norm": 0.04697828367352486,
      "learning_rate": 7.74754489035403e-07,
      "loss": 0.0015,
      "step": 57500
    },
    {
      "epoch": 10454.545454545454,
      "eval_loss": 4.970674991607666,
      "eval_runtime": 0.9518,
      "eval_samples_per_second": 10.507,
      "eval_steps_per_second": 5.253,
      "step": 57500
    },
    {
      "epoch": 10456.363636363636,
      "grad_norm": 0.04676603525876999,
      "learning_rate": 7.74657268175595e-07,
      "loss": 0.0009,
      "step": 57510
    },
    {
      "epoch": 10458.181818181818,
      "grad_norm": 0.001453543547540903,
      "learning_rate": 7.745600324419403e-07,
      "loss": 0.0009,
      "step": 57520
    },
    {
      "epoch": 10460.0,
      "grad_norm": 0.23084066808223724,
      "learning_rate": 7.744627818397045e-07,
      "loss": 0.0012,
      "step": 57530
    },
    {
      "epoch": 10461.818181818182,
      "grad_norm": 0.001697530155070126,
      "learning_rate": 7.743655163741543e-07,
      "loss": 0.0009,
      "step": 57540
    },
    {
      "epoch": 10463.636363636364,
      "grad_norm": 0.19124287366867065,
      "learning_rate": 7.742682360505568e-07,
      "loss": 0.0014,
      "step": 57550
    },
    {
      "epoch": 10465.454545454546,
      "grad_norm": 0.24958445131778717,
      "learning_rate": 7.741709408741803e-07,
      "loss": 0.0011,
      "step": 57560
    },
    {
      "epoch": 10467.272727272728,
      "grad_norm": 0.2661830186843872,
      "learning_rate": 7.740736308502938e-07,
      "loss": 0.0011,
      "step": 57570
    },
    {
      "epoch": 10469.09090909091,
      "grad_norm": 0.22378166019916534,
      "learning_rate": 7.739763059841669e-07,
      "loss": 0.0011,
      "step": 57580
    },
    {
      "epoch": 10470.90909090909,
      "grad_norm": 0.21517769992351532,
      "learning_rate": 7.738789662810701e-07,
      "loss": 0.0012,
      "step": 57590
    },
    {
      "epoch": 10472.727272727272,
      "grad_norm": 0.29126793146133423,
      "learning_rate": 7.737816117462751e-07,
      "loss": 0.0011,
      "step": 57600
    },
    {
      "epoch": 10474.545454545454,
      "grad_norm": 0.22487443685531616,
      "learning_rate": 7.736842423850537e-07,
      "loss": 0.0011,
      "step": 57610
    },
    {
      "epoch": 10476.363636363636,
      "grad_norm": 0.23304295539855957,
      "learning_rate": 7.73586858202679e-07,
      "loss": 0.0013,
      "step": 57620
    },
    {
      "epoch": 10478.181818181818,
      "grad_norm": 0.06375949084758759,
      "learning_rate": 7.734894592044248e-07,
      "loss": 0.0008,
      "step": 57630
    },
    {
      "epoch": 10480.0,
      "grad_norm": 0.1824341118335724,
      "learning_rate": 7.733920453955656e-07,
      "loss": 0.0013,
      "step": 57640
    },
    {
      "epoch": 10481.818181818182,
      "grad_norm": 0.21676822006702423,
      "learning_rate": 7.732946167813768e-07,
      "loss": 0.0011,
      "step": 57650
    },
    {
      "epoch": 10483.636363636364,
      "grad_norm": 0.0016252086497843266,
      "learning_rate": 7.731971733671345e-07,
      "loss": 0.0011,
      "step": 57660
    },
    {
      "epoch": 10485.454545454546,
      "grad_norm": 0.00292876991443336,
      "learning_rate": 7.730997151581158e-07,
      "loss": 0.0011,
      "step": 57670
    },
    {
      "epoch": 10487.272727272728,
      "grad_norm": 0.20749551057815552,
      "learning_rate": 7.730022421595983e-07,
      "loss": 0.0014,
      "step": 57680
    },
    {
      "epoch": 10489.09090909091,
      "grad_norm": 0.002328597940504551,
      "learning_rate": 7.729047543768608e-07,
      "loss": 0.0009,
      "step": 57690
    },
    {
      "epoch": 10490.90909090909,
      "grad_norm": 0.0011639355216175318,
      "learning_rate": 7.728072518151825e-07,
      "loss": 0.0011,
      "step": 57700
    },
    {
      "epoch": 10492.727272727272,
      "grad_norm": 0.23688891530036926,
      "learning_rate": 7.727097344798435e-07,
      "loss": 0.0013,
      "step": 57710
    },
    {
      "epoch": 10494.545454545454,
      "grad_norm": 0.0012351350160315633,
      "learning_rate": 7.72612202376125e-07,
      "loss": 0.0008,
      "step": 57720
    },
    {
      "epoch": 10496.363636363636,
      "grad_norm": 0.19134868681430817,
      "learning_rate": 7.725146555093087e-07,
      "loss": 0.0011,
      "step": 57730
    },
    {
      "epoch": 10498.181818181818,
      "grad_norm": 0.22794802486896515,
      "learning_rate": 7.724170938846771e-07,
      "loss": 0.0014,
      "step": 57740
    },
    {
      "epoch": 10500.0,
      "grad_norm": 0.18266098201274872,
      "learning_rate": 7.723195175075135e-07,
      "loss": 0.0011,
      "step": 57750
    },
    {
      "epoch": 10501.818181818182,
      "grad_norm": 0.19873157143592834,
      "learning_rate": 7.722219263831022e-07,
      "loss": 0.0012,
      "step": 57760
    },
    {
      "epoch": 10503.636363636364,
      "grad_norm": 0.21878312528133392,
      "learning_rate": 7.721243205167282e-07,
      "loss": 0.001,
      "step": 57770
    },
    {
      "epoch": 10505.454545454546,
      "grad_norm": 0.24563869833946228,
      "learning_rate": 7.720266999136773e-07,
      "loss": 0.0012,
      "step": 57780
    },
    {
      "epoch": 10507.272727272728,
      "grad_norm": 0.23035036027431488,
      "learning_rate": 7.71929064579236e-07,
      "loss": 0.0009,
      "step": 57790
    },
    {
      "epoch": 10509.09090909091,
      "grad_norm": 0.0010691966162994504,
      "learning_rate": 7.718314145186916e-07,
      "loss": 0.0011,
      "step": 57800
    },
    {
      "epoch": 10510.90909090909,
      "grad_norm": 0.002198818139731884,
      "learning_rate": 7.717337497373324e-07,
      "loss": 0.0012,
      "step": 57810
    },
    {
      "epoch": 10512.727272727272,
      "grad_norm": 0.16916488111019135,
      "learning_rate": 7.716360702404472e-07,
      "loss": 0.001,
      "step": 57820
    },
    {
      "epoch": 10514.545454545454,
      "grad_norm": 0.001950983889400959,
      "learning_rate": 7.715383760333257e-07,
      "loss": 0.0011,
      "step": 57830
    },
    {
      "epoch": 10516.363636363636,
      "grad_norm": 0.263781875371933,
      "learning_rate": 7.714406671212588e-07,
      "loss": 0.0012,
      "step": 57840
    },
    {
      "epoch": 10518.181818181818,
      "grad_norm": 0.0019128237618133426,
      "learning_rate": 7.713429435095374e-07,
      "loss": 0.0009,
      "step": 57850
    },
    {
      "epoch": 10520.0,
      "grad_norm": 0.002685477724298835,
      "learning_rate": 7.71245205203454e-07,
      "loss": 0.0013,
      "step": 57860
    },
    {
      "epoch": 10521.818181818182,
      "grad_norm": 0.001766599714756012,
      "learning_rate": 7.711474522083015e-07,
      "loss": 0.0009,
      "step": 57870
    },
    {
      "epoch": 10523.636363636364,
      "grad_norm": 0.21531137824058533,
      "learning_rate": 7.710496845293735e-07,
      "loss": 0.0014,
      "step": 57880
    },
    {
      "epoch": 10525.454545454546,
      "grad_norm": 0.0015379601391032338,
      "learning_rate": 7.709519021719643e-07,
      "loss": 0.0009,
      "step": 57890
    },
    {
      "epoch": 10527.272727272728,
      "grad_norm": 0.26000553369522095,
      "learning_rate": 7.7085410514137e-07,
      "loss": 0.0015,
      "step": 57900
    },
    {
      "epoch": 10529.09090909091,
      "grad_norm": 0.19623883068561554,
      "learning_rate": 7.70756293442886e-07,
      "loss": 0.0009,
      "step": 57910
    },
    {
      "epoch": 10530.90909090909,
      "grad_norm": 0.0021209532860666513,
      "learning_rate": 7.706584670818093e-07,
      "loss": 0.0012,
      "step": 57920
    },
    {
      "epoch": 10532.727272727272,
      "grad_norm": 0.002914123237133026,
      "learning_rate": 7.705606260634379e-07,
      "loss": 0.0009,
      "step": 57930
    },
    {
      "epoch": 10534.545454545454,
      "grad_norm": 0.2657618820667267,
      "learning_rate": 7.704627703930702e-07,
      "loss": 0.0013,
      "step": 57940
    },
    {
      "epoch": 10536.363636363636,
      "grad_norm": 0.2143050730228424,
      "learning_rate": 7.703649000760053e-07,
      "loss": 0.0009,
      "step": 57950
    },
    {
      "epoch": 10538.181818181818,
      "grad_norm": 0.1867830902338028,
      "learning_rate": 7.702670151175434e-07,
      "loss": 0.0014,
      "step": 57960
    },
    {
      "epoch": 10540.0,
      "grad_norm": 0.000962928170338273,
      "learning_rate": 7.701691155229857e-07,
      "loss": 0.001,
      "step": 57970
    },
    {
      "epoch": 10541.818181818182,
      "grad_norm": 0.21126942336559296,
      "learning_rate": 7.700712012976335e-07,
      "loss": 0.001,
      "step": 57980
    },
    {
      "epoch": 10543.636363636364,
      "grad_norm": 0.22065290808677673,
      "learning_rate": 7.699732724467893e-07,
      "loss": 0.0014,
      "step": 57990
    },
    {
      "epoch": 10545.454545454546,
      "grad_norm": 0.1753157377243042,
      "learning_rate": 7.698753289757564e-07,
      "loss": 0.0009,
      "step": 58000
    },
    {
      "epoch": 10545.454545454546,
      "eval_loss": 4.9068403244018555,
      "eval_runtime": 0.9492,
      "eval_samples_per_second": 10.535,
      "eval_steps_per_second": 5.267,
      "step": 58000
    },
    {
      "epoch": 10547.272727272728,
      "grad_norm": 0.010905754752457142,
      "learning_rate": 7.697773708898389e-07,
      "loss": 0.0012,
      "step": 58010
    },
    {
      "epoch": 10549.09090909091,
      "grad_norm": 0.21421149373054504,
      "learning_rate": 7.696793981943417e-07,
      "loss": 0.001,
      "step": 58020
    },
    {
      "epoch": 10550.90909090909,
      "grad_norm": 0.17544879019260406,
      "learning_rate": 7.695814108945703e-07,
      "loss": 0.0009,
      "step": 58030
    },
    {
      "epoch": 10552.727272727272,
      "grad_norm": 0.22821494936943054,
      "learning_rate": 7.694834089958311e-07,
      "loss": 0.0009,
      "step": 58040
    },
    {
      "epoch": 10554.545454545454,
      "grad_norm": 0.23196345567703247,
      "learning_rate": 7.693853925034314e-07,
      "loss": 0.0014,
      "step": 58050
    },
    {
      "epoch": 10556.363636363636,
      "grad_norm": 0.22208823263645172,
      "learning_rate": 7.692873614226794e-07,
      "loss": 0.001,
      "step": 58060
    },
    {
      "epoch": 10558.181818181818,
      "grad_norm": 0.23646412789821625,
      "learning_rate": 7.691893157588836e-07,
      "loss": 0.0015,
      "step": 58070
    },
    {
      "epoch": 10560.0,
      "grad_norm": 0.24607491493225098,
      "learning_rate": 7.690912555173535e-07,
      "loss": 0.0008,
      "step": 58080
    },
    {
      "epoch": 10561.818181818182,
      "grad_norm": 0.2896645665168762,
      "learning_rate": 7.689931807033998e-07,
      "loss": 0.0011,
      "step": 58090
    },
    {
      "epoch": 10563.636363636364,
      "grad_norm": 0.00204039947129786,
      "learning_rate": 7.688950913223336e-07,
      "loss": 0.0013,
      "step": 58100
    },
    {
      "epoch": 10565.454545454546,
      "grad_norm": 0.18405823409557343,
      "learning_rate": 7.687969873794667e-07,
      "loss": 0.0009,
      "step": 58110
    },
    {
      "epoch": 10567.272727272728,
      "grad_norm": 0.25141283869743347,
      "learning_rate": 7.686988688801118e-07,
      "loss": 0.0012,
      "step": 58120
    },
    {
      "epoch": 10569.09090909091,
      "grad_norm": 0.053462158888578415,
      "learning_rate": 7.686007358295828e-07,
      "loss": 0.0011,
      "step": 58130
    },
    {
      "epoch": 10570.90909090909,
      "grad_norm": 0.0034531624987721443,
      "learning_rate": 7.685025882331935e-07,
      "loss": 0.001,
      "step": 58140
    },
    {
      "epoch": 10572.727272727272,
      "grad_norm": 0.0014484910061582923,
      "learning_rate": 7.684044260962592e-07,
      "loss": 0.0013,
      "step": 58150
    },
    {
      "epoch": 10574.545454545454,
      "grad_norm": 0.16612966358661652,
      "learning_rate": 7.683062494240961e-07,
      "loss": 0.0012,
      "step": 58160
    },
    {
      "epoch": 10576.363636363636,
      "grad_norm": 0.0010682919528335333,
      "learning_rate": 7.682080582220205e-07,
      "loss": 0.0011,
      "step": 58170
    },
    {
      "epoch": 10578.181818181818,
      "grad_norm": 0.2775867283344269,
      "learning_rate": 7.681098524953501e-07,
      "loss": 0.0011,
      "step": 58180
    },
    {
      "epoch": 10580.0,
      "grad_norm": 0.309524804353714,
      "learning_rate": 7.68011632249403e-07,
      "loss": 0.001,
      "step": 58190
    },
    {
      "epoch": 10581.818181818182,
      "grad_norm": 0.0010303125018253922,
      "learning_rate": 7.679133974894982e-07,
      "loss": 0.0011,
      "step": 58200
    },
    {
      "epoch": 10583.636363636364,
      "grad_norm": 0.1846199631690979,
      "learning_rate": 7.678151482209558e-07,
      "loss": 0.0013,
      "step": 58210
    },
    {
      "epoch": 10585.454545454546,
      "grad_norm": 0.21997897326946259,
      "learning_rate": 7.677168844490963e-07,
      "loss": 0.0008,
      "step": 58220
    },
    {
      "epoch": 10587.272727272728,
      "grad_norm": 0.0010410454124212265,
      "learning_rate": 7.676186061792407e-07,
      "loss": 0.001,
      "step": 58230
    },
    {
      "epoch": 10589.09090909091,
      "grad_norm": 0.003715899307280779,
      "learning_rate": 7.675203134167117e-07,
      "loss": 0.0012,
      "step": 58240
    },
    {
      "epoch": 10590.90909090909,
      "grad_norm": 0.0018744299886748195,
      "learning_rate": 7.674220061668322e-07,
      "loss": 0.001,
      "step": 58250
    },
    {
      "epoch": 10592.727272727272,
      "grad_norm": 0.2956741154193878,
      "learning_rate": 7.673236844349256e-07,
      "loss": 0.0012,
      "step": 58260
    },
    {
      "epoch": 10594.545454545454,
      "grad_norm": 0.18955980241298676,
      "learning_rate": 7.672253482263168e-07,
      "loss": 0.0011,
      "step": 58270
    },
    {
      "epoch": 10596.363636363636,
      "grad_norm": 0.0064743272960186005,
      "learning_rate": 7.67126997546331e-07,
      "loss": 0.0011,
      "step": 58280
    },
    {
      "epoch": 10598.181818181818,
      "grad_norm": 0.19395099580287933,
      "learning_rate": 7.670286324002942e-07,
      "loss": 0.0013,
      "step": 58290
    },
    {
      "epoch": 10600.0,
      "grad_norm": 0.0018829351756721735,
      "learning_rate": 7.669302527935334e-07,
      "loss": 0.001,
      "step": 58300
    },
    {
      "epoch": 10601.818181818182,
      "grad_norm": 0.0013400516472756863,
      "learning_rate": 7.668318587313763e-07,
      "loss": 0.001,
      "step": 58310
    },
    {
      "epoch": 10603.636363636364,
      "grad_norm": 0.0026332426350563765,
      "learning_rate": 7.667334502191514e-07,
      "loss": 0.0012,
      "step": 58320
    },
    {
      "epoch": 10605.454545454546,
      "grad_norm": 0.002605908550322056,
      "learning_rate": 7.666350272621875e-07,
      "loss": 0.0008,
      "step": 58330
    },
    {
      "epoch": 10607.272727272728,
      "grad_norm": 0.27043813467025757,
      "learning_rate": 7.665365898658153e-07,
      "loss": 0.0015,
      "step": 58340
    },
    {
      "epoch": 10609.09090909091,
      "grad_norm": 0.21378493309020996,
      "learning_rate": 7.664381380353649e-07,
      "loss": 0.0009,
      "step": 58350
    },
    {
      "epoch": 10610.90909090909,
      "grad_norm": 0.19095981121063232,
      "learning_rate": 7.663396717761686e-07,
      "loss": 0.0011,
      "step": 58360
    },
    {
      "epoch": 10612.727272727272,
      "grad_norm": 0.0038874640595167875,
      "learning_rate": 7.662411910935582e-07,
      "loss": 0.0009,
      "step": 58370
    },
    {
      "epoch": 10614.545454545454,
      "grad_norm": 0.22721360623836517,
      "learning_rate": 7.66142695992867e-07,
      "loss": 0.0014,
      "step": 58380
    },
    {
      "epoch": 10616.363636363636,
      "grad_norm": 0.01838718354701996,
      "learning_rate": 7.660441864794289e-07,
      "loss": 0.0011,
      "step": 58390
    },
    {
      "epoch": 10618.181818181818,
      "grad_norm": 0.0010980336228385568,
      "learning_rate": 7.659456625585788e-07,
      "loss": 0.0009,
      "step": 58400
    },
    {
      "epoch": 10620.0,
      "grad_norm": 0.0012765040155500174,
      "learning_rate": 7.65847124235652e-07,
      "loss": 0.0012,
      "step": 58410
    },
    {
      "epoch": 10621.818181818182,
      "grad_norm": 0.001444879686459899,
      "learning_rate": 7.657485715159848e-07,
      "loss": 0.001,
      "step": 58420
    },
    {
      "epoch": 10623.636363636364,
      "grad_norm": 0.21364250779151917,
      "learning_rate": 7.656500044049143e-07,
      "loss": 0.0011,
      "step": 58430
    },
    {
      "epoch": 10625.454545454546,
      "grad_norm": 0.28896838426589966,
      "learning_rate": 7.655514229077783e-07,
      "loss": 0.0015,
      "step": 58440
    },
    {
      "epoch": 10627.272727272728,
      "grad_norm": 0.3008485436439514,
      "learning_rate": 7.654528270299154e-07,
      "loss": 0.0009,
      "step": 58450
    },
    {
      "epoch": 10629.09090909091,
      "grad_norm": 0.0028003475163131952,
      "learning_rate": 7.653542167766649e-07,
      "loss": 0.0009,
      "step": 58460
    },
    {
      "epoch": 10630.90909090909,
      "grad_norm": 0.00220129475928843,
      "learning_rate": 7.65255592153367e-07,
      "loss": 0.0012,
      "step": 58470
    },
    {
      "epoch": 10632.727272727272,
      "grad_norm": 0.21169482171535492,
      "learning_rate": 7.651569531653628e-07,
      "loss": 0.0009,
      "step": 58480
    },
    {
      "epoch": 10634.545454545454,
      "grad_norm": 0.1894730180501938,
      "learning_rate": 7.650582998179937e-07,
      "loss": 0.0016,
      "step": 58490
    },
    {
      "epoch": 10636.363636363636,
      "grad_norm": 0.17340855300426483,
      "learning_rate": 7.649596321166024e-07,
      "loss": 0.0007,
      "step": 58500
    },
    {
      "epoch": 10636.363636363636,
      "eval_loss": 4.9152116775512695,
      "eval_runtime": 0.9516,
      "eval_samples_per_second": 10.509,
      "eval_steps_per_second": 5.254,
      "step": 58500
    },
    {
      "epoch": 10638.181818181818,
      "grad_norm": 0.18605951964855194,
      "learning_rate": 7.648609500665323e-07,
      "loss": 0.0013,
      "step": 58510
    },
    {
      "epoch": 10640.0,
      "grad_norm": 0.0022893743589520454,
      "learning_rate": 7.647622536731272e-07,
      "loss": 0.001,
      "step": 58520
    },
    {
      "epoch": 10641.818181818182,
      "grad_norm": 0.19201236963272095,
      "learning_rate": 7.64663542941732e-07,
      "loss": 0.0012,
      "step": 58530
    },
    {
      "epoch": 10643.636363636364,
      "grad_norm": 0.0010927547700703144,
      "learning_rate": 7.645648178776923e-07,
      "loss": 0.0011,
      "step": 58540
    },
    {
      "epoch": 10645.454545454546,
      "grad_norm": 0.2008047252893448,
      "learning_rate": 7.644660784863546e-07,
      "loss": 0.0009,
      "step": 58550
    },
    {
      "epoch": 10647.272727272728,
      "grad_norm": 0.23019345104694366,
      "learning_rate": 7.643673247730658e-07,
      "loss": 0.0014,
      "step": 58560
    },
    {
      "epoch": 10649.09090909091,
      "grad_norm": 0.002342168241739273,
      "learning_rate": 7.642685567431741e-07,
      "loss": 0.0009,
      "step": 58570
    },
    {
      "epoch": 10650.90909090909,
      "grad_norm": 0.22737771272659302,
      "learning_rate": 7.641697744020281e-07,
      "loss": 0.0012,
      "step": 58580
    },
    {
      "epoch": 10652.727272727272,
      "grad_norm": 0.0014556969981640577,
      "learning_rate": 7.640709777549772e-07,
      "loss": 0.0009,
      "step": 58590
    },
    {
      "epoch": 10654.545454545454,
      "grad_norm": 0.2716538608074188,
      "learning_rate": 7.639721668073717e-07,
      "loss": 0.0017,
      "step": 58600
    },
    {
      "epoch": 10656.363636363636,
      "grad_norm": 0.04427432641386986,
      "learning_rate": 7.638733415645628e-07,
      "loss": 0.0012,
      "step": 58610
    },
    {
      "epoch": 10658.181818181818,
      "grad_norm": 0.004103608429431915,
      "learning_rate": 7.637745020319018e-07,
      "loss": 0.0007,
      "step": 58620
    },
    {
      "epoch": 10660.0,
      "grad_norm": 0.18375445902347565,
      "learning_rate": 7.63675648214742e-07,
      "loss": 0.0012,
      "step": 58630
    },
    {
      "epoch": 10661.818181818182,
      "grad_norm": 0.002075397176668048,
      "learning_rate": 7.635767801184362e-07,
      "loss": 0.0012,
      "step": 58640
    },
    {
      "epoch": 10663.636363636364,
      "grad_norm": 0.21159109473228455,
      "learning_rate": 7.634778977483388e-07,
      "loss": 0.0009,
      "step": 58650
    },
    {
      "epoch": 10665.454545454546,
      "grad_norm": 0.0014921565307304263,
      "learning_rate": 7.633790011098045e-07,
      "loss": 0.0011,
      "step": 58660
    },
    {
      "epoch": 10667.272727272728,
      "grad_norm": 0.2407233566045761,
      "learning_rate": 7.632800902081891e-07,
      "loss": 0.0012,
      "step": 58670
    },
    {
      "epoch": 10669.09090909091,
      "grad_norm": 0.0018171046394854784,
      "learning_rate": 7.631811650488488e-07,
      "loss": 0.0011,
      "step": 58680
    },
    {
      "epoch": 10670.90909090909,
      "grad_norm": 0.0012833387590944767,
      "learning_rate": 7.630822256371414e-07,
      "loss": 0.0011,
      "step": 58690
    },
    {
      "epoch": 10672.727272727272,
      "grad_norm": 0.0023065994028002024,
      "learning_rate": 7.629832719784244e-07,
      "loss": 0.0009,
      "step": 58700
    },
    {
      "epoch": 10674.545454545454,
      "grad_norm": 0.001014131004922092,
      "learning_rate": 7.628843040780566e-07,
      "loss": 0.001,
      "step": 58710
    },
    {
      "epoch": 10676.363636363636,
      "grad_norm": 0.20644088089466095,
      "learning_rate": 7.627853219413975e-07,
      "loss": 0.0012,
      "step": 58720
    },
    {
      "epoch": 10678.181818181818,
      "grad_norm": 0.2917154133319855,
      "learning_rate": 7.626863255738075e-07,
      "loss": 0.0014,
      "step": 58730
    },
    {
      "epoch": 10680.0,
      "grad_norm": 0.3264591097831726,
      "learning_rate": 7.625873149806479e-07,
      "loss": 0.001,
      "step": 58740
    },
    {
      "epoch": 10681.818181818182,
      "grad_norm": 0.0008623383473604918,
      "learning_rate": 7.6248829016728e-07,
      "loss": 0.001,
      "step": 58750
    },
    {
      "epoch": 10683.636363636364,
      "grad_norm": 0.16200579702854156,
      "learning_rate": 7.62389251139067e-07,
      "loss": 0.0013,
      "step": 58760
    },
    {
      "epoch": 10685.454545454546,
      "grad_norm": 0.17301149666309357,
      "learning_rate": 7.622901979013716e-07,
      "loss": 0.0012,
      "step": 58770
    },
    {
      "epoch": 10687.272727272728,
      "grad_norm": 0.0012146936496719718,
      "learning_rate": 7.621911304595588e-07,
      "loss": 0.0009,
      "step": 58780
    },
    {
      "epoch": 10689.09090909091,
      "grad_norm": 0.2042638659477234,
      "learning_rate": 7.620920488189928e-07,
      "loss": 0.0012,
      "step": 58790
    },
    {
      "epoch": 10690.90909090909,
      "grad_norm": 0.171438530087471,
      "learning_rate": 7.619929529850396e-07,
      "loss": 0.0011,
      "step": 58800
    },
    {
      "epoch": 10692.727272727272,
      "grad_norm": 0.0017922038678079844,
      "learning_rate": 7.618938429630655e-07,
      "loss": 0.0011,
      "step": 58810
    },
    {
      "epoch": 10694.545454545454,
      "grad_norm": 0.0014115107478573918,
      "learning_rate": 7.617947187584379e-07,
      "loss": 0.0013,
      "step": 58820
    },
    {
      "epoch": 10696.363636363636,
      "grad_norm": 0.29541540145874023,
      "learning_rate": 7.616955803765248e-07,
      "loss": 0.0012,
      "step": 58830
    },
    {
      "epoch": 10698.181818181818,
      "grad_norm": 0.0012934829574078321,
      "learning_rate": 7.615964278226948e-07,
      "loss": 0.0008,
      "step": 58840
    },
    {
      "epoch": 10700.0,
      "grad_norm": 0.24015991389751434,
      "learning_rate": 7.614972611023176e-07,
      "loss": 0.0012,
      "step": 58850
    },
    {
      "epoch": 10701.818181818182,
      "grad_norm": 0.22943904995918274,
      "learning_rate": 7.613980802207632e-07,
      "loss": 0.0012,
      "step": 58860
    },
    {
      "epoch": 10703.636363636364,
      "grad_norm": 0.003078663954511285,
      "learning_rate": 7.612988851834032e-07,
      "loss": 0.001,
      "step": 58870
    },
    {
      "epoch": 10705.454545454546,
      "grad_norm": 0.179655522108078,
      "learning_rate": 7.611996759956088e-07,
      "loss": 0.001,
      "step": 58880
    },
    {
      "epoch": 10707.272727272728,
      "grad_norm": 0.0033410554751753807,
      "learning_rate": 7.611004526627529e-07,
      "loss": 0.0009,
      "step": 58890
    },
    {
      "epoch": 10709.09090909091,
      "grad_norm": 0.21333284676074982,
      "learning_rate": 7.61001215190209e-07,
      "loss": 0.0014,
      "step": 58900
    },
    {
      "epoch": 10710.90909090909,
      "grad_norm": 0.276745080947876,
      "learning_rate": 7.609019635833511e-07,
      "loss": 0.0012,
      "step": 58910
    },
    {
      "epoch": 10712.727272727272,
      "grad_norm": 0.18265433609485626,
      "learning_rate": 7.608026978475539e-07,
      "loss": 0.0011,
      "step": 58920
    },
    {
      "epoch": 10714.545454545454,
      "grad_norm": 0.18725775182247162,
      "learning_rate": 7.607034179881933e-07,
      "loss": 0.0011,
      "step": 58930
    },
    {
      "epoch": 10716.363636363636,
      "grad_norm": 0.20311960577964783,
      "learning_rate": 7.606041240106456e-07,
      "loss": 0.0011,
      "step": 58940
    },
    {
      "epoch": 10718.181818181818,
      "grad_norm": 0.002107583452016115,
      "learning_rate": 7.605048159202882e-07,
      "loss": 0.0009,
      "step": 58950
    },
    {
      "epoch": 10720.0,
      "grad_norm": 0.00295618106611073,
      "learning_rate": 7.604054937224989e-07,
      "loss": 0.0012,
      "step": 58960
    },
    {
      "epoch": 10721.818181818182,
      "grad_norm": 0.0013498518383130431,
      "learning_rate": 7.603061574226561e-07,
      "loss": 0.001,
      "step": 58970
    },
    {
      "epoch": 10723.636363636364,
      "grad_norm": 0.0023824989330023527,
      "learning_rate": 7.602068070261398e-07,
      "loss": 0.0011,
      "step": 58980
    },
    {
      "epoch": 10725.454545454546,
      "grad_norm": 0.21093705296516418,
      "learning_rate": 7.6010744253833e-07,
      "loss": 0.0014,
      "step": 58990
    },
    {
      "epoch": 10727.272727272728,
      "grad_norm": 0.2268083095550537,
      "learning_rate": 7.600080639646076e-07,
      "loss": 0.0009,
      "step": 59000
    },
    {
      "epoch": 10727.272727272728,
      "eval_loss": 5.02321720123291,
      "eval_runtime": 0.9513,
      "eval_samples_per_second": 10.512,
      "eval_steps_per_second": 5.256,
      "step": 59000
    },
    {
      "epoch": 10729.09090909091,
      "grad_norm": 0.19334758818149567,
      "learning_rate": 7.599086713103546e-07,
      "loss": 0.0013,
      "step": 59010
    },
    {
      "epoch": 10730.90909090909,
      "grad_norm": 0.222902312874794,
      "learning_rate": 7.598092645809535e-07,
      "loss": 0.0011,
      "step": 59020
    },
    {
      "epoch": 10732.727272727272,
      "grad_norm": 0.001072548795491457,
      "learning_rate": 7.597098437817875e-07,
      "loss": 0.0012,
      "step": 59030
    },
    {
      "epoch": 10734.545454545454,
      "grad_norm": 0.3028549551963806,
      "learning_rate": 7.596104089182407e-07,
      "loss": 0.001,
      "step": 59040
    },
    {
      "epoch": 10736.363636363636,
      "grad_norm": 0.2980799973011017,
      "learning_rate": 7.595109599956976e-07,
      "loss": 0.0012,
      "step": 59050
    },
    {
      "epoch": 10738.181818181818,
      "grad_norm": 0.00247280765324831,
      "learning_rate": 7.594114970195445e-07,
      "loss": 0.0009,
      "step": 59060
    },
    {
      "epoch": 10740.0,
      "grad_norm": 0.0007395432912744582,
      "learning_rate": 7.59312019995167e-07,
      "loss": 0.0012,
      "step": 59070
    },
    {
      "epoch": 10741.818181818182,
      "grad_norm": 0.0019091739086434245,
      "learning_rate": 7.592125289279527e-07,
      "loss": 0.0012,
      "step": 59080
    },
    {
      "epoch": 10743.636363636364,
      "grad_norm": 0.19693219661712646,
      "learning_rate": 7.591130238232891e-07,
      "loss": 0.0009,
      "step": 59090
    },
    {
      "epoch": 10745.454545454546,
      "grad_norm": 0.1980162262916565,
      "learning_rate": 7.590135046865651e-07,
      "loss": 0.0012,
      "step": 59100
    },
    {
      "epoch": 10747.272727272728,
      "grad_norm": 0.16960586607456207,
      "learning_rate": 7.589139715231699e-07,
      "loss": 0.001,
      "step": 59110
    },
    {
      "epoch": 10749.09090909091,
      "grad_norm": 0.0027702676597982645,
      "learning_rate": 7.588144243384937e-07,
      "loss": 0.001,
      "step": 59120
    },
    {
      "epoch": 10750.90909090909,
      "grad_norm": 0.2449067085981369,
      "learning_rate": 7.587148631379274e-07,
      "loss": 0.0012,
      "step": 59130
    },
    {
      "epoch": 10752.727272727272,
      "grad_norm": 0.001215814845636487,
      "learning_rate": 7.586152879268628e-07,
      "loss": 0.001,
      "step": 59140
    },
    {
      "epoch": 10754.545454545454,
      "grad_norm": 0.3168782591819763,
      "learning_rate": 7.58515698710692e-07,
      "loss": 0.0014,
      "step": 59150
    },
    {
      "epoch": 10756.363636363636,
      "grad_norm": 0.0187615305185318,
      "learning_rate": 7.584160954948084e-07,
      "loss": 0.0012,
      "step": 59160
    },
    {
      "epoch": 10758.181818181818,
      "grad_norm": 0.19763220846652985,
      "learning_rate": 7.583164782846059e-07,
      "loss": 0.001,
      "step": 59170
    },
    {
      "epoch": 10760.0,
      "grad_norm": 0.28310373425483704,
      "learning_rate": 7.582168470854793e-07,
      "loss": 0.0011,
      "step": 59180
    },
    {
      "epoch": 10761.818181818182,
      "grad_norm": 0.0018060392467305064,
      "learning_rate": 7.581172019028236e-07,
      "loss": 0.0012,
      "step": 59190
    },
    {
      "epoch": 10763.636363636364,
      "grad_norm": 0.0016152228927239776,
      "learning_rate": 7.580175427420357e-07,
      "loss": 0.0009,
      "step": 59200
    },
    {
      "epoch": 10765.454545454546,
      "grad_norm": 0.20444312691688538,
      "learning_rate": 7.57917869608512e-07,
      "loss": 0.0014,
      "step": 59210
    },
    {
      "epoch": 10767.272727272728,
      "grad_norm": 0.20736780762672424,
      "learning_rate": 7.578181825076505e-07,
      "loss": 0.0012,
      "step": 59220
    },
    {
      "epoch": 10769.09090909091,
      "grad_norm": 0.0026192860677838326,
      "learning_rate": 7.577184814448495e-07,
      "loss": 0.0007,
      "step": 59230
    },
    {
      "epoch": 10770.90909090909,
      "grad_norm": 0.0012525852071121335,
      "learning_rate": 7.576187664255083e-07,
      "loss": 0.0012,
      "step": 59240
    },
    {
      "epoch": 10772.727272727272,
      "grad_norm": 0.21997614204883575,
      "learning_rate": 7.575190374550271e-07,
      "loss": 0.0011,
      "step": 59250
    },
    {
      "epoch": 10774.545454545454,
      "grad_norm": 0.18988171219825745,
      "learning_rate": 7.574192945388063e-07,
      "loss": 0.0011,
      "step": 59260
    },
    {
      "epoch": 10776.363636363636,
      "grad_norm": 0.0013563325628638268,
      "learning_rate": 7.573195376822476e-07,
      "loss": 0.0009,
      "step": 59270
    },
    {
      "epoch": 10778.181818181818,
      "grad_norm": 0.002084305975586176,
      "learning_rate": 7.572197668907531e-07,
      "loss": 0.0012,
      "step": 59280
    },
    {
      "epoch": 10780.0,
      "grad_norm": 0.0019531119614839554,
      "learning_rate": 7.571199821697262e-07,
      "loss": 0.0012,
      "step": 59290
    },
    {
      "epoch": 10781.818181818182,
      "grad_norm": 0.0013290069764479995,
      "learning_rate": 7.570201835245702e-07,
      "loss": 0.0011,
      "step": 59300
    },
    {
      "epoch": 10783.636363636364,
      "grad_norm": 0.002624157117679715,
      "learning_rate": 7.569203709606897e-07,
      "loss": 0.0012,
      "step": 59310
    },
    {
      "epoch": 10785.454545454546,
      "grad_norm": 0.19561946392059326,
      "learning_rate": 7.568205444834901e-07,
      "loss": 0.0009,
      "step": 59320
    },
    {
      "epoch": 10787.272727272728,
      "grad_norm": 0.22796005010604858,
      "learning_rate": 7.567207040983775e-07,
      "loss": 0.0012,
      "step": 59330
    },
    {
      "epoch": 10789.09090909091,
      "grad_norm": 0.0014010191662237048,
      "learning_rate": 7.566208498107585e-07,
      "loss": 0.001,
      "step": 59340
    },
    {
      "epoch": 10790.90909090909,
      "grad_norm": 0.0017762590432539582,
      "learning_rate": 7.565209816260406e-07,
      "loss": 0.001,
      "step": 59350
    },
    {
      "epoch": 10792.727272727272,
      "grad_norm": 0.29362183809280396,
      "learning_rate": 7.564210995496323e-07,
      "loss": 0.0012,
      "step": 59360
    },
    {
      "epoch": 10794.545454545454,
      "grad_norm": 0.000876377453096211,
      "learning_rate": 7.563212035869425e-07,
      "loss": 0.0009,
      "step": 59370
    },
    {
      "epoch": 10796.363636363636,
      "grad_norm": 0.0010992875322699547,
      "learning_rate": 7.562212937433809e-07,
      "loss": 0.0015,
      "step": 59380
    },
    {
      "epoch": 10798.181818181818,
      "grad_norm": 0.17920273542404175,
      "learning_rate": 7.561213700243583e-07,
      "loss": 0.0009,
      "step": 59390
    },
    {
      "epoch": 10800.0,
      "grad_norm": 0.001425181864760816,
      "learning_rate": 7.560214324352858e-07,
      "loss": 0.0011,
      "step": 59400
    },
    {
      "epoch": 10801.818181818182,
      "grad_norm": 0.20916306972503662,
      "learning_rate": 7.559214809815754e-07,
      "loss": 0.0012,
      "step": 59410
    },
    {
      "epoch": 10803.636363636364,
      "grad_norm": 0.0018446360481902957,
      "learning_rate": 7.558215156686399e-07,
      "loss": 0.0012,
      "step": 59420
    },
    {
      "epoch": 10805.454545454546,
      "grad_norm": 0.20467738807201385,
      "learning_rate": 7.55721536501893e-07,
      "loss": 0.0011,
      "step": 59430
    },
    {
      "epoch": 10807.272727272728,
      "grad_norm": 0.002253336366266012,
      "learning_rate": 7.556215434867489e-07,
      "loss": 0.0009,
      "step": 59440
    },
    {
      "epoch": 10809.09090909091,
      "grad_norm": 0.0014460516395047307,
      "learning_rate": 7.555215366286226e-07,
      "loss": 0.0011,
      "step": 59450
    },
    {
      "epoch": 10810.90909090909,
      "grad_norm": 0.0010972839081659913,
      "learning_rate": 7.554215159329299e-07,
      "loss": 0.0012,
      "step": 59460
    },
    {
      "epoch": 10812.727272727272,
      "grad_norm": 0.0019305169116705656,
      "learning_rate": 7.553214814050875e-07,
      "loss": 0.0012,
      "step": 59470
    },
    {
      "epoch": 10814.545454545454,
      "grad_norm": 0.0035517553333193064,
      "learning_rate": 7.552214330505124e-07,
      "loss": 0.0011,
      "step": 59480
    },
    {
      "epoch": 10816.363636363636,
      "grad_norm": 0.0020773715805262327,
      "learning_rate": 7.551213708746229e-07,
      "loss": 0.0012,
      "step": 59490
    },
    {
      "epoch": 10818.181818181818,
      "grad_norm": 0.21402746438980103,
      "learning_rate": 7.550212948828376e-07,
      "loss": 0.0011,
      "step": 59500
    },
    {
      "epoch": 10818.181818181818,
      "eval_loss": 4.902442932128906,
      "eval_runtime": 0.9503,
      "eval_samples_per_second": 10.522,
      "eval_steps_per_second": 5.261,
      "step": 59500
    },
    {
      "epoch": 10820.0,
      "grad_norm": 0.0021017855033278465,
      "learning_rate": 7.549212050805761e-07,
      "loss": 0.0011,
      "step": 59510
    },
    {
      "epoch": 10821.818181818182,
      "grad_norm": 0.18553678691387177,
      "learning_rate": 7.548211014732589e-07,
      "loss": 0.0012,
      "step": 59520
    },
    {
      "epoch": 10823.636363636364,
      "grad_norm": 0.22413557767868042,
      "learning_rate": 7.547209840663067e-07,
      "loss": 0.0011,
      "step": 59530
    },
    {
      "epoch": 10825.454545454546,
      "grad_norm": 0.27425968647003174,
      "learning_rate": 7.546208528651414e-07,
      "loss": 0.0012,
      "step": 59540
    },
    {
      "epoch": 10827.272727272728,
      "grad_norm": 0.045871611684560776,
      "learning_rate": 7.545207078751857e-07,
      "loss": 0.0014,
      "step": 59550
    },
    {
      "epoch": 10829.09090909091,
      "grad_norm": 0.39022329449653625,
      "learning_rate": 7.544205491018625e-07,
      "loss": 0.0009,
      "step": 59560
    },
    {
      "epoch": 10830.90909090909,
      "grad_norm": 0.20219750702381134,
      "learning_rate": 7.543203765505961e-07,
      "loss": 0.0011,
      "step": 59570
    },
    {
      "epoch": 10832.727272727272,
      "grad_norm": 0.24886876344680786,
      "learning_rate": 7.542201902268114e-07,
      "loss": 0.0011,
      "step": 59580
    },
    {
      "epoch": 10834.545454545454,
      "grad_norm": 0.2629632353782654,
      "learning_rate": 7.541199901359334e-07,
      "loss": 0.0011,
      "step": 59590
    },
    {
      "epoch": 10836.363636363636,
      "grad_norm": 0.0011940361000597477,
      "learning_rate": 7.540197762833889e-07,
      "loss": 0.0011,
      "step": 59600
    },
    {
      "epoch": 10838.181818181818,
      "grad_norm": 0.0036401899997144938,
      "learning_rate": 7.539195486746046e-07,
      "loss": 0.0009,
      "step": 59610
    },
    {
      "epoch": 10840.0,
      "grad_norm": 0.005090024787932634,
      "learning_rate": 7.538193073150083e-07,
      "loss": 0.0012,
      "step": 59620
    },
    {
      "epoch": 10841.818181818182,
      "grad_norm": 0.0020705298520624638,
      "learning_rate": 7.537190522100285e-07,
      "loss": 0.0012,
      "step": 59630
    },
    {
      "epoch": 10843.636363636364,
      "grad_norm": 0.23092608153820038,
      "learning_rate": 7.536187833650945e-07,
      "loss": 0.0009,
      "step": 59640
    },
    {
      "epoch": 10845.454545454546,
      "grad_norm": 0.21711772680282593,
      "learning_rate": 7.535185007856362e-07,
      "loss": 0.0012,
      "step": 59650
    },
    {
      "epoch": 10847.272727272728,
      "grad_norm": 0.008710986003279686,
      "learning_rate": 7.534182044770842e-07,
      "loss": 0.0011,
      "step": 59660
    },
    {
      "epoch": 10849.09090909091,
      "grad_norm": 0.00130969169549644,
      "learning_rate": 7.533178944448703e-07,
      "loss": 0.0009,
      "step": 59670
    },
    {
      "epoch": 10850.90909090909,
      "grad_norm": 0.22669717669487,
      "learning_rate": 7.532175706944266e-07,
      "loss": 0.001,
      "step": 59680
    },
    {
      "epoch": 10852.727272727272,
      "grad_norm": 0.21111968159675598,
      "learning_rate": 7.53117233231186e-07,
      "loss": 0.001,
      "step": 59690
    },
    {
      "epoch": 10854.545454545454,
      "grad_norm": 0.000789332902058959,
      "learning_rate": 7.530168820605818e-07,
      "loss": 0.0009,
      "step": 59700
    },
    {
      "epoch": 10856.363636363636,
      "grad_norm": 0.0012245968682691455,
      "learning_rate": 7.529165171880492e-07,
      "loss": 0.0012,
      "step": 59710
    },
    {
      "epoch": 10858.181818181818,
      "grad_norm": 0.0037580025382339954,
      "learning_rate": 7.528161386190228e-07,
      "loss": 0.0012,
      "step": 59720
    },
    {
      "epoch": 10860.0,
      "grad_norm": 0.0011802174849435687,
      "learning_rate": 7.527157463589388e-07,
      "loss": 0.0012,
      "step": 59730
    },
    {
      "epoch": 10861.818181818182,
      "grad_norm": 0.0008420803351327777,
      "learning_rate": 7.526153404132336e-07,
      "loss": 0.0011,
      "step": 59740
    },
    {
      "epoch": 10863.636363636364,
      "grad_norm": 0.18185709416866302,
      "learning_rate": 7.52514920787345e-07,
      "loss": 0.0012,
      "step": 59750
    },
    {
      "epoch": 10865.454545454546,
      "grad_norm": 0.00137651851400733,
      "learning_rate": 7.524144874867109e-07,
      "loss": 0.0012,
      "step": 59760
    },
    {
      "epoch": 10867.272727272728,
      "grad_norm": 0.21303483843803406,
      "learning_rate": 7.523140405167701e-07,
      "loss": 0.0009,
      "step": 59770
    },
    {
      "epoch": 10869.09090909091,
      "grad_norm": 0.2615026533603668,
      "learning_rate": 7.522135798829623e-07,
      "loss": 0.0013,
      "step": 59780
    },
    {
      "epoch": 10870.90909090909,
      "grad_norm": 0.2968452274799347,
      "learning_rate": 7.521131055907282e-07,
      "loss": 0.0011,
      "step": 59790
    },
    {
      "epoch": 10872.727272727272,
      "grad_norm": 0.0009622607030905783,
      "learning_rate": 7.520126176455082e-07,
      "loss": 0.0009,
      "step": 59800
    },
    {
      "epoch": 10874.545454545454,
      "grad_norm": 0.22768856585025787,
      "learning_rate": 7.519121160527448e-07,
      "loss": 0.0011,
      "step": 59810
    },
    {
      "epoch": 10876.363636363636,
      "grad_norm": 0.011048738844692707,
      "learning_rate": 7.518116008178804e-07,
      "loss": 0.0012,
      "step": 59820
    },
    {
      "epoch": 10878.181818181818,
      "grad_norm": 0.1665433794260025,
      "learning_rate": 7.517110719463582e-07,
      "loss": 0.0011,
      "step": 59830
    },
    {
      "epoch": 10880.0,
      "grad_norm": 0.0015501908492296934,
      "learning_rate": 7.516105294436224e-07,
      "loss": 0.0011,
      "step": 59840
    },
    {
      "epoch": 10881.818181818182,
      "grad_norm": 0.19129562377929688,
      "learning_rate": 7.515099733151175e-07,
      "loss": 0.0011,
      "step": 59850
    },
    {
      "epoch": 10883.636363636364,
      "grad_norm": 0.0011599824065342546,
      "learning_rate": 7.514094035662894e-07,
      "loss": 0.001,
      "step": 59860
    },
    {
      "epoch": 10885.454545454546,
      "grad_norm": 0.2161499410867691,
      "learning_rate": 7.513088202025842e-07,
      "loss": 0.0013,
      "step": 59870
    },
    {
      "epoch": 10887.272727272728,
      "grad_norm": 0.3361503481864929,
      "learning_rate": 7.51208223229449e-07,
      "loss": 0.0014,
      "step": 59880
    },
    {
      "epoch": 10889.09090909091,
      "grad_norm": 0.24171458184719086,
      "learning_rate": 7.511076126523315e-07,
      "loss": 0.0008,
      "step": 59890
    },
    {
      "epoch": 10890.90909090909,
      "grad_norm": 0.20248103141784668,
      "learning_rate": 7.510069884766801e-07,
      "loss": 0.0012,
      "step": 59900
    },
    {
      "epoch": 10892.727272727272,
      "grad_norm": 0.18659625947475433,
      "learning_rate": 7.509063507079441e-07,
      "loss": 0.0009,
      "step": 59910
    },
    {
      "epoch": 10894.545454545454,
      "grad_norm": 0.001698778592981398,
      "learning_rate": 7.508056993515736e-07,
      "loss": 0.0012,
      "step": 59920
    },
    {
      "epoch": 10896.363636363636,
      "grad_norm": 0.0018281511729583144,
      "learning_rate": 7.50705034413019e-07,
      "loss": 0.0009,
      "step": 59930
    },
    {
      "epoch": 10898.181818181818,
      "grad_norm": 0.2294456958770752,
      "learning_rate": 7.506043558977321e-07,
      "loss": 0.0014,
      "step": 59940
    },
    {
      "epoch": 10900.0,
      "grad_norm": 0.298829048871994,
      "learning_rate": 7.505036638111647e-07,
      "loss": 0.001,
      "step": 59950
    },
    {
      "epoch": 10901.818181818182,
      "grad_norm": 0.22911770641803741,
      "learning_rate": 7.5040295815877e-07,
      "loss": 0.0011,
      "step": 59960
    },
    {
      "epoch": 10903.636363636364,
      "grad_norm": 0.0026064706034958363,
      "learning_rate": 7.503022389460013e-07,
      "loss": 0.0012,
      "step": 59970
    },
    {
      "epoch": 10905.454545454546,
      "grad_norm": 0.3206784725189209,
      "learning_rate": 7.502015061783132e-07,
      "loss": 0.0013,
      "step": 59980
    },
    {
      "epoch": 10907.272727272728,
      "grad_norm": 0.1842680126428604,
      "learning_rate": 7.501007598611608e-07,
      "loss": 0.0008,
      "step": 59990
    },
    {
      "epoch": 10909.09090909091,
      "grad_norm": 0.22702129185199738,
      "learning_rate": 7.5e-07,
      "loss": 0.0011,
      "step": 60000
    },
    {
      "epoch": 10909.09090909091,
      "eval_loss": 4.937065124511719,
      "eval_runtime": 0.9526,
      "eval_samples_per_second": 10.498,
      "eval_steps_per_second": 5.249,
      "step": 60000
    },
    {
      "epoch": 10910.90909090909,
      "grad_norm": 2.1942543983459473,
      "learning_rate": 7.498992266002871e-07,
      "loss": 0.0012,
      "step": 60010
    },
    {
      "epoch": 10912.727272727272,
      "grad_norm": 0.21191143989562988,
      "learning_rate": 7.497984396674797e-07,
      "loss": 0.0009,
      "step": 60020
    },
    {
      "epoch": 10914.545454545454,
      "grad_norm": 0.2546330392360687,
      "learning_rate": 7.496976392070357e-07,
      "loss": 0.0046,
      "step": 60030
    },
    {
      "epoch": 10916.363636363636,
      "grad_norm": 0.25007936358451843,
      "learning_rate": 7.495968252244138e-07,
      "loss": 0.0012,
      "step": 60040
    },
    {
      "epoch": 10918.181818181818,
      "grad_norm": 0.5552321076393127,
      "learning_rate": 7.494959977250736e-07,
      "loss": 0.0037,
      "step": 60050
    },
    {
      "epoch": 10920.0,
      "grad_norm": 0.46393442153930664,
      "learning_rate": 7.493951567144754e-07,
      "loss": 0.0011,
      "step": 60060
    },
    {
      "epoch": 10921.818181818182,
      "grad_norm": 0.013773847371339798,
      "learning_rate": 7.4929430219808e-07,
      "loss": 0.001,
      "step": 60070
    },
    {
      "epoch": 10923.636363636364,
      "grad_norm": 0.00601285370066762,
      "learning_rate": 7.491934341813493e-07,
      "loss": 0.0012,
      "step": 60080
    },
    {
      "epoch": 10925.454545454546,
      "grad_norm": 0.21311302483081818,
      "learning_rate": 7.490925526697454e-07,
      "loss": 0.0015,
      "step": 60090
    },
    {
      "epoch": 10927.272727272728,
      "grad_norm": 0.22585639357566833,
      "learning_rate": 7.489916576687317e-07,
      "loss": 0.0012,
      "step": 60100
    },
    {
      "epoch": 10929.09090909091,
      "grad_norm": 0.02772260643541813,
      "learning_rate": 7.488907491837721e-07,
      "loss": 0.0009,
      "step": 60110
    },
    {
      "epoch": 10930.90909090909,
      "grad_norm": 0.3141607940196991,
      "learning_rate": 7.487898272203312e-07,
      "loss": 0.0012,
      "step": 60120
    },
    {
      "epoch": 10932.727272727272,
      "grad_norm": 0.004444626159965992,
      "learning_rate": 7.486888917838743e-07,
      "loss": 0.0011,
      "step": 60130
    },
    {
      "epoch": 10934.545454545454,
      "grad_norm": 0.2281656712293625,
      "learning_rate": 7.485879428798674e-07,
      "loss": 0.0014,
      "step": 60140
    },
    {
      "epoch": 10936.363636363636,
      "grad_norm": 0.004246044903993607,
      "learning_rate": 7.484869805137777e-07,
      "loss": 0.0006,
      "step": 60150
    },
    {
      "epoch": 10938.181818181818,
      "grad_norm": 0.21074403822422028,
      "learning_rate": 7.483860046910721e-07,
      "loss": 0.0013,
      "step": 60160
    },
    {
      "epoch": 10940.0,
      "grad_norm": 0.018216721713542938,
      "learning_rate": 7.482850154172195e-07,
      "loss": 0.001,
      "step": 60170
    },
    {
      "epoch": 10941.818181818182,
      "grad_norm": 0.17912523448467255,
      "learning_rate": 7.481840126976883e-07,
      "loss": 0.0012,
      "step": 60180
    },
    {
      "epoch": 10943.636363636364,
      "grad_norm": 0.20926311612129211,
      "learning_rate": 7.480829965379489e-07,
      "loss": 0.0011,
      "step": 60190
    },
    {
      "epoch": 10945.454545454546,
      "grad_norm": 0.003891630796715617,
      "learning_rate": 7.47981966943471e-07,
      "loss": 0.001,
      "step": 60200
    },
    {
      "epoch": 10947.272727272728,
      "grad_norm": 0.2203717976808548,
      "learning_rate": 7.478809239197264e-07,
      "loss": 0.0012,
      "step": 60210
    },
    {
      "epoch": 10949.09090909091,
      "grad_norm": 0.2278166264295578,
      "learning_rate": 7.477798674721867e-07,
      "loss": 0.0011,
      "step": 60220
    },
    {
      "epoch": 10950.90909090909,
      "grad_norm": 0.0035997533705085516,
      "learning_rate": 7.476787976063245e-07,
      "loss": 0.0011,
      "step": 60230
    },
    {
      "epoch": 10952.727272727272,
      "grad_norm": 0.00393755454570055,
      "learning_rate": 7.475777143276132e-07,
      "loss": 0.0012,
      "step": 60240
    },
    {
      "epoch": 10954.545454545454,
      "grad_norm": 0.27905091643333435,
      "learning_rate": 7.47476617641527e-07,
      "loss": 0.001,
      "step": 60250
    },
    {
      "epoch": 10956.363636363636,
      "grad_norm": 0.18774306774139404,
      "learning_rate": 7.473755075535408e-07,
      "loss": 0.0013,
      "step": 60260
    },
    {
      "epoch": 10958.181818181818,
      "grad_norm": 0.0026459398213773966,
      "learning_rate": 7.472743840691298e-07,
      "loss": 0.0009,
      "step": 60270
    },
    {
      "epoch": 10960.0,
      "grad_norm": 0.0020798298064619303,
      "learning_rate": 7.471732471937705e-07,
      "loss": 0.0012,
      "step": 60280
    },
    {
      "epoch": 10961.818181818182,
      "grad_norm": 0.29987868666648865,
      "learning_rate": 7.470720969329398e-07,
      "loss": 0.0012,
      "step": 60290
    },
    {
      "epoch": 10963.636363636364,
      "grad_norm": 0.0020718141458928585,
      "learning_rate": 7.469709332921154e-07,
      "loss": 0.0011,
      "step": 60300
    },
    {
      "epoch": 10965.454545454546,
      "grad_norm": 0.23705025017261505,
      "learning_rate": 7.468697562767759e-07,
      "loss": 0.0012,
      "step": 60310
    },
    {
      "epoch": 10967.272727272728,
      "grad_norm": 0.2369886338710785,
      "learning_rate": 7.467685658924003e-07,
      "loss": 0.0012,
      "step": 60320
    },
    {
      "epoch": 10969.09090909091,
      "grad_norm": 0.0010638715466484427,
      "learning_rate": 7.466673621444684e-07,
      "loss": 0.0009,
      "step": 60330
    },
    {
      "epoch": 10970.90909090909,
      "grad_norm": 0.19202779233455658,
      "learning_rate": 7.465661450384612e-07,
      "loss": 0.0011,
      "step": 60340
    },
    {
      "epoch": 10972.727272727272,
      "grad_norm": 0.238285630941391,
      "learning_rate": 7.464649145798597e-07,
      "loss": 0.0013,
      "step": 60350
    },
    {
      "epoch": 10974.545454545454,
      "grad_norm": 0.17586749792099,
      "learning_rate": 7.463636707741458e-07,
      "loss": 0.0009,
      "step": 60360
    },
    {
      "epoch": 10976.363636363636,
      "grad_norm": 0.0013686508173123002,
      "learning_rate": 7.462624136268026e-07,
      "loss": 0.0011,
      "step": 60370
    },
    {
      "epoch": 10978.181818181818,
      "grad_norm": 0.002430211752653122,
      "learning_rate": 7.461611431433134e-07,
      "loss": 0.001,
      "step": 60380
    },
    {
      "epoch": 10980.0,
      "grad_norm": 0.19246011972427368,
      "learning_rate": 7.460598593291627e-07,
      "loss": 0.0012,
      "step": 60390
    },
    {
      "epoch": 10981.818181818182,
      "grad_norm": 0.2886655628681183,
      "learning_rate": 7.459585621898352e-07,
      "loss": 0.0012,
      "step": 60400
    },
    {
      "epoch": 10983.636363636364,
      "grad_norm": 0.2962396442890167,
      "learning_rate": 7.458572517308168e-07,
      "loss": 0.0011,
      "step": 60410
    },
    {
      "epoch": 10985.454545454546,
      "grad_norm": 0.2769404351711273,
      "learning_rate": 7.457559279575934e-07,
      "loss": 0.001,
      "step": 60420
    },
    {
      "epoch": 10987.272727272728,
      "grad_norm": 0.00447418075054884,
      "learning_rate": 7.456545908756527e-07,
      "loss": 0.0009,
      "step": 60430
    },
    {
      "epoch": 10989.09090909091,
      "grad_norm": 0.18379589915275574,
      "learning_rate": 7.455532404904821e-07,
      "loss": 0.0014,
      "step": 60440
    },
    {
      "epoch": 10990.90909090909,
      "grad_norm": 0.0016135652549564838,
      "learning_rate": 7.454518768075703e-07,
      "loss": 0.001,
      "step": 60450
    },
    {
      "epoch": 10992.727272727272,
      "grad_norm": 0.21480347216129303,
      "learning_rate": 7.453504998324067e-07,
      "loss": 0.0009,
      "step": 60460
    },
    {
      "epoch": 10994.545454545454,
      "grad_norm": 0.21351514756679535,
      "learning_rate": 7.452491095704813e-07,
      "loss": 0.0012,
      "step": 60470
    },
    {
      "epoch": 10996.363636363636,
      "grad_norm": 0.18272319436073303,
      "learning_rate": 7.451477060272843e-07,
      "loss": 0.0012,
      "step": 60480
    },
    {
      "epoch": 10998.181818181818,
      "grad_norm": 0.2232368141412735,
      "learning_rate": 7.450462892083078e-07,
      "loss": 0.0011,
      "step": 60490
    },
    {
      "epoch": 11000.0,
      "grad_norm": 0.0010090736905112863,
      "learning_rate": 7.449448591190434e-07,
      "loss": 0.0011,
      "step": 60500
    },
    {
      "epoch": 11000.0,
      "eval_loss": 4.903170585632324,
      "eval_runtime": 0.9516,
      "eval_samples_per_second": 10.509,
      "eval_steps_per_second": 5.254,
      "step": 60500
    },
    {
      "epoch": 11001.818181818182,
      "grad_norm": 0.21382036805152893,
      "learning_rate": 7.448434157649846e-07,
      "loss": 0.0012,
      "step": 60510
    },
    {
      "epoch": 11003.636363636364,
      "grad_norm": 0.22109344601631165,
      "learning_rate": 7.447419591516242e-07,
      "loss": 0.0009,
      "step": 60520
    },
    {
      "epoch": 11005.454545454546,
      "grad_norm": 0.22407935559749603,
      "learning_rate": 7.446404892844571e-07,
      "loss": 0.0012,
      "step": 60530
    },
    {
      "epoch": 11007.272727272728,
      "grad_norm": 0.000988222542218864,
      "learning_rate": 7.445390061689782e-07,
      "loss": 0.0009,
      "step": 60540
    },
    {
      "epoch": 11009.09090909091,
      "grad_norm": 0.17106230556964874,
      "learning_rate": 7.44437509810683e-07,
      "loss": 0.0013,
      "step": 60550
    },
    {
      "epoch": 11010.90909090909,
      "grad_norm": 0.00208627013489604,
      "learning_rate": 7.443360002150683e-07,
      "loss": 0.0011,
      "step": 60560
    },
    {
      "epoch": 11012.727272727272,
      "grad_norm": 0.2196747064590454,
      "learning_rate": 7.442344773876309e-07,
      "loss": 0.0011,
      "step": 60570
    },
    {
      "epoch": 11014.545454545454,
      "grad_norm": 0.1747010201215744,
      "learning_rate": 7.441329413338688e-07,
      "loss": 0.0012,
      "step": 60580
    },
    {
      "epoch": 11016.363636363636,
      "grad_norm": 0.18717524409294128,
      "learning_rate": 7.440313920592809e-07,
      "loss": 0.0012,
      "step": 60590
    },
    {
      "epoch": 11018.181818181818,
      "grad_norm": 0.19566567242145538,
      "learning_rate": 7.439298295693663e-07,
      "loss": 0.0014,
      "step": 60600
    },
    {
      "epoch": 11020.0,
      "grad_norm": 0.22374227643013,
      "learning_rate": 7.43828253869625e-07,
      "loss": 0.0007,
      "step": 60610
    },
    {
      "epoch": 11021.818181818182,
      "grad_norm": 0.0013585624983534217,
      "learning_rate": 7.437266649655578e-07,
      "loss": 0.0011,
      "step": 60620
    },
    {
      "epoch": 11023.636363636364,
      "grad_norm": 0.17131417989730835,
      "learning_rate": 7.436250628626661e-07,
      "loss": 0.0012,
      "step": 60630
    },
    {
      "epoch": 11025.454545454546,
      "grad_norm": 0.2360200583934784,
      "learning_rate": 7.435234475664522e-07,
      "loss": 0.0009,
      "step": 60640
    },
    {
      "epoch": 11027.272727272728,
      "grad_norm": 0.0018371311016380787,
      "learning_rate": 7.43421819082419e-07,
      "loss": 0.0012,
      "step": 60650
    },
    {
      "epoch": 11029.09090909091,
      "grad_norm": 0.2124285250902176,
      "learning_rate": 7.4332017741607e-07,
      "loss": 0.0012,
      "step": 60660
    },
    {
      "epoch": 11030.90909090909,
      "grad_norm": 0.0017386149847880006,
      "learning_rate": 7.432185225729094e-07,
      "loss": 0.0012,
      "step": 60670
    },
    {
      "epoch": 11032.727272727272,
      "grad_norm": 0.0014483564300462604,
      "learning_rate": 7.431168545584427e-07,
      "loss": 0.0008,
      "step": 60680
    },
    {
      "epoch": 11034.545454545454,
      "grad_norm": 0.23688824474811554,
      "learning_rate": 7.430151733781751e-07,
      "loss": 0.0012,
      "step": 60690
    },
    {
      "epoch": 11036.363636363636,
      "grad_norm": 0.18949465453624725,
      "learning_rate": 7.429134790376134e-07,
      "loss": 0.0014,
      "step": 60700
    },
    {
      "epoch": 11038.181818181818,
      "grad_norm": 0.00248142471536994,
      "learning_rate": 7.428117715422647e-07,
      "loss": 0.0009,
      "step": 60710
    },
    {
      "epoch": 11040.0,
      "grad_norm": 0.22034093737602234,
      "learning_rate": 7.42710050897637e-07,
      "loss": 0.0013,
      "step": 60720
    },
    {
      "epoch": 11041.818181818182,
      "grad_norm": 0.2715117633342743,
      "learning_rate": 7.426083171092386e-07,
      "loss": 0.0012,
      "step": 60730
    },
    {
      "epoch": 11043.636363636364,
      "grad_norm": 0.2040846198797226,
      "learning_rate": 7.425065701825792e-07,
      "loss": 0.001,
      "step": 60740
    },
    {
      "epoch": 11045.454545454546,
      "grad_norm": 0.19403204321861267,
      "learning_rate": 7.424048101231686e-07,
      "loss": 0.001,
      "step": 60750
    },
    {
      "epoch": 11047.272727272728,
      "grad_norm": 0.011641101911664009,
      "learning_rate": 7.423030369365174e-07,
      "loss": 0.0014,
      "step": 60760
    },
    {
      "epoch": 11049.09090909091,
      "grad_norm": 0.1911832094192505,
      "learning_rate": 7.422012506281372e-07,
      "loss": 0.001,
      "step": 60770
    },
    {
      "epoch": 11050.90909090909,
      "grad_norm": 0.21623580157756805,
      "learning_rate": 7.420994512035403e-07,
      "loss": 0.0011,
      "step": 60780
    },
    {
      "epoch": 11052.727272727272,
      "grad_norm": 0.271519273519516,
      "learning_rate": 7.419976386682394e-07,
      "loss": 0.0012,
      "step": 60790
    },
    {
      "epoch": 11054.545454545454,
      "grad_norm": 0.0015832835342735052,
      "learning_rate": 7.418958130277482e-07,
      "loss": 0.001,
      "step": 60800
    },
    {
      "epoch": 11056.363636363636,
      "grad_norm": 0.0013239765539765358,
      "learning_rate": 7.417939742875807e-07,
      "loss": 0.001,
      "step": 60810
    },
    {
      "epoch": 11058.181818181818,
      "grad_norm": 0.28097766637802124,
      "learning_rate": 7.416921224532522e-07,
      "loss": 0.0013,
      "step": 60820
    },
    {
      "epoch": 11060.0,
      "grad_norm": 0.0015373140340670943,
      "learning_rate": 7.415902575302784e-07,
      "loss": 0.0009,
      "step": 60830
    },
    {
      "epoch": 11061.818181818182,
      "grad_norm": 0.28937581181526184,
      "learning_rate": 7.414883795241753e-07,
      "loss": 0.0012,
      "step": 60840
    },
    {
      "epoch": 11063.636363636364,
      "grad_norm": 0.17257918417453766,
      "learning_rate": 7.413864884404607e-07,
      "loss": 0.0007,
      "step": 60850
    },
    {
      "epoch": 11065.454545454546,
      "grad_norm": 0.22106483578681946,
      "learning_rate": 7.412845842846518e-07,
      "loss": 0.0016,
      "step": 60860
    },
    {
      "epoch": 11067.272727272728,
      "grad_norm": 0.2100481390953064,
      "learning_rate": 7.411826670622675e-07,
      "loss": 0.001,
      "step": 60870
    },
    {
      "epoch": 11069.09090909091,
      "grad_norm": 0.001863354817032814,
      "learning_rate": 7.41080736778827e-07,
      "loss": 0.0009,
      "step": 60880
    },
    {
      "epoch": 11070.90909090909,
      "grad_norm": 0.1769275963306427,
      "learning_rate": 7.409787934398502e-07,
      "loss": 0.001,
      "step": 60890
    },
    {
      "epoch": 11072.727272727272,
      "grad_norm": 0.0021159141324460506,
      "learning_rate": 7.408768370508576e-07,
      "loss": 0.0012,
      "step": 60900
    },
    {
      "epoch": 11074.545454545454,
      "grad_norm": 0.22733324766159058,
      "learning_rate": 7.407748676173708e-07,
      "loss": 0.0009,
      "step": 60910
    },
    {
      "epoch": 11076.363636363636,
      "grad_norm": 0.18107233941555023,
      "learning_rate": 7.406728851449118e-07,
      "loss": 0.0012,
      "step": 60920
    },
    {
      "epoch": 11078.181818181818,
      "grad_norm": 0.17701971530914307,
      "learning_rate": 7.405708896390036e-07,
      "loss": 0.0012,
      "step": 60930
    },
    {
      "epoch": 11080.0,
      "grad_norm": 0.002857581013813615,
      "learning_rate": 7.404688811051692e-07,
      "loss": 0.0011,
      "step": 60940
    },
    {
      "epoch": 11081.818181818182,
      "grad_norm": 0.0012087944196537137,
      "learning_rate": 7.403668595489331e-07,
      "loss": 0.0012,
      "step": 60950
    },
    {
      "epoch": 11083.636363636364,
      "grad_norm": 0.012225283309817314,
      "learning_rate": 7.402648249758203e-07,
      "loss": 0.001,
      "step": 60960
    },
    {
      "epoch": 11085.454545454546,
      "grad_norm": 0.006142991129308939,
      "learning_rate": 7.401627773913563e-07,
      "loss": 0.0012,
      "step": 60970
    },
    {
      "epoch": 11087.272727272728,
      "grad_norm": 0.0009873573435470462,
      "learning_rate": 7.400607168010672e-07,
      "loss": 0.0009,
      "step": 60980
    },
    {
      "epoch": 11089.09090909091,
      "grad_norm": 0.001207653316669166,
      "learning_rate": 7.399586432104803e-07,
      "loss": 0.0011,
      "step": 60990
    },
    {
      "epoch": 11090.90909090909,
      "grad_norm": 0.23682555556297302,
      "learning_rate": 7.398565566251232e-07,
      "loss": 0.0012,
      "step": 61000
    },
    {
      "epoch": 11090.90909090909,
      "eval_loss": 4.850096225738525,
      "eval_runtime": 0.952,
      "eval_samples_per_second": 10.504,
      "eval_steps_per_second": 5.252,
      "step": 61000
    },
    {
      "epoch": 11092.727272727272,
      "grad_norm": 0.18659046292304993,
      "learning_rate": 7.397544570505242e-07,
      "loss": 0.0011,
      "step": 61010
    },
    {
      "epoch": 11094.545454545454,
      "grad_norm": 0.22041259706020355,
      "learning_rate": 7.396523444922124e-07,
      "loss": 0.0008,
      "step": 61020
    },
    {
      "epoch": 11096.363636363636,
      "grad_norm": 0.22236312925815582,
      "learning_rate": 7.39550218955718e-07,
      "loss": 0.0012,
      "step": 61030
    },
    {
      "epoch": 11098.181818181818,
      "grad_norm": 0.0012061243178322911,
      "learning_rate": 7.394480804465712e-07,
      "loss": 0.0011,
      "step": 61040
    },
    {
      "epoch": 11100.0,
      "grad_norm": 0.0018135163700208068,
      "learning_rate": 7.393459289703035e-07,
      "loss": 0.0012,
      "step": 61050
    },
    {
      "epoch": 11101.818181818182,
      "grad_norm": 0.001297003822401166,
      "learning_rate": 7.392437645324463e-07,
      "loss": 0.001,
      "step": 61060
    },
    {
      "epoch": 11103.636363636364,
      "grad_norm": 0.21997538208961487,
      "learning_rate": 7.391415871385328e-07,
      "loss": 0.0012,
      "step": 61070
    },
    {
      "epoch": 11105.454545454546,
      "grad_norm": 0.2215900421142578,
      "learning_rate": 7.390393967940962e-07,
      "loss": 0.001,
      "step": 61080
    },
    {
      "epoch": 11107.272727272728,
      "grad_norm": 0.20780177414417267,
      "learning_rate": 7.389371935046702e-07,
      "loss": 0.0012,
      "step": 61090
    },
    {
      "epoch": 11109.09090909091,
      "grad_norm": 0.20493917167186737,
      "learning_rate": 7.3883497727579e-07,
      "loss": 0.001,
      "step": 61100
    },
    {
      "epoch": 11110.90909090909,
      "grad_norm": 0.011227179318666458,
      "learning_rate": 7.387327481129906e-07,
      "loss": 0.001,
      "step": 61110
    },
    {
      "epoch": 11112.727272727272,
      "grad_norm": 0.0018459476996213198,
      "learning_rate": 7.386305060218086e-07,
      "loss": 0.0012,
      "step": 61120
    },
    {
      "epoch": 11114.545454545454,
      "grad_norm": 0.001991596072912216,
      "learning_rate": 7.385282510077805e-07,
      "loss": 0.0011,
      "step": 61130
    },
    {
      "epoch": 11116.363636363636,
      "grad_norm": 0.0021092791575938463,
      "learning_rate": 7.384259830764439e-07,
      "loss": 0.0011,
      "step": 61140
    },
    {
      "epoch": 11118.181818181818,
      "grad_norm": 0.2725487947463989,
      "learning_rate": 7.383237022333372e-07,
      "loss": 0.0014,
      "step": 61150
    },
    {
      "epoch": 11120.0,
      "grad_norm": 0.17038771510124207,
      "learning_rate": 7.382214084839993e-07,
      "loss": 0.0011,
      "step": 61160
    },
    {
      "epoch": 11121.818181818182,
      "grad_norm": 0.0011698725866153836,
      "learning_rate": 7.381191018339695e-07,
      "loss": 0.0012,
      "step": 61170
    },
    {
      "epoch": 11123.636363636364,
      "grad_norm": 0.17283745110034943,
      "learning_rate": 7.380167822887886e-07,
      "loss": 0.0009,
      "step": 61180
    },
    {
      "epoch": 11125.454545454546,
      "grad_norm": 0.2514442205429077,
      "learning_rate": 7.379144498539974e-07,
      "loss": 0.0012,
      "step": 61190
    },
    {
      "epoch": 11127.272727272728,
      "grad_norm": 0.003987219650298357,
      "learning_rate": 7.378121045351377e-07,
      "loss": 0.0011,
      "step": 61200
    },
    {
      "epoch": 11129.09090909091,
      "grad_norm": 0.24948783218860626,
      "learning_rate": 7.377097463377518e-07,
      "loss": 0.0012,
      "step": 61210
    },
    {
      "epoch": 11130.90909090909,
      "grad_norm": 0.001321162679232657,
      "learning_rate": 7.376073752673828e-07,
      "loss": 0.0013,
      "step": 61220
    },
    {
      "epoch": 11132.727272727272,
      "grad_norm": 0.001437123166397214,
      "learning_rate": 7.375049913295749e-07,
      "loss": 0.0012,
      "step": 61230
    },
    {
      "epoch": 11134.545454545454,
      "grad_norm": 0.17198596894741058,
      "learning_rate": 7.374025945298723e-07,
      "loss": 0.0011,
      "step": 61240
    },
    {
      "epoch": 11136.363636363636,
      "grad_norm": 0.24541616439819336,
      "learning_rate": 7.373001848738202e-07,
      "loss": 0.0011,
      "step": 61250
    },
    {
      "epoch": 11138.181818181818,
      "grad_norm": 0.14160461723804474,
      "learning_rate": 7.371977623669645e-07,
      "loss": 0.0013,
      "step": 61260
    },
    {
      "epoch": 11140.0,
      "grad_norm": 0.001123146153986454,
      "learning_rate": 7.370953270148522e-07,
      "loss": 0.0011,
      "step": 61270
    },
    {
      "epoch": 11141.818181818182,
      "grad_norm": 0.0019998836796730757,
      "learning_rate": 7.369928788230301e-07,
      "loss": 0.0009,
      "step": 61280
    },
    {
      "epoch": 11143.636363636364,
      "grad_norm": 0.002492278115823865,
      "learning_rate": 7.368904177970465e-07,
      "loss": 0.0012,
      "step": 61290
    },
    {
      "epoch": 11145.454545454546,
      "grad_norm": 0.18042676150798798,
      "learning_rate": 7.367879439424499e-07,
      "loss": 0.001,
      "step": 61300
    },
    {
      "epoch": 11147.272727272728,
      "grad_norm": 0.18496590852737427,
      "learning_rate": 7.366854572647901e-07,
      "loss": 0.0014,
      "step": 61310
    },
    {
      "epoch": 11149.09090909091,
      "grad_norm": 0.20787577331066132,
      "learning_rate": 7.365829577696165e-07,
      "loss": 0.0012,
      "step": 61320
    },
    {
      "epoch": 11150.90909090909,
      "grad_norm": 0.0032011240255087614,
      "learning_rate": 7.364804454624805e-07,
      "loss": 0.0007,
      "step": 61330
    },
    {
      "epoch": 11152.727272727272,
      "grad_norm": 0.22225090861320496,
      "learning_rate": 7.363779203489333e-07,
      "loss": 0.0012,
      "step": 61340
    },
    {
      "epoch": 11154.545454545454,
      "grad_norm": 0.0019448844250291586,
      "learning_rate": 7.362753824345271e-07,
      "loss": 0.0009,
      "step": 61350
    },
    {
      "epoch": 11156.363636363636,
      "grad_norm": 0.2122914344072342,
      "learning_rate": 7.361728317248146e-07,
      "loss": 0.0012,
      "step": 61360
    },
    {
      "epoch": 11158.181818181818,
      "grad_norm": 0.29107949137687683,
      "learning_rate": 7.360702682253496e-07,
      "loss": 0.0013,
      "step": 61370
    },
    {
      "epoch": 11160.0,
      "grad_norm": 0.0012887049233540893,
      "learning_rate": 7.359676919416864e-07,
      "loss": 0.0009,
      "step": 61380
    },
    {
      "epoch": 11161.818181818182,
      "grad_norm": 0.28913265466690063,
      "learning_rate": 7.358651028793796e-07,
      "loss": 0.0012,
      "step": 61390
    },
    {
      "epoch": 11163.636363636364,
      "grad_norm": 0.004501702263951302,
      "learning_rate": 7.357625010439852e-07,
      "loss": 0.0009,
      "step": 61400
    },
    {
      "epoch": 11165.454545454546,
      "grad_norm": 0.001207551802508533,
      "learning_rate": 7.356598864410591e-07,
      "loss": 0.001,
      "step": 61410
    },
    {
      "epoch": 11167.272727272728,
      "grad_norm": 0.0015017192345112562,
      "learning_rate": 7.355572590761588e-07,
      "loss": 0.0012,
      "step": 61420
    },
    {
      "epoch": 11169.09090909091,
      "grad_norm": 0.22320352494716644,
      "learning_rate": 7.354546189548417e-07,
      "loss": 0.0013,
      "step": 61430
    },
    {
      "epoch": 11170.90909090909,
      "grad_norm": 0.0014237905852496624,
      "learning_rate": 7.353519660826664e-07,
      "loss": 0.0011,
      "step": 61440
    },
    {
      "epoch": 11172.727272727272,
      "grad_norm": 0.17566095292568207,
      "learning_rate": 7.352493004651915e-07,
      "loss": 0.0009,
      "step": 61450
    },
    {
      "epoch": 11174.545454545454,
      "grad_norm": 0.01814512349665165,
      "learning_rate": 7.351466221079774e-07,
      "loss": 0.0014,
      "step": 61460
    },
    {
      "epoch": 11176.363636363636,
      "grad_norm": 0.001289181993342936,
      "learning_rate": 7.350439310165841e-07,
      "loss": 0.0008,
      "step": 61470
    },
    {
      "epoch": 11178.181818181818,
      "grad_norm": 0.0008349469862878323,
      "learning_rate": 7.34941227196573e-07,
      "loss": 0.0012,
      "step": 61480
    },
    {
      "epoch": 11180.0,
      "grad_norm": 0.002545703435316682,
      "learning_rate": 7.348385106535059e-07,
      "loss": 0.0012,
      "step": 61490
    },
    {
      "epoch": 11181.818181818182,
      "grad_norm": 0.22411450743675232,
      "learning_rate": 7.347357813929454e-07,
      "loss": 0.0012,
      "step": 61500
    },
    {
      "epoch": 11181.818181818182,
      "eval_loss": 4.9556355476379395,
      "eval_runtime": 0.9515,
      "eval_samples_per_second": 10.51,
      "eval_steps_per_second": 5.255,
      "step": 61500
    },
    {
      "epoch": 11183.636363636364,
      "grad_norm": 0.0008258632151409984,
      "learning_rate": 7.346330394204545e-07,
      "loss": 0.0009,
      "step": 61510
    },
    {
      "epoch": 11185.454545454546,
      "grad_norm": 0.18980517983436584,
      "learning_rate": 7.345302847415973e-07,
      "loss": 0.0014,
      "step": 61520
    },
    {
      "epoch": 11187.272727272728,
      "grad_norm": 0.0015065651386976242,
      "learning_rate": 7.344275173619384e-07,
      "loss": 0.0008,
      "step": 61530
    },
    {
      "epoch": 11189.09090909091,
      "grad_norm": 0.27507850527763367,
      "learning_rate": 7.343247372870429e-07,
      "loss": 0.0012,
      "step": 61540
    },
    {
      "epoch": 11190.90909090909,
      "grad_norm": 0.0012166791129857302,
      "learning_rate": 7.34221944522477e-07,
      "loss": 0.0013,
      "step": 61550
    },
    {
      "epoch": 11192.727272727272,
      "grad_norm": 0.17593549191951752,
      "learning_rate": 7.341191390738072e-07,
      "loss": 0.0012,
      "step": 61560
    },
    {
      "epoch": 11194.545454545454,
      "grad_norm": 0.0013621603138744831,
      "learning_rate": 7.34016320946601e-07,
      "loss": 0.001,
      "step": 61570
    },
    {
      "epoch": 11196.363636363636,
      "grad_norm": 0.2100168913602829,
      "learning_rate": 7.339134901464264e-07,
      "loss": 0.0011,
      "step": 61580
    },
    {
      "epoch": 11198.181818181818,
      "grad_norm": 0.0023021625820547342,
      "learning_rate": 7.33810646678852e-07,
      "loss": 0.0011,
      "step": 61590
    },
    {
      "epoch": 11200.0,
      "grad_norm": 0.0036296281032264233,
      "learning_rate": 7.337077905494471e-07,
      "loss": 0.0012,
      "step": 61600
    },
    {
      "epoch": 11201.818181818182,
      "grad_norm": 0.0013280141865834594,
      "learning_rate": 7.336049217637821e-07,
      "loss": 0.0011,
      "step": 61610
    },
    {
      "epoch": 11203.636363636364,
      "grad_norm": 0.19302749633789062,
      "learning_rate": 7.335020403274277e-07,
      "loss": 0.0012,
      "step": 61620
    },
    {
      "epoch": 11205.454545454546,
      "grad_norm": 0.001755662844516337,
      "learning_rate": 7.333991462459552e-07,
      "loss": 0.0008,
      "step": 61630
    },
    {
      "epoch": 11207.272727272728,
      "grad_norm": 0.0018477977719157934,
      "learning_rate": 7.332962395249368e-07,
      "loss": 0.0013,
      "step": 61640
    },
    {
      "epoch": 11209.09090909091,
      "grad_norm": 0.001125246868468821,
      "learning_rate": 7.331933201699457e-07,
      "loss": 0.0012,
      "step": 61650
    },
    {
      "epoch": 11210.90909090909,
      "grad_norm": 0.0015145448269322515,
      "learning_rate": 7.330903881865548e-07,
      "loss": 0.0011,
      "step": 61660
    },
    {
      "epoch": 11212.727272727272,
      "grad_norm": 0.0009718640358187258,
      "learning_rate": 7.329874435803387e-07,
      "loss": 0.0012,
      "step": 61670
    },
    {
      "epoch": 11214.545454545454,
      "grad_norm": 0.00258948327973485,
      "learning_rate": 7.328844863568721e-07,
      "loss": 0.001,
      "step": 61680
    },
    {
      "epoch": 11216.363636363636,
      "grad_norm": 0.21873639523983002,
      "learning_rate": 7.327815165217309e-07,
      "loss": 0.0011,
      "step": 61690
    },
    {
      "epoch": 11218.181818181818,
      "grad_norm": 0.0015638364711776376,
      "learning_rate": 7.326785340804907e-07,
      "loss": 0.0011,
      "step": 61700
    },
    {
      "epoch": 11220.0,
      "grad_norm": 0.2202807515859604,
      "learning_rate": 7.325755390387292e-07,
      "loss": 0.0012,
      "step": 61710
    },
    {
      "epoch": 11221.818181818182,
      "grad_norm": 0.20410265028476715,
      "learning_rate": 7.324725314020235e-07,
      "loss": 0.001,
      "step": 61720
    },
    {
      "epoch": 11223.636363636364,
      "grad_norm": 0.18973131477832794,
      "learning_rate": 7.32369511175952e-07,
      "loss": 0.0012,
      "step": 61730
    },
    {
      "epoch": 11225.454545454546,
      "grad_norm": 0.0006807157187722623,
      "learning_rate": 7.322664783660939e-07,
      "loss": 0.0012,
      "step": 61740
    },
    {
      "epoch": 11227.272727272728,
      "grad_norm": 0.19265340268611908,
      "learning_rate": 7.321634329780285e-07,
      "loss": 0.001,
      "step": 61750
    },
    {
      "epoch": 11229.09090909091,
      "grad_norm": 0.19242414832115173,
      "learning_rate": 7.320603750173364e-07,
      "loss": 0.0011,
      "step": 61760
    },
    {
      "epoch": 11230.90909090909,
      "grad_norm": 0.0006446537445299327,
      "learning_rate": 7.319573044895984e-07,
      "loss": 0.0012,
      "step": 61770
    },
    {
      "epoch": 11232.727272727272,
      "grad_norm": 0.0018223174847662449,
      "learning_rate": 7.318542214003967e-07,
      "loss": 0.0012,
      "step": 61780
    },
    {
      "epoch": 11234.545454545454,
      "grad_norm": 0.0022433409467339516,
      "learning_rate": 7.317511257553129e-07,
      "loss": 0.0014,
      "step": 61790
    },
    {
      "epoch": 11236.363636363636,
      "grad_norm": 0.0010235707741230726,
      "learning_rate": 7.316480175599308e-07,
      "loss": 0.0009,
      "step": 61800
    },
    {
      "epoch": 11238.181818181818,
      "grad_norm": 0.1582447588443756,
      "learning_rate": 7.315448968198338e-07,
      "loss": 0.0013,
      "step": 61810
    },
    {
      "epoch": 11240.0,
      "grad_norm": 0.3197677731513977,
      "learning_rate": 7.314417635406064e-07,
      "loss": 0.0011,
      "step": 61820
    },
    {
      "epoch": 11241.818181818182,
      "grad_norm": 0.19246138632297516,
      "learning_rate": 7.313386177278335e-07,
      "loss": 0.0012,
      "step": 61830
    },
    {
      "epoch": 11243.636363636364,
      "grad_norm": 0.18813996016979218,
      "learning_rate": 7.312354593871012e-07,
      "loss": 0.001,
      "step": 61840
    },
    {
      "epoch": 11245.454545454546,
      "grad_norm": 0.20834627747535706,
      "learning_rate": 7.311322885239956e-07,
      "loss": 0.001,
      "step": 61850
    },
    {
      "epoch": 11247.272727272728,
      "grad_norm": 0.04383737966418266,
      "learning_rate": 7.310291051441043e-07,
      "loss": 0.0014,
      "step": 61860
    },
    {
      "epoch": 11249.09090909091,
      "grad_norm": 0.00098612648434937,
      "learning_rate": 7.309259092530146e-07,
      "loss": 0.0009,
      "step": 61870
    },
    {
      "epoch": 11250.90909090909,
      "grad_norm": 0.22392697632312775,
      "learning_rate": 7.308227008563155e-07,
      "loss": 0.0012,
      "step": 61880
    },
    {
      "epoch": 11252.727272727272,
      "grad_norm": 0.2992362678050995,
      "learning_rate": 7.307194799595957e-07,
      "loss": 0.0009,
      "step": 61890
    },
    {
      "epoch": 11254.545454545454,
      "grad_norm": 0.0010156159987673163,
      "learning_rate": 7.306162465684453e-07,
      "loss": 0.001,
      "step": 61900
    },
    {
      "epoch": 11256.363636363636,
      "grad_norm": 0.01891501061618328,
      "learning_rate": 7.305130006884549e-07,
      "loss": 0.0013,
      "step": 61910
    },
    {
      "epoch": 11258.181818181818,
      "grad_norm": 0.29855549335479736,
      "learning_rate": 7.304097423252154e-07,
      "loss": 0.0012,
      "step": 61920
    },
    {
      "epoch": 11260.0,
      "grad_norm": 0.0027461834251880646,
      "learning_rate": 7.30306471484319e-07,
      "loss": 0.0009,
      "step": 61930
    },
    {
      "epoch": 11261.818181818182,
      "grad_norm": 0.001146232127211988,
      "learning_rate": 7.302031881713581e-07,
      "loss": 0.001,
      "step": 61940
    },
    {
      "epoch": 11263.636363636364,
      "grad_norm": 0.20085693895816803,
      "learning_rate": 7.300998923919258e-07,
      "loss": 0.001,
      "step": 61950
    },
    {
      "epoch": 11265.454545454546,
      "grad_norm": 0.2093975692987442,
      "learning_rate": 7.299965841516162e-07,
      "loss": 0.0013,
      "step": 61960
    },
    {
      "epoch": 11267.272727272728,
      "grad_norm": 0.001423611887730658,
      "learning_rate": 7.298932634560239e-07,
      "loss": 0.001,
      "step": 61970
    },
    {
      "epoch": 11269.09090909091,
      "grad_norm": 0.20176970958709717,
      "learning_rate": 7.29789930310744e-07,
      "loss": 0.0014,
      "step": 61980
    },
    {
      "epoch": 11270.90909090909,
      "grad_norm": 0.27991217374801636,
      "learning_rate": 7.296865847213723e-07,
      "loss": 0.0009,
      "step": 61990
    },
    {
      "epoch": 11272.727272727272,
      "grad_norm": 0.17481471598148346,
      "learning_rate": 7.295832266935058e-07,
      "loss": 0.001,
      "step": 62000
    },
    {
      "epoch": 11272.727272727272,
      "eval_loss": 4.869383811950684,
      "eval_runtime": 0.9523,
      "eval_samples_per_second": 10.5,
      "eval_steps_per_second": 5.25,
      "step": 62000
    },
    {
      "epoch": 11274.545454545454,
      "grad_norm": 0.1754244863986969,
      "learning_rate": 7.294798562327415e-07,
      "loss": 0.0013,
      "step": 62010
    },
    {
      "epoch": 11276.363636363636,
      "grad_norm": 0.22391773760318756,
      "learning_rate": 7.293764733446775e-07,
      "loss": 0.0009,
      "step": 62020
    },
    {
      "epoch": 11278.181818181818,
      "grad_norm": 0.002728346036747098,
      "learning_rate": 7.292730780349121e-07,
      "loss": 0.0011,
      "step": 62030
    },
    {
      "epoch": 11280.0,
      "grad_norm": 0.22326020896434784,
      "learning_rate": 7.291696703090449e-07,
      "loss": 0.0012,
      "step": 62040
    },
    {
      "epoch": 11281.818181818182,
      "grad_norm": 0.002267452422529459,
      "learning_rate": 7.290662501726758e-07,
      "loss": 0.001,
      "step": 62050
    },
    {
      "epoch": 11283.636363636364,
      "grad_norm": 0.18031255900859833,
      "learning_rate": 7.289628176314054e-07,
      "loss": 0.0014,
      "step": 62060
    },
    {
      "epoch": 11285.454545454546,
      "grad_norm": 0.2965925633907318,
      "learning_rate": 7.28859372690835e-07,
      "loss": 0.0009,
      "step": 62070
    },
    {
      "epoch": 11287.272727272728,
      "grad_norm": 0.0010771339293569326,
      "learning_rate": 7.287559153565665e-07,
      "loss": 0.0012,
      "step": 62080
    },
    {
      "epoch": 11289.09090909091,
      "grad_norm": 0.001966714859008789,
      "learning_rate": 7.286524456342029e-07,
      "loss": 0.0011,
      "step": 62090
    },
    {
      "epoch": 11290.90909090909,
      "grad_norm": 0.22518913447856903,
      "learning_rate": 7.285489635293471e-07,
      "loss": 0.001,
      "step": 62100
    },
    {
      "epoch": 11292.727272727272,
      "grad_norm": 0.17939801514148712,
      "learning_rate": 7.284454690476032e-07,
      "loss": 0.0015,
      "step": 62110
    },
    {
      "epoch": 11294.545454545454,
      "grad_norm": 0.001759944250807166,
      "learning_rate": 7.283419621945761e-07,
      "loss": 0.001,
      "step": 62120
    },
    {
      "epoch": 11296.363636363636,
      "grad_norm": 0.0007071048603393137,
      "learning_rate": 7.282384429758708e-07,
      "loss": 0.0008,
      "step": 62130
    },
    {
      "epoch": 11298.181818181818,
      "grad_norm": 0.010790357366204262,
      "learning_rate": 7.281349113970934e-07,
      "loss": 0.0015,
      "step": 62140
    },
    {
      "epoch": 11300.0,
      "grad_norm": 0.0013787706848233938,
      "learning_rate": 7.280313674638507e-07,
      "loss": 0.0009,
      "step": 62150
    },
    {
      "epoch": 11301.818181818182,
      "grad_norm": 0.24012742936611176,
      "learning_rate": 7.2792781118175e-07,
      "loss": 0.0011,
      "step": 62160
    },
    {
      "epoch": 11303.636363636364,
      "grad_norm": 0.0009636727045290172,
      "learning_rate": 7.278242425563992e-07,
      "loss": 0.0012,
      "step": 62170
    },
    {
      "epoch": 11305.454545454546,
      "grad_norm": 0.002357685938477516,
      "learning_rate": 7.277206615934071e-07,
      "loss": 0.001,
      "step": 62180
    },
    {
      "epoch": 11307.272727272728,
      "grad_norm": 0.19500993192195892,
      "learning_rate": 7.276170682983829e-07,
      "loss": 0.0014,
      "step": 62190
    },
    {
      "epoch": 11309.09090909091,
      "grad_norm": 0.28371164202690125,
      "learning_rate": 7.275134626769368e-07,
      "loss": 0.0011,
      "step": 62200
    },
    {
      "epoch": 11310.90909090909,
      "grad_norm": 0.20927810668945312,
      "learning_rate": 7.274098447346794e-07,
      "loss": 0.0009,
      "step": 62210
    },
    {
      "epoch": 11312.727272727272,
      "grad_norm": 0.22323717176914215,
      "learning_rate": 7.27306214477222e-07,
      "loss": 0.0009,
      "step": 62220
    },
    {
      "epoch": 11314.545454545454,
      "grad_norm": 0.23380255699157715,
      "learning_rate": 7.272025719101765e-07,
      "loss": 0.0017,
      "step": 62230
    },
    {
      "epoch": 11316.363636363636,
      "grad_norm": 0.0009781774133443832,
      "learning_rate": 7.27098917039156e-07,
      "loss": 0.0009,
      "step": 62240
    },
    {
      "epoch": 11318.181818181818,
      "grad_norm": 0.011554347351193428,
      "learning_rate": 7.269952498697734e-07,
      "loss": 0.0012,
      "step": 62250
    },
    {
      "epoch": 11320.0,
      "grad_norm": 0.23406502604484558,
      "learning_rate": 7.268915704076429e-07,
      "loss": 0.0009,
      "step": 62260
    },
    {
      "epoch": 11321.818181818182,
      "grad_norm": 0.2786540389060974,
      "learning_rate": 7.267878786583791e-07,
      "loss": 0.001,
      "step": 62270
    },
    {
      "epoch": 11323.636363636364,
      "grad_norm": 0.21107928454875946,
      "learning_rate": 7.266841746275976e-07,
      "loss": 0.0009,
      "step": 62280
    },
    {
      "epoch": 11325.454545454546,
      "grad_norm": 0.003675946732982993,
      "learning_rate": 7.265804583209141e-07,
      "loss": 0.0013,
      "step": 62290
    },
    {
      "epoch": 11327.272727272728,
      "grad_norm": 0.002274482510983944,
      "learning_rate": 7.264767297439454e-07,
      "loss": 0.0009,
      "step": 62300
    },
    {
      "epoch": 11329.09090909091,
      "grad_norm": 0.0011742929928004742,
      "learning_rate": 7.263729889023089e-07,
      "loss": 0.0012,
      "step": 62310
    },
    {
      "epoch": 11330.90909090909,
      "grad_norm": 0.011657741852104664,
      "learning_rate": 7.262692358016225e-07,
      "loss": 0.0012,
      "step": 62320
    },
    {
      "epoch": 11332.727272727272,
      "grad_norm": 0.001391979749314487,
      "learning_rate": 7.261654704475049e-07,
      "loss": 0.0011,
      "step": 62330
    },
    {
      "epoch": 11334.545454545454,
      "grad_norm": 0.21563853323459625,
      "learning_rate": 7.260616928455753e-07,
      "loss": 0.0009,
      "step": 62340
    },
    {
      "epoch": 11336.363636363636,
      "grad_norm": 0.0075548142194747925,
      "learning_rate": 7.25957903001454e-07,
      "loss": 0.0014,
      "step": 62350
    },
    {
      "epoch": 11338.181818181818,
      "grad_norm": 0.21148903667926788,
      "learning_rate": 7.258541009207615e-07,
      "loss": 0.001,
      "step": 62360
    },
    {
      "epoch": 11340.0,
      "grad_norm": 0.2279784083366394,
      "learning_rate": 7.25750286609119e-07,
      "loss": 0.0011,
      "step": 62370
    },
    {
      "epoch": 11341.818181818182,
      "grad_norm": 0.002080608857795596,
      "learning_rate": 7.256464600721485e-07,
      "loss": 0.001,
      "step": 62380
    },
    {
      "epoch": 11343.636363636364,
      "grad_norm": 0.28170138597488403,
      "learning_rate": 7.25542621315473e-07,
      "loss": 0.0014,
      "step": 62390
    },
    {
      "epoch": 11345.454545454546,
      "grad_norm": 0.23719193041324615,
      "learning_rate": 7.254387703447153e-07,
      "loss": 0.0011,
      "step": 62400
    },
    {
      "epoch": 11347.272727272728,
      "grad_norm": 0.28282052278518677,
      "learning_rate": 7.253349071654997e-07,
      "loss": 0.0012,
      "step": 62410
    },
    {
      "epoch": 11349.09090909091,
      "grad_norm": 0.18842121958732605,
      "learning_rate": 7.252310317834507e-07,
      "loss": 0.001,
      "step": 62420
    },
    {
      "epoch": 11350.90909090909,
      "grad_norm": 0.0012628581607714295,
      "learning_rate": 7.251271442041938e-07,
      "loss": 0.0012,
      "step": 62430
    },
    {
      "epoch": 11352.727272727272,
      "grad_norm": 0.20213086903095245,
      "learning_rate": 7.250232444333546e-07,
      "loss": 0.0012,
      "step": 62440
    },
    {
      "epoch": 11354.545454545454,
      "grad_norm": 0.0010092504089698195,
      "learning_rate": 7.249193324765598e-07,
      "loss": 0.001,
      "step": 62450
    },
    {
      "epoch": 11356.363636363636,
      "grad_norm": 0.2132246047258377,
      "learning_rate": 7.248154083394369e-07,
      "loss": 0.0011,
      "step": 62460
    },
    {
      "epoch": 11358.181818181818,
      "grad_norm": 0.003225331660360098,
      "learning_rate": 7.247114720276136e-07,
      "loss": 0.001,
      "step": 62470
    },
    {
      "epoch": 11360.0,
      "grad_norm": 0.0017092054476961493,
      "learning_rate": 7.246075235467186e-07,
      "loss": 0.0012,
      "step": 62480
    },
    {
      "epoch": 11361.818181818182,
      "grad_norm": 0.2088528722524643,
      "learning_rate": 7.245035629023812e-07,
      "loss": 0.0012,
      "step": 62490
    },
    {
      "epoch": 11363.636363636364,
      "grad_norm": 0.224837988615036,
      "learning_rate": 7.243995901002311e-07,
      "loss": 0.0011,
      "step": 62500
    },
    {
      "epoch": 11363.636363636364,
      "eval_loss": 4.899765968322754,
      "eval_runtime": 0.9509,
      "eval_samples_per_second": 10.516,
      "eval_steps_per_second": 5.258,
      "step": 62500
    },
    {
      "epoch": 11365.454545454546,
      "grad_norm": 0.21030159294605255,
      "learning_rate": 7.24295605145899e-07,
      "loss": 0.001,
      "step": 62510
    },
    {
      "epoch": 11367.272727272728,
      "grad_norm": 0.0013230089098215103,
      "learning_rate": 7.241916080450162e-07,
      "loss": 0.001,
      "step": 62520
    },
    {
      "epoch": 11369.09090909091,
      "grad_norm": 0.2792429029941559,
      "learning_rate": 7.240875988032142e-07,
      "loss": 0.0015,
      "step": 62530
    },
    {
      "epoch": 11370.90909090909,
      "grad_norm": 0.0011658447328954935,
      "learning_rate": 7.239835774261263e-07,
      "loss": 0.0008,
      "step": 62540
    },
    {
      "epoch": 11372.727272727272,
      "grad_norm": 0.00218712049536407,
      "learning_rate": 7.238795439193848e-07,
      "loss": 0.0012,
      "step": 62550
    },
    {
      "epoch": 11374.545454545454,
      "grad_norm": 0.0023785014636814594,
      "learning_rate": 7.237754982886243e-07,
      "loss": 0.0012,
      "step": 62560
    },
    {
      "epoch": 11376.363636363636,
      "grad_norm": 0.0012049092911183834,
      "learning_rate": 7.236714405394786e-07,
      "loss": 0.0011,
      "step": 62570
    },
    {
      "epoch": 11378.181818181818,
      "grad_norm": 0.141138955950737,
      "learning_rate": 7.235673706775835e-07,
      "loss": 0.0012,
      "step": 62580
    },
    {
      "epoch": 11380.0,
      "grad_norm": 0.0011401865631341934,
      "learning_rate": 7.234632887085746e-07,
      "loss": 0.0011,
      "step": 62590
    },
    {
      "epoch": 11381.818181818182,
      "grad_norm": 0.22132933139801025,
      "learning_rate": 7.233591946380884e-07,
      "loss": 0.0011,
      "step": 62600
    },
    {
      "epoch": 11383.636363636364,
      "grad_norm": 0.22070395946502686,
      "learning_rate": 7.232550884717616e-07,
      "loss": 0.0012,
      "step": 62610
    },
    {
      "epoch": 11385.454545454546,
      "grad_norm": 0.003143439767882228,
      "learning_rate": 7.231509702152327e-07,
      "loss": 0.0009,
      "step": 62620
    },
    {
      "epoch": 11387.272727272728,
      "grad_norm": 0.1905040293931961,
      "learning_rate": 7.230468398741399e-07,
      "loss": 0.0012,
      "step": 62630
    },
    {
      "epoch": 11389.09090909091,
      "grad_norm": 0.0017434590263292193,
      "learning_rate": 7.229426974541221e-07,
      "loss": 0.001,
      "step": 62640
    },
    {
      "epoch": 11390.90909090909,
      "grad_norm": 0.001620985334739089,
      "learning_rate": 7.22838542960819e-07,
      "loss": 0.0012,
      "step": 62650
    },
    {
      "epoch": 11392.727272727272,
      "grad_norm": 0.0009628371917642653,
      "learning_rate": 7.227343763998712e-07,
      "loss": 0.0012,
      "step": 62660
    },
    {
      "epoch": 11394.545454545454,
      "grad_norm": 0.22345739603042603,
      "learning_rate": 7.226301977769198e-07,
      "loss": 0.001,
      "step": 62670
    },
    {
      "epoch": 11396.363636363636,
      "grad_norm": 0.29302284121513367,
      "learning_rate": 7.225260070976064e-07,
      "loss": 0.0014,
      "step": 62680
    },
    {
      "epoch": 11398.181818181818,
      "grad_norm": 0.0010182383703067899,
      "learning_rate": 7.224218043675734e-07,
      "loss": 0.0006,
      "step": 62690
    },
    {
      "epoch": 11400.0,
      "grad_norm": 0.0021021245047450066,
      "learning_rate": 7.223175895924637e-07,
      "loss": 0.0012,
      "step": 62700
    },
    {
      "epoch": 11401.818181818182,
      "grad_norm": 0.2191658616065979,
      "learning_rate": 7.222133627779212e-07,
      "loss": 0.0012,
      "step": 62710
    },
    {
      "epoch": 11403.636363636364,
      "grad_norm": 0.0010685214074328542,
      "learning_rate": 7.2210912392959e-07,
      "loss": 0.001,
      "step": 62720
    },
    {
      "epoch": 11405.454545454546,
      "grad_norm": 0.17686134576797485,
      "learning_rate": 7.220048730531153e-07,
      "loss": 0.001,
      "step": 62730
    },
    {
      "epoch": 11407.272727272728,
      "grad_norm": 0.15034307539463043,
      "learning_rate": 7.219006101541425e-07,
      "loss": 0.0012,
      "step": 62740
    },
    {
      "epoch": 11409.09090909091,
      "grad_norm": 0.0011316629825159907,
      "learning_rate": 7.217963352383181e-07,
      "loss": 0.0009,
      "step": 62750
    },
    {
      "epoch": 11410.90909090909,
      "grad_norm": 0.22217662632465363,
      "learning_rate": 7.216920483112885e-07,
      "loss": 0.0012,
      "step": 62760
    },
    {
      "epoch": 11412.727272727272,
      "grad_norm": 0.0014102838467806578,
      "learning_rate": 7.21587749378702e-07,
      "loss": 0.0012,
      "step": 62770
    },
    {
      "epoch": 11414.545454545454,
      "grad_norm": 0.0013354690745472908,
      "learning_rate": 7.214834384462064e-07,
      "loss": 0.0008,
      "step": 62780
    },
    {
      "epoch": 11416.363636363636,
      "grad_norm": 0.17556068301200867,
      "learning_rate": 7.213791155194508e-07,
      "loss": 0.0012,
      "step": 62790
    },
    {
      "epoch": 11418.181818181818,
      "grad_norm": 0.004406152293086052,
      "learning_rate": 7.212747806040844e-07,
      "loss": 0.0011,
      "step": 62800
    },
    {
      "epoch": 11420.0,
      "grad_norm": 0.0011361622018739581,
      "learning_rate": 7.211704337057577e-07,
      "loss": 0.0012,
      "step": 62810
    },
    {
      "epoch": 11421.818181818182,
      "grad_norm": 0.18820318579673767,
      "learning_rate": 7.210660748301212e-07,
      "loss": 0.0008,
      "step": 62820
    },
    {
      "epoch": 11423.636363636364,
      "grad_norm": 0.29109519720077515,
      "learning_rate": 7.209617039828269e-07,
      "loss": 0.0015,
      "step": 62830
    },
    {
      "epoch": 11425.454545454546,
      "grad_norm": 0.0006920272717252374,
      "learning_rate": 7.208573211695264e-07,
      "loss": 0.0009,
      "step": 62840
    },
    {
      "epoch": 11427.272727272728,
      "grad_norm": 0.21005551517009735,
      "learning_rate": 7.207529263958726e-07,
      "loss": 0.0013,
      "step": 62850
    },
    {
      "epoch": 11429.09090909091,
      "grad_norm": 0.0011737782042473555,
      "learning_rate": 7.20648519667519e-07,
      "loss": 0.0011,
      "step": 62860
    },
    {
      "epoch": 11430.90909090909,
      "grad_norm": 0.1854417473077774,
      "learning_rate": 7.205441009901196e-07,
      "loss": 0.0011,
      "step": 62870
    },
    {
      "epoch": 11432.727272727272,
      "grad_norm": 0.0025631135795265436,
      "learning_rate": 7.204396703693293e-07,
      "loss": 0.0013,
      "step": 62880
    },
    {
      "epoch": 11434.545454545454,
      "grad_norm": 0.20150703191757202,
      "learning_rate": 7.203352278108031e-07,
      "loss": 0.0011,
      "step": 62890
    },
    {
      "epoch": 11436.363636363636,
      "grad_norm": 0.2787361741065979,
      "learning_rate": 7.202307733201974e-07,
      "loss": 0.0012,
      "step": 62900
    },
    {
      "epoch": 11438.181818181818,
      "grad_norm": 0.18582016229629517,
      "learning_rate": 7.201263069031686e-07,
      "loss": 0.0009,
      "step": 62910
    },
    {
      "epoch": 11440.0,
      "grad_norm": 0.23691986501216888,
      "learning_rate": 7.200218285653739e-07,
      "loss": 0.001,
      "step": 62920
    },
    {
      "epoch": 11441.818181818182,
      "grad_norm": 0.27528122067451477,
      "learning_rate": 7.199173383124715e-07,
      "loss": 0.0009,
      "step": 62930
    },
    {
      "epoch": 11443.636363636364,
      "grad_norm": 0.0017468139994889498,
      "learning_rate": 7.198128361501199e-07,
      "loss": 0.0015,
      "step": 62940
    },
    {
      "epoch": 11445.454545454546,
      "grad_norm": 0.229370579123497,
      "learning_rate": 7.197083220839783e-07,
      "loss": 0.0009,
      "step": 62950
    },
    {
      "epoch": 11447.272727272728,
      "grad_norm": 0.0010202290723100305,
      "learning_rate": 7.196037961197066e-07,
      "loss": 0.001,
      "step": 62960
    },
    {
      "epoch": 11449.09090909091,
      "grad_norm": 0.1796082705259323,
      "learning_rate": 7.194992582629653e-07,
      "loss": 0.0014,
      "step": 62970
    },
    {
      "epoch": 11450.90909090909,
      "grad_norm": 0.018535181879997253,
      "learning_rate": 7.193947085194157e-07,
      "loss": 0.0011,
      "step": 62980
    },
    {
      "epoch": 11452.727272727272,
      "grad_norm": 0.22734379768371582,
      "learning_rate": 7.192901468947192e-07,
      "loss": 0.0012,
      "step": 62990
    },
    {
      "epoch": 11454.545454545454,
      "grad_norm": 0.22295324504375458,
      "learning_rate": 7.191855733945386e-07,
      "loss": 0.001,
      "step": 63000
    },
    {
      "epoch": 11454.545454545454,
      "eval_loss": 4.907447338104248,
      "eval_runtime": 0.9545,
      "eval_samples_per_second": 10.476,
      "eval_steps_per_second": 5.238,
      "step": 63000
    },
    {
      "epoch": 11456.363636363636,
      "grad_norm": 0.22469718754291534,
      "learning_rate": 7.19080988024537e-07,
      "loss": 0.0012,
      "step": 63010
    },
    {
      "epoch": 11458.181818181818,
      "grad_norm": 0.19372566044330597,
      "learning_rate": 7.189763907903782e-07,
      "loss": 0.0009,
      "step": 63020
    },
    {
      "epoch": 11460.0,
      "grad_norm": 0.3433973491191864,
      "learning_rate": 7.188717816977264e-07,
      "loss": 0.0012,
      "step": 63030
    },
    {
      "epoch": 11461.818181818182,
      "grad_norm": 0.000884737994056195,
      "learning_rate": 7.187671607522466e-07,
      "loss": 0.001,
      "step": 63040
    },
    {
      "epoch": 11463.636363636364,
      "grad_norm": 0.0008231883402913809,
      "learning_rate": 7.186625279596044e-07,
      "loss": 0.0011,
      "step": 63050
    },
    {
      "epoch": 11465.454545454546,
      "grad_norm": 0.2522308826446533,
      "learning_rate": 7.185578833254664e-07,
      "loss": 0.0013,
      "step": 63060
    },
    {
      "epoch": 11467.272727272728,
      "grad_norm": 0.17845991253852844,
      "learning_rate": 7.184532268554995e-07,
      "loss": 0.0009,
      "step": 63070
    },
    {
      "epoch": 11469.09090909091,
      "grad_norm": 0.22715692222118378,
      "learning_rate": 7.18348558555371e-07,
      "loss": 0.0013,
      "step": 63080
    },
    {
      "epoch": 11470.90909090909,
      "grad_norm": 0.0009886556072160602,
      "learning_rate": 7.182438784307494e-07,
      "loss": 0.0008,
      "step": 63090
    },
    {
      "epoch": 11472.727272727272,
      "grad_norm": 0.18278764188289642,
      "learning_rate": 7.181391864873034e-07,
      "loss": 0.0012,
      "step": 63100
    },
    {
      "epoch": 11474.545454545454,
      "grad_norm": 0.21164992451667786,
      "learning_rate": 7.180344827307026e-07,
      "loss": 0.0012,
      "step": 63110
    },
    {
      "epoch": 11476.363636363636,
      "grad_norm": 0.2827862799167633,
      "learning_rate": 7.17929767166617e-07,
      "loss": 0.0012,
      "step": 63120
    },
    {
      "epoch": 11478.181818181818,
      "grad_norm": 0.0008630815427750349,
      "learning_rate": 7.178250398007177e-07,
      "loss": 0.0007,
      "step": 63130
    },
    {
      "epoch": 11480.0,
      "grad_norm": 0.2771531939506531,
      "learning_rate": 7.177203006386759e-07,
      "loss": 0.0014,
      "step": 63140
    },
    {
      "epoch": 11481.818181818182,
      "grad_norm": 0.0018574332352727652,
      "learning_rate": 7.176155496861638e-07,
      "loss": 0.0011,
      "step": 63150
    },
    {
      "epoch": 11483.636363636364,
      "grad_norm": 0.17335103452205658,
      "learning_rate": 7.175107869488538e-07,
      "loss": 0.0012,
      "step": 63160
    },
    {
      "epoch": 11485.454545454546,
      "grad_norm": 0.0023309167008847,
      "learning_rate": 7.174060124324197e-07,
      "loss": 0.0013,
      "step": 63170
    },
    {
      "epoch": 11487.272727272728,
      "grad_norm": 0.2638450860977173,
      "learning_rate": 7.173012261425351e-07,
      "loss": 0.0011,
      "step": 63180
    },
    {
      "epoch": 11489.09090909091,
      "grad_norm": 0.2934776842594147,
      "learning_rate": 7.171964280848748e-07,
      "loss": 0.0011,
      "step": 63190
    },
    {
      "epoch": 11490.90909090909,
      "grad_norm": 0.16918130218982697,
      "learning_rate": 7.17091618265114e-07,
      "loss": 0.0012,
      "step": 63200
    },
    {
      "epoch": 11492.727272727272,
      "grad_norm": 0.0020696402061730623,
      "learning_rate": 7.169867966889287e-07,
      "loss": 0.0009,
      "step": 63210
    },
    {
      "epoch": 11494.545454545454,
      "grad_norm": 0.16626553237438202,
      "learning_rate": 7.168819633619953e-07,
      "loss": 0.0013,
      "step": 63220
    },
    {
      "epoch": 11496.363636363636,
      "grad_norm": 0.0011999467387795448,
      "learning_rate": 7.167771182899909e-07,
      "loss": 0.0007,
      "step": 63230
    },
    {
      "epoch": 11498.181818181818,
      "grad_norm": 0.0026079982053488493,
      "learning_rate": 7.166722614785936e-07,
      "loss": 0.0012,
      "step": 63240
    },
    {
      "epoch": 11500.0,
      "grad_norm": 0.2035577893257141,
      "learning_rate": 7.165673929334815e-07,
      "loss": 0.0012,
      "step": 63250
    },
    {
      "epoch": 11501.818181818182,
      "grad_norm": 0.0010130590526387095,
      "learning_rate": 7.164625126603339e-07,
      "loss": 0.0011,
      "step": 63260
    },
    {
      "epoch": 11503.636363636364,
      "grad_norm": 0.0013764073373749852,
      "learning_rate": 7.163576206648303e-07,
      "loss": 0.0012,
      "step": 63270
    },
    {
      "epoch": 11505.454545454546,
      "grad_norm": 0.2100065052509308,
      "learning_rate": 7.162527169526513e-07,
      "loss": 0.0011,
      "step": 63280
    },
    {
      "epoch": 11507.272727272728,
      "grad_norm": 0.2750425636768341,
      "learning_rate": 7.161478015294777e-07,
      "loss": 0.0013,
      "step": 63290
    },
    {
      "epoch": 11509.09090909091,
      "grad_norm": 0.0010938022751361132,
      "learning_rate": 7.160428744009912e-07,
      "loss": 0.0009,
      "step": 63300
    },
    {
      "epoch": 11510.90909090909,
      "grad_norm": 0.16715586185455322,
      "learning_rate": 7.159379355728739e-07,
      "loss": 0.0012,
      "step": 63310
    },
    {
      "epoch": 11512.727272727272,
      "grad_norm": 0.0015590767143294215,
      "learning_rate": 7.15832985050809e-07,
      "loss": 0.0012,
      "step": 63320
    },
    {
      "epoch": 11514.545454545454,
      "grad_norm": 0.002200533403083682,
      "learning_rate": 7.157280228404795e-07,
      "loss": 0.0009,
      "step": 63330
    },
    {
      "epoch": 11516.363636363636,
      "grad_norm": 0.22259585559368134,
      "learning_rate": 7.156230489475699e-07,
      "loss": 0.0015,
      "step": 63340
    },
    {
      "epoch": 11518.181818181818,
      "grad_norm": 0.0012795610819011927,
      "learning_rate": 7.15518063377765e-07,
      "loss": 0.0007,
      "step": 63350
    },
    {
      "epoch": 11520.0,
      "grad_norm": 0.3008902668952942,
      "learning_rate": 7.154130661367502e-07,
      "loss": 0.0012,
      "step": 63360
    },
    {
      "epoch": 11521.818181818182,
      "grad_norm": 0.16639652848243713,
      "learning_rate": 7.153080572302113e-07,
      "loss": 0.0011,
      "step": 63370
    },
    {
      "epoch": 11523.636363636364,
      "grad_norm": 0.27313560247421265,
      "learning_rate": 7.152030366638353e-07,
      "loss": 0.0011,
      "step": 63380
    },
    {
      "epoch": 11525.454545454546,
      "grad_norm": 0.2086203396320343,
      "learning_rate": 7.150980044433093e-07,
      "loss": 0.0011,
      "step": 63390
    },
    {
      "epoch": 11527.272727272728,
      "grad_norm": 0.0008214068948291242,
      "learning_rate": 7.149929605743214e-07,
      "loss": 0.0012,
      "step": 63400
    },
    {
      "epoch": 11529.09090909091,
      "grad_norm": 0.0018265927210450172,
      "learning_rate": 7.148879050625598e-07,
      "loss": 0.0011,
      "step": 63410
    },
    {
      "epoch": 11530.90909090909,
      "grad_norm": 0.24105112254619598,
      "learning_rate": 7.14782837913714e-07,
      "loss": 0.0011,
      "step": 63420
    },
    {
      "epoch": 11532.727272727272,
      "grad_norm": 0.0008748604450374842,
      "learning_rate": 7.146777591334741e-07,
      "loss": 0.0012,
      "step": 63430
    },
    {
      "epoch": 11534.545454545454,
      "grad_norm": 0.0008373821619898081,
      "learning_rate": 7.1457266872753e-07,
      "loss": 0.0013,
      "step": 63440
    },
    {
      "epoch": 11536.363636363636,
      "grad_norm": 0.22697141766548157,
      "learning_rate": 7.144675667015729e-07,
      "loss": 0.0009,
      "step": 63450
    },
    {
      "epoch": 11538.181818181818,
      "grad_norm": 0.22493807971477509,
      "learning_rate": 7.143624530612949e-07,
      "loss": 0.0012,
      "step": 63460
    },
    {
      "epoch": 11540.0,
      "grad_norm": 0.22872351109981537,
      "learning_rate": 7.142573278123879e-07,
      "loss": 0.0011,
      "step": 63470
    },
    {
      "epoch": 11541.818181818182,
      "grad_norm": 0.27186453342437744,
      "learning_rate": 7.141521909605451e-07,
      "loss": 0.0012,
      "step": 63480
    },
    {
      "epoch": 11543.636363636364,
      "grad_norm": 0.22225727140903473,
      "learning_rate": 7.140470425114602e-07,
      "loss": 0.001,
      "step": 63490
    },
    {
      "epoch": 11545.454545454546,
      "grad_norm": 0.003307015635073185,
      "learning_rate": 7.139418824708271e-07,
      "loss": 0.0012,
      "step": 63500
    },
    {
      "epoch": 11545.454545454546,
      "eval_loss": 4.911076545715332,
      "eval_runtime": 0.9516,
      "eval_samples_per_second": 10.509,
      "eval_steps_per_second": 5.255,
      "step": 63500
    },
    {
      "epoch": 11547.272727272728,
      "grad_norm": 0.0010715106036514044,
      "learning_rate": 7.138367108443411e-07,
      "loss": 0.0009,
      "step": 63510
    },
    {
      "epoch": 11549.09090909091,
      "grad_norm": 0.00530979223549366,
      "learning_rate": 7.137315276376972e-07,
      "loss": 0.0012,
      "step": 63520
    },
    {
      "epoch": 11550.90909090909,
      "grad_norm": 0.0009820816339924932,
      "learning_rate": 7.136263328565919e-07,
      "loss": 0.0012,
      "step": 63530
    },
    {
      "epoch": 11552.727272727272,
      "grad_norm": 0.0009968314552679658,
      "learning_rate": 7.135211265067216e-07,
      "loss": 0.0012,
      "step": 63540
    },
    {
      "epoch": 11554.545454545454,
      "grad_norm": 0.2890284061431885,
      "learning_rate": 7.134159085937841e-07,
      "loss": 0.0011,
      "step": 63550
    },
    {
      "epoch": 11556.363636363636,
      "grad_norm": 0.22542504966259003,
      "learning_rate": 7.133106791234771e-07,
      "loss": 0.0009,
      "step": 63560
    },
    {
      "epoch": 11558.181818181818,
      "grad_norm": 0.16729938983917236,
      "learning_rate": 7.132054381014994e-07,
      "loss": 0.0012,
      "step": 63570
    },
    {
      "epoch": 11560.0,
      "grad_norm": 0.21464882791042328,
      "learning_rate": 7.131001855335498e-07,
      "loss": 0.0011,
      "step": 63580
    },
    {
      "epoch": 11561.818181818182,
      "grad_norm": 0.18148493766784668,
      "learning_rate": 7.129949214253287e-07,
      "loss": 0.0014,
      "step": 63590
    },
    {
      "epoch": 11563.636363636364,
      "grad_norm": 0.0015853968216106296,
      "learning_rate": 7.128896457825363e-07,
      "loss": 0.0009,
      "step": 63600
    },
    {
      "epoch": 11565.454545454546,
      "grad_norm": 0.13875488936901093,
      "learning_rate": 7.127843586108739e-07,
      "loss": 0.0012,
      "step": 63610
    },
    {
      "epoch": 11567.272727272728,
      "grad_norm": 0.2216995805501938,
      "learning_rate": 7.126790599160431e-07,
      "loss": 0.0011,
      "step": 63620
    },
    {
      "epoch": 11569.09090909091,
      "grad_norm": 0.0013379944721236825,
      "learning_rate": 7.125737497037462e-07,
      "loss": 0.0011,
      "step": 63630
    },
    {
      "epoch": 11570.90909090909,
      "grad_norm": 0.0012513177935034037,
      "learning_rate": 7.124684279796864e-07,
      "loss": 0.0012,
      "step": 63640
    },
    {
      "epoch": 11572.727272727272,
      "grad_norm": 0.15050619840621948,
      "learning_rate": 7.123630947495671e-07,
      "loss": 0.0012,
      "step": 63650
    },
    {
      "epoch": 11574.545454545454,
      "grad_norm": 0.17078891396522522,
      "learning_rate": 7.12257750019093e-07,
      "loss": 0.001,
      "step": 63660
    },
    {
      "epoch": 11576.363636363636,
      "grad_norm": 0.0008616741397418082,
      "learning_rate": 7.121523937939683e-07,
      "loss": 0.0009,
      "step": 63670
    },
    {
      "epoch": 11578.181818181818,
      "grad_norm": 0.0012003056472167373,
      "learning_rate": 7.12047026079899e-07,
      "loss": 0.001,
      "step": 63680
    },
    {
      "epoch": 11580.0,
      "grad_norm": 0.0010167596628889441,
      "learning_rate": 7.119416468825908e-07,
      "loss": 0.0012,
      "step": 63690
    },
    {
      "epoch": 11581.818181818182,
      "grad_norm": 0.002200725022703409,
      "learning_rate": 7.118362562077506e-07,
      "loss": 0.0012,
      "step": 63700
    },
    {
      "epoch": 11583.636363636364,
      "grad_norm": 0.21286380290985107,
      "learning_rate": 7.11730854061086e-07,
      "loss": 0.0012,
      "step": 63710
    },
    {
      "epoch": 11585.454545454546,
      "grad_norm": 0.20755207538604736,
      "learning_rate": 7.116254404483047e-07,
      "loss": 0.0009,
      "step": 63720
    },
    {
      "epoch": 11587.272727272728,
      "grad_norm": 0.0012489634100347757,
      "learning_rate": 7.115200153751152e-07,
      "loss": 0.0009,
      "step": 63730
    },
    {
      "epoch": 11589.09090909091,
      "grad_norm": 0.0011215390404686332,
      "learning_rate": 7.11414578847227e-07,
      "loss": 0.0012,
      "step": 63740
    },
    {
      "epoch": 11590.90909090909,
      "grad_norm": 0.0008701345650479198,
      "learning_rate": 7.113091308703497e-07,
      "loss": 0.0012,
      "step": 63750
    },
    {
      "epoch": 11592.727272727272,
      "grad_norm": 0.00748389633372426,
      "learning_rate": 7.112036714501939e-07,
      "loss": 0.0012,
      "step": 63760
    },
    {
      "epoch": 11594.545454545454,
      "grad_norm": 0.047405533492565155,
      "learning_rate": 7.110982005924704e-07,
      "loss": 0.0012,
      "step": 63770
    },
    {
      "epoch": 11596.363636363636,
      "grad_norm": 0.225167915225029,
      "learning_rate": 7.109927183028915e-07,
      "loss": 0.0008,
      "step": 63780
    },
    {
      "epoch": 11598.181818181818,
      "grad_norm": 0.0010982205858454108,
      "learning_rate": 7.108872245871686e-07,
      "loss": 0.0011,
      "step": 63790
    },
    {
      "epoch": 11600.0,
      "grad_norm": 0.23772677779197693,
      "learning_rate": 7.107817194510156e-07,
      "loss": 0.0012,
      "step": 63800
    },
    {
      "epoch": 11601.818181818182,
      "grad_norm": 0.0010871000122278929,
      "learning_rate": 7.106762029001454e-07,
      "loss": 0.0011,
      "step": 63810
    },
    {
      "epoch": 11603.636363636364,
      "grad_norm": 0.19930371642112732,
      "learning_rate": 7.105706749402723e-07,
      "loss": 0.0012,
      "step": 63820
    },
    {
      "epoch": 11605.454545454546,
      "grad_norm": 0.0019783417228609324,
      "learning_rate": 7.104651355771111e-07,
      "loss": 0.0011,
      "step": 63830
    },
    {
      "epoch": 11607.272727272728,
      "grad_norm": 0.2248108685016632,
      "learning_rate": 7.103595848163774e-07,
      "loss": 0.001,
      "step": 63840
    },
    {
      "epoch": 11609.09090909091,
      "grad_norm": 0.17692138254642487,
      "learning_rate": 7.102540226637869e-07,
      "loss": 0.0012,
      "step": 63850
    },
    {
      "epoch": 11610.90909090909,
      "grad_norm": 0.16495241224765778,
      "learning_rate": 7.101484491250563e-07,
      "loss": 0.0011,
      "step": 63860
    },
    {
      "epoch": 11612.727272727272,
      "grad_norm": 0.2102774679660797,
      "learning_rate": 7.100428642059032e-07,
      "loss": 0.0012,
      "step": 63870
    },
    {
      "epoch": 11614.545454545454,
      "grad_norm": 0.002181997522711754,
      "learning_rate": 7.09937267912045e-07,
      "loss": 0.0009,
      "step": 63880
    },
    {
      "epoch": 11616.363636363636,
      "grad_norm": 0.22468949854373932,
      "learning_rate": 7.098316602492004e-07,
      "loss": 0.0014,
      "step": 63890
    },
    {
      "epoch": 11618.181818181818,
      "grad_norm": 0.002265077317133546,
      "learning_rate": 7.097260412230885e-07,
      "loss": 0.0007,
      "step": 63900
    },
    {
      "epoch": 11620.0,
      "grad_norm": 0.17076928913593292,
      "learning_rate": 7.096204108394291e-07,
      "loss": 0.0012,
      "step": 63910
    },
    {
      "epoch": 11621.818181818182,
      "grad_norm": 0.001170914270915091,
      "learning_rate": 7.095147691039423e-07,
      "loss": 0.0009,
      "step": 63920
    },
    {
      "epoch": 11623.636363636364,
      "grad_norm": 0.0013724790187552571,
      "learning_rate": 7.094091160223492e-07,
      "loss": 0.0012,
      "step": 63930
    },
    {
      "epoch": 11625.454545454546,
      "grad_norm": 0.0010440056212246418,
      "learning_rate": 7.093034516003714e-07,
      "loss": 0.001,
      "step": 63940
    },
    {
      "epoch": 11627.272727272728,
      "grad_norm": 0.006943203508853912,
      "learning_rate": 7.091977758437311e-07,
      "loss": 0.0013,
      "step": 63950
    },
    {
      "epoch": 11629.09090909091,
      "grad_norm": 0.17278414964675903,
      "learning_rate": 7.090920887581506e-07,
      "loss": 0.0009,
      "step": 63960
    },
    {
      "epoch": 11630.90909090909,
      "grad_norm": 0.000913352589122951,
      "learning_rate": 7.089863903493541e-07,
      "loss": 0.0012,
      "step": 63970
    },
    {
      "epoch": 11632.727272727272,
      "grad_norm": 0.001056245411746204,
      "learning_rate": 7.088806806230651e-07,
      "loss": 0.0011,
      "step": 63980
    },
    {
      "epoch": 11634.545454545454,
      "grad_norm": 0.18325453996658325,
      "learning_rate": 7.087749595850083e-07,
      "loss": 0.0012,
      "step": 63990
    },
    {
      "epoch": 11636.363636363636,
      "grad_norm": 0.23771212995052338,
      "learning_rate": 7.086692272409089e-07,
      "loss": 0.0011,
      "step": 64000
    },
    {
      "epoch": 11636.363636363636,
      "eval_loss": 4.899580001831055,
      "eval_runtime": 0.9526,
      "eval_samples_per_second": 10.497,
      "eval_steps_per_second": 5.249,
      "step": 64000
    },
    {
      "epoch": 11638.181818181818,
      "grad_norm": 0.2238302081823349,
      "learning_rate": 7.085634835964929e-07,
      "loss": 0.001,
      "step": 64010
    },
    {
      "epoch": 11640.0,
      "grad_norm": 0.30887454748153687,
      "learning_rate": 7.084577286574867e-07,
      "loss": 0.0011,
      "step": 64020
    },
    {
      "epoch": 11641.818181818182,
      "grad_norm": 0.28367525339126587,
      "learning_rate": 7.083519624296173e-07,
      "loss": 0.0012,
      "step": 64030
    },
    {
      "epoch": 11643.636363636364,
      "grad_norm": 0.17758269608020782,
      "learning_rate": 7.082461849186125e-07,
      "loss": 0.0011,
      "step": 64040
    },
    {
      "epoch": 11645.454545454546,
      "grad_norm": 0.015101495198905468,
      "learning_rate": 7.081403961302006e-07,
      "loss": 0.0013,
      "step": 64050
    },
    {
      "epoch": 11647.272727272728,
      "grad_norm": 0.03725587949156761,
      "learning_rate": 7.080345960701104e-07,
      "loss": 0.001,
      "step": 64060
    },
    {
      "epoch": 11649.09090909091,
      "grad_norm": 0.0011330044362694025,
      "learning_rate": 7.079287847440715e-07,
      "loss": 0.0009,
      "step": 64070
    },
    {
      "epoch": 11650.90909090909,
      "grad_norm": 0.010080251842737198,
      "learning_rate": 7.078229621578139e-07,
      "loss": 0.0012,
      "step": 64080
    },
    {
      "epoch": 11652.727272727272,
      "grad_norm": 0.19006173312664032,
      "learning_rate": 7.077171283170685e-07,
      "loss": 0.0011,
      "step": 64090
    },
    {
      "epoch": 11654.545454545454,
      "grad_norm": 0.011461720801889896,
      "learning_rate": 7.076112832275666e-07,
      "loss": 0.0012,
      "step": 64100
    },
    {
      "epoch": 11656.363636363636,
      "grad_norm": 0.002082148566842079,
      "learning_rate": 7.075054268950401e-07,
      "loss": 0.0008,
      "step": 64110
    },
    {
      "epoch": 11658.181818181818,
      "grad_norm": 0.18168118596076965,
      "learning_rate": 7.073995593252217e-07,
      "loss": 0.0015,
      "step": 64120
    },
    {
      "epoch": 11660.0,
      "grad_norm": 0.21894662082195282,
      "learning_rate": 7.072936805238443e-07,
      "loss": 0.0009,
      "step": 64130
    },
    {
      "epoch": 11661.818181818182,
      "grad_norm": 0.29857969284057617,
      "learning_rate": 7.071877904966422e-07,
      "loss": 0.0012,
      "step": 64140
    },
    {
      "epoch": 11663.636363636364,
      "grad_norm": 0.0011892981128767133,
      "learning_rate": 7.070818892493491e-07,
      "loss": 0.0008,
      "step": 64150
    },
    {
      "epoch": 11665.454545454546,
      "grad_norm": 0.0009889776119962335,
      "learning_rate": 7.069759767877005e-07,
      "loss": 0.0015,
      "step": 64160
    },
    {
      "epoch": 11667.272727272728,
      "grad_norm": 0.0036925075110048056,
      "learning_rate": 7.068700531174319e-07,
      "loss": 0.0009,
      "step": 64170
    },
    {
      "epoch": 11669.09090909091,
      "grad_norm": 0.011698494665324688,
      "learning_rate": 7.067641182442793e-07,
      "loss": 0.0012,
      "step": 64180
    },
    {
      "epoch": 11670.90909090909,
      "grad_norm": 0.17918670177459717,
      "learning_rate": 7.0665817217398e-07,
      "loss": 0.0012,
      "step": 64190
    },
    {
      "epoch": 11672.727272727272,
      "grad_norm": 0.18255141377449036,
      "learning_rate": 7.065522149122709e-07,
      "loss": 0.0012,
      "step": 64200
    },
    {
      "epoch": 11674.545454545454,
      "grad_norm": 0.001490401686169207,
      "learning_rate": 7.064462464648904e-07,
      "loss": 0.0007,
      "step": 64210
    },
    {
      "epoch": 11676.363636363636,
      "grad_norm": 0.19701319932937622,
      "learning_rate": 7.063402668375767e-07,
      "loss": 0.0014,
      "step": 64220
    },
    {
      "epoch": 11678.181818181818,
      "grad_norm": 0.30807313323020935,
      "learning_rate": 7.062342760360696e-07,
      "loss": 0.0012,
      "step": 64230
    },
    {
      "epoch": 11680.0,
      "grad_norm": 0.0012101810425519943,
      "learning_rate": 7.061282740661086e-07,
      "loss": 0.0009,
      "step": 64240
    },
    {
      "epoch": 11681.818181818182,
      "grad_norm": 0.0008726544911041856,
      "learning_rate": 7.060222609334342e-07,
      "loss": 0.0009,
      "step": 64250
    },
    {
      "epoch": 11683.636363636364,
      "grad_norm": 0.19291339814662933,
      "learning_rate": 7.059162366437874e-07,
      "loss": 0.0014,
      "step": 64260
    },
    {
      "epoch": 11685.454545454546,
      "grad_norm": 0.0007749139331281185,
      "learning_rate": 7.058102012029102e-07,
      "loss": 0.001,
      "step": 64270
    },
    {
      "epoch": 11687.272727272728,
      "grad_norm": 0.0010355159174650908,
      "learning_rate": 7.057041546165443e-07,
      "loss": 0.0012,
      "step": 64280
    },
    {
      "epoch": 11689.09090909091,
      "grad_norm": 0.1730155646800995,
      "learning_rate": 7.055980968904332e-07,
      "loss": 0.0011,
      "step": 64290
    },
    {
      "epoch": 11690.90909090909,
      "grad_norm": 0.22296647727489471,
      "learning_rate": 7.054920280303198e-07,
      "loss": 0.0012,
      "step": 64300
    },
    {
      "epoch": 11692.727272727272,
      "grad_norm": 0.0011997180990874767,
      "learning_rate": 7.053859480419485e-07,
      "loss": 0.0009,
      "step": 64310
    },
    {
      "epoch": 11694.545454545454,
      "grad_norm": 0.190912663936615,
      "learning_rate": 7.052798569310639e-07,
      "loss": 0.0013,
      "step": 64320
    },
    {
      "epoch": 11696.363636363636,
      "grad_norm": 0.0009311658795922995,
      "learning_rate": 7.051737547034114e-07,
      "loss": 0.0011,
      "step": 64330
    },
    {
      "epoch": 11698.181818181818,
      "grad_norm": 0.0011564291780814528,
      "learning_rate": 7.050676413647367e-07,
      "loss": 0.0011,
      "step": 64340
    },
    {
      "epoch": 11700.0,
      "grad_norm": 0.01808619312942028,
      "learning_rate": 7.049615169207863e-07,
      "loss": 0.0012,
      "step": 64350
    },
    {
      "epoch": 11701.818181818182,
      "grad_norm": 0.00077273283386603,
      "learning_rate": 7.048553813773074e-07,
      "loss": 0.001,
      "step": 64360
    },
    {
      "epoch": 11703.636363636364,
      "grad_norm": 0.001656402018852532,
      "learning_rate": 7.047492347400477e-07,
      "loss": 0.001,
      "step": 64370
    },
    {
      "epoch": 11705.454545454546,
      "grad_norm": 0.20721237361431122,
      "learning_rate": 7.046430770147553e-07,
      "loss": 0.0011,
      "step": 64380
    },
    {
      "epoch": 11707.272727272728,
      "grad_norm": 0.18821723759174347,
      "learning_rate": 7.045369082071791e-07,
      "loss": 0.0011,
      "step": 64390
    },
    {
      "epoch": 11709.09090909091,
      "grad_norm": 0.218176931142807,
      "learning_rate": 7.044307283230689e-07,
      "loss": 0.0012,
      "step": 64400
    },
    {
      "epoch": 11710.90909090909,
      "grad_norm": 0.018797626718878746,
      "learning_rate": 7.043245373681746e-07,
      "loss": 0.0012,
      "step": 64410
    },
    {
      "epoch": 11712.727272727272,
      "grad_norm": 0.18944773077964783,
      "learning_rate": 7.042183353482467e-07,
      "loss": 0.0009,
      "step": 64420
    },
    {
      "epoch": 11714.545454545454,
      "grad_norm": 0.1882997751235962,
      "learning_rate": 7.041121222690366e-07,
      "loss": 0.0014,
      "step": 64430
    },
    {
      "epoch": 11716.363636363636,
      "grad_norm": 0.2027708739042282,
      "learning_rate": 7.040058981362965e-07,
      "loss": 0.0012,
      "step": 64440
    },
    {
      "epoch": 11718.181818181818,
      "grad_norm": 0.24499483406543732,
      "learning_rate": 7.038996629557783e-07,
      "loss": 0.0011,
      "step": 64450
    },
    {
      "epoch": 11720.0,
      "grad_norm": 0.2952222228050232,
      "learning_rate": 7.037934167332356e-07,
      "loss": 0.001,
      "step": 64460
    },
    {
      "epoch": 11721.818181818182,
      "grad_norm": 0.002176474779844284,
      "learning_rate": 7.036871594744217e-07,
      "loss": 0.001,
      "step": 64470
    },
    {
      "epoch": 11723.636363636364,
      "grad_norm": 0.0012503742473199964,
      "learning_rate": 7.035808911850914e-07,
      "loss": 0.0012,
      "step": 64480
    },
    {
      "epoch": 11725.454545454546,
      "grad_norm": 0.29699262976646423,
      "learning_rate": 7.034746118709988e-07,
      "loss": 0.0011,
      "step": 64490
    },
    {
      "epoch": 11727.272727272728,
      "grad_norm": 0.0016301573486998677,
      "learning_rate": 7.033683215379002e-07,
      "loss": 0.0011,
      "step": 64500
    },
    {
      "epoch": 11727.272727272728,
      "eval_loss": 4.968481540679932,
      "eval_runtime": 0.9494,
      "eval_samples_per_second": 10.533,
      "eval_steps_per_second": 5.267,
      "step": 64500
    },
    {
      "epoch": 11729.09090909091,
      "grad_norm": 0.0009429411147721112,
      "learning_rate": 7.032620201915509e-07,
      "loss": 0.0012,
      "step": 64510
    },
    {
      "epoch": 11730.90909090909,
      "grad_norm": 0.0013274673838168383,
      "learning_rate": 7.031557078377081e-07,
      "loss": 0.0012,
      "step": 64520
    },
    {
      "epoch": 11732.727272727272,
      "grad_norm": 0.21847327053546906,
      "learning_rate": 7.03049384482129e-07,
      "loss": 0.0012,
      "step": 64530
    },
    {
      "epoch": 11734.545454545454,
      "grad_norm": 0.20707115530967712,
      "learning_rate": 7.029430501305714e-07,
      "loss": 0.0012,
      "step": 64540
    },
    {
      "epoch": 11736.363636363636,
      "grad_norm": 0.2785450220108032,
      "learning_rate": 7.028367047887934e-07,
      "loss": 0.0012,
      "step": 64550
    },
    {
      "epoch": 11738.181818181818,
      "grad_norm": 0.2693990170955658,
      "learning_rate": 7.027303484625547e-07,
      "loss": 0.001,
      "step": 64560
    },
    {
      "epoch": 11740.0,
      "grad_norm": 0.0008711802074685693,
      "learning_rate": 7.026239811576143e-07,
      "loss": 0.0011,
      "step": 64570
    },
    {
      "epoch": 11741.818181818182,
      "grad_norm": 0.18345075845718384,
      "learning_rate": 7.025176028797328e-07,
      "loss": 0.0012,
      "step": 64580
    },
    {
      "epoch": 11743.636363636364,
      "grad_norm": 0.0012658488703891635,
      "learning_rate": 7.024112136346712e-07,
      "loss": 0.0012,
      "step": 64590
    },
    {
      "epoch": 11745.454545454546,
      "grad_norm": 0.012769844383001328,
      "learning_rate": 7.023048134281906e-07,
      "loss": 0.001,
      "step": 64600
    },
    {
      "epoch": 11747.272727272728,
      "grad_norm": 0.0021855421364307404,
      "learning_rate": 7.02198402266053e-07,
      "loss": 0.0011,
      "step": 64610
    },
    {
      "epoch": 11749.09090909091,
      "grad_norm": 0.0008240199531428516,
      "learning_rate": 7.02091980154021e-07,
      "loss": 0.001,
      "step": 64620
    },
    {
      "epoch": 11750.90909090909,
      "grad_norm": 0.0008703706553205848,
      "learning_rate": 7.019855470978582e-07,
      "loss": 0.0012,
      "step": 64630
    },
    {
      "epoch": 11752.727272727272,
      "grad_norm": 0.20293648540973663,
      "learning_rate": 7.018791031033281e-07,
      "loss": 0.0013,
      "step": 64640
    },
    {
      "epoch": 11754.545454545454,
      "grad_norm": 0.001195813063532114,
      "learning_rate": 7.017726481761951e-07,
      "loss": 0.0008,
      "step": 64650
    },
    {
      "epoch": 11756.363636363636,
      "grad_norm": 0.002209253376349807,
      "learning_rate": 7.01666182322224e-07,
      "loss": 0.0012,
      "step": 64660
    },
    {
      "epoch": 11758.181818181818,
      "grad_norm": 0.1590069681406021,
      "learning_rate": 7.015597055471808e-07,
      "loss": 0.0012,
      "step": 64670
    },
    {
      "epoch": 11760.0,
      "grad_norm": 0.0013266740133985877,
      "learning_rate": 7.014532178568313e-07,
      "loss": 0.0011,
      "step": 64680
    },
    {
      "epoch": 11761.818181818182,
      "grad_norm": 0.0032882834784686565,
      "learning_rate": 7.013467192569426e-07,
      "loss": 0.0012,
      "step": 64690
    },
    {
      "epoch": 11763.636363636364,
      "grad_norm": 0.2035716027021408,
      "learning_rate": 7.012402097532814e-07,
      "loss": 0.001,
      "step": 64700
    },
    {
      "epoch": 11765.454545454546,
      "grad_norm": 0.0007867416716180742,
      "learning_rate": 7.011336893516166e-07,
      "loss": 0.0011,
      "step": 64710
    },
    {
      "epoch": 11767.272727272728,
      "grad_norm": 0.27555444836616516,
      "learning_rate": 7.010271580577159e-07,
      "loss": 0.0014,
      "step": 64720
    },
    {
      "epoch": 11769.09090909091,
      "grad_norm": 0.0007725271279923618,
      "learning_rate": 7.009206158773489e-07,
      "loss": 0.0009,
      "step": 64730
    },
    {
      "epoch": 11770.90909090909,
      "grad_norm": 0.21867045760154724,
      "learning_rate": 7.00814062816285e-07,
      "loss": 0.0012,
      "step": 64740
    },
    {
      "epoch": 11772.727272727272,
      "grad_norm": 0.1891154944896698,
      "learning_rate": 7.007074988802946e-07,
      "loss": 0.0007,
      "step": 64750
    },
    {
      "epoch": 11774.545454545454,
      "grad_norm": 0.01117321290075779,
      "learning_rate": 7.006009240751487e-07,
      "loss": 0.0015,
      "step": 64760
    },
    {
      "epoch": 11776.363636363636,
      "grad_norm": 0.165878027677536,
      "learning_rate": 7.004943384066186e-07,
      "loss": 0.0009,
      "step": 64770
    },
    {
      "epoch": 11778.181818181818,
      "grad_norm": 0.2768472731113434,
      "learning_rate": 7.003877418804766e-07,
      "loss": 0.0014,
      "step": 64780
    },
    {
      "epoch": 11780.0,
      "grad_norm": 0.0017166343750432134,
      "learning_rate": 7.00281134502495e-07,
      "loss": 0.0009,
      "step": 64790
    },
    {
      "epoch": 11781.818181818182,
      "grad_norm": 0.21027684211730957,
      "learning_rate": 7.001745162784475e-07,
      "loss": 0.0011,
      "step": 64800
    },
    {
      "epoch": 11783.636363636364,
      "grad_norm": 0.0010823841439560056,
      "learning_rate": 7.000678872141074e-07,
      "loss": 0.0009,
      "step": 64810
    },
    {
      "epoch": 11785.454545454546,
      "grad_norm": 0.0014738565078005195,
      "learning_rate": 6.999612473152497e-07,
      "loss": 0.0012,
      "step": 64820
    },
    {
      "epoch": 11787.272727272728,
      "grad_norm": 0.20395305752754211,
      "learning_rate": 6.998545965876488e-07,
      "loss": 0.0012,
      "step": 64830
    },
    {
      "epoch": 11789.09090909091,
      "grad_norm": 0.251810759305954,
      "learning_rate": 6.997479350370807e-07,
      "loss": 0.0012,
      "step": 64840
    },
    {
      "epoch": 11790.90909090909,
      "grad_norm": 0.2168491631746292,
      "learning_rate": 6.996412626693213e-07,
      "loss": 0.0012,
      "step": 64850
    },
    {
      "epoch": 11792.727272727272,
      "grad_norm": 0.18946218490600586,
      "learning_rate": 6.995345794901476e-07,
      "loss": 0.0012,
      "step": 64860
    },
    {
      "epoch": 11794.545454545454,
      "grad_norm": 0.0031338571570813656,
      "learning_rate": 6.994278855053369e-07,
      "loss": 0.0006,
      "step": 64870
    },
    {
      "epoch": 11796.363636363636,
      "grad_norm": 0.001227638334967196,
      "learning_rate": 6.993211807206669e-07,
      "loss": 0.0012,
      "step": 64880
    },
    {
      "epoch": 11798.181818181818,
      "grad_norm": 0.006118853576481342,
      "learning_rate": 6.992144651419163e-07,
      "loss": 0.0012,
      "step": 64890
    },
    {
      "epoch": 11800.0,
      "grad_norm": 0.22564631700515747,
      "learning_rate": 6.991077387748643e-07,
      "loss": 0.0012,
      "step": 64900
    },
    {
      "epoch": 11801.818181818182,
      "grad_norm": 0.19896261394023895,
      "learning_rate": 6.990010016252903e-07,
      "loss": 0.0013,
      "step": 64910
    },
    {
      "epoch": 11803.636363636364,
      "grad_norm": 0.19622042775154114,
      "learning_rate": 6.988942536989749e-07,
      "loss": 0.0012,
      "step": 64920
    },
    {
      "epoch": 11805.454545454546,
      "grad_norm": 0.0010291925864294171,
      "learning_rate": 6.987874950016987e-07,
      "loss": 0.0009,
      "step": 64930
    },
    {
      "epoch": 11807.272727272728,
      "grad_norm": 0.18612626194953918,
      "learning_rate": 6.98680725539243e-07,
      "loss": 0.0013,
      "step": 64940
    },
    {
      "epoch": 11809.09090909091,
      "grad_norm": 0.21964344382286072,
      "learning_rate": 6.985739453173902e-07,
      "loss": 0.001,
      "step": 64950
    },
    {
      "epoch": 11810.90909090909,
      "grad_norm": 0.18953905999660492,
      "learning_rate": 6.984671543419228e-07,
      "loss": 0.0011,
      "step": 64960
    },
    {
      "epoch": 11812.727272727272,
      "grad_norm": 0.29904863238334656,
      "learning_rate": 6.983603526186238e-07,
      "loss": 0.0012,
      "step": 64970
    },
    {
      "epoch": 11814.545454545454,
      "grad_norm": 0.0014534136280417442,
      "learning_rate": 6.98253540153277e-07,
      "loss": 0.0014,
      "step": 64980
    },
    {
      "epoch": 11816.363636363636,
      "grad_norm": 0.2218937873840332,
      "learning_rate": 6.98146716951667e-07,
      "loss": 0.0008,
      "step": 64990
    },
    {
      "epoch": 11818.181818181818,
      "grad_norm": 0.002497296780347824,
      "learning_rate": 6.980398830195784e-07,
      "loss": 0.001,
      "step": 65000
    },
    {
      "epoch": 11818.181818181818,
      "eval_loss": 4.832819938659668,
      "eval_runtime": 0.9532,
      "eval_samples_per_second": 10.491,
      "eval_steps_per_second": 5.246,
      "step": 65000
    },
    {
      "epoch": 11820.0,
      "grad_norm": 0.2296539843082428,
      "learning_rate": 6.979330383627968e-07,
      "loss": 0.0012,
      "step": 65010
    },
    {
      "epoch": 11821.818181818182,
      "grad_norm": 0.0013561059022322297,
      "learning_rate": 6.978261829871084e-07,
      "loss": 0.0011,
      "step": 65020
    },
    {
      "epoch": 11823.636363636364,
      "grad_norm": 0.001078888075426221,
      "learning_rate": 6.977193168982999e-07,
      "loss": 0.0012,
      "step": 65030
    },
    {
      "epoch": 11825.454545454546,
      "grad_norm": 0.23419159650802612,
      "learning_rate": 6.976124401021582e-07,
      "loss": 0.0008,
      "step": 65040
    },
    {
      "epoch": 11827.272727272728,
      "grad_norm": 0.20369717478752136,
      "learning_rate": 6.975055526044715e-07,
      "loss": 0.0012,
      "step": 65050
    },
    {
      "epoch": 11829.09090909091,
      "grad_norm": 0.17977190017700195,
      "learning_rate": 6.973986544110281e-07,
      "loss": 0.0012,
      "step": 65060
    },
    {
      "epoch": 11830.90909090909,
      "grad_norm": 0.0015511757228523493,
      "learning_rate": 6.972917455276169e-07,
      "loss": 0.0011,
      "step": 65070
    },
    {
      "epoch": 11832.727272727272,
      "grad_norm": 0.0015376481460407376,
      "learning_rate": 6.971848259600275e-07,
      "loss": 0.0009,
      "step": 65080
    },
    {
      "epoch": 11834.545454545454,
      "grad_norm": 0.0008923608111217618,
      "learning_rate": 6.970778957140502e-07,
      "loss": 0.0012,
      "step": 65090
    },
    {
      "epoch": 11836.363636363636,
      "grad_norm": 0.3022870719432831,
      "learning_rate": 6.969709547954755e-07,
      "loss": 0.0013,
      "step": 65100
    },
    {
      "epoch": 11838.181818181818,
      "grad_norm": 0.26256367564201355,
      "learning_rate": 6.96864003210095e-07,
      "loss": 0.001,
      "step": 65110
    },
    {
      "epoch": 11840.0,
      "grad_norm": 0.19179359078407288,
      "learning_rate": 6.967570409637002e-07,
      "loss": 0.0009,
      "step": 65120
    },
    {
      "epoch": 11841.818181818182,
      "grad_norm": 0.0009143449133262038,
      "learning_rate": 6.966500680620836e-07,
      "loss": 0.0011,
      "step": 65130
    },
    {
      "epoch": 11843.636363636364,
      "grad_norm": 0.0011118925176560879,
      "learning_rate": 6.965430845110385e-07,
      "loss": 0.001,
      "step": 65140
    },
    {
      "epoch": 11845.454545454546,
      "grad_norm": 0.0011871119495481253,
      "learning_rate": 6.964360903163585e-07,
      "loss": 0.0012,
      "step": 65150
    },
    {
      "epoch": 11847.272727272728,
      "grad_norm": 0.0015441564610227942,
      "learning_rate": 6.963290854838376e-07,
      "loss": 0.001,
      "step": 65160
    },
    {
      "epoch": 11849.09090909091,
      "grad_norm": 0.2135545164346695,
      "learning_rate": 6.962220700192704e-07,
      "loss": 0.0014,
      "step": 65170
    },
    {
      "epoch": 11850.90909090909,
      "grad_norm": 0.2521298825740814,
      "learning_rate": 6.961150439284528e-07,
      "loss": 0.001,
      "step": 65180
    },
    {
      "epoch": 11852.727272727272,
      "grad_norm": 0.0027044680900871754,
      "learning_rate": 6.960080072171801e-07,
      "loss": 0.0012,
      "step": 65190
    },
    {
      "epoch": 11854.545454545454,
      "grad_norm": 0.22383709251880646,
      "learning_rate": 6.959009598912492e-07,
      "loss": 0.0009,
      "step": 65200
    },
    {
      "epoch": 11856.363636363636,
      "grad_norm": 0.17495621740818024,
      "learning_rate": 6.95793901956457e-07,
      "loss": 0.0011,
      "step": 65210
    },
    {
      "epoch": 11858.181818181818,
      "grad_norm": 0.0011524120345711708,
      "learning_rate": 6.956868334186012e-07,
      "loss": 0.001,
      "step": 65220
    },
    {
      "epoch": 11860.0,
      "grad_norm": 0.165684774518013,
      "learning_rate": 6.955797542834799e-07,
      "loss": 0.0012,
      "step": 65230
    },
    {
      "epoch": 11861.818181818182,
      "grad_norm": 0.001054997555911541,
      "learning_rate": 6.95472664556892e-07,
      "loss": 0.0011,
      "step": 65240
    },
    {
      "epoch": 11863.636363636364,
      "grad_norm": 0.0010946939000859857,
      "learning_rate": 6.953655642446367e-07,
      "loss": 0.001,
      "step": 65250
    },
    {
      "epoch": 11865.454545454546,
      "grad_norm": 0.2945004105567932,
      "learning_rate": 6.952584533525144e-07,
      "loss": 0.0012,
      "step": 65260
    },
    {
      "epoch": 11867.272727272728,
      "grad_norm": 0.2804369628429413,
      "learning_rate": 6.95151331886325e-07,
      "loss": 0.0012,
      "step": 65270
    },
    {
      "epoch": 11869.09090909091,
      "grad_norm": 0.4402739703655243,
      "learning_rate": 6.950441998518698e-07,
      "loss": 0.0011,
      "step": 65280
    },
    {
      "epoch": 11870.90909090909,
      "grad_norm": 0.1785602867603302,
      "learning_rate": 6.949370572549505e-07,
      "loss": 0.0009,
      "step": 65290
    },
    {
      "epoch": 11872.727272727272,
      "grad_norm": 0.18795126676559448,
      "learning_rate": 6.948299041013695e-07,
      "loss": 0.0012,
      "step": 65300
    },
    {
      "epoch": 11874.545454545454,
      "grad_norm": 0.002180415205657482,
      "learning_rate": 6.947227403969293e-07,
      "loss": 0.001,
      "step": 65310
    },
    {
      "epoch": 11876.363636363636,
      "grad_norm": 0.2171449065208435,
      "learning_rate": 6.946155661474332e-07,
      "loss": 0.0012,
      "step": 65320
    },
    {
      "epoch": 11878.181818181818,
      "grad_norm": 0.0020320855546742678,
      "learning_rate": 6.945083813586856e-07,
      "loss": 0.0009,
      "step": 65330
    },
    {
      "epoch": 11880.0,
      "grad_norm": 0.0014090024633333087,
      "learning_rate": 6.944011860364904e-07,
      "loss": 0.0012,
      "step": 65340
    },
    {
      "epoch": 11881.818181818182,
      "grad_norm": 0.2198084145784378,
      "learning_rate": 6.942939801866532e-07,
      "loss": 0.001,
      "step": 65350
    },
    {
      "epoch": 11883.636363636364,
      "grad_norm": 0.21176877617835999,
      "learning_rate": 6.941867638149794e-07,
      "loss": 0.0011,
      "step": 65360
    },
    {
      "epoch": 11885.454545454546,
      "grad_norm": 0.001738120918162167,
      "learning_rate": 6.940795369272753e-07,
      "loss": 0.0009,
      "step": 65370
    },
    {
      "epoch": 11887.272727272728,
      "grad_norm": 0.0016576676862314343,
      "learning_rate": 6.939722995293476e-07,
      "loss": 0.0013,
      "step": 65380
    },
    {
      "epoch": 11889.09090909091,
      "grad_norm": 0.30314308404922485,
      "learning_rate": 6.938650516270038e-07,
      "loss": 0.0012,
      "step": 65390
    },
    {
      "epoch": 11890.90909090909,
      "grad_norm": 0.22148358821868896,
      "learning_rate": 6.937577932260514e-07,
      "loss": 0.001,
      "step": 65400
    },
    {
      "epoch": 11892.727272727272,
      "grad_norm": 0.0010409998940303922,
      "learning_rate": 6.936505243322995e-07,
      "loss": 0.0007,
      "step": 65410
    },
    {
      "epoch": 11894.545454545454,
      "grad_norm": 0.23530006408691406,
      "learning_rate": 6.935432449515568e-07,
      "loss": 0.0017,
      "step": 65420
    },
    {
      "epoch": 11896.363636363636,
      "grad_norm": 0.1886957883834839,
      "learning_rate": 6.934359550896331e-07,
      "loss": 0.0008,
      "step": 65430
    },
    {
      "epoch": 11898.181818181818,
      "grad_norm": 0.17941370606422424,
      "learning_rate": 6.933286547523382e-07,
      "loss": 0.0012,
      "step": 65440
    },
    {
      "epoch": 11900.0,
      "grad_norm": 0.0007766231428831816,
      "learning_rate": 6.932213439454836e-07,
      "loss": 0.001,
      "step": 65450
    },
    {
      "epoch": 11901.818181818182,
      "grad_norm": 0.29974040389060974,
      "learning_rate": 6.9311402267488e-07,
      "loss": 0.001,
      "step": 65460
    },
    {
      "epoch": 11903.636363636364,
      "grad_norm": 0.21840736269950867,
      "learning_rate": 6.930066909463396e-07,
      "loss": 0.0013,
      "step": 65470
    },
    {
      "epoch": 11905.454545454546,
      "grad_norm": 0.19237054884433746,
      "learning_rate": 6.928993487656745e-07,
      "loss": 0.0011,
      "step": 65480
    },
    {
      "epoch": 11907.272727272728,
      "grad_norm": 0.21746370196342468,
      "learning_rate": 6.927919961386983e-07,
      "loss": 0.0012,
      "step": 65490
    },
    {
      "epoch": 11909.09090909091,
      "grad_norm": 0.0008555276435799897,
      "learning_rate": 6.926846330712241e-07,
      "loss": 0.0011,
      "step": 65500
    },
    {
      "epoch": 11909.09090909091,
      "eval_loss": 4.951150417327881,
      "eval_runtime": 0.9495,
      "eval_samples_per_second": 10.532,
      "eval_steps_per_second": 5.266,
      "step": 65500
    },
    {
      "epoch": 11910.90909090909,
      "grad_norm": 0.036808956414461136,
      "learning_rate": 6.925772595690663e-07,
      "loss": 0.0012,
      "step": 65510
    },
    {
      "epoch": 11912.727272727272,
      "grad_norm": 0.23114919662475586,
      "learning_rate": 6.924698756380397e-07,
      "loss": 0.0012,
      "step": 65520
    },
    {
      "epoch": 11914.545454545454,
      "grad_norm": 0.253329873085022,
      "learning_rate": 6.923624812839593e-07,
      "loss": 0.0008,
      "step": 65530
    },
    {
      "epoch": 11916.363636363636,
      "grad_norm": 0.20643508434295654,
      "learning_rate": 6.922550765126413e-07,
      "loss": 0.0012,
      "step": 65540
    },
    {
      "epoch": 11918.181818181818,
      "grad_norm": 0.17693136632442474,
      "learning_rate": 6.921476613299018e-07,
      "loss": 0.0011,
      "step": 65550
    },
    {
      "epoch": 11920.0,
      "grad_norm": 0.21032293140888214,
      "learning_rate": 6.920402357415581e-07,
      "loss": 0.001,
      "step": 65560
    },
    {
      "epoch": 11921.818181818182,
      "grad_norm": 0.04338812083005905,
      "learning_rate": 6.919327997534274e-07,
      "loss": 0.0012,
      "step": 65570
    },
    {
      "epoch": 11923.636363636364,
      "grad_norm": 0.2310183048248291,
      "learning_rate": 6.918253533713282e-07,
      "loss": 0.0011,
      "step": 65580
    },
    {
      "epoch": 11925.454545454546,
      "grad_norm": 0.20267042517662048,
      "learning_rate": 6.917178966010787e-07,
      "loss": 0.0009,
      "step": 65590
    },
    {
      "epoch": 11927.272727272728,
      "grad_norm": 0.0008164684986695647,
      "learning_rate": 6.916104294484987e-07,
      "loss": 0.001,
      "step": 65600
    },
    {
      "epoch": 11929.09090909091,
      "grad_norm": 0.22978734970092773,
      "learning_rate": 6.915029519194075e-07,
      "loss": 0.0012,
      "step": 65610
    },
    {
      "epoch": 11930.90909090909,
      "grad_norm": 0.0010183199774473906,
      "learning_rate": 6.913954640196258e-07,
      "loss": 0.001,
      "step": 65620
    },
    {
      "epoch": 11932.727272727272,
      "grad_norm": 0.22106719017028809,
      "learning_rate": 6.912879657549741e-07,
      "loss": 0.0013,
      "step": 65630
    },
    {
      "epoch": 11934.545454545454,
      "grad_norm": 0.2879681885242462,
      "learning_rate": 6.911804571312745e-07,
      "loss": 0.001,
      "step": 65640
    },
    {
      "epoch": 11936.363636363636,
      "grad_norm": 0.0009450070792809129,
      "learning_rate": 6.910729381543485e-07,
      "loss": 0.0009,
      "step": 65650
    },
    {
      "epoch": 11938.181818181818,
      "grad_norm": 0.0016860623145475984,
      "learning_rate": 6.909654088300191e-07,
      "loss": 0.001,
      "step": 65660
    },
    {
      "epoch": 11940.0,
      "grad_norm": 0.002126098843291402,
      "learning_rate": 6.908578691641091e-07,
      "loss": 0.0012,
      "step": 65670
    },
    {
      "epoch": 11941.818181818182,
      "grad_norm": 0.19056986272335052,
      "learning_rate": 6.907503191624426e-07,
      "loss": 0.0012,
      "step": 65680
    },
    {
      "epoch": 11943.636363636364,
      "grad_norm": 0.17379413545131683,
      "learning_rate": 6.906427588308436e-07,
      "loss": 0.0012,
      "step": 65690
    },
    {
      "epoch": 11945.454545454546,
      "grad_norm": 0.0010378219885751605,
      "learning_rate": 6.905351881751371e-07,
      "loss": 0.0009,
      "step": 65700
    },
    {
      "epoch": 11947.272727272728,
      "grad_norm": 0.2100277990102768,
      "learning_rate": 6.904276072011484e-07,
      "loss": 0.0012,
      "step": 65710
    },
    {
      "epoch": 11949.09090909091,
      "grad_norm": 0.0027182167395949364,
      "learning_rate": 6.903200159147033e-07,
      "loss": 0.0011,
      "step": 65720
    },
    {
      "epoch": 11950.90909090909,
      "grad_norm": 0.0018037984846159816,
      "learning_rate": 6.902124143216288e-07,
      "loss": 0.0012,
      "step": 65730
    },
    {
      "epoch": 11952.727272727272,
      "grad_norm": 0.0009416567627340555,
      "learning_rate": 6.901048024277516e-07,
      "loss": 0.0012,
      "step": 65740
    },
    {
      "epoch": 11954.545454545454,
      "grad_norm": 0.20532813668251038,
      "learning_rate": 6.899971802388995e-07,
      "loss": 0.001,
      "step": 65750
    },
    {
      "epoch": 11956.363636363636,
      "grad_norm": 0.001624194672331214,
      "learning_rate": 6.898895477609005e-07,
      "loss": 0.0008,
      "step": 65760
    },
    {
      "epoch": 11958.181818181818,
      "grad_norm": 0.0010334727121517062,
      "learning_rate": 6.897819049995836e-07,
      "loss": 0.0012,
      "step": 65770
    },
    {
      "epoch": 11960.0,
      "grad_norm": 0.17819719016551971,
      "learning_rate": 6.896742519607778e-07,
      "loss": 0.0012,
      "step": 65780
    },
    {
      "epoch": 11961.818181818182,
      "grad_norm": 0.20323485136032104,
      "learning_rate": 6.895665886503135e-07,
      "loss": 0.0013,
      "step": 65790
    },
    {
      "epoch": 11963.636363636364,
      "grad_norm": 0.1724047064781189,
      "learning_rate": 6.894589150740207e-07,
      "loss": 0.0012,
      "step": 65800
    },
    {
      "epoch": 11965.454545454546,
      "grad_norm": 0.17996788024902344,
      "learning_rate": 6.893512312377303e-07,
      "loss": 0.0009,
      "step": 65810
    },
    {
      "epoch": 11967.272727272728,
      "grad_norm": 0.28107884526252747,
      "learning_rate": 6.89243537147274e-07,
      "loss": 0.0012,
      "step": 65820
    },
    {
      "epoch": 11969.09090909091,
      "grad_norm": 0.0010919432388618588,
      "learning_rate": 6.89135832808484e-07,
      "loss": 0.0009,
      "step": 65830
    },
    {
      "epoch": 11970.90909090909,
      "grad_norm": 0.0018099287990480661,
      "learning_rate": 6.890281182271928e-07,
      "loss": 0.001,
      "step": 65840
    },
    {
      "epoch": 11972.727272727272,
      "grad_norm": 0.2803671956062317,
      "learning_rate": 6.889203934092335e-07,
      "loss": 0.0014,
      "step": 65850
    },
    {
      "epoch": 11974.545454545454,
      "grad_norm": 0.23543310165405273,
      "learning_rate": 6.8881265836044e-07,
      "loss": 0.0011,
      "step": 65860
    },
    {
      "epoch": 11976.363636363636,
      "grad_norm": 0.0007767039933241904,
      "learning_rate": 6.887049130866467e-07,
      "loss": 0.0009,
      "step": 65870
    },
    {
      "epoch": 11978.181818181818,
      "grad_norm": 0.000849320727866143,
      "learning_rate": 6.885971575936883e-07,
      "loss": 0.0012,
      "step": 65880
    },
    {
      "epoch": 11980.0,
      "grad_norm": 0.0012628453550860286,
      "learning_rate": 6.884893918874003e-07,
      "loss": 0.0012,
      "step": 65890
    },
    {
      "epoch": 11981.818181818182,
      "grad_norm": 0.2279752790927887,
      "learning_rate": 6.883816159736186e-07,
      "loss": 0.001,
      "step": 65900
    },
    {
      "epoch": 11983.636363636364,
      "grad_norm": 0.0014587135519832373,
      "learning_rate": 6.882738298581797e-07,
      "loss": 0.001,
      "step": 65910
    },
    {
      "epoch": 11985.454545454546,
      "grad_norm": 0.15997302532196045,
      "learning_rate": 6.881660335469206e-07,
      "loss": 0.001,
      "step": 65920
    },
    {
      "epoch": 11987.272727272728,
      "grad_norm": 0.18692247569561005,
      "learning_rate": 6.880582270456792e-07,
      "loss": 0.0013,
      "step": 65930
    },
    {
      "epoch": 11989.09090909091,
      "grad_norm": 0.18352480232715607,
      "learning_rate": 6.879504103602933e-07,
      "loss": 0.0011,
      "step": 65940
    },
    {
      "epoch": 11990.90909090909,
      "grad_norm": 0.017927369102835655,
      "learning_rate": 6.87842583496602e-07,
      "loss": 0.0012,
      "step": 65950
    },
    {
      "epoch": 11992.727272727272,
      "grad_norm": 0.1866566389799118,
      "learning_rate": 6.877347464604445e-07,
      "loss": 0.0013,
      "step": 65960
    },
    {
      "epoch": 11994.545454545454,
      "grad_norm": 0.23206709325313568,
      "learning_rate": 6.876268992576604e-07,
      "loss": 0.0013,
      "step": 65970
    },
    {
      "epoch": 11996.363636363636,
      "grad_norm": 0.2728330194950104,
      "learning_rate": 6.875190418940903e-07,
      "loss": 0.0012,
      "step": 65980
    },
    {
      "epoch": 11998.181818181818,
      "grad_norm": 0.1648411601781845,
      "learning_rate": 6.874111743755749e-07,
      "loss": 0.0009,
      "step": 65990
    },
    {
      "epoch": 12000.0,
      "grad_norm": 0.0015660912031307817,
      "learning_rate": 6.87303296707956e-07,
      "loss": 0.001,
      "step": 66000
    },
    {
      "epoch": 12000.0,
      "eval_loss": 4.989241123199463,
      "eval_runtime": 0.9486,
      "eval_samples_per_second": 10.542,
      "eval_steps_per_second": 5.271,
      "step": 66000
    },
    {
      "epoch": 12001.818181818182,
      "grad_norm": 0.2218090295791626,
      "learning_rate": 6.871954088970755e-07,
      "loss": 0.0011,
      "step": 66010
    },
    {
      "epoch": 12003.636363636364,
      "grad_norm": 0.20613694190979004,
      "learning_rate": 6.870875109487758e-07,
      "loss": 0.0011,
      "step": 66020
    },
    {
      "epoch": 12005.454545454546,
      "grad_norm": 0.0018138251034542918,
      "learning_rate": 6.869796028689001e-07,
      "loss": 0.0011,
      "step": 66030
    },
    {
      "epoch": 12007.272727272728,
      "grad_norm": 0.00078562245471403,
      "learning_rate": 6.868716846632925e-07,
      "loss": 0.0009,
      "step": 66040
    },
    {
      "epoch": 12009.09090909091,
      "grad_norm": 0.20405595004558563,
      "learning_rate": 6.867637563377966e-07,
      "loss": 0.0013,
      "step": 66050
    },
    {
      "epoch": 12010.90909090909,
      "grad_norm": 0.18684515357017517,
      "learning_rate": 6.866558178982574e-07,
      "loss": 0.001,
      "step": 66060
    },
    {
      "epoch": 12012.727272727272,
      "grad_norm": 0.1867394894361496,
      "learning_rate": 6.865478693505204e-07,
      "loss": 0.0012,
      "step": 66070
    },
    {
      "epoch": 12014.545454545454,
      "grad_norm": 0.0014503938145935535,
      "learning_rate": 6.864399107004313e-07,
      "loss": 0.0008,
      "step": 66080
    },
    {
      "epoch": 12016.363636363636,
      "grad_norm": 0.17027494311332703,
      "learning_rate": 6.863319419538365e-07,
      "loss": 0.0013,
      "step": 66090
    },
    {
      "epoch": 12018.181818181818,
      "grad_norm": 0.1894901543855667,
      "learning_rate": 6.862239631165831e-07,
      "loss": 0.0012,
      "step": 66100
    },
    {
      "epoch": 12020.0,
      "grad_norm": 0.20081692934036255,
      "learning_rate": 6.861159741945185e-07,
      "loss": 0.0011,
      "step": 66110
    },
    {
      "epoch": 12021.818181818182,
      "grad_norm": 0.18226328492164612,
      "learning_rate": 6.860079751934907e-07,
      "loss": 0.001,
      "step": 66120
    },
    {
      "epoch": 12023.636363636364,
      "grad_norm": 0.173566997051239,
      "learning_rate": 6.858999661193484e-07,
      "loss": 0.0012,
      "step": 66130
    },
    {
      "epoch": 12025.454545454546,
      "grad_norm": 0.16943611204624176,
      "learning_rate": 6.857919469779407e-07,
      "loss": 0.0009,
      "step": 66140
    },
    {
      "epoch": 12027.272727272728,
      "grad_norm": 0.20612391829490662,
      "learning_rate": 6.856839177751175e-07,
      "loss": 0.0012,
      "step": 66150
    },
    {
      "epoch": 12029.09090909091,
      "grad_norm": 0.19992399215698242,
      "learning_rate": 6.855758785167286e-07,
      "loss": 0.0012,
      "step": 66160
    },
    {
      "epoch": 12030.90909090909,
      "grad_norm": 0.0009765928843989968,
      "learning_rate": 6.854678292086251e-07,
      "loss": 0.001,
      "step": 66170
    },
    {
      "epoch": 12032.727272727272,
      "grad_norm": 0.1713189333677292,
      "learning_rate": 6.853597698566582e-07,
      "loss": 0.0012,
      "step": 66180
    },
    {
      "epoch": 12034.545454545454,
      "grad_norm": 0.17649151384830475,
      "learning_rate": 6.8525170046668e-07,
      "loss": 0.0011,
      "step": 66190
    },
    {
      "epoch": 12036.363636363636,
      "grad_norm": 0.21542055904865265,
      "learning_rate": 6.851436210445426e-07,
      "loss": 0.0012,
      "step": 66200
    },
    {
      "epoch": 12038.181818181818,
      "grad_norm": 0.21726562082767487,
      "learning_rate": 6.850355315960992e-07,
      "loss": 0.001,
      "step": 66210
    },
    {
      "epoch": 12040.0,
      "grad_norm": 0.2622761130332947,
      "learning_rate": 6.849274321272029e-07,
      "loss": 0.0012,
      "step": 66220
    },
    {
      "epoch": 12041.818181818182,
      "grad_norm": 0.0011241915635764599,
      "learning_rate": 6.848193226437085e-07,
      "loss": 0.0012,
      "step": 66230
    },
    {
      "epoch": 12043.636363636364,
      "grad_norm": 0.0016545914113521576,
      "learning_rate": 6.847112031514697e-07,
      "loss": 0.0007,
      "step": 66240
    },
    {
      "epoch": 12045.454545454546,
      "grad_norm": 0.0010334489634260535,
      "learning_rate": 6.846030736563422e-07,
      "loss": 0.0013,
      "step": 66250
    },
    {
      "epoch": 12047.272727272728,
      "grad_norm": 0.0012937737628817558,
      "learning_rate": 6.844949341641815e-07,
      "loss": 0.0009,
      "step": 66260
    },
    {
      "epoch": 12049.09090909091,
      "grad_norm": 0.22806666791439056,
      "learning_rate": 6.843867846808438e-07,
      "loss": 0.0014,
      "step": 66270
    },
    {
      "epoch": 12050.90909090909,
      "grad_norm": 0.2276933640241623,
      "learning_rate": 6.842786252121858e-07,
      "loss": 0.0011,
      "step": 66280
    },
    {
      "epoch": 12052.727272727272,
      "grad_norm": 0.18429090082645416,
      "learning_rate": 6.841704557640649e-07,
      "loss": 0.0011,
      "step": 66290
    },
    {
      "epoch": 12054.545454545454,
      "grad_norm": 0.002134466078132391,
      "learning_rate": 6.840622763423391e-07,
      "loss": 0.001,
      "step": 66300
    },
    {
      "epoch": 12056.363636363636,
      "grad_norm": 0.0022606258280575275,
      "learning_rate": 6.839540869528663e-07,
      "loss": 0.001,
      "step": 66310
    },
    {
      "epoch": 12058.181818181818,
      "grad_norm": 0.0006107590161263943,
      "learning_rate": 6.838458876015057e-07,
      "loss": 0.001,
      "step": 66320
    },
    {
      "epoch": 12060.0,
      "grad_norm": 0.0009497132850810885,
      "learning_rate": 6.837376782941166e-07,
      "loss": 0.0012,
      "step": 66330
    },
    {
      "epoch": 12061.818181818182,
      "grad_norm": 0.0010899837361648679,
      "learning_rate": 6.836294590365594e-07,
      "loss": 0.001,
      "step": 66340
    },
    {
      "epoch": 12063.636363636364,
      "grad_norm": 0.0008898663800209761,
      "learning_rate": 6.83521229834694e-07,
      "loss": 0.001,
      "step": 66350
    },
    {
      "epoch": 12065.454545454546,
      "grad_norm": 0.21521656215190887,
      "learning_rate": 6.83412990694382e-07,
      "loss": 0.0017,
      "step": 66360
    },
    {
      "epoch": 12067.272727272728,
      "grad_norm": 0.28090420365333557,
      "learning_rate": 6.833047416214847e-07,
      "loss": 0.001,
      "step": 66370
    },
    {
      "epoch": 12069.09090909091,
      "grad_norm": 0.001901972689665854,
      "learning_rate": 6.831964826218646e-07,
      "loss": 0.0009,
      "step": 66380
    },
    {
      "epoch": 12070.90909090909,
      "grad_norm": 0.0015958216972649097,
      "learning_rate": 6.830882137013838e-07,
      "loss": 0.0011,
      "step": 66390
    },
    {
      "epoch": 12072.727272727272,
      "grad_norm": 0.17967109382152557,
      "learning_rate": 6.82979934865906e-07,
      "loss": 0.0012,
      "step": 66400
    },
    {
      "epoch": 12074.545454545454,
      "grad_norm": 0.25416991114616394,
      "learning_rate": 6.828716461212948e-07,
      "loss": 0.0013,
      "step": 66410
    },
    {
      "epoch": 12076.363636363636,
      "grad_norm": 0.16524375975131989,
      "learning_rate": 6.827633474734144e-07,
      "loss": 0.0008,
      "step": 66420
    },
    {
      "epoch": 12078.181818181818,
      "grad_norm": 0.0046622054651379585,
      "learning_rate": 6.826550389281299e-07,
      "loss": 0.0011,
      "step": 66430
    },
    {
      "epoch": 12080.0,
      "grad_norm": 0.1622914969921112,
      "learning_rate": 6.825467204913063e-07,
      "loss": 0.0012,
      "step": 66440
    },
    {
      "epoch": 12081.818181818182,
      "grad_norm": 0.001086996402591467,
      "learning_rate": 6.824383921688097e-07,
      "loss": 0.0012,
      "step": 66450
    },
    {
      "epoch": 12083.636363636364,
      "grad_norm": 0.16979029774665833,
      "learning_rate": 6.823300539665067e-07,
      "loss": 0.0009,
      "step": 66460
    },
    {
      "epoch": 12085.454545454546,
      "grad_norm": 0.0024434358347207308,
      "learning_rate": 6.82221705890264e-07,
      "loss": 0.0015,
      "step": 66470
    },
    {
      "epoch": 12087.272727272728,
      "grad_norm": 0.29470527172088623,
      "learning_rate": 6.821133479459491e-07,
      "loss": 0.0011,
      "step": 66480
    },
    {
      "epoch": 12089.09090909091,
      "grad_norm": 0.26574358344078064,
      "learning_rate": 6.820049801394302e-07,
      "loss": 0.0011,
      "step": 66490
    },
    {
      "epoch": 12090.90909090909,
      "grad_norm": 0.3075527846813202,
      "learning_rate": 6.818966024765757e-07,
      "loss": 0.0011,
      "step": 66500
    },
    {
      "epoch": 12090.90909090909,
      "eval_loss": 4.906780242919922,
      "eval_runtime": 0.9559,
      "eval_samples_per_second": 10.462,
      "eval_steps_per_second": 5.231,
      "step": 66500
    },
    {
      "epoch": 12092.727272727272,
      "grad_norm": 0.22309917211532593,
      "learning_rate": 6.817882149632549e-07,
      "loss": 0.001,
      "step": 66510
    },
    {
      "epoch": 12094.545454545454,
      "grad_norm": 0.21331459283828735,
      "learning_rate": 6.816798176053373e-07,
      "loss": 0.001,
      "step": 66520
    },
    {
      "epoch": 12096.363636363636,
      "grad_norm": 0.2156224548816681,
      "learning_rate": 6.815714104086932e-07,
      "loss": 0.0011,
      "step": 66530
    },
    {
      "epoch": 12098.181818181818,
      "grad_norm": 0.0017935257637873292,
      "learning_rate": 6.814629933791931e-07,
      "loss": 0.001,
      "step": 66540
    },
    {
      "epoch": 12100.0,
      "grad_norm": 0.1708475798368454,
      "learning_rate": 6.813545665227085e-07,
      "loss": 0.0012,
      "step": 66550
    },
    {
      "epoch": 12101.818181818182,
      "grad_norm": 0.0009369651088491082,
      "learning_rate": 6.812461298451109e-07,
      "loss": 0.0012,
      "step": 66560
    },
    {
      "epoch": 12103.636363636364,
      "grad_norm": 0.003480401122942567,
      "learning_rate": 6.811376833522729e-07,
      "loss": 0.0009,
      "step": 66570
    },
    {
      "epoch": 12105.454545454546,
      "grad_norm": 0.0009846505708992481,
      "learning_rate": 6.81029227050067e-07,
      "loss": 0.0009,
      "step": 66580
    },
    {
      "epoch": 12107.272727272728,
      "grad_norm": 0.18449820578098297,
      "learning_rate": 6.809207609443669e-07,
      "loss": 0.0015,
      "step": 66590
    },
    {
      "epoch": 12109.09090909091,
      "grad_norm": 0.004018711391836405,
      "learning_rate": 6.80812285041046e-07,
      "loss": 0.001,
      "step": 66600
    },
    {
      "epoch": 12110.90909090909,
      "grad_norm": 0.0010384072083979845,
      "learning_rate": 6.807037993459794e-07,
      "loss": 0.001,
      "step": 66610
    },
    {
      "epoch": 12112.727272727272,
      "grad_norm": 0.0012005351018160582,
      "learning_rate": 6.805953038650416e-07,
      "loss": 0.0013,
      "step": 66620
    },
    {
      "epoch": 12114.545454545454,
      "grad_norm": 0.005079291760921478,
      "learning_rate": 6.804867986041083e-07,
      "loss": 0.0011,
      "step": 66630
    },
    {
      "epoch": 12116.363636363636,
      "grad_norm": 0.0012275747722014785,
      "learning_rate": 6.803782835690552e-07,
      "loss": 0.0012,
      "step": 66640
    },
    {
      "epoch": 12118.181818181818,
      "grad_norm": 0.0011524766450747848,
      "learning_rate": 6.802697587657594e-07,
      "loss": 0.0009,
      "step": 66650
    },
    {
      "epoch": 12120.0,
      "grad_norm": 0.0011075176298618317,
      "learning_rate": 6.801612242000974e-07,
      "loss": 0.0012,
      "step": 66660
    },
    {
      "epoch": 12121.818181818182,
      "grad_norm": 0.008709203451871872,
      "learning_rate": 6.80052679877947e-07,
      "loss": 0.0012,
      "step": 66670
    },
    {
      "epoch": 12123.636363636364,
      "grad_norm": 0.17749559879302979,
      "learning_rate": 6.799441258051865e-07,
      "loss": 0.0009,
      "step": 66680
    },
    {
      "epoch": 12125.454545454546,
      "grad_norm": 0.0014179425779730082,
      "learning_rate": 6.798355619876943e-07,
      "loss": 0.001,
      "step": 66690
    },
    {
      "epoch": 12127.272727272728,
      "grad_norm": 0.0011827627895399928,
      "learning_rate": 6.797269884313499e-07,
      "loss": 0.0011,
      "step": 66700
    },
    {
      "epoch": 12129.09090909091,
      "grad_norm": 0.2770736813545227,
      "learning_rate": 6.796184051420326e-07,
      "loss": 0.0014,
      "step": 66710
    },
    {
      "epoch": 12130.90909090909,
      "grad_norm": 0.0009530516108497977,
      "learning_rate": 6.795098121256231e-07,
      "loss": 0.001,
      "step": 66720
    },
    {
      "epoch": 12132.727272727272,
      "grad_norm": 0.018218176439404488,
      "learning_rate": 6.794012093880018e-07,
      "loss": 0.0012,
      "step": 66730
    },
    {
      "epoch": 12134.545454545454,
      "grad_norm": 0.0011709434911608696,
      "learning_rate": 6.792925969350503e-07,
      "loss": 0.0009,
      "step": 66740
    },
    {
      "epoch": 12136.363636363636,
      "grad_norm": 0.0008728859247639775,
      "learning_rate": 6.7918397477265e-07,
      "loss": 0.0011,
      "step": 66750
    },
    {
      "epoch": 12138.181818181818,
      "grad_norm": 0.20652523636817932,
      "learning_rate": 6.790753429066838e-07,
      "loss": 0.0012,
      "step": 66760
    },
    {
      "epoch": 12140.0,
      "grad_norm": 0.0009828839683905244,
      "learning_rate": 6.789667013430342e-07,
      "loss": 0.001,
      "step": 66770
    },
    {
      "epoch": 12141.818181818182,
      "grad_norm": 0.0018523851176723838,
      "learning_rate": 6.788580500875846e-07,
      "loss": 0.001,
      "step": 66780
    },
    {
      "epoch": 12143.636363636364,
      "grad_norm": 0.0009433728409931064,
      "learning_rate": 6.78749389146219e-07,
      "loss": 0.0013,
      "step": 66790
    },
    {
      "epoch": 12145.454545454546,
      "grad_norm": 0.0009630928980186582,
      "learning_rate": 6.78640718524822e-07,
      "loss": 0.0009,
      "step": 66800
    },
    {
      "epoch": 12147.272727272728,
      "grad_norm": 0.002073958981782198,
      "learning_rate": 6.785320382292783e-07,
      "loss": 0.0011,
      "step": 66810
    },
    {
      "epoch": 12149.09090909091,
      "grad_norm": 0.1595245897769928,
      "learning_rate": 6.784233482654735e-07,
      "loss": 0.0012,
      "step": 66820
    },
    {
      "epoch": 12150.90909090909,
      "grad_norm": 0.0008105353335849941,
      "learning_rate": 6.783146486392937e-07,
      "loss": 0.0012,
      "step": 66830
    },
    {
      "epoch": 12152.727272727272,
      "grad_norm": 0.20180100202560425,
      "learning_rate": 6.782059393566253e-07,
      "loss": 0.0012,
      "step": 66840
    },
    {
      "epoch": 12154.545454545454,
      "grad_norm": 0.1715373396873474,
      "learning_rate": 6.780972204233555e-07,
      "loss": 0.0009,
      "step": 66850
    },
    {
      "epoch": 12156.363636363636,
      "grad_norm": 0.2648647427558899,
      "learning_rate": 6.779884918453719e-07,
      "loss": 0.0015,
      "step": 66860
    },
    {
      "epoch": 12158.181818181818,
      "grad_norm": 0.20613031089305878,
      "learning_rate": 6.778797536285624e-07,
      "loss": 0.0007,
      "step": 66870
    },
    {
      "epoch": 12160.0,
      "grad_norm": 0.0010970444418489933,
      "learning_rate": 6.777710057788158e-07,
      "loss": 0.0011,
      "step": 66880
    },
    {
      "epoch": 12161.818181818182,
      "grad_norm": 0.1934230774641037,
      "learning_rate": 6.776622483020213e-07,
      "loss": 0.0012,
      "step": 66890
    },
    {
      "epoch": 12163.636363636364,
      "grad_norm": 0.20439386367797852,
      "learning_rate": 6.775534812040686e-07,
      "loss": 0.0007,
      "step": 66900
    },
    {
      "epoch": 12165.454545454546,
      "grad_norm": 0.002274196594953537,
      "learning_rate": 6.774447044908476e-07,
      "loss": 0.0013,
      "step": 66910
    },
    {
      "epoch": 12167.272727272728,
      "grad_norm": 0.0007745924522168934,
      "learning_rate": 6.773359181682492e-07,
      "loss": 0.0011,
      "step": 66920
    },
    {
      "epoch": 12169.09090909091,
      "grad_norm": 0.0015014243545010686,
      "learning_rate": 6.772271222421648e-07,
      "loss": 0.0012,
      "step": 66930
    },
    {
      "epoch": 12170.90909090909,
      "grad_norm": 0.012220448814332485,
      "learning_rate": 6.77118316718486e-07,
      "loss": 0.0012,
      "step": 66940
    },
    {
      "epoch": 12172.727272727272,
      "grad_norm": 0.0013081489596515894,
      "learning_rate": 6.77009501603105e-07,
      "loss": 0.0009,
      "step": 66950
    },
    {
      "epoch": 12174.545454545454,
      "grad_norm": 0.2053634226322174,
      "learning_rate": 6.769006769019147e-07,
      "loss": 0.0013,
      "step": 66960
    },
    {
      "epoch": 12176.363636363636,
      "grad_norm": 0.16650079190731049,
      "learning_rate": 6.767918426208084e-07,
      "loss": 0.001,
      "step": 66970
    },
    {
      "epoch": 12178.181818181818,
      "grad_norm": 0.18131089210510254,
      "learning_rate": 6.7668299876568e-07,
      "loss": 0.0012,
      "step": 66980
    },
    {
      "epoch": 12180.0,
      "grad_norm": 0.001980114495381713,
      "learning_rate": 6.765741453424236e-07,
      "loss": 0.001,
      "step": 66990
    },
    {
      "epoch": 12181.818181818182,
      "grad_norm": 0.2135574221611023,
      "learning_rate": 6.764652823569343e-07,
      "loss": 0.0012,
      "step": 67000
    },
    {
      "epoch": 12181.818181818182,
      "eval_loss": 4.905532360076904,
      "eval_runtime": 0.9545,
      "eval_samples_per_second": 10.477,
      "eval_steps_per_second": 5.238,
      "step": 67000
    },
    {
      "epoch": 12183.636363636364,
      "grad_norm": 0.22897925972938538,
      "learning_rate": 6.763564098151078e-07,
      "loss": 0.0009,
      "step": 67010
    },
    {
      "epoch": 12185.454545454546,
      "grad_norm": 0.18467335402965546,
      "learning_rate": 6.762475277228392e-07,
      "loss": 0.0013,
      "step": 67020
    },
    {
      "epoch": 12187.272727272728,
      "grad_norm": 0.0008278806344605982,
      "learning_rate": 6.761386360860257e-07,
      "loss": 0.0009,
      "step": 67030
    },
    {
      "epoch": 12189.09090909091,
      "grad_norm": 0.17274904251098633,
      "learning_rate": 6.760297349105636e-07,
      "loss": 0.0012,
      "step": 67040
    },
    {
      "epoch": 12190.90909090909,
      "grad_norm": 0.28734225034713745,
      "learning_rate": 6.759208242023509e-07,
      "loss": 0.0011,
      "step": 67050
    },
    {
      "epoch": 12192.727272727272,
      "grad_norm": 0.04193514958024025,
      "learning_rate": 6.758119039672853e-07,
      "loss": 0.0011,
      "step": 67060
    },
    {
      "epoch": 12194.545454545454,
      "grad_norm": 0.1976817101240158,
      "learning_rate": 6.757029742112652e-07,
      "loss": 0.001,
      "step": 67070
    },
    {
      "epoch": 12196.363636363636,
      "grad_norm": 0.18417692184448242,
      "learning_rate": 6.7559403494019e-07,
      "loss": 0.0014,
      "step": 67080
    },
    {
      "epoch": 12198.181818181818,
      "grad_norm": 0.2759424149990082,
      "learning_rate": 6.754850861599588e-07,
      "loss": 0.0011,
      "step": 67090
    },
    {
      "epoch": 12200.0,
      "grad_norm": 0.19823412597179413,
      "learning_rate": 6.753761278764718e-07,
      "loss": 0.001,
      "step": 67100
    },
    {
      "epoch": 12201.818181818182,
      "grad_norm": 0.2145717591047287,
      "learning_rate": 6.752671600956295e-07,
      "loss": 0.0009,
      "step": 67110
    },
    {
      "epoch": 12203.636363636364,
      "grad_norm": 0.1675092577934265,
      "learning_rate": 6.75158182823333e-07,
      "loss": 0.0015,
      "step": 67120
    },
    {
      "epoch": 12205.454545454546,
      "grad_norm": 0.21311578154563904,
      "learning_rate": 6.750491960654841e-07,
      "loss": 0.0009,
      "step": 67130
    },
    {
      "epoch": 12207.272727272728,
      "grad_norm": 0.17168787121772766,
      "learning_rate": 6.749401998279845e-07,
      "loss": 0.001,
      "step": 67140
    },
    {
      "epoch": 12209.09090909091,
      "grad_norm": 0.17227694392204285,
      "learning_rate": 6.748311941167369e-07,
      "loss": 0.001,
      "step": 67150
    },
    {
      "epoch": 12210.90909090909,
      "grad_norm": 0.27004292607307434,
      "learning_rate": 6.747221789376446e-07,
      "loss": 0.0012,
      "step": 67160
    },
    {
      "epoch": 12212.727272727272,
      "grad_norm": 0.21999816596508026,
      "learning_rate": 6.746131542966112e-07,
      "loss": 0.001,
      "step": 67170
    },
    {
      "epoch": 12214.545454545454,
      "grad_norm": 0.2824625074863434,
      "learning_rate": 6.745041201995405e-07,
      "loss": 0.0011,
      "step": 67180
    },
    {
      "epoch": 12216.363636363636,
      "grad_norm": 0.0008189431973733008,
      "learning_rate": 6.743950766523376e-07,
      "loss": 0.0013,
      "step": 67190
    },
    {
      "epoch": 12218.181818181818,
      "grad_norm": 0.0029328244272619486,
      "learning_rate": 6.742860236609076e-07,
      "loss": 0.0009,
      "step": 67200
    },
    {
      "epoch": 12220.0,
      "grad_norm": 0.22837474942207336,
      "learning_rate": 6.741769612311559e-07,
      "loss": 0.0012,
      "step": 67210
    },
    {
      "epoch": 12221.818181818182,
      "grad_norm": 0.2087940126657486,
      "learning_rate": 6.740678893689891e-07,
      "loss": 0.0009,
      "step": 67220
    },
    {
      "epoch": 12223.636363636364,
      "grad_norm": 0.1481763869524002,
      "learning_rate": 6.739588080803133e-07,
      "loss": 0.0013,
      "step": 67230
    },
    {
      "epoch": 12225.454545454546,
      "grad_norm": 0.19926872849464417,
      "learning_rate": 6.738497173710365e-07,
      "loss": 0.0012,
      "step": 67240
    },
    {
      "epoch": 12227.272727272728,
      "grad_norm": 0.17395779490470886,
      "learning_rate": 6.737406172470657e-07,
      "loss": 0.0012,
      "step": 67250
    },
    {
      "epoch": 12229.09090909091,
      "grad_norm": 0.0008132359362207353,
      "learning_rate": 6.736315077143094e-07,
      "loss": 0.0008,
      "step": 67260
    },
    {
      "epoch": 12230.90909090909,
      "grad_norm": 0.00104400678537786,
      "learning_rate": 6.735223887786766e-07,
      "loss": 0.001,
      "step": 67270
    },
    {
      "epoch": 12232.727272727272,
      "grad_norm": 0.01864461973309517,
      "learning_rate": 6.734132604460762e-07,
      "loss": 0.0013,
      "step": 67280
    },
    {
      "epoch": 12234.545454545454,
      "grad_norm": 0.0012624794617295265,
      "learning_rate": 6.733041227224181e-07,
      "loss": 0.0009,
      "step": 67290
    },
    {
      "epoch": 12236.363636363636,
      "grad_norm": 0.16914424300193787,
      "learning_rate": 6.731949756136124e-07,
      "loss": 0.0012,
      "step": 67300
    },
    {
      "epoch": 12238.181818181818,
      "grad_norm": 0.0009398756083101034,
      "learning_rate": 6.730858191255702e-07,
      "loss": 0.0009,
      "step": 67310
    },
    {
      "epoch": 12240.0,
      "grad_norm": 0.0008827417041175067,
      "learning_rate": 6.729766532642024e-07,
      "loss": 0.0012,
      "step": 67320
    },
    {
      "epoch": 12241.818181818182,
      "grad_norm": 0.17094510793685913,
      "learning_rate": 6.728674780354211e-07,
      "loss": 0.0014,
      "step": 67330
    },
    {
      "epoch": 12243.636363636364,
      "grad_norm": 0.19126808643341064,
      "learning_rate": 6.727582934451383e-07,
      "loss": 0.0012,
      "step": 67340
    },
    {
      "epoch": 12245.454545454546,
      "grad_norm": 0.1741720587015152,
      "learning_rate": 6.726490994992673e-07,
      "loss": 0.0011,
      "step": 67350
    },
    {
      "epoch": 12247.272727272728,
      "grad_norm": 0.0020112309139221907,
      "learning_rate": 6.72539896203721e-07,
      "loss": 0.0007,
      "step": 67360
    },
    {
      "epoch": 12249.09090909091,
      "grad_norm": 0.27724555134773254,
      "learning_rate": 6.724306835644132e-07,
      "loss": 0.0014,
      "step": 67370
    },
    {
      "epoch": 12250.90909090909,
      "grad_norm": 0.0010490879649296403,
      "learning_rate": 6.723214615872585e-07,
      "loss": 0.0009,
      "step": 67380
    },
    {
      "epoch": 12252.727272727272,
      "grad_norm": 0.0010300123831257224,
      "learning_rate": 6.722122302781715e-07,
      "loss": 0.0012,
      "step": 67390
    },
    {
      "epoch": 12254.545454545454,
      "grad_norm": 0.2181217521429062,
      "learning_rate": 6.721029896430677e-07,
      "loss": 0.0012,
      "step": 67400
    },
    {
      "epoch": 12256.363636363636,
      "grad_norm": 0.0016772056696936488,
      "learning_rate": 6.719937396878627e-07,
      "loss": 0.0009,
      "step": 67410
    },
    {
      "epoch": 12258.181818181818,
      "grad_norm": 0.0007819582242518663,
      "learning_rate": 6.71884480418473e-07,
      "loss": 0.001,
      "step": 67420
    },
    {
      "epoch": 12260.0,
      "grad_norm": 0.0010200460674241185,
      "learning_rate": 6.717752118408157e-07,
      "loss": 0.0012,
      "step": 67430
    },
    {
      "epoch": 12261.818181818182,
      "grad_norm": 0.23030920326709747,
      "learning_rate": 6.716659339608076e-07,
      "loss": 0.0011,
      "step": 67440
    },
    {
      "epoch": 12263.636363636364,
      "grad_norm": 0.001562673831358552,
      "learning_rate": 6.715566467843669e-07,
      "loss": 0.0009,
      "step": 67450
    },
    {
      "epoch": 12265.454545454546,
      "grad_norm": 0.000830699282232672,
      "learning_rate": 6.714473503174121e-07,
      "loss": 0.0013,
      "step": 67460
    },
    {
      "epoch": 12267.272727272728,
      "grad_norm": 0.06251596659421921,
      "learning_rate": 6.713380445658617e-07,
      "loss": 0.0013,
      "step": 67470
    },
    {
      "epoch": 12269.09090909091,
      "grad_norm": 0.0011800144566223025,
      "learning_rate": 6.712287295356353e-07,
      "loss": 0.0009,
      "step": 67480
    },
    {
      "epoch": 12270.90909090909,
      "grad_norm": 0.0018378294771537185,
      "learning_rate": 6.711194052326527e-07,
      "loss": 0.0011,
      "step": 67490
    },
    {
      "epoch": 12272.727272727272,
      "grad_norm": 0.0009883120656013489,
      "learning_rate": 6.710100716628344e-07,
      "loss": 0.0011,
      "step": 67500
    },
    {
      "epoch": 12272.727272727272,
      "eval_loss": 4.938327789306641,
      "eval_runtime": 0.9521,
      "eval_samples_per_second": 10.503,
      "eval_steps_per_second": 5.251,
      "step": 67500
    },
    {
      "epoch": 12274.545454545454,
      "grad_norm": 0.0014641863526776433,
      "learning_rate": 6.70900728832101e-07,
      "loss": 0.0012,
      "step": 67510
    },
    {
      "epoch": 12276.363636363636,
      "grad_norm": 0.18112322688102722,
      "learning_rate": 6.707913767463743e-07,
      "loss": 0.0013,
      "step": 67520
    },
    {
      "epoch": 12278.181818181818,
      "grad_norm": 0.20444126427173615,
      "learning_rate": 6.706820154115754e-07,
      "loss": 0.0009,
      "step": 67530
    },
    {
      "epoch": 12280.0,
      "grad_norm": 0.0008175148977898061,
      "learning_rate": 6.705726448336277e-07,
      "loss": 0.001,
      "step": 67540
    },
    {
      "epoch": 12281.818181818182,
      "grad_norm": 0.0025326840113848448,
      "learning_rate": 6.704632650184532e-07,
      "loss": 0.0011,
      "step": 67550
    },
    {
      "epoch": 12283.636363636364,
      "grad_norm": 0.17852284014225006,
      "learning_rate": 6.703538759719759e-07,
      "loss": 0.0012,
      "step": 67560
    },
    {
      "epoch": 12285.454545454546,
      "grad_norm": 0.011129675433039665,
      "learning_rate": 6.702444777001191e-07,
      "loss": 0.0012,
      "step": 67570
    },
    {
      "epoch": 12287.272727272728,
      "grad_norm": 0.1741141974925995,
      "learning_rate": 6.701350702088078e-07,
      "loss": 0.001,
      "step": 67580
    },
    {
      "epoch": 12289.09090909091,
      "grad_norm": 0.001108878175728023,
      "learning_rate": 6.700256535039664e-07,
      "loss": 0.0009,
      "step": 67590
    },
    {
      "epoch": 12290.90909090909,
      "grad_norm": 0.1873914748430252,
      "learning_rate": 6.699162275915206e-07,
      "loss": 0.0011,
      "step": 67600
    },
    {
      "epoch": 12292.727272727272,
      "grad_norm": 0.2845804989337921,
      "learning_rate": 6.69806792477396e-07,
      "loss": 0.0014,
      "step": 67610
    },
    {
      "epoch": 12294.545454545454,
      "grad_norm": 0.0010926190298050642,
      "learning_rate": 6.69697348167519e-07,
      "loss": 0.0007,
      "step": 67620
    },
    {
      "epoch": 12296.363636363636,
      "grad_norm": 0.18486841022968292,
      "learning_rate": 6.695878946678167e-07,
      "loss": 0.0012,
      "step": 67630
    },
    {
      "epoch": 12298.181818181818,
      "grad_norm": 0.19828537106513977,
      "learning_rate": 6.694784319842163e-07,
      "loss": 0.0011,
      "step": 67640
    },
    {
      "epoch": 12300.0,
      "grad_norm": 0.0022029245737940073,
      "learning_rate": 6.693689601226458e-07,
      "loss": 0.001,
      "step": 67650
    },
    {
      "epoch": 12301.818181818182,
      "grad_norm": 0.0007494866149500012,
      "learning_rate": 6.692594790890333e-07,
      "loss": 0.0009,
      "step": 67660
    },
    {
      "epoch": 12303.636363636364,
      "grad_norm": 0.2258647382259369,
      "learning_rate": 6.691499888893079e-07,
      "loss": 0.0012,
      "step": 67670
    },
    {
      "epoch": 12305.454545454546,
      "grad_norm": 0.17689640820026398,
      "learning_rate": 6.690404895293987e-07,
      "loss": 0.0012,
      "step": 67680
    },
    {
      "epoch": 12307.272727272728,
      "grad_norm": 0.0009142540511675179,
      "learning_rate": 6.689309810152358e-07,
      "loss": 0.0011,
      "step": 67690
    },
    {
      "epoch": 12309.09090909091,
      "grad_norm": 0.009765168651938438,
      "learning_rate": 6.688214633527495e-07,
      "loss": 0.0012,
      "step": 67700
    },
    {
      "epoch": 12310.90909090909,
      "grad_norm": 0.2791806757450104,
      "learning_rate": 6.687119365478706e-07,
      "loss": 0.0009,
      "step": 67710
    },
    {
      "epoch": 12312.727272727272,
      "grad_norm": 0.19938571751117706,
      "learning_rate": 6.686024006065303e-07,
      "loss": 0.0014,
      "step": 67720
    },
    {
      "epoch": 12314.545454545454,
      "grad_norm": 0.2149026095867157,
      "learning_rate": 6.684928555346608e-07,
      "loss": 0.0011,
      "step": 67730
    },
    {
      "epoch": 12316.363636363636,
      "grad_norm": 0.0014674525009468198,
      "learning_rate": 6.683833013381941e-07,
      "loss": 0.0009,
      "step": 67740
    },
    {
      "epoch": 12318.181818181818,
      "grad_norm": 0.1689477115869522,
      "learning_rate": 6.682737380230632e-07,
      "loss": 0.0012,
      "step": 67750
    },
    {
      "epoch": 12320.0,
      "grad_norm": 0.17013461887836456,
      "learning_rate": 6.681641655952012e-07,
      "loss": 0.001,
      "step": 67760
    },
    {
      "epoch": 12321.818181818182,
      "grad_norm": 0.014776225201785564,
      "learning_rate": 6.680545840605421e-07,
      "loss": 0.0012,
      "step": 67770
    },
    {
      "epoch": 12323.636363636364,
      "grad_norm": 0.2106143832206726,
      "learning_rate": 6.679449934250202e-07,
      "loss": 0.0011,
      "step": 67780
    },
    {
      "epoch": 12325.454545454546,
      "grad_norm": 0.0007983377436175942,
      "learning_rate": 6.678353936945703e-07,
      "loss": 0.0007,
      "step": 67790
    },
    {
      "epoch": 12327.272727272728,
      "grad_norm": 0.2700023949146271,
      "learning_rate": 6.677257848751276e-07,
      "loss": 0.0015,
      "step": 67800
    },
    {
      "epoch": 12329.09090909091,
      "grad_norm": 0.25905394554138184,
      "learning_rate": 6.676161669726277e-07,
      "loss": 0.0011,
      "step": 67810
    },
    {
      "epoch": 12330.90909090909,
      "grad_norm": 0.17035998404026031,
      "learning_rate": 6.675065399930072e-07,
      "loss": 0.0009,
      "step": 67820
    },
    {
      "epoch": 12332.727272727272,
      "grad_norm": 0.20438843965530396,
      "learning_rate": 6.673969039422029e-07,
      "loss": 0.001,
      "step": 67830
    },
    {
      "epoch": 12334.545454545454,
      "grad_norm": 0.21549035608768463,
      "learning_rate": 6.672872588261517e-07,
      "loss": 0.001,
      "step": 67840
    },
    {
      "epoch": 12336.363636363636,
      "grad_norm": 0.15998472273349762,
      "learning_rate": 6.671776046507915e-07,
      "loss": 0.0015,
      "step": 67850
    },
    {
      "epoch": 12338.181818181818,
      "grad_norm": 0.16284877061843872,
      "learning_rate": 6.670679414220608e-07,
      "loss": 0.0008,
      "step": 67860
    },
    {
      "epoch": 12340.0,
      "grad_norm": 0.0009346726583316922,
      "learning_rate": 6.669582691458979e-07,
      "loss": 0.001,
      "step": 67870
    },
    {
      "epoch": 12341.818181818182,
      "grad_norm": 0.17375630140304565,
      "learning_rate": 6.668485878282422e-07,
      "loss": 0.001,
      "step": 67880
    },
    {
      "epoch": 12343.636363636364,
      "grad_norm": 0.0017097906675189734,
      "learning_rate": 6.667388974750335e-07,
      "loss": 0.0012,
      "step": 67890
    },
    {
      "epoch": 12345.454545454546,
      "grad_norm": 0.15628571808338165,
      "learning_rate": 6.666291980922121e-07,
      "loss": 0.001,
      "step": 67900
    },
    {
      "epoch": 12347.272727272728,
      "grad_norm": 0.001447595190256834,
      "learning_rate": 6.665194896857181e-07,
      "loss": 0.0009,
      "step": 67910
    },
    {
      "epoch": 12349.09090909091,
      "grad_norm": 0.0011617806740105152,
      "learning_rate": 6.664097722614933e-07,
      "loss": 0.0012,
      "step": 67920
    },
    {
      "epoch": 12350.90909090909,
      "grad_norm": 0.26282697916030884,
      "learning_rate": 6.663000458254791e-07,
      "loss": 0.0012,
      "step": 67930
    },
    {
      "epoch": 12352.727272727272,
      "grad_norm": 0.3001672923564911,
      "learning_rate": 6.661903103836177e-07,
      "loss": 0.0012,
      "step": 67940
    },
    {
      "epoch": 12354.545454545454,
      "grad_norm": 0.0012160460464656353,
      "learning_rate": 6.660805659418516e-07,
      "loss": 0.0006,
      "step": 67950
    },
    {
      "epoch": 12356.363636363636,
      "grad_norm": 0.0012682063970714808,
      "learning_rate": 6.659708125061241e-07,
      "loss": 0.0014,
      "step": 67960
    },
    {
      "epoch": 12358.181818181818,
      "grad_norm": 0.0008280218462459743,
      "learning_rate": 6.658610500823787e-07,
      "loss": 0.001,
      "step": 67970
    },
    {
      "epoch": 12360.0,
      "grad_norm": 0.1713491976261139,
      "learning_rate": 6.657512786765598e-07,
      "loss": 0.0012,
      "step": 67980
    },
    {
      "epoch": 12361.818181818182,
      "grad_norm": 0.0013760545989498496,
      "learning_rate": 6.656414982946115e-07,
      "loss": 0.0009,
      "step": 67990
    },
    {
      "epoch": 12363.636363636364,
      "grad_norm": 0.017515338957309723,
      "learning_rate": 6.65531708942479e-07,
      "loss": 0.0013,
      "step": 68000
    },
    {
      "epoch": 12363.636363636364,
      "eval_loss": 4.928593158721924,
      "eval_runtime": 0.9544,
      "eval_samples_per_second": 10.478,
      "eval_steps_per_second": 5.239,
      "step": 68000
    },
    {
      "epoch": 12365.454545454546,
      "grad_norm": 0.16526231169700623,
      "learning_rate": 6.654219106261081e-07,
      "loss": 0.0011,
      "step": 68010
    },
    {
      "epoch": 12367.272727272728,
      "grad_norm": 0.2600114047527313,
      "learning_rate": 6.653121033514447e-07,
      "loss": 0.0012,
      "step": 68020
    },
    {
      "epoch": 12369.09090909091,
      "grad_norm": 0.2008085697889328,
      "learning_rate": 6.652022871244353e-07,
      "loss": 0.0009,
      "step": 68030
    },
    {
      "epoch": 12370.90909090909,
      "grad_norm": 0.2018783539533615,
      "learning_rate": 6.650924619510267e-07,
      "loss": 0.0012,
      "step": 68040
    },
    {
      "epoch": 12372.727272727272,
      "grad_norm": 0.0012827488826587796,
      "learning_rate": 6.64982627837167e-07,
      "loss": 0.0011,
      "step": 68050
    },
    {
      "epoch": 12374.545454545454,
      "grad_norm": 0.2731914818286896,
      "learning_rate": 6.648727847888036e-07,
      "loss": 0.001,
      "step": 68060
    },
    {
      "epoch": 12376.363636363636,
      "grad_norm": 0.0013274207012727857,
      "learning_rate": 6.647629328118851e-07,
      "loss": 0.0009,
      "step": 68070
    },
    {
      "epoch": 12378.181818181818,
      "grad_norm": 0.0010707925539463758,
      "learning_rate": 6.646530719123607e-07,
      "loss": 0.0012,
      "step": 68080
    },
    {
      "epoch": 12380.0,
      "grad_norm": 0.17194323241710663,
      "learning_rate": 6.645432020961795e-07,
      "loss": 0.0014,
      "step": 68090
    },
    {
      "epoch": 12381.818181818182,
      "grad_norm": 0.00156869413331151,
      "learning_rate": 6.644333233692916e-07,
      "loss": 0.0011,
      "step": 68100
    },
    {
      "epoch": 12383.636363636364,
      "grad_norm": 0.23111756145954132,
      "learning_rate": 6.643234357376474e-07,
      "loss": 0.0011,
      "step": 68110
    },
    {
      "epoch": 12385.454545454546,
      "grad_norm": 0.20465631783008575,
      "learning_rate": 6.642135392071976e-07,
      "loss": 0.0012,
      "step": 68120
    },
    {
      "epoch": 12387.272727272728,
      "grad_norm": 0.26637792587280273,
      "learning_rate": 6.641036337838939e-07,
      "loss": 0.0012,
      "step": 68130
    },
    {
      "epoch": 12389.09090909091,
      "grad_norm": 0.19959551095962524,
      "learning_rate": 6.639937194736877e-07,
      "loss": 0.0011,
      "step": 68140
    },
    {
      "epoch": 12390.90909090909,
      "grad_norm": 0.16987408697605133,
      "learning_rate": 6.638837962825316e-07,
      "loss": 0.0011,
      "step": 68150
    },
    {
      "epoch": 12392.727272727272,
      "grad_norm": 0.0009999097092077136,
      "learning_rate": 6.637738642163783e-07,
      "loss": 0.0009,
      "step": 68160
    },
    {
      "epoch": 12394.545454545454,
      "grad_norm": 0.1985657662153244,
      "learning_rate": 6.636639232811814e-07,
      "loss": 0.0012,
      "step": 68170
    },
    {
      "epoch": 12396.363636363636,
      "grad_norm": 0.0008528498001396656,
      "learning_rate": 6.635539734828942e-07,
      "loss": 0.001,
      "step": 68180
    },
    {
      "epoch": 12398.181818181818,
      "grad_norm": 0.15770985186100006,
      "learning_rate": 6.634440148274711e-07,
      "loss": 0.0013,
      "step": 68190
    },
    {
      "epoch": 12400.0,
      "grad_norm": 0.17626729607582092,
      "learning_rate": 6.633340473208672e-07,
      "loss": 0.0012,
      "step": 68200
    },
    {
      "epoch": 12401.818181818182,
      "grad_norm": 0.21579422056674957,
      "learning_rate": 6.632240709690371e-07,
      "loss": 0.0013,
      "step": 68210
    },
    {
      "epoch": 12403.636363636364,
      "grad_norm": 0.0010690286289900541,
      "learning_rate": 6.631140857779367e-07,
      "loss": 0.0011,
      "step": 68220
    },
    {
      "epoch": 12405.454545454546,
      "grad_norm": 0.0027900682762265205,
      "learning_rate": 6.630040917535223e-07,
      "loss": 0.0009,
      "step": 68230
    },
    {
      "epoch": 12407.272727272728,
      "grad_norm": 0.27547961473464966,
      "learning_rate": 6.628940889017505e-07,
      "loss": 0.0014,
      "step": 68240
    },
    {
      "epoch": 12409.09090909091,
      "grad_norm": 0.19179673492908478,
      "learning_rate": 6.627840772285783e-07,
      "loss": 0.0008,
      "step": 68250
    },
    {
      "epoch": 12410.90909090909,
      "grad_norm": 0.0011687027290463448,
      "learning_rate": 6.626740567399634e-07,
      "loss": 0.001,
      "step": 68260
    },
    {
      "epoch": 12412.727272727272,
      "grad_norm": 0.16631409525871277,
      "learning_rate": 6.625640274418638e-07,
      "loss": 0.0012,
      "step": 68270
    },
    {
      "epoch": 12414.545454545454,
      "grad_norm": 0.0010826458455994725,
      "learning_rate": 6.624539893402382e-07,
      "loss": 0.0012,
      "step": 68280
    },
    {
      "epoch": 12416.363636363636,
      "grad_norm": 0.2172471582889557,
      "learning_rate": 6.623439424410455e-07,
      "loss": 0.0011,
      "step": 68290
    },
    {
      "epoch": 12418.181818181818,
      "grad_norm": 0.21704532206058502,
      "learning_rate": 6.622338867502451e-07,
      "loss": 0.001,
      "step": 68300
    },
    {
      "epoch": 12420.0,
      "grad_norm": 0.20438387989997864,
      "learning_rate": 6.621238222737971e-07,
      "loss": 0.0011,
      "step": 68310
    },
    {
      "epoch": 12421.818181818182,
      "grad_norm": 0.001410087221302092,
      "learning_rate": 6.620137490176622e-07,
      "loss": 0.0009,
      "step": 68320
    },
    {
      "epoch": 12423.636363636364,
      "grad_norm": 0.19172629714012146,
      "learning_rate": 6.619036669878009e-07,
      "loss": 0.0014,
      "step": 68330
    },
    {
      "epoch": 12425.454545454546,
      "grad_norm": 0.0023433275055140257,
      "learning_rate": 6.617935761901747e-07,
      "loss": 0.0009,
      "step": 68340
    },
    {
      "epoch": 12427.272727272728,
      "grad_norm": 0.012155672535300255,
      "learning_rate": 6.616834766307457e-07,
      "loss": 0.0014,
      "step": 68350
    },
    {
      "epoch": 12429.09090909091,
      "grad_norm": 0.0008569209603592753,
      "learning_rate": 6.61573368315476e-07,
      "loss": 0.0009,
      "step": 68360
    },
    {
      "epoch": 12430.90909090909,
      "grad_norm": 0.18718531727790833,
      "learning_rate": 6.614632512503288e-07,
      "loss": 0.0012,
      "step": 68370
    },
    {
      "epoch": 12432.727272727272,
      "grad_norm": 0.2051689773797989,
      "learning_rate": 6.613531254412668e-07,
      "loss": 0.0009,
      "step": 68380
    },
    {
      "epoch": 12434.545454545454,
      "grad_norm": 0.0010370054515078664,
      "learning_rate": 6.612429908942545e-07,
      "loss": 0.0012,
      "step": 68390
    },
    {
      "epoch": 12436.363636363636,
      "grad_norm": 0.00669959606602788,
      "learning_rate": 6.611328476152556e-07,
      "loss": 0.0013,
      "step": 68400
    },
    {
      "epoch": 12438.181818181818,
      "grad_norm": 0.008659177459776402,
      "learning_rate": 6.610226956102349e-07,
      "loss": 0.0008,
      "step": 68410
    },
    {
      "epoch": 12440.0,
      "grad_norm": 0.0018586911028251052,
      "learning_rate": 6.609125348851577e-07,
      "loss": 0.0012,
      "step": 68420
    },
    {
      "epoch": 12441.818181818182,
      "grad_norm": 0.22389809787273407,
      "learning_rate": 6.608023654459898e-07,
      "loss": 0.0009,
      "step": 68430
    },
    {
      "epoch": 12443.636363636364,
      "grad_norm": 0.01725577749311924,
      "learning_rate": 6.606921872986973e-07,
      "loss": 0.0012,
      "step": 68440
    },
    {
      "epoch": 12445.454545454546,
      "grad_norm": 0.0012936292914673686,
      "learning_rate": 6.605820004492466e-07,
      "loss": 0.0009,
      "step": 68450
    },
    {
      "epoch": 12447.272727272728,
      "grad_norm": 0.2740159332752228,
      "learning_rate": 6.604718049036047e-07,
      "loss": 0.0015,
      "step": 68460
    },
    {
      "epoch": 12449.09090909091,
      "grad_norm": 0.0010528725106269121,
      "learning_rate": 6.603616006677395e-07,
      "loss": 0.0009,
      "step": 68470
    },
    {
      "epoch": 12450.90909090909,
      "grad_norm": 0.21430544555187225,
      "learning_rate": 6.602513877476189e-07,
      "loss": 0.0012,
      "step": 68480
    },
    {
      "epoch": 12452.727272727272,
      "grad_norm": 0.21212738752365112,
      "learning_rate": 6.601411661492113e-07,
      "loss": 0.0012,
      "step": 68490
    },
    {
      "epoch": 12454.545454545454,
      "grad_norm": 0.0020019866060465574,
      "learning_rate": 6.600309358784857e-07,
      "loss": 0.0008,
      "step": 68500
    },
    {
      "epoch": 12454.545454545454,
      "eval_loss": 4.883238792419434,
      "eval_runtime": 0.9536,
      "eval_samples_per_second": 10.487,
      "eval_steps_per_second": 5.243,
      "step": 68500
    },
    {
      "epoch": 12456.363636363636,
      "grad_norm": 0.001574167050421238,
      "learning_rate": 6.599206969414117e-07,
      "loss": 0.0012,
      "step": 68510
    },
    {
      "epoch": 12458.181818181818,
      "grad_norm": 0.0008985213935375214,
      "learning_rate": 6.598104493439589e-07,
      "loss": 0.0012,
      "step": 68520
    },
    {
      "epoch": 12460.0,
      "grad_norm": 0.16692589223384857,
      "learning_rate": 6.59700193092098e-07,
      "loss": 0.0012,
      "step": 68530
    },
    {
      "epoch": 12461.818181818182,
      "grad_norm": 0.21382947266101837,
      "learning_rate": 6.595899281917995e-07,
      "loss": 0.0012,
      "step": 68540
    },
    {
      "epoch": 12463.636363636364,
      "grad_norm": 0.0007676387904211879,
      "learning_rate": 6.59479654649035e-07,
      "loss": 0.0009,
      "step": 68550
    },
    {
      "epoch": 12465.454545454546,
      "grad_norm": 0.0010729897767305374,
      "learning_rate": 6.593693724697761e-07,
      "loss": 0.0011,
      "step": 68560
    },
    {
      "epoch": 12467.272727272728,
      "grad_norm": 0.0007096274057403207,
      "learning_rate": 6.59259081659995e-07,
      "loss": 0.001,
      "step": 68570
    },
    {
      "epoch": 12469.09090909091,
      "grad_norm": 0.2662855088710785,
      "learning_rate": 6.591487822256647e-07,
      "loss": 0.0014,
      "step": 68580
    },
    {
      "epoch": 12470.90909090909,
      "grad_norm": 0.0007336105336435139,
      "learning_rate": 6.590384741727582e-07,
      "loss": 0.0009,
      "step": 68590
    },
    {
      "epoch": 12472.727272727272,
      "grad_norm": 0.00045691756531596184,
      "learning_rate": 6.58928157507249e-07,
      "loss": 0.0012,
      "step": 68600
    },
    {
      "epoch": 12474.545454545454,
      "grad_norm": 0.18008120357990265,
      "learning_rate": 6.588178322351112e-07,
      "loss": 0.0011,
      "step": 68610
    },
    {
      "epoch": 12476.363636363636,
      "grad_norm": 0.1658656746149063,
      "learning_rate": 6.587074983623199e-07,
      "loss": 0.001,
      "step": 68620
    },
    {
      "epoch": 12478.181818181818,
      "grad_norm": 0.0007947311969473958,
      "learning_rate": 6.585971558948495e-07,
      "loss": 0.001,
      "step": 68630
    },
    {
      "epoch": 12480.0,
      "grad_norm": 0.0034382629673928022,
      "learning_rate": 6.584868048386759e-07,
      "loss": 0.0012,
      "step": 68640
    },
    {
      "epoch": 12481.818181818182,
      "grad_norm": 0.18800708651542664,
      "learning_rate": 6.583764451997748e-07,
      "loss": 0.001,
      "step": 68650
    },
    {
      "epoch": 12483.636363636364,
      "grad_norm": 0.19357463717460632,
      "learning_rate": 6.58266076984123e-07,
      "loss": 0.0015,
      "step": 68660
    },
    {
      "epoch": 12485.454545454546,
      "grad_norm": 0.0009996861917898059,
      "learning_rate": 6.58155700197697e-07,
      "loss": 0.0006,
      "step": 68670
    },
    {
      "epoch": 12487.272727272728,
      "grad_norm": 0.21381279826164246,
      "learning_rate": 6.580453148464745e-07,
      "loss": 0.0016,
      "step": 68680
    },
    {
      "epoch": 12489.09090909091,
      "grad_norm": 0.20187124609947205,
      "learning_rate": 6.579349209364331e-07,
      "loss": 0.0007,
      "step": 68690
    },
    {
      "epoch": 12490.90909090909,
      "grad_norm": 0.01747567020356655,
      "learning_rate": 6.578245184735512e-07,
      "loss": 0.0012,
      "step": 68700
    },
    {
      "epoch": 12492.727272727272,
      "grad_norm": 0.017841914668679237,
      "learning_rate": 6.577141074638075e-07,
      "loss": 0.0012,
      "step": 68710
    },
    {
      "epoch": 12494.545454545454,
      "grad_norm": 0.003248940221965313,
      "learning_rate": 6.576036879131814e-07,
      "loss": 0.0008,
      "step": 68720
    },
    {
      "epoch": 12496.363636363636,
      "grad_norm": 0.0011624455219134688,
      "learning_rate": 6.574932598276524e-07,
      "loss": 0.001,
      "step": 68730
    },
    {
      "epoch": 12498.181818181818,
      "grad_norm": 0.18363837897777557,
      "learning_rate": 6.573828232132006e-07,
      "loss": 0.0013,
      "step": 68740
    },
    {
      "epoch": 12500.0,
      "grad_norm": 0.2683269679546356,
      "learning_rate": 6.572723780758068e-07,
      "loss": 0.0011,
      "step": 68750
    },
    {
      "epoch": 12501.818181818182,
      "grad_norm": 0.21754740178585052,
      "learning_rate": 6.57161924421452e-07,
      "loss": 0.0011,
      "step": 68760
    },
    {
      "epoch": 12503.636363636364,
      "grad_norm": 0.19360984861850739,
      "learning_rate": 6.570514622561176e-07,
      "loss": 0.0012,
      "step": 68770
    },
    {
      "epoch": 12505.454545454546,
      "grad_norm": 0.1492302268743515,
      "learning_rate": 6.569409915857858e-07,
      "loss": 0.0013,
      "step": 68780
    },
    {
      "epoch": 12507.272727272728,
      "grad_norm": 0.16728569567203522,
      "learning_rate": 6.568305124164388e-07,
      "loss": 0.0012,
      "step": 68790
    },
    {
      "epoch": 12509.09090909091,
      "grad_norm": 0.01308958325535059,
      "learning_rate": 6.567200247540598e-07,
      "loss": 0.0012,
      "step": 68800
    },
    {
      "epoch": 12510.90909090909,
      "grad_norm": 0.17738023400306702,
      "learning_rate": 6.56609528604632e-07,
      "loss": 0.0011,
      "step": 68810
    },
    {
      "epoch": 12512.727272727272,
      "grad_norm": 0.011183136142790318,
      "learning_rate": 6.564990239741391e-07,
      "loss": 0.0009,
      "step": 68820
    },
    {
      "epoch": 12514.545454545454,
      "grad_norm": 0.20601166784763336,
      "learning_rate": 6.563885108685657e-07,
      "loss": 0.0012,
      "step": 68830
    },
    {
      "epoch": 12516.363636363636,
      "grad_norm": 0.14326336979866028,
      "learning_rate": 6.562779892938964e-07,
      "loss": 0.0012,
      "step": 68840
    },
    {
      "epoch": 12518.181818181818,
      "grad_norm": 0.0009164070361293852,
      "learning_rate": 6.561674592561163e-07,
      "loss": 0.001,
      "step": 68850
    },
    {
      "epoch": 12520.0,
      "grad_norm": 0.20091040432453156,
      "learning_rate": 6.560569207612112e-07,
      "loss": 0.0012,
      "step": 68860
    },
    {
      "epoch": 12521.818181818182,
      "grad_norm": 0.21384604275226593,
      "learning_rate": 6.559463738151673e-07,
      "loss": 0.0012,
      "step": 68870
    },
    {
      "epoch": 12523.636363636364,
      "grad_norm": 0.0011929285246878862,
      "learning_rate": 6.558358184239709e-07,
      "loss": 0.0009,
      "step": 68880
    },
    {
      "epoch": 12525.454545454546,
      "grad_norm": 0.20412902534008026,
      "learning_rate": 6.557252545936094e-07,
      "loss": 0.0012,
      "step": 68890
    },
    {
      "epoch": 12527.272727272728,
      "grad_norm": 0.01099306344985962,
      "learning_rate": 6.5561468233007e-07,
      "loss": 0.0013,
      "step": 68900
    },
    {
      "epoch": 12529.09090909091,
      "grad_norm": 0.0021695520263165236,
      "learning_rate": 6.555041016393409e-07,
      "loss": 0.0007,
      "step": 68910
    },
    {
      "epoch": 12530.90909090909,
      "grad_norm": 0.2026796191930771,
      "learning_rate": 6.553935125274101e-07,
      "loss": 0.0011,
      "step": 68920
    },
    {
      "epoch": 12532.727272727272,
      "grad_norm": 0.001229328801855445,
      "learning_rate": 6.55282915000267e-07,
      "loss": 0.001,
      "step": 68930
    },
    {
      "epoch": 12534.545454545454,
      "grad_norm": 0.0018369967583566904,
      "learning_rate": 6.551723090639006e-07,
      "loss": 0.0009,
      "step": 68940
    },
    {
      "epoch": 12536.363636363636,
      "grad_norm": 0.0008378734346479177,
      "learning_rate": 6.550616947243009e-07,
      "loss": 0.0013,
      "step": 68950
    },
    {
      "epoch": 12538.181818181818,
      "grad_norm": 0.002580939792096615,
      "learning_rate": 6.549510719874577e-07,
      "loss": 0.0012,
      "step": 68960
    },
    {
      "epoch": 12540.0,
      "grad_norm": 0.21567219495773315,
      "learning_rate": 6.548404408593621e-07,
      "loss": 0.0012,
      "step": 68970
    },
    {
      "epoch": 12541.818181818182,
      "grad_norm": 0.2281738817691803,
      "learning_rate": 6.547298013460052e-07,
      "loss": 0.0012,
      "step": 68980
    },
    {
      "epoch": 12543.636363636364,
      "grad_norm": 0.003157044295221567,
      "learning_rate": 6.546191534533783e-07,
      "loss": 0.0009,
      "step": 68990
    },
    {
      "epoch": 12545.454545454546,
      "grad_norm": 0.010707981884479523,
      "learning_rate": 6.545084971874736e-07,
      "loss": 0.0011,
      "step": 69000
    },
    {
      "epoch": 12545.454545454546,
      "eval_loss": 4.878233432769775,
      "eval_runtime": 0.9512,
      "eval_samples_per_second": 10.513,
      "eval_steps_per_second": 5.257,
      "step": 69000
    },
    {
      "epoch": 12547.272727272728,
      "grad_norm": 0.0011437209323048592,
      "learning_rate": 6.543978325542839e-07,
      "loss": 0.0012,
      "step": 69010
    },
    {
      "epoch": 12549.09090909091,
      "grad_norm": 0.41470950841903687,
      "learning_rate": 6.542871595598019e-07,
      "loss": 0.0015,
      "step": 69020
    },
    {
      "epoch": 12550.90909090909,
      "grad_norm": 0.001182860229164362,
      "learning_rate": 6.541764782100208e-07,
      "loss": 0.0009,
      "step": 69030
    },
    {
      "epoch": 12552.727272727272,
      "grad_norm": 0.27396950125694275,
      "learning_rate": 6.540657885109347e-07,
      "loss": 0.0012,
      "step": 69040
    },
    {
      "epoch": 12554.545454545454,
      "grad_norm": 0.0010510683059692383,
      "learning_rate": 6.539550904685379e-07,
      "loss": 0.0008,
      "step": 69050
    },
    {
      "epoch": 12556.363636363636,
      "grad_norm": 0.25598910450935364,
      "learning_rate": 6.538443840888253e-07,
      "loss": 0.0014,
      "step": 69060
    },
    {
      "epoch": 12558.181818181818,
      "grad_norm": 0.21477538347244263,
      "learning_rate": 6.53733669377792e-07,
      "loss": 0.0009,
      "step": 69070
    },
    {
      "epoch": 12560.0,
      "grad_norm": 0.001681582652963698,
      "learning_rate": 6.536229463414334e-07,
      "loss": 0.0011,
      "step": 69080
    },
    {
      "epoch": 12561.818181818182,
      "grad_norm": 0.006207322236150503,
      "learning_rate": 6.535122149857459e-07,
      "loss": 0.0012,
      "step": 69090
    },
    {
      "epoch": 12563.636363636364,
      "grad_norm": 0.0022925978992134333,
      "learning_rate": 6.534014753167262e-07,
      "loss": 0.0007,
      "step": 69100
    },
    {
      "epoch": 12565.454545454546,
      "grad_norm": 0.0012568540405482054,
      "learning_rate": 6.53290727340371e-07,
      "loss": 0.0012,
      "step": 69110
    },
    {
      "epoch": 12567.272727272728,
      "grad_norm": 0.0009963756892830133,
      "learning_rate": 6.531799710626778e-07,
      "loss": 0.0011,
      "step": 69120
    },
    {
      "epoch": 12569.09090909091,
      "grad_norm": 0.41416624188423157,
      "learning_rate": 6.530692064896448e-07,
      "loss": 0.0015,
      "step": 69130
    },
    {
      "epoch": 12570.90909090909,
      "grad_norm": 0.28595679998397827,
      "learning_rate": 6.529584336272702e-07,
      "loss": 0.0009,
      "step": 69140
    },
    {
      "epoch": 12572.727272727272,
      "grad_norm": 0.170771986246109,
      "learning_rate": 6.528476524815528e-07,
      "loss": 0.001,
      "step": 69150
    },
    {
      "epoch": 12574.545454545454,
      "grad_norm": 0.0014544649748131633,
      "learning_rate": 6.527368630584919e-07,
      "loss": 0.0014,
      "step": 69160
    },
    {
      "epoch": 12576.363636363636,
      "grad_norm": 0.0009445399045944214,
      "learning_rate": 6.526260653640872e-07,
      "loss": 0.0009,
      "step": 69170
    },
    {
      "epoch": 12578.181818181818,
      "grad_norm": 0.002010559430345893,
      "learning_rate": 6.525152594043388e-07,
      "loss": 0.001,
      "step": 69180
    },
    {
      "epoch": 12580.0,
      "grad_norm": 0.17054443061351776,
      "learning_rate": 6.524044451852473e-07,
      "loss": 0.0012,
      "step": 69190
    },
    {
      "epoch": 12581.818181818182,
      "grad_norm": 0.15571430325508118,
      "learning_rate": 6.522936227128138e-07,
      "loss": 0.0012,
      "step": 69200
    },
    {
      "epoch": 12583.636363636364,
      "grad_norm": 0.0010315327672287822,
      "learning_rate": 6.521827919930401e-07,
      "loss": 0.0009,
      "step": 69210
    },
    {
      "epoch": 12585.454545454546,
      "grad_norm": 0.002320988103747368,
      "learning_rate": 6.520719530319276e-07,
      "loss": 0.0012,
      "step": 69220
    },
    {
      "epoch": 12587.272727272728,
      "grad_norm": 0.0011092505883425474,
      "learning_rate": 6.51961105835479e-07,
      "loss": 0.0012,
      "step": 69230
    },
    {
      "epoch": 12589.09090909091,
      "grad_norm": 0.0013078944757580757,
      "learning_rate": 6.518502504096971e-07,
      "loss": 0.0012,
      "step": 69240
    },
    {
      "epoch": 12590.90909090909,
      "grad_norm": 0.27121981978416443,
      "learning_rate": 6.517393867605854e-07,
      "loss": 0.0012,
      "step": 69250
    },
    {
      "epoch": 12592.727272727272,
      "grad_norm": 0.002113248687237501,
      "learning_rate": 6.516285148941472e-07,
      "loss": 0.0009,
      "step": 69260
    },
    {
      "epoch": 12594.545454545454,
      "grad_norm": 0.1597425490617752,
      "learning_rate": 6.51517634816387e-07,
      "loss": 0.0013,
      "step": 69270
    },
    {
      "epoch": 12596.363636363636,
      "grad_norm": 0.15590430796146393,
      "learning_rate": 6.514067465333093e-07,
      "loss": 0.0009,
      "step": 69280
    },
    {
      "epoch": 12598.181818181818,
      "grad_norm": 0.0012425645254552364,
      "learning_rate": 6.512958500509193e-07,
      "loss": 0.0011,
      "step": 69290
    },
    {
      "epoch": 12600.0,
      "grad_norm": 0.16327078640460968,
      "learning_rate": 6.511849453752223e-07,
      "loss": 0.0012,
      "step": 69300
    },
    {
      "epoch": 12601.818181818182,
      "grad_norm": 0.0015485933981835842,
      "learning_rate": 6.510740325122244e-07,
      "loss": 0.0009,
      "step": 69310
    },
    {
      "epoch": 12603.636363636364,
      "grad_norm": 0.0009361827978864312,
      "learning_rate": 6.509631114679318e-07,
      "loss": 0.0014,
      "step": 69320
    },
    {
      "epoch": 12605.454545454546,
      "grad_norm": 0.2054336816072464,
      "learning_rate": 6.508521822483517e-07,
      "loss": 0.0009,
      "step": 69330
    },
    {
      "epoch": 12607.272727272728,
      "grad_norm": 0.0010566005948930979,
      "learning_rate": 6.507412448594912e-07,
      "loss": 0.0011,
      "step": 69340
    },
    {
      "epoch": 12609.09090909091,
      "grad_norm": 0.0025461919140070677,
      "learning_rate": 6.506302993073579e-07,
      "loss": 0.0012,
      "step": 69350
    },
    {
      "epoch": 12610.90909090909,
      "grad_norm": 0.32035282254219055,
      "learning_rate": 6.505193455979603e-07,
      "loss": 0.0044,
      "step": 69360
    },
    {
      "epoch": 12612.727272727272,
      "grad_norm": 0.004309343174099922,
      "learning_rate": 6.504083837373066e-07,
      "loss": 0.0009,
      "step": 69370
    },
    {
      "epoch": 12614.545454545454,
      "grad_norm": 0.023456763476133347,
      "learning_rate": 6.502974137314062e-07,
      "loss": 0.0015,
      "step": 69380
    },
    {
      "epoch": 12616.363636363636,
      "grad_norm": 0.3094024658203125,
      "learning_rate": 6.501864355862681e-07,
      "loss": 0.0011,
      "step": 69390
    },
    {
      "epoch": 12618.181818181818,
      "grad_norm": 0.01150087546557188,
      "learning_rate": 6.500754493079028e-07,
      "loss": 0.0017,
      "step": 69400
    },
    {
      "epoch": 12620.0,
      "grad_norm": 0.2500603497028351,
      "learning_rate": 6.499644549023204e-07,
      "loss": 0.0014,
      "step": 69410
    },
    {
      "epoch": 12621.818181818182,
      "grad_norm": 0.01653788797557354,
      "learning_rate": 6.498534523755319e-07,
      "loss": 0.0012,
      "step": 69420
    },
    {
      "epoch": 12623.636363636364,
      "grad_norm": 0.17232540249824524,
      "learning_rate": 6.497424417335482e-07,
      "loss": 0.0014,
      "step": 69430
    },
    {
      "epoch": 12625.454545454546,
      "grad_norm": 0.0027537166606634855,
      "learning_rate": 6.496314229823813e-07,
      "loss": 0.001,
      "step": 69440
    },
    {
      "epoch": 12627.272727272728,
      "grad_norm": 0.07458065450191498,
      "learning_rate": 6.495203961280433e-07,
      "loss": 0.0009,
      "step": 69450
    },
    {
      "epoch": 12629.09090909091,
      "grad_norm": 0.026812855154275894,
      "learning_rate": 6.494093611765468e-07,
      "loss": 0.0012,
      "step": 69460
    },
    {
      "epoch": 12630.90909090909,
      "grad_norm": 0.005650101229548454,
      "learning_rate": 6.492983181339045e-07,
      "loss": 0.0012,
      "step": 69470
    },
    {
      "epoch": 12632.727272727272,
      "grad_norm": 0.007125521078705788,
      "learning_rate": 6.491872670061301e-07,
      "loss": 0.0012,
      "step": 69480
    },
    {
      "epoch": 12634.545454545454,
      "grad_norm": 0.010936660692095757,
      "learning_rate": 6.490762077992376e-07,
      "loss": 0.001,
      "step": 69490
    },
    {
      "epoch": 12636.363636363636,
      "grad_norm": 0.2052503526210785,
      "learning_rate": 6.489651405192409e-07,
      "loss": 0.0009,
      "step": 69500
    },
    {
      "epoch": 12636.363636363636,
      "eval_loss": 5.065428733825684,
      "eval_runtime": 0.9594,
      "eval_samples_per_second": 10.423,
      "eval_steps_per_second": 5.212,
      "step": 69500
    },
    {
      "epoch": 12638.181818181818,
      "grad_norm": 0.004637637175619602,
      "learning_rate": 6.488540651721553e-07,
      "loss": 0.0011,
      "step": 69510
    },
    {
      "epoch": 12640.0,
      "grad_norm": 0.0029937210492789745,
      "learning_rate": 6.487429817639956e-07,
      "loss": 0.0012,
      "step": 69520
    },
    {
      "epoch": 12641.818181818182,
      "grad_norm": 0.21691283583641052,
      "learning_rate": 6.486318903007777e-07,
      "loss": 0.0011,
      "step": 69530
    },
    {
      "epoch": 12643.636363636364,
      "grad_norm": 0.16933606564998627,
      "learning_rate": 6.485207907885174e-07,
      "loss": 0.0012,
      "step": 69540
    },
    {
      "epoch": 12645.454545454546,
      "grad_norm": 0.02883232571184635,
      "learning_rate": 6.484096832332316e-07,
      "loss": 0.0011,
      "step": 69550
    },
    {
      "epoch": 12647.272727272728,
      "grad_norm": 0.27284398674964905,
      "learning_rate": 6.482985676409367e-07,
      "loss": 0.0012,
      "step": 69560
    },
    {
      "epoch": 12649.09090909091,
      "grad_norm": 0.21453151106834412,
      "learning_rate": 6.481874440176504e-07,
      "loss": 0.0009,
      "step": 69570
    },
    {
      "epoch": 12650.90909090909,
      "grad_norm": 0.24555173516273499,
      "learning_rate": 6.480763123693905e-07,
      "loss": 0.0011,
      "step": 69580
    },
    {
      "epoch": 12652.727272727272,
      "grad_norm": 0.004063115455210209,
      "learning_rate": 6.479651727021753e-07,
      "loss": 0.0009,
      "step": 69590
    },
    {
      "epoch": 12654.545454545454,
      "grad_norm": 0.09383774548768997,
      "learning_rate": 6.478540250220233e-07,
      "loss": 0.0011,
      "step": 69600
    },
    {
      "epoch": 12656.363636363636,
      "grad_norm": 0.17779886722564697,
      "learning_rate": 6.477428693349539e-07,
      "loss": 0.0012,
      "step": 69610
    },
    {
      "epoch": 12658.181818181818,
      "grad_norm": 0.004293831065297127,
      "learning_rate": 6.476317056469862e-07,
      "loss": 0.0011,
      "step": 69620
    },
    {
      "epoch": 12660.0,
      "grad_norm": 0.21550065279006958,
      "learning_rate": 6.475205339641406e-07,
      "loss": 0.0013,
      "step": 69630
    },
    {
      "epoch": 12661.818181818182,
      "grad_norm": 0.22603362798690796,
      "learning_rate": 6.474093542924374e-07,
      "loss": 0.0009,
      "step": 69640
    },
    {
      "epoch": 12663.636363636364,
      "grad_norm": 0.0019529842538759112,
      "learning_rate": 6.472981666378974e-07,
      "loss": 0.0011,
      "step": 69650
    },
    {
      "epoch": 12665.454545454546,
      "grad_norm": 0.0029911308083683252,
      "learning_rate": 6.471869710065417e-07,
      "loss": 0.0015,
      "step": 69660
    },
    {
      "epoch": 12667.272727272728,
      "grad_norm": 0.035882964730262756,
      "learning_rate": 6.470757674043924e-07,
      "loss": 0.0013,
      "step": 69670
    },
    {
      "epoch": 12669.09090909091,
      "grad_norm": 0.00919611006975174,
      "learning_rate": 6.469645558374714e-07,
      "loss": 0.0008,
      "step": 69680
    },
    {
      "epoch": 12670.90909090909,
      "grad_norm": 0.006709475535899401,
      "learning_rate": 6.468533363118014e-07,
      "loss": 0.0012,
      "step": 69690
    },
    {
      "epoch": 12672.727272727272,
      "grad_norm": 0.01858069933950901,
      "learning_rate": 6.467421088334052e-07,
      "loss": 0.0012,
      "step": 69700
    },
    {
      "epoch": 12674.545454545454,
      "grad_norm": 0.2660579979419708,
      "learning_rate": 6.466308734083064e-07,
      "loss": 0.001,
      "step": 69710
    },
    {
      "epoch": 12676.363636363636,
      "grad_norm": 0.26327773928642273,
      "learning_rate": 6.465196300425286e-07,
      "loss": 0.0012,
      "step": 69720
    },
    {
      "epoch": 12678.181818181818,
      "grad_norm": 0.0015107685467228293,
      "learning_rate": 6.464083787420967e-07,
      "loss": 0.0009,
      "step": 69730
    },
    {
      "epoch": 12680.0,
      "grad_norm": 0.0051745763048529625,
      "learning_rate": 6.462971195130348e-07,
      "loss": 0.0012,
      "step": 69740
    },
    {
      "epoch": 12681.818181818182,
      "grad_norm": 0.21299852430820465,
      "learning_rate": 6.461858523613684e-07,
      "loss": 0.0012,
      "step": 69750
    },
    {
      "epoch": 12683.636363636364,
      "grad_norm": 0.21842879056930542,
      "learning_rate": 6.46074577293123e-07,
      "loss": 0.001,
      "step": 69760
    },
    {
      "epoch": 12685.454545454546,
      "grad_norm": 0.0020599064882844687,
      "learning_rate": 6.459632943143245e-07,
      "loss": 0.001,
      "step": 69770
    },
    {
      "epoch": 12687.272727272728,
      "grad_norm": 0.001816366333514452,
      "learning_rate": 6.458520034309995e-07,
      "loss": 0.0011,
      "step": 69780
    },
    {
      "epoch": 12689.09090909091,
      "grad_norm": 0.0017244465416297317,
      "learning_rate": 6.457407046491747e-07,
      "loss": 0.0012,
      "step": 69790
    },
    {
      "epoch": 12690.90909090909,
      "grad_norm": 0.19857798516750336,
      "learning_rate": 6.456293979748777e-07,
      "loss": 0.0012,
      "step": 69800
    },
    {
      "epoch": 12692.727272727272,
      "grad_norm": 0.003303281730040908,
      "learning_rate": 6.455180834141359e-07,
      "loss": 0.0007,
      "step": 69810
    },
    {
      "epoch": 12694.545454545454,
      "grad_norm": 0.20315788686275482,
      "learning_rate": 6.454067609729776e-07,
      "loss": 0.0013,
      "step": 69820
    },
    {
      "epoch": 12696.363636363636,
      "grad_norm": 0.002741447417065501,
      "learning_rate": 6.452954306574314e-07,
      "loss": 0.0009,
      "step": 69830
    },
    {
      "epoch": 12698.181818181818,
      "grad_norm": 0.0029951841570436954,
      "learning_rate": 6.451840924735263e-07,
      "loss": 0.0013,
      "step": 69840
    },
    {
      "epoch": 12700.0,
      "grad_norm": 0.0029631636571139097,
      "learning_rate": 6.450727464272916e-07,
      "loss": 0.0012,
      "step": 69850
    },
    {
      "epoch": 12701.818181818182,
      "grad_norm": 0.16532015800476074,
      "learning_rate": 6.449613925247573e-07,
      "loss": 0.0012,
      "step": 69860
    },
    {
      "epoch": 12703.636363636364,
      "grad_norm": 0.16522400081157684,
      "learning_rate": 6.448500307719537e-07,
      "loss": 0.0011,
      "step": 69870
    },
    {
      "epoch": 12705.454545454546,
      "grad_norm": 0.17178228497505188,
      "learning_rate": 6.447386611749115e-07,
      "loss": 0.0011,
      "step": 69880
    },
    {
      "epoch": 12707.272727272728,
      "grad_norm": 0.001797530916519463,
      "learning_rate": 6.446272837396616e-07,
      "loss": 0.0009,
      "step": 69890
    },
    {
      "epoch": 12709.09090909091,
      "grad_norm": 0.1557396948337555,
      "learning_rate": 6.445158984722358e-07,
      "loss": 0.0014,
      "step": 69900
    },
    {
      "epoch": 12710.90909090909,
      "grad_norm": 0.21470864117145538,
      "learning_rate": 6.444045053786659e-07,
      "loss": 0.0012,
      "step": 69910
    },
    {
      "epoch": 12712.727272727272,
      "grad_norm": 0.21353204548358917,
      "learning_rate": 6.442931044649847e-07,
      "loss": 0.001,
      "step": 69920
    },
    {
      "epoch": 12714.545454545454,
      "grad_norm": 0.21441562473773956,
      "learning_rate": 6.441816957372245e-07,
      "loss": 0.001,
      "step": 69930
    },
    {
      "epoch": 12716.363636363636,
      "grad_norm": 0.15120594203472137,
      "learning_rate": 6.44070279201419e-07,
      "loss": 0.0012,
      "step": 69940
    },
    {
      "epoch": 12718.181818181818,
      "grad_norm": 0.16500899195671082,
      "learning_rate": 6.439588548636016e-07,
      "loss": 0.001,
      "step": 69950
    },
    {
      "epoch": 12720.0,
      "grad_norm": 0.001024743658490479,
      "learning_rate": 6.438474227298064e-07,
      "loss": 0.0011,
      "step": 69960
    },
    {
      "epoch": 12721.818181818182,
      "grad_norm": 0.16777192056179047,
      "learning_rate": 6.43735982806068e-07,
      "loss": 0.0012,
      "step": 69970
    },
    {
      "epoch": 12723.636363636364,
      "grad_norm": 0.011698844842612743,
      "learning_rate": 6.436245350984213e-07,
      "loss": 0.0012,
      "step": 69980
    },
    {
      "epoch": 12725.454545454546,
      "grad_norm": 0.1727217137813568,
      "learning_rate": 6.435130796129018e-07,
      "loss": 0.0009,
      "step": 69990
    },
    {
      "epoch": 12727.272727272728,
      "grad_norm": 0.0010843620402738452,
      "learning_rate": 6.434016163555451e-07,
      "loss": 0.0009,
      "step": 70000
    },
    {
      "epoch": 12727.272727272728,
      "eval_loss": 4.995287895202637,
      "eval_runtime": 0.9527,
      "eval_samples_per_second": 10.497,
      "eval_steps_per_second": 5.248,
      "step": 70000
    },
    {
      "epoch": 12729.09090909091,
      "grad_norm": 0.16986507177352905,
      "learning_rate": 6.432901453323876e-07,
      "loss": 0.0013,
      "step": 70010
    },
    {
      "epoch": 12730.90909090909,
      "grad_norm": 0.0016071907011792064,
      "learning_rate": 6.431786665494657e-07,
      "loss": 0.0012,
      "step": 70020
    },
    {
      "epoch": 12732.727272727272,
      "grad_norm": 0.0015311777824535966,
      "learning_rate": 6.430671800128165e-07,
      "loss": 0.0009,
      "step": 70030
    },
    {
      "epoch": 12734.545454545454,
      "grad_norm": 0.0013093444285914302,
      "learning_rate": 6.429556857284776e-07,
      "loss": 0.001,
      "step": 70040
    },
    {
      "epoch": 12736.363636363636,
      "grad_norm": 0.16342708468437195,
      "learning_rate": 6.428441837024868e-07,
      "loss": 0.0014,
      "step": 70050
    },
    {
      "epoch": 12738.181818181818,
      "grad_norm": 0.0016219289973378181,
      "learning_rate": 6.427326739408822e-07,
      "loss": 0.0011,
      "step": 70060
    },
    {
      "epoch": 12740.0,
      "grad_norm": 0.001505439868196845,
      "learning_rate": 6.42621156449703e-07,
      "loss": 0.0013,
      "step": 70070
    },
    {
      "epoch": 12741.818181818182,
      "grad_norm": 0.0025033371057361364,
      "learning_rate": 6.42509631234988e-07,
      "loss": 0.0012,
      "step": 70080
    },
    {
      "epoch": 12743.636363636364,
      "grad_norm": 0.18008048832416534,
      "learning_rate": 6.423980983027768e-07,
      "loss": 0.0009,
      "step": 70090
    },
    {
      "epoch": 12745.454545454546,
      "grad_norm": 0.004000348038971424,
      "learning_rate": 6.422865576591094e-07,
      "loss": 0.001,
      "step": 70100
    },
    {
      "epoch": 12747.272727272728,
      "grad_norm": 0.18614737689495087,
      "learning_rate": 6.421750093100263e-07,
      "loss": 0.0014,
      "step": 70110
    },
    {
      "epoch": 12749.09090909091,
      "grad_norm": 0.0012570716207847,
      "learning_rate": 6.420634532615681e-07,
      "loss": 0.0009,
      "step": 70120
    },
    {
      "epoch": 12750.90909090909,
      "grad_norm": 0.006692610681056976,
      "learning_rate": 6.419518895197762e-07,
      "loss": 0.0012,
      "step": 70130
    },
    {
      "epoch": 12752.727272727272,
      "grad_norm": 0.0009773981291800737,
      "learning_rate": 6.418403180906922e-07,
      "loss": 0.0011,
      "step": 70140
    },
    {
      "epoch": 12754.545454545454,
      "grad_norm": 0.1727922409772873,
      "learning_rate": 6.41728738980358e-07,
      "loss": 0.0009,
      "step": 70150
    },
    {
      "epoch": 12756.363636363636,
      "grad_norm": 0.2864937484264374,
      "learning_rate": 6.416171521948164e-07,
      "loss": 0.0015,
      "step": 70160
    },
    {
      "epoch": 12758.181818181818,
      "grad_norm": 0.26755666732788086,
      "learning_rate": 6.4150555774011e-07,
      "loss": 0.0011,
      "step": 70170
    },
    {
      "epoch": 12760.0,
      "grad_norm": 0.0360637903213501,
      "learning_rate": 6.413939556222825e-07,
      "loss": 0.0009,
      "step": 70180
    },
    {
      "epoch": 12761.818181818182,
      "grad_norm": 0.19632089138031006,
      "learning_rate": 6.412823458473771e-07,
      "loss": 0.0012,
      "step": 70190
    },
    {
      "epoch": 12763.636363636364,
      "grad_norm": 0.19809985160827637,
      "learning_rate": 6.411707284214383e-07,
      "loss": 0.0009,
      "step": 70200
    },
    {
      "epoch": 12765.454545454546,
      "grad_norm": 0.21884369850158691,
      "learning_rate": 6.410591033505104e-07,
      "loss": 0.0012,
      "step": 70210
    },
    {
      "epoch": 12767.272727272728,
      "grad_norm": 0.16357511281967163,
      "learning_rate": 6.409474706406387e-07,
      "loss": 0.0014,
      "step": 70220
    },
    {
      "epoch": 12769.09090909091,
      "grad_norm": 0.1650499403476715,
      "learning_rate": 6.408358302978683e-07,
      "loss": 0.001,
      "step": 70230
    },
    {
      "epoch": 12770.90909090909,
      "grad_norm": 0.0017816744511947036,
      "learning_rate": 6.40724182328245e-07,
      "loss": 0.0011,
      "step": 70240
    },
    {
      "epoch": 12772.727272727272,
      "grad_norm": 0.16721180081367493,
      "learning_rate": 6.406125267378153e-07,
      "loss": 0.001,
      "step": 70250
    },
    {
      "epoch": 12774.545454545454,
      "grad_norm": 0.2147466093301773,
      "learning_rate": 6.405008635326255e-07,
      "loss": 0.001,
      "step": 70260
    },
    {
      "epoch": 12776.363636363636,
      "grad_norm": 0.011596654541790485,
      "learning_rate": 6.403891927187228e-07,
      "loss": 0.0014,
      "step": 70270
    },
    {
      "epoch": 12778.181818181818,
      "grad_norm": 0.19079026579856873,
      "learning_rate": 6.402775143021546e-07,
      "loss": 0.0011,
      "step": 70280
    },
    {
      "epoch": 12780.0,
      "grad_norm": 0.21288995444774628,
      "learning_rate": 6.401658282889688e-07,
      "loss": 0.0012,
      "step": 70290
    },
    {
      "epoch": 12781.818181818182,
      "grad_norm": 0.18875345587730408,
      "learning_rate": 6.400541346852135e-07,
      "loss": 0.0012,
      "step": 70300
    },
    {
      "epoch": 12783.636363636364,
      "grad_norm": 0.0012004744494333863,
      "learning_rate": 6.399424334969375e-07,
      "loss": 0.0008,
      "step": 70310
    },
    {
      "epoch": 12785.454545454546,
      "grad_norm": 0.15965107083320618,
      "learning_rate": 6.3983072473019e-07,
      "loss": 0.0013,
      "step": 70320
    },
    {
      "epoch": 12787.272727272728,
      "grad_norm": 0.0024872212670743465,
      "learning_rate": 6.397190083910203e-07,
      "loss": 0.0012,
      "step": 70330
    },
    {
      "epoch": 12789.09090909091,
      "grad_norm": 0.43005257844924927,
      "learning_rate": 6.396072844854784e-07,
      "loss": 0.0012,
      "step": 70340
    },
    {
      "epoch": 12790.90909090909,
      "grad_norm": 0.001988956006243825,
      "learning_rate": 6.394955530196147e-07,
      "loss": 0.001,
      "step": 70350
    },
    {
      "epoch": 12792.727272727272,
      "grad_norm": 0.1690775752067566,
      "learning_rate": 6.393838139994796e-07,
      "loss": 0.0009,
      "step": 70360
    },
    {
      "epoch": 12794.545454545454,
      "grad_norm": 0.018564607948064804,
      "learning_rate": 6.392720674311248e-07,
      "loss": 0.0012,
      "step": 70370
    },
    {
      "epoch": 12796.363636363636,
      "grad_norm": 0.0013421423500403762,
      "learning_rate": 6.391603133206014e-07,
      "loss": 0.001,
      "step": 70380
    },
    {
      "epoch": 12798.181818181818,
      "grad_norm": 0.21353447437286377,
      "learning_rate": 6.390485516739615e-07,
      "loss": 0.0013,
      "step": 70390
    },
    {
      "epoch": 12800.0,
      "grad_norm": 0.16756173968315125,
      "learning_rate": 6.389367824972573e-07,
      "loss": 0.0011,
      "step": 70400
    },
    {
      "epoch": 12801.818181818182,
      "grad_norm": 0.22794093191623688,
      "learning_rate": 6.388250057965419e-07,
      "loss": 0.0007,
      "step": 70410
    },
    {
      "epoch": 12803.636363636364,
      "grad_norm": 0.0025414596311748028,
      "learning_rate": 6.387132215778683e-07,
      "loss": 0.0013,
      "step": 70420
    },
    {
      "epoch": 12805.454545454546,
      "grad_norm": 0.0007975205080583692,
      "learning_rate": 6.3860142984729e-07,
      "loss": 0.0011,
      "step": 70430
    },
    {
      "epoch": 12807.272727272728,
      "grad_norm": 0.26851221919059753,
      "learning_rate": 6.384896306108612e-07,
      "loss": 0.0015,
      "step": 70440
    },
    {
      "epoch": 12809.09090909091,
      "grad_norm": 0.2179165482521057,
      "learning_rate": 6.38377823874636e-07,
      "loss": 0.0009,
      "step": 70450
    },
    {
      "epoch": 12810.90909090909,
      "grad_norm": 0.20426228642463684,
      "learning_rate": 6.382660096446695e-07,
      "loss": 0.001,
      "step": 70460
    },
    {
      "epoch": 12812.727272727272,
      "grad_norm": 0.2715323865413666,
      "learning_rate": 6.381541879270168e-07,
      "loss": 0.0013,
      "step": 70470
    },
    {
      "epoch": 12814.545454545454,
      "grad_norm": 0.0024025565944612026,
      "learning_rate": 6.380423587277335e-07,
      "loss": 0.0006,
      "step": 70480
    },
    {
      "epoch": 12816.363636363636,
      "grad_norm": 0.0014844824327155948,
      "learning_rate": 6.379305220528756e-07,
      "loss": 0.0014,
      "step": 70490
    },
    {
      "epoch": 12818.181818181818,
      "grad_norm": 0.0014352337457239628,
      "learning_rate": 6.378186779084995e-07,
      "loss": 0.001,
      "step": 70500
    },
    {
      "epoch": 12818.181818181818,
      "eval_loss": 4.950237274169922,
      "eval_runtime": 0.9518,
      "eval_samples_per_second": 10.506,
      "eval_steps_per_second": 5.253,
      "step": 70500
    },
    {
      "epoch": 12820.0,
      "grad_norm": 0.0010862858034670353,
      "learning_rate": 6.377068263006622e-07,
      "loss": 0.0012,
      "step": 70510
    },
    {
      "epoch": 12821.818181818182,
      "grad_norm": 0.0017093817004933953,
      "learning_rate": 6.375949672354208e-07,
      "loss": 0.0009,
      "step": 70520
    },
    {
      "epoch": 12823.636363636364,
      "grad_norm": 0.0013479673070833087,
      "learning_rate": 6.374831007188331e-07,
      "loss": 0.0012,
      "step": 70530
    },
    {
      "epoch": 12825.454545454546,
      "grad_norm": 0.266731858253479,
      "learning_rate": 6.373712267569569e-07,
      "loss": 0.0013,
      "step": 70540
    },
    {
      "epoch": 12827.272727272728,
      "grad_norm": 0.2564091384410858,
      "learning_rate": 6.372593453558504e-07,
      "loss": 0.0013,
      "step": 70550
    },
    {
      "epoch": 12829.09090909091,
      "grad_norm": 0.16643916070461273,
      "learning_rate": 6.371474565215733e-07,
      "loss": 0.0008,
      "step": 70560
    },
    {
      "epoch": 12830.90909090909,
      "grad_norm": 0.0009958096779882908,
      "learning_rate": 6.370355602601841e-07,
      "loss": 0.0011,
      "step": 70570
    },
    {
      "epoch": 12832.727272727272,
      "grad_norm": 0.0011177480919286609,
      "learning_rate": 6.369236565777428e-07,
      "loss": 0.0009,
      "step": 70580
    },
    {
      "epoch": 12834.545454545454,
      "grad_norm": 0.15195295214653015,
      "learning_rate": 6.368117454803092e-07,
      "loss": 0.0016,
      "step": 70590
    },
    {
      "epoch": 12836.363636363636,
      "grad_norm": 0.010944098234176636,
      "learning_rate": 6.366998269739441e-07,
      "loss": 0.0009,
      "step": 70600
    },
    {
      "epoch": 12838.181818181818,
      "grad_norm": 0.0010687881149351597,
      "learning_rate": 6.365879010647081e-07,
      "loss": 0.0009,
      "step": 70610
    },
    {
      "epoch": 12840.0,
      "grad_norm": 0.1632966250181198,
      "learning_rate": 6.364759677586627e-07,
      "loss": 0.0012,
      "step": 70620
    },
    {
      "epoch": 12841.818181818182,
      "grad_norm": 0.26628828048706055,
      "learning_rate": 6.363640270618691e-07,
      "loss": 0.0012,
      "step": 70630
    },
    {
      "epoch": 12843.636363636364,
      "grad_norm": 0.0011198510183021426,
      "learning_rate": 6.362520789803898e-07,
      "loss": 0.0009,
      "step": 70640
    },
    {
      "epoch": 12845.454545454546,
      "grad_norm": 0.15950322151184082,
      "learning_rate": 6.361401235202871e-07,
      "loss": 0.001,
      "step": 70650
    },
    {
      "epoch": 12847.272727272728,
      "grad_norm": 0.16802573204040527,
      "learning_rate": 6.36028160687624e-07,
      "loss": 0.0012,
      "step": 70660
    },
    {
      "epoch": 12849.09090909091,
      "grad_norm": 0.007605304941534996,
      "learning_rate": 6.359161904884635e-07,
      "loss": 0.0014,
      "step": 70670
    },
    {
      "epoch": 12850.90909090909,
      "grad_norm": 0.1603209376335144,
      "learning_rate": 6.358042129288693e-07,
      "loss": 0.0008,
      "step": 70680
    },
    {
      "epoch": 12852.727272727272,
      "grad_norm": 0.1713760793209076,
      "learning_rate": 6.356922280149057e-07,
      "loss": 0.0011,
      "step": 70690
    },
    {
      "epoch": 12854.545454545454,
      "grad_norm": 0.0011395827168598771,
      "learning_rate": 6.355802357526369e-07,
      "loss": 0.0011,
      "step": 70700
    },
    {
      "epoch": 12856.363636363636,
      "grad_norm": 0.0009624528465792537,
      "learning_rate": 6.354682361481279e-07,
      "loss": 0.0014,
      "step": 70710
    },
    {
      "epoch": 12858.181818181818,
      "grad_norm": 0.21276237070560455,
      "learning_rate": 6.353562292074439e-07,
      "loss": 0.0009,
      "step": 70720
    },
    {
      "epoch": 12860.0,
      "grad_norm": 0.041938066482543945,
      "learning_rate": 6.352442149366506e-07,
      "loss": 0.0012,
      "step": 70730
    },
    {
      "epoch": 12861.818181818182,
      "grad_norm": 0.2508871853351593,
      "learning_rate": 6.351321933418139e-07,
      "loss": 0.0012,
      "step": 70740
    },
    {
      "epoch": 12863.636363636364,
      "grad_norm": 0.0008328447001986206,
      "learning_rate": 6.350201644290004e-07,
      "loss": 0.0007,
      "step": 70750
    },
    {
      "epoch": 12865.454545454546,
      "grad_norm": 0.0013894628500565886,
      "learning_rate": 6.349081282042767e-07,
      "loss": 0.0012,
      "step": 70760
    },
    {
      "epoch": 12867.272727272728,
      "grad_norm": 0.19756431877613068,
      "learning_rate": 6.347960846737104e-07,
      "loss": 0.0013,
      "step": 70770
    },
    {
      "epoch": 12869.09090909091,
      "grad_norm": 0.20077286660671234,
      "learning_rate": 6.346840338433689e-07,
      "loss": 0.0009,
      "step": 70780
    },
    {
      "epoch": 12870.90909090909,
      "grad_norm": 0.0012012118240818381,
      "learning_rate": 6.345719757193202e-07,
      "loss": 0.0012,
      "step": 70790
    },
    {
      "epoch": 12872.727272727272,
      "grad_norm": 0.0011564751621335745,
      "learning_rate": 6.344599103076328e-07,
      "loss": 0.0008,
      "step": 70800
    },
    {
      "epoch": 12874.545454545454,
      "grad_norm": 0.001351481070742011,
      "learning_rate": 6.343478376143756e-07,
      "loss": 0.001,
      "step": 70810
    },
    {
      "epoch": 12876.363636363636,
      "grad_norm": 0.0016567428829148412,
      "learning_rate": 6.342357576456174e-07,
      "loss": 0.0012,
      "step": 70820
    },
    {
      "epoch": 12878.181818181818,
      "grad_norm": 0.005627823993563652,
      "learning_rate": 6.341236704074284e-07,
      "loss": 0.0017,
      "step": 70830
    },
    {
      "epoch": 12880.0,
      "grad_norm": 0.04950981214642525,
      "learning_rate": 6.340115759058782e-07,
      "loss": 0.001,
      "step": 70840
    },
    {
      "epoch": 12881.818181818182,
      "grad_norm": 0.26288142800331116,
      "learning_rate": 6.338994741470373e-07,
      "loss": 0.0012,
      "step": 70850
    },
    {
      "epoch": 12883.636363636364,
      "grad_norm": 0.011157718487083912,
      "learning_rate": 6.337873651369763e-07,
      "loss": 0.0009,
      "step": 70860
    },
    {
      "epoch": 12885.454545454546,
      "grad_norm": 0.0007751973462291062,
      "learning_rate": 6.336752488817667e-07,
      "loss": 0.0014,
      "step": 70870
    },
    {
      "epoch": 12887.272727272728,
      "grad_norm": 0.2742512822151184,
      "learning_rate": 6.335631253874799e-07,
      "loss": 0.0012,
      "step": 70880
    },
    {
      "epoch": 12889.09090909091,
      "grad_norm": 0.0007578851655125618,
      "learning_rate": 6.334509946601878e-07,
      "loss": 0.001,
      "step": 70890
    },
    {
      "epoch": 12890.90909090909,
      "grad_norm": 0.2156660258769989,
      "learning_rate": 6.333388567059627e-07,
      "loss": 0.0011,
      "step": 70900
    },
    {
      "epoch": 12892.727272727272,
      "grad_norm": 0.003714532358571887,
      "learning_rate": 6.332267115308777e-07,
      "loss": 0.0011,
      "step": 70910
    },
    {
      "epoch": 12894.545454545454,
      "grad_norm": 0.0013658027164638042,
      "learning_rate": 6.331145591410057e-07,
      "loss": 0.0009,
      "step": 70920
    },
    {
      "epoch": 12896.363636363636,
      "grad_norm": 0.0017540550325065851,
      "learning_rate": 6.3300239954242e-07,
      "loss": 0.0011,
      "step": 70930
    },
    {
      "epoch": 12898.181818181818,
      "grad_norm": 0.17586569488048553,
      "learning_rate": 6.328902327411947e-07,
      "loss": 0.0012,
      "step": 70940
    },
    {
      "epoch": 12900.0,
      "grad_norm": 0.0010300950380042195,
      "learning_rate": 6.327780587434044e-07,
      "loss": 0.0011,
      "step": 70950
    },
    {
      "epoch": 12901.818181818182,
      "grad_norm": 0.0035682865418493748,
      "learning_rate": 6.326658775551235e-07,
      "loss": 0.0011,
      "step": 70960
    },
    {
      "epoch": 12903.636363636364,
      "grad_norm": 0.21566088497638702,
      "learning_rate": 6.325536891824269e-07,
      "loss": 0.0011,
      "step": 70970
    },
    {
      "epoch": 12905.454545454546,
      "grad_norm": 0.1610267460346222,
      "learning_rate": 6.324414936313904e-07,
      "loss": 0.0009,
      "step": 70980
    },
    {
      "epoch": 12907.272727272728,
      "grad_norm": 0.18440468609333038,
      "learning_rate": 6.323292909080896e-07,
      "loss": 0.0015,
      "step": 70990
    },
    {
      "epoch": 12909.09090909091,
      "grad_norm": 0.2010236531496048,
      "learning_rate": 6.322170810186011e-07,
      "loss": 0.0009,
      "step": 71000
    },
    {
      "epoch": 12909.09090909091,
      "eval_loss": 4.940985202789307,
      "eval_runtime": 0.9569,
      "eval_samples_per_second": 10.45,
      "eval_steps_per_second": 5.225,
      "step": 71000
    },
    {
      "epoch": 12910.90909090909,
      "grad_norm": 0.20486703515052795,
      "learning_rate": 6.321048639690012e-07,
      "loss": 0.001,
      "step": 71010
    },
    {
      "epoch": 12912.727272727272,
      "grad_norm": 0.21523417532444,
      "learning_rate": 6.319926397653672e-07,
      "loss": 0.0009,
      "step": 71020
    },
    {
      "epoch": 12914.545454545454,
      "grad_norm": 0.000884287292137742,
      "learning_rate": 6.318804084137762e-07,
      "loss": 0.0013,
      "step": 71030
    },
    {
      "epoch": 12916.363636363636,
      "grad_norm": 0.0022801831364631653,
      "learning_rate": 6.317681699203064e-07,
      "loss": 0.0011,
      "step": 71040
    },
    {
      "epoch": 12918.181818181818,
      "grad_norm": 0.0010631757322698832,
      "learning_rate": 6.316559242910356e-07,
      "loss": 0.001,
      "step": 71050
    },
    {
      "epoch": 12920.0,
      "grad_norm": 0.0008274632855318487,
      "learning_rate": 6.315436715320424e-07,
      "loss": 0.0012,
      "step": 71060
    },
    {
      "epoch": 12921.818181818182,
      "grad_norm": 0.20316992700099945,
      "learning_rate": 6.314314116494061e-07,
      "loss": 0.001,
      "step": 71070
    },
    {
      "epoch": 12923.636363636364,
      "grad_norm": 0.21699440479278564,
      "learning_rate": 6.313191446492056e-07,
      "loss": 0.0012,
      "step": 71080
    },
    {
      "epoch": 12925.454545454546,
      "grad_norm": 0.1696380376815796,
      "learning_rate": 6.312068705375211e-07,
      "loss": 0.0011,
      "step": 71090
    },
    {
      "epoch": 12927.272727272728,
      "grad_norm": 0.000824378861580044,
      "learning_rate": 6.310945893204324e-07,
      "loss": 0.0009,
      "step": 71100
    },
    {
      "epoch": 12929.09090909091,
      "grad_norm": 0.0015250822762027383,
      "learning_rate": 6.309823010040201e-07,
      "loss": 0.0012,
      "step": 71110
    },
    {
      "epoch": 12930.90909090909,
      "grad_norm": 0.002023625886067748,
      "learning_rate": 6.30870005594365e-07,
      "loss": 0.0012,
      "step": 71120
    },
    {
      "epoch": 12932.727272727272,
      "grad_norm": 0.001078669331036508,
      "learning_rate": 6.307577030975484e-07,
      "loss": 0.0007,
      "step": 71130
    },
    {
      "epoch": 12934.545454545454,
      "grad_norm": 0.16342997550964355,
      "learning_rate": 6.30645393519652e-07,
      "loss": 0.0013,
      "step": 71140
    },
    {
      "epoch": 12936.363636363636,
      "grad_norm": 0.0007517788326367736,
      "learning_rate": 6.305330768667581e-07,
      "loss": 0.001,
      "step": 71150
    },
    {
      "epoch": 12938.181818181818,
      "grad_norm": 0.2587166130542755,
      "learning_rate": 6.304207531449484e-07,
      "loss": 0.0013,
      "step": 71160
    },
    {
      "epoch": 12940.0,
      "grad_norm": 0.16220204532146454,
      "learning_rate": 6.303084223603063e-07,
      "loss": 0.0009,
      "step": 71170
    },
    {
      "epoch": 12941.818181818182,
      "grad_norm": 0.1591995805501938,
      "learning_rate": 6.301960845189149e-07,
      "loss": 0.001,
      "step": 71180
    },
    {
      "epoch": 12943.636363636364,
      "grad_norm": 0.0008165401523001492,
      "learning_rate": 6.300837396268577e-07,
      "loss": 0.0012,
      "step": 71190
    },
    {
      "epoch": 12945.454545454546,
      "grad_norm": 0.0011179266730323434,
      "learning_rate": 6.299713876902187e-07,
      "loss": 0.0011,
      "step": 71200
    },
    {
      "epoch": 12947.272727272728,
      "grad_norm": 0.21263276040554047,
      "learning_rate": 6.298590287150821e-07,
      "loss": 0.001,
      "step": 71210
    },
    {
      "epoch": 12949.09090909091,
      "grad_norm": 0.19998207688331604,
      "learning_rate": 6.297466627075326e-07,
      "loss": 0.0012,
      "step": 71220
    },
    {
      "epoch": 12950.90909090909,
      "grad_norm": 0.000919634010642767,
      "learning_rate": 6.296342896736556e-07,
      "loss": 0.001,
      "step": 71230
    },
    {
      "epoch": 12952.727272727272,
      "grad_norm": 0.3186356723308563,
      "learning_rate": 6.295219096195361e-07,
      "loss": 0.0012,
      "step": 71240
    },
    {
      "epoch": 12954.545454545454,
      "grad_norm": 0.17015717923641205,
      "learning_rate": 6.294095225512604e-07,
      "loss": 0.0011,
      "step": 71250
    },
    {
      "epoch": 12956.363636363636,
      "grad_norm": 0.17871005833148956,
      "learning_rate": 6.292971284749144e-07,
      "loss": 0.0013,
      "step": 71260
    },
    {
      "epoch": 12958.181818181818,
      "grad_norm": 0.0024139436427503824,
      "learning_rate": 6.291847273965849e-07,
      "loss": 0.0006,
      "step": 71270
    },
    {
      "epoch": 12960.0,
      "grad_norm": 0.001230961992405355,
      "learning_rate": 6.290723193223588e-07,
      "loss": 0.0012,
      "step": 71280
    },
    {
      "epoch": 12961.818181818182,
      "grad_norm": 0.0009760549874044955,
      "learning_rate": 6.289599042583236e-07,
      "loss": 0.0013,
      "step": 71290
    },
    {
      "epoch": 12963.636363636364,
      "grad_norm": 0.2623160481452942,
      "learning_rate": 6.28847482210567e-07,
      "loss": 0.0012,
      "step": 71300
    },
    {
      "epoch": 12965.454545454546,
      "grad_norm": 0.16157792508602142,
      "learning_rate": 6.28735053185177e-07,
      "loss": 0.001,
      "step": 71310
    },
    {
      "epoch": 12967.272727272728,
      "grad_norm": 0.00170963816344738,
      "learning_rate": 6.286226171882422e-07,
      "loss": 0.0008,
      "step": 71320
    },
    {
      "epoch": 12969.09090909091,
      "grad_norm": 0.21700969338417053,
      "learning_rate": 6.285101742258513e-07,
      "loss": 0.0012,
      "step": 71330
    },
    {
      "epoch": 12970.90909090909,
      "grad_norm": 0.0007048610714264214,
      "learning_rate": 6.283977243040939e-07,
      "loss": 0.0012,
      "step": 71340
    },
    {
      "epoch": 12972.727272727272,
      "grad_norm": 0.20403040945529938,
      "learning_rate": 6.282852674290594e-07,
      "loss": 0.0011,
      "step": 71350
    },
    {
      "epoch": 12974.545454545454,
      "grad_norm": 0.19297100603580475,
      "learning_rate": 6.281728036068379e-07,
      "loss": 0.0014,
      "step": 71360
    },
    {
      "epoch": 12976.363636363636,
      "grad_norm": 0.2620808482170105,
      "learning_rate": 6.280603328435197e-07,
      "loss": 0.001,
      "step": 71370
    },
    {
      "epoch": 12978.181818181818,
      "grad_norm": 0.2709672749042511,
      "learning_rate": 6.279478551451959e-07,
      "loss": 0.0012,
      "step": 71380
    },
    {
      "epoch": 12980.0,
      "grad_norm": 0.2051827311515808,
      "learning_rate": 6.278353705179571e-07,
      "loss": 0.0009,
      "step": 71390
    },
    {
      "epoch": 12981.818181818182,
      "grad_norm": 0.18698953092098236,
      "learning_rate": 6.277228789678953e-07,
      "loss": 0.0012,
      "step": 71400
    },
    {
      "epoch": 12983.636363636364,
      "grad_norm": 0.21362900733947754,
      "learning_rate": 6.276103805011019e-07,
      "loss": 0.001,
      "step": 71410
    },
    {
      "epoch": 12985.454545454546,
      "grad_norm": 0.20512603223323822,
      "learning_rate": 6.274978751236698e-07,
      "loss": 0.0012,
      "step": 71420
    },
    {
      "epoch": 12987.272727272728,
      "grad_norm": 0.21272841095924377,
      "learning_rate": 6.273853628416911e-07,
      "loss": 0.001,
      "step": 71430
    },
    {
      "epoch": 12989.09090909091,
      "grad_norm": 0.0012487705098465085,
      "learning_rate": 6.27272843661259e-07,
      "loss": 0.001,
      "step": 71440
    },
    {
      "epoch": 12990.90909090909,
      "grad_norm": 0.21244271099567413,
      "learning_rate": 6.27160317588467e-07,
      "loss": 0.0011,
      "step": 71450
    },
    {
      "epoch": 12992.727272727272,
      "grad_norm": 0.0108669213950634,
      "learning_rate": 6.270477846294086e-07,
      "loss": 0.0011,
      "step": 71460
    },
    {
      "epoch": 12994.545454545454,
      "grad_norm": 0.2038394957780838,
      "learning_rate": 6.269352447901781e-07,
      "loss": 0.001,
      "step": 71470
    },
    {
      "epoch": 12996.363636363636,
      "grad_norm": 0.0016789365326985717,
      "learning_rate": 6.2682269807687e-07,
      "loss": 0.001,
      "step": 71480
    },
    {
      "epoch": 12998.181818181818,
      "grad_norm": 0.26567843556404114,
      "learning_rate": 6.267101444955791e-07,
      "loss": 0.0013,
      "step": 71490
    },
    {
      "epoch": 13000.0,
      "grad_norm": 0.1857644021511078,
      "learning_rate": 6.265975840524009e-07,
      "loss": 0.001,
      "step": 71500
    },
    {
      "epoch": 13000.0,
      "eval_loss": 4.967164993286133,
      "eval_runtime": 0.9488,
      "eval_samples_per_second": 10.54,
      "eval_steps_per_second": 5.27,
      "step": 71500
    },
    {
      "epoch": 13001.818181818182,
      "grad_norm": 0.20362186431884766,
      "learning_rate": 6.264850167534307e-07,
      "loss": 0.0014,
      "step": 71510
    },
    {
      "epoch": 13003.636363636364,
      "grad_norm": 0.21483682096004486,
      "learning_rate": 6.263724426047646e-07,
      "loss": 0.0011,
      "step": 71520
    },
    {
      "epoch": 13005.454545454546,
      "grad_norm": 0.21185500919818878,
      "learning_rate": 6.262598616124991e-07,
      "loss": 0.0013,
      "step": 71530
    },
    {
      "epoch": 13007.272727272728,
      "grad_norm": 0.26294785737991333,
      "learning_rate": 6.261472737827309e-07,
      "loss": 0.0012,
      "step": 71540
    },
    {
      "epoch": 13009.09090909091,
      "grad_norm": 0.0015936598647385836,
      "learning_rate": 6.26034679121557e-07,
      "loss": 0.0009,
      "step": 71550
    },
    {
      "epoch": 13010.90909090909,
      "grad_norm": 0.18091236054897308,
      "learning_rate": 6.259220776350745e-07,
      "loss": 0.001,
      "step": 71560
    },
    {
      "epoch": 13012.727272727272,
      "grad_norm": 0.16342593729496002,
      "learning_rate": 6.25809469329382e-07,
      "loss": 0.0012,
      "step": 71570
    },
    {
      "epoch": 13014.545454545454,
      "grad_norm": 0.19848743081092834,
      "learning_rate": 6.256968542105774e-07,
      "loss": 0.0009,
      "step": 71580
    },
    {
      "epoch": 13016.363636363636,
      "grad_norm": 0.0011680411407724023,
      "learning_rate": 6.255842322847593e-07,
      "loss": 0.001,
      "step": 71590
    },
    {
      "epoch": 13018.181818181818,
      "grad_norm": 0.0030660550110042095,
      "learning_rate": 6.254716035580264e-07,
      "loss": 0.0012,
      "step": 71600
    },
    {
      "epoch": 13020.0,
      "grad_norm": 0.0028244471177458763,
      "learning_rate": 6.253589680364783e-07,
      "loss": 0.0012,
      "step": 71610
    },
    {
      "epoch": 13021.818181818182,
      "grad_norm": 0.0010358873987570405,
      "learning_rate": 6.252463257262146e-07,
      "loss": 0.0009,
      "step": 71620
    },
    {
      "epoch": 13023.636363636364,
      "grad_norm": 0.005603731144219637,
      "learning_rate": 6.251336766333355e-07,
      "loss": 0.0013,
      "step": 71630
    },
    {
      "epoch": 13025.454545454546,
      "grad_norm": 0.22420628368854523,
      "learning_rate": 6.25021020763941e-07,
      "loss": 0.0009,
      "step": 71640
    },
    {
      "epoch": 13027.272727272728,
      "grad_norm": 0.1681976467370987,
      "learning_rate": 6.249083581241323e-07,
      "loss": 0.0012,
      "step": 71650
    },
    {
      "epoch": 13029.09090909091,
      "grad_norm": 0.27833178639411926,
      "learning_rate": 6.247956887200107e-07,
      "loss": 0.0012,
      "step": 71660
    },
    {
      "epoch": 13030.90909090909,
      "grad_norm": 0.0015951670939102769,
      "learning_rate": 6.24683012557677e-07,
      "loss": 0.001,
      "step": 71670
    },
    {
      "epoch": 13032.727272727272,
      "grad_norm": 0.0016429794486612082,
      "learning_rate": 6.245703296432338e-07,
      "loss": 0.0012,
      "step": 71680
    },
    {
      "epoch": 13034.545454545454,
      "grad_norm": 0.1786538064479828,
      "learning_rate": 6.24457639982783e-07,
      "loss": 0.0009,
      "step": 71690
    },
    {
      "epoch": 13036.363636363636,
      "grad_norm": 0.0011851791059598327,
      "learning_rate": 6.243449435824276e-07,
      "loss": 0.0009,
      "step": 71700
    },
    {
      "epoch": 13038.181818181818,
      "grad_norm": 0.0010824096389114857,
      "learning_rate": 6.242322404482697e-07,
      "loss": 0.0012,
      "step": 71710
    },
    {
      "epoch": 13040.0,
      "grad_norm": 0.1932777315378189,
      "learning_rate": 6.241195305864137e-07,
      "loss": 0.0012,
      "step": 71720
    },
    {
      "epoch": 13041.818181818182,
      "grad_norm": 0.2830455005168915,
      "learning_rate": 6.240068140029627e-07,
      "loss": 0.0012,
      "step": 71730
    },
    {
      "epoch": 13043.636363636364,
      "grad_norm": 0.001213614596053958,
      "learning_rate": 6.238940907040211e-07,
      "loss": 0.0007,
      "step": 71740
    },
    {
      "epoch": 13045.454545454546,
      "grad_norm": 0.20288459956645966,
      "learning_rate": 6.237813606956929e-07,
      "loss": 0.0013,
      "step": 71750
    },
    {
      "epoch": 13047.272727272728,
      "grad_norm": 0.0011435915948823094,
      "learning_rate": 6.236686239840835e-07,
      "loss": 0.0009,
      "step": 71760
    },
    {
      "epoch": 13049.09090909091,
      "grad_norm": 0.0008541273418813944,
      "learning_rate": 6.235558805752975e-07,
      "loss": 0.0012,
      "step": 71770
    },
    {
      "epoch": 13050.90909090909,
      "grad_norm": 0.002131636021658778,
      "learning_rate": 6.234431304754409e-07,
      "loss": 0.0009,
      "step": 71780
    },
    {
      "epoch": 13052.727272727272,
      "grad_norm": 0.0008655793499201536,
      "learning_rate": 6.233303736906193e-07,
      "loss": 0.0014,
      "step": 71790
    },
    {
      "epoch": 13054.545454545454,
      "grad_norm": 0.001172448042780161,
      "learning_rate": 6.232176102269388e-07,
      "loss": 0.0009,
      "step": 71800
    },
    {
      "epoch": 13056.363636363636,
      "grad_norm": 0.0014721627812832594,
      "learning_rate": 6.231048400905064e-07,
      "loss": 0.0015,
      "step": 71810
    },
    {
      "epoch": 13058.181818181818,
      "grad_norm": 0.1733684092760086,
      "learning_rate": 6.22992063287429e-07,
      "loss": 0.0013,
      "step": 71820
    },
    {
      "epoch": 13060.0,
      "grad_norm": 0.1848396211862564,
      "learning_rate": 6.228792798238139e-07,
      "loss": 0.001,
      "step": 71830
    },
    {
      "epoch": 13061.818181818182,
      "grad_norm": 0.000890215567778796,
      "learning_rate": 6.227664897057685e-07,
      "loss": 0.0012,
      "step": 71840
    },
    {
      "epoch": 13063.636363636364,
      "grad_norm": 0.001529971486888826,
      "learning_rate": 6.226536929394013e-07,
      "loss": 0.0009,
      "step": 71850
    },
    {
      "epoch": 13065.454545454546,
      "grad_norm": 0.001320183859206736,
      "learning_rate": 6.225408895308205e-07,
      "loss": 0.0012,
      "step": 71860
    },
    {
      "epoch": 13067.272727272728,
      "grad_norm": 0.040484506636857986,
      "learning_rate": 6.224280794861348e-07,
      "loss": 0.0015,
      "step": 71870
    },
    {
      "epoch": 13069.09090909091,
      "grad_norm": 0.0009686663979664445,
      "learning_rate": 6.223152628114536e-07,
      "loss": 0.0008,
      "step": 71880
    },
    {
      "epoch": 13070.90909090909,
      "grad_norm": 0.0010027404641732574,
      "learning_rate": 6.222024395128863e-07,
      "loss": 0.0012,
      "step": 71890
    },
    {
      "epoch": 13072.727272727272,
      "grad_norm": 0.21571728587150574,
      "learning_rate": 6.220896095965427e-07,
      "loss": 0.0012,
      "step": 71900
    },
    {
      "epoch": 13074.545454545454,
      "grad_norm": 0.0007780025480315089,
      "learning_rate": 6.219767730685328e-07,
      "loss": 0.0009,
      "step": 71910
    },
    {
      "epoch": 13076.363636363636,
      "grad_norm": 0.17500337958335876,
      "learning_rate": 6.218639299349675e-07,
      "loss": 0.0011,
      "step": 71920
    },
    {
      "epoch": 13078.181818181818,
      "grad_norm": 0.0009892303496599197,
      "learning_rate": 6.217510802019578e-07,
      "loss": 0.001,
      "step": 71930
    },
    {
      "epoch": 13080.0,
      "grad_norm": 0.21313433349132538,
      "learning_rate": 6.216382238756146e-07,
      "loss": 0.0012,
      "step": 71940
    },
    {
      "epoch": 13081.818181818182,
      "grad_norm": 0.00113145902287215,
      "learning_rate": 6.215253609620497e-07,
      "loss": 0.0012,
      "step": 71950
    },
    {
      "epoch": 13083.636363636364,
      "grad_norm": 0.18865209817886353,
      "learning_rate": 6.214124914673754e-07,
      "loss": 0.0011,
      "step": 71960
    },
    {
      "epoch": 13085.454545454546,
      "grad_norm": 0.22750212252140045,
      "learning_rate": 6.212996153977037e-07,
      "loss": 0.0012,
      "step": 71970
    },
    {
      "epoch": 13087.272727272728,
      "grad_norm": 0.19983887672424316,
      "learning_rate": 6.211867327591475e-07,
      "loss": 0.0009,
      "step": 71980
    },
    {
      "epoch": 13089.09090909091,
      "grad_norm": 0.17842690646648407,
      "learning_rate": 6.210738435578196e-07,
      "loss": 0.0012,
      "step": 71990
    },
    {
      "epoch": 13090.90909090909,
      "grad_norm": 0.0020198316778987646,
      "learning_rate": 6.209609477998338e-07,
      "loss": 0.0011,
      "step": 72000
    },
    {
      "epoch": 13090.90909090909,
      "eval_loss": 4.978378772735596,
      "eval_runtime": 0.9494,
      "eval_samples_per_second": 10.533,
      "eval_steps_per_second": 5.267,
      "step": 72000
    },
    {
      "epoch": 13092.727272727272,
      "grad_norm": 0.0004983521648682654,
      "learning_rate": 6.208480454913037e-07,
      "loss": 0.0011,
      "step": 72010
    },
    {
      "epoch": 13094.545454545454,
      "grad_norm": 0.0009034454706124961,
      "learning_rate": 6.207351366383434e-07,
      "loss": 0.0013,
      "step": 72020
    },
    {
      "epoch": 13096.363636363636,
      "grad_norm": 0.0008681901963427663,
      "learning_rate": 6.206222212470674e-07,
      "loss": 0.0014,
      "step": 72030
    },
    {
      "epoch": 13098.181818181818,
      "grad_norm": 0.000944211904425174,
      "learning_rate": 6.205092993235907e-07,
      "loss": 0.0007,
      "step": 72040
    },
    {
      "epoch": 13100.0,
      "grad_norm": 0.17007005214691162,
      "learning_rate": 6.203963708740283e-07,
      "loss": 0.0012,
      "step": 72050
    },
    {
      "epoch": 13101.818181818182,
      "grad_norm": 0.28007957339286804,
      "learning_rate": 6.202834359044958e-07,
      "loss": 0.001,
      "step": 72060
    },
    {
      "epoch": 13103.636363636364,
      "grad_norm": 0.0008266764343716204,
      "learning_rate": 6.201704944211091e-07,
      "loss": 0.0012,
      "step": 72070
    },
    {
      "epoch": 13105.454545454546,
      "grad_norm": 0.17682920396327972,
      "learning_rate": 6.200575464299847e-07,
      "loss": 0.0012,
      "step": 72080
    },
    {
      "epoch": 13107.272727272728,
      "grad_norm": 0.27466511726379395,
      "learning_rate": 6.199445919372388e-07,
      "loss": 0.0012,
      "step": 72090
    },
    {
      "epoch": 13109.09090909091,
      "grad_norm": 0.0013103389646857977,
      "learning_rate": 6.198316309489884e-07,
      "loss": 0.0009,
      "step": 72100
    },
    {
      "epoch": 13110.90909090909,
      "grad_norm": 0.0011125848395749927,
      "learning_rate": 6.197186634713512e-07,
      "loss": 0.0012,
      "step": 72110
    },
    {
      "epoch": 13112.727272727272,
      "grad_norm": 0.0011018591467291117,
      "learning_rate": 6.196056895104447e-07,
      "loss": 0.001,
      "step": 72120
    },
    {
      "epoch": 13114.545454545454,
      "grad_norm": 0.0007935570320114493,
      "learning_rate": 6.194927090723867e-07,
      "loss": 0.0009,
      "step": 72130
    },
    {
      "epoch": 13116.363636363636,
      "grad_norm": 0.21426816284656525,
      "learning_rate": 6.193797221632958e-07,
      "loss": 0.0012,
      "step": 72140
    },
    {
      "epoch": 13118.181818181818,
      "grad_norm": 0.27276530861854553,
      "learning_rate": 6.192667287892904e-07,
      "loss": 0.0014,
      "step": 72150
    },
    {
      "epoch": 13120.0,
      "grad_norm": 0.0099555142223835,
      "learning_rate": 6.191537289564899e-07,
      "loss": 0.0009,
      "step": 72160
    },
    {
      "epoch": 13121.818181818182,
      "grad_norm": 0.2798537313938141,
      "learning_rate": 6.190407226710137e-07,
      "loss": 0.0012,
      "step": 72170
    },
    {
      "epoch": 13123.636363636364,
      "grad_norm": 0.0018981144530698657,
      "learning_rate": 6.189277099389815e-07,
      "loss": 0.0009,
      "step": 72180
    },
    {
      "epoch": 13125.454545454546,
      "grad_norm": 0.0010008467361330986,
      "learning_rate": 6.188146907665133e-07,
      "loss": 0.0012,
      "step": 72190
    },
    {
      "epoch": 13127.272727272728,
      "grad_norm": 0.0006964360363781452,
      "learning_rate": 6.187016651597298e-07,
      "loss": 0.0009,
      "step": 72200
    },
    {
      "epoch": 13129.09090909091,
      "grad_norm": 0.27028894424438477,
      "learning_rate": 6.185886331247515e-07,
      "loss": 0.0015,
      "step": 72210
    },
    {
      "epoch": 13130.90909090909,
      "grad_norm": 0.18398091197013855,
      "learning_rate": 6.184755946676997e-07,
      "loss": 0.001,
      "step": 72220
    },
    {
      "epoch": 13132.727272727272,
      "grad_norm": 0.17032258212566376,
      "learning_rate": 6.183625497946961e-07,
      "loss": 0.0009,
      "step": 72230
    },
    {
      "epoch": 13134.545454545454,
      "grad_norm": 0.00693989172577858,
      "learning_rate": 6.182494985118624e-07,
      "loss": 0.0014,
      "step": 72240
    },
    {
      "epoch": 13136.363636363636,
      "grad_norm": 0.00982594583183527,
      "learning_rate": 6.181364408253207e-07,
      "loss": 0.001,
      "step": 72250
    },
    {
      "epoch": 13138.181818181818,
      "grad_norm": 0.0008946761372499168,
      "learning_rate": 6.180233767411936e-07,
      "loss": 0.0009,
      "step": 72260
    },
    {
      "epoch": 13140.0,
      "grad_norm": 0.0017002158565446734,
      "learning_rate": 6.179103062656042e-07,
      "loss": 0.0012,
      "step": 72270
    },
    {
      "epoch": 13141.818181818182,
      "grad_norm": 0.000691829074639827,
      "learning_rate": 6.177972294046753e-07,
      "loss": 0.001,
      "step": 72280
    },
    {
      "epoch": 13143.636363636364,
      "grad_norm": 0.28026407957077026,
      "learning_rate": 6.17684146164531e-07,
      "loss": 0.0014,
      "step": 72290
    },
    {
      "epoch": 13145.454545454546,
      "grad_norm": 0.0008343199733644724,
      "learning_rate": 6.17571056551295e-07,
      "loss": 0.0008,
      "step": 72300
    },
    {
      "epoch": 13147.272727272728,
      "grad_norm": 0.1752510666847229,
      "learning_rate": 6.174579605710915e-07,
      "loss": 0.0012,
      "step": 72310
    },
    {
      "epoch": 13149.09090909091,
      "grad_norm": 0.0008466313011012971,
      "learning_rate": 6.173448582300453e-07,
      "loss": 0.001,
      "step": 72320
    },
    {
      "epoch": 13150.90909090909,
      "grad_norm": 0.0007713968516327441,
      "learning_rate": 6.172317495342811e-07,
      "loss": 0.0012,
      "step": 72330
    },
    {
      "epoch": 13152.727272727272,
      "grad_norm": 0.18622353672981262,
      "learning_rate": 6.171186344899245e-07,
      "loss": 0.001,
      "step": 72340
    },
    {
      "epoch": 13154.545454545454,
      "grad_norm": 0.28854453563690186,
      "learning_rate": 6.170055131031011e-07,
      "loss": 0.0013,
      "step": 72350
    },
    {
      "epoch": 13156.363636363636,
      "grad_norm": 0.017167488113045692,
      "learning_rate": 6.168923853799368e-07,
      "loss": 0.0011,
      "step": 72360
    },
    {
      "epoch": 13158.181818181818,
      "grad_norm": 0.1672653704881668,
      "learning_rate": 6.16779251326558e-07,
      "loss": 0.0011,
      "step": 72370
    },
    {
      "epoch": 13160.0,
      "grad_norm": 0.1681957244873047,
      "learning_rate": 6.166661109490913e-07,
      "loss": 0.0011,
      "step": 72380
    },
    {
      "epoch": 13161.818181818182,
      "grad_norm": 0.000984215410426259,
      "learning_rate": 6.165529642536639e-07,
      "loss": 0.0012,
      "step": 72390
    },
    {
      "epoch": 13163.636363636364,
      "grad_norm": 0.22105975449085236,
      "learning_rate": 6.164398112464029e-07,
      "loss": 0.0011,
      "step": 72400
    },
    {
      "epoch": 13165.454545454546,
      "grad_norm": 0.22127029299736023,
      "learning_rate": 6.163266519334363e-07,
      "loss": 0.001,
      "step": 72410
    },
    {
      "epoch": 13167.272727272728,
      "grad_norm": 0.2045862078666687,
      "learning_rate": 6.16213486320892e-07,
      "loss": 0.001,
      "step": 72420
    },
    {
      "epoch": 13169.09090909091,
      "grad_norm": 0.21904335916042328,
      "learning_rate": 6.161003144148984e-07,
      "loss": 0.0012,
      "step": 72430
    },
    {
      "epoch": 13170.90909090909,
      "grad_norm": 0.000924798077903688,
      "learning_rate": 6.159871362215844e-07,
      "loss": 0.001,
      "step": 72440
    },
    {
      "epoch": 13172.727272727272,
      "grad_norm": 0.16470389068126678,
      "learning_rate": 6.158739517470786e-07,
      "loss": 0.001,
      "step": 72450
    },
    {
      "epoch": 13174.545454545454,
      "grad_norm": 0.2225925326347351,
      "learning_rate": 6.157607609975111e-07,
      "loss": 0.0012,
      "step": 72460
    },
    {
      "epoch": 13176.363636363636,
      "grad_norm": 0.0010810913518071175,
      "learning_rate": 6.156475639790111e-07,
      "loss": 0.001,
      "step": 72470
    },
    {
      "epoch": 13178.181818181818,
      "grad_norm": 0.0011795193422585726,
      "learning_rate": 6.155343606977091e-07,
      "loss": 0.001,
      "step": 72480
    },
    {
      "epoch": 13180.0,
      "grad_norm": 0.0013362031895667315,
      "learning_rate": 6.154211511597349e-07,
      "loss": 0.0012,
      "step": 72490
    },
    {
      "epoch": 13181.818181818182,
      "grad_norm": 0.0005567407933995128,
      "learning_rate": 6.153079353712201e-07,
      "loss": 0.0012,
      "step": 72500
    },
    {
      "epoch": 13181.818181818182,
      "eval_loss": 4.97628116607666,
      "eval_runtime": 0.9518,
      "eval_samples_per_second": 10.507,
      "eval_steps_per_second": 5.253,
      "step": 72500
    },
    {
      "epoch": 13183.636363636364,
      "grad_norm": 0.21462854743003845,
      "learning_rate": 6.151947133382953e-07,
      "loss": 0.0011,
      "step": 72510
    },
    {
      "epoch": 13185.454545454546,
      "grad_norm": 0.20873460173606873,
      "learning_rate": 6.150814850670921e-07,
      "loss": 0.001,
      "step": 72520
    },
    {
      "epoch": 13187.272727272728,
      "grad_norm": 0.17342765629291534,
      "learning_rate": 6.149682505637421e-07,
      "loss": 0.0011,
      "step": 72530
    },
    {
      "epoch": 13189.09090909091,
      "grad_norm": 0.0009082346223294735,
      "learning_rate": 6.148550098343777e-07,
      "loss": 0.001,
      "step": 72540
    },
    {
      "epoch": 13190.90909090909,
      "grad_norm": 0.1522316336631775,
      "learning_rate": 6.147417628851313e-07,
      "loss": 0.0012,
      "step": 72550
    },
    {
      "epoch": 13192.727272727272,
      "grad_norm": 0.0012584659270942211,
      "learning_rate": 6.146285097221357e-07,
      "loss": 0.0012,
      "step": 72560
    },
    {
      "epoch": 13194.545454545454,
      "grad_norm": 0.1613723635673523,
      "learning_rate": 6.145152503515238e-07,
      "loss": 0.0007,
      "step": 72570
    },
    {
      "epoch": 13196.363636363636,
      "grad_norm": 0.20150284469127655,
      "learning_rate": 6.144019847794293e-07,
      "loss": 0.0014,
      "step": 72580
    },
    {
      "epoch": 13198.181818181818,
      "grad_norm": 0.13905221223831177,
      "learning_rate": 6.14288713011986e-07,
      "loss": 0.001,
      "step": 72590
    },
    {
      "epoch": 13200.0,
      "grad_norm": 0.16100828349590302,
      "learning_rate": 6.141754350553279e-07,
      "loss": 0.001,
      "step": 72600
    },
    {
      "epoch": 13201.818181818182,
      "grad_norm": 0.001731779775582254,
      "learning_rate": 6.140621509155897e-07,
      "loss": 0.0012,
      "step": 72610
    },
    {
      "epoch": 13203.636363636364,
      "grad_norm": 0.21978352963924408,
      "learning_rate": 6.139488605989059e-07,
      "loss": 0.0011,
      "step": 72620
    },
    {
      "epoch": 13205.454545454546,
      "grad_norm": 0.2075119912624359,
      "learning_rate": 6.13835564111412e-07,
      "loss": 0.001,
      "step": 72630
    },
    {
      "epoch": 13207.272727272728,
      "grad_norm": 0.2199525237083435,
      "learning_rate": 6.137222614592431e-07,
      "loss": 0.0012,
      "step": 72640
    },
    {
      "epoch": 13209.09090909091,
      "grad_norm": 0.24587547779083252,
      "learning_rate": 6.136089526485356e-07,
      "loss": 0.0011,
      "step": 72650
    },
    {
      "epoch": 13210.90909090909,
      "grad_norm": 0.0009207524126395583,
      "learning_rate": 6.13495637685425e-07,
      "loss": 0.0012,
      "step": 72660
    },
    {
      "epoch": 13212.727272727272,
      "grad_norm": 0.0009705572738312185,
      "learning_rate": 6.133823165760481e-07,
      "loss": 0.0012,
      "step": 72670
    },
    {
      "epoch": 13214.545454545454,
      "grad_norm": 0.003068635007366538,
      "learning_rate": 6.132689893265415e-07,
      "loss": 0.0009,
      "step": 72680
    },
    {
      "epoch": 13216.363636363636,
      "grad_norm": 0.0010002917842939496,
      "learning_rate": 6.13155655943043e-07,
      "loss": 0.0015,
      "step": 72690
    },
    {
      "epoch": 13218.181818181818,
      "grad_norm": 0.2059813141822815,
      "learning_rate": 6.130423164316893e-07,
      "loss": 0.0009,
      "step": 72700
    },
    {
      "epoch": 13220.0,
      "grad_norm": 0.001326580299064517,
      "learning_rate": 6.129289707986186e-07,
      "loss": 0.0011,
      "step": 72710
    },
    {
      "epoch": 13221.818181818182,
      "grad_norm": 0.21610748767852783,
      "learning_rate": 6.128156190499687e-07,
      "loss": 0.0011,
      "step": 72720
    },
    {
      "epoch": 13223.636363636364,
      "grad_norm": 0.26017025113105774,
      "learning_rate": 6.127022611918786e-07,
      "loss": 0.001,
      "step": 72730
    },
    {
      "epoch": 13225.454545454546,
      "grad_norm": 0.0008940289844758809,
      "learning_rate": 6.125888972304868e-07,
      "loss": 0.0009,
      "step": 72740
    },
    {
      "epoch": 13227.272727272728,
      "grad_norm": 0.17504751682281494,
      "learning_rate": 6.124755271719326e-07,
      "loss": 0.0013,
      "step": 72750
    },
    {
      "epoch": 13229.09090909091,
      "grad_norm": 0.21577668190002441,
      "learning_rate": 6.123621510223551e-07,
      "loss": 0.0012,
      "step": 72760
    },
    {
      "epoch": 13230.90909090909,
      "grad_norm": 0.21739456057548523,
      "learning_rate": 6.122487687878944e-07,
      "loss": 0.0011,
      "step": 72770
    },
    {
      "epoch": 13232.727272727272,
      "grad_norm": 0.21541281044483185,
      "learning_rate": 6.121353804746905e-07,
      "loss": 0.0011,
      "step": 72780
    },
    {
      "epoch": 13234.545454545454,
      "grad_norm": 0.18226052820682526,
      "learning_rate": 6.120219860888841e-07,
      "loss": 0.0009,
      "step": 72790
    },
    {
      "epoch": 13236.363636363636,
      "grad_norm": 0.0009669288992881775,
      "learning_rate": 6.119085856366157e-07,
      "loss": 0.0014,
      "step": 72800
    },
    {
      "epoch": 13238.181818181818,
      "grad_norm": 0.18266816437244415,
      "learning_rate": 6.117951791240265e-07,
      "loss": 0.001,
      "step": 72810
    },
    {
      "epoch": 13240.0,
      "grad_norm": 0.20179249346256256,
      "learning_rate": 6.11681766557258e-07,
      "loss": 0.0011,
      "step": 72820
    },
    {
      "epoch": 13241.818181818182,
      "grad_norm": 0.21249152719974518,
      "learning_rate": 6.115683479424518e-07,
      "loss": 0.0011,
      "step": 72830
    },
    {
      "epoch": 13243.636363636364,
      "grad_norm": 0.20361880958080292,
      "learning_rate": 6.114549232857502e-07,
      "loss": 0.001,
      "step": 72840
    },
    {
      "epoch": 13245.454545454546,
      "grad_norm": 0.018042922019958496,
      "learning_rate": 6.113414925932955e-07,
      "loss": 0.0015,
      "step": 72850
    },
    {
      "epoch": 13247.272727272728,
      "grad_norm": 0.001198521931655705,
      "learning_rate": 6.112280558712305e-07,
      "loss": 0.0008,
      "step": 72860
    },
    {
      "epoch": 13249.09090909091,
      "grad_norm": 0.0007738522836007178,
      "learning_rate": 6.111146131256982e-07,
      "loss": 0.0012,
      "step": 72870
    },
    {
      "epoch": 13250.90909090909,
      "grad_norm": 0.0009961468167603016,
      "learning_rate": 6.110011643628421e-07,
      "loss": 0.0011,
      "step": 72880
    },
    {
      "epoch": 13252.727272727272,
      "grad_norm": 0.17007941007614136,
      "learning_rate": 6.108877095888059e-07,
      "loss": 0.0012,
      "step": 72890
    },
    {
      "epoch": 13254.545454545454,
      "grad_norm": 0.20468254387378693,
      "learning_rate": 6.107742488097338e-07,
      "loss": 0.0009,
      "step": 72900
    },
    {
      "epoch": 13256.363636363636,
      "grad_norm": 0.000917519093491137,
      "learning_rate": 6.106607820317696e-07,
      "loss": 0.0012,
      "step": 72910
    },
    {
      "epoch": 13258.181818181818,
      "grad_norm": 0.0008972241193987429,
      "learning_rate": 6.105473092610588e-07,
      "loss": 0.001,
      "step": 72920
    },
    {
      "epoch": 13260.0,
      "grad_norm": 0.0009631516295485198,
      "learning_rate": 6.104338305037459e-07,
      "loss": 0.0012,
      "step": 72930
    },
    {
      "epoch": 13261.818181818182,
      "grad_norm": 0.18884719908237457,
      "learning_rate": 6.103203457659765e-07,
      "loss": 0.001,
      "step": 72940
    },
    {
      "epoch": 13263.636363636364,
      "grad_norm": 0.0008681872277520597,
      "learning_rate": 6.102068550538962e-07,
      "loss": 0.0011,
      "step": 72950
    },
    {
      "epoch": 13265.454545454546,
      "grad_norm": 0.001089909696020186,
      "learning_rate": 6.100933583736507e-07,
      "loss": 0.0011,
      "step": 72960
    },
    {
      "epoch": 13267.272727272728,
      "grad_norm": 0.167708158493042,
      "learning_rate": 6.099798557313869e-07,
      "loss": 0.0012,
      "step": 72970
    },
    {
      "epoch": 13269.09090909091,
      "grad_norm": 0.035614486783742905,
      "learning_rate": 6.098663471332511e-07,
      "loss": 0.0012,
      "step": 72980
    },
    {
      "epoch": 13270.90909090909,
      "grad_norm": 0.0008388362475670874,
      "learning_rate": 6.097528325853903e-07,
      "loss": 0.001,
      "step": 72990
    },
    {
      "epoch": 13272.727272727272,
      "grad_norm": 0.20943212509155273,
      "learning_rate": 6.096393120939516e-07,
      "loss": 0.0012,
      "step": 73000
    },
    {
      "epoch": 13272.727272727272,
      "eval_loss": 4.941694736480713,
      "eval_runtime": 0.9556,
      "eval_samples_per_second": 10.465,
      "eval_steps_per_second": 5.232,
      "step": 73000
    },
    {
      "epoch": 13274.545454545454,
      "grad_norm": 0.2662844657897949,
      "learning_rate": 6.095257856650831e-07,
      "loss": 0.001,
      "step": 73010
    },
    {
      "epoch": 13276.363636363636,
      "grad_norm": 0.19862282276153564,
      "learning_rate": 6.094122533049323e-07,
      "loss": 0.0009,
      "step": 73020
    },
    {
      "epoch": 13278.181818181818,
      "grad_norm": 0.0011509429896250367,
      "learning_rate": 6.092987150196476e-07,
      "loss": 0.001,
      "step": 73030
    },
    {
      "epoch": 13280.0,
      "grad_norm": 0.011749804019927979,
      "learning_rate": 6.091851708153777e-07,
      "loss": 0.0012,
      "step": 73040
    },
    {
      "epoch": 13281.818181818182,
      "grad_norm": 0.20578700304031372,
      "learning_rate": 6.090716206982713e-07,
      "loss": 0.001,
      "step": 73050
    },
    {
      "epoch": 13283.636363636364,
      "grad_norm": 0.0007888408144935966,
      "learning_rate": 6.089580646744778e-07,
      "loss": 0.0012,
      "step": 73060
    },
    {
      "epoch": 13285.454545454546,
      "grad_norm": 0.2027052640914917,
      "learning_rate": 6.088445027501464e-07,
      "loss": 0.0012,
      "step": 73070
    },
    {
      "epoch": 13287.272727272728,
      "grad_norm": 0.0012769504683092237,
      "learning_rate": 6.087309349314274e-07,
      "loss": 0.0011,
      "step": 73080
    },
    {
      "epoch": 13289.09090909091,
      "grad_norm": 0.0028934727888554335,
      "learning_rate": 6.086173612244707e-07,
      "loss": 0.0011,
      "step": 73090
    },
    {
      "epoch": 13290.90909090909,
      "grad_norm": 0.0014245384372770786,
      "learning_rate": 6.085037816354268e-07,
      "loss": 0.0012,
      "step": 73100
    },
    {
      "epoch": 13292.727272727272,
      "grad_norm": 0.0010800415184348822,
      "learning_rate": 6.083901961704466e-07,
      "loss": 0.0011,
      "step": 73110
    },
    {
      "epoch": 13294.545454545454,
      "grad_norm": 0.16893121600151062,
      "learning_rate": 6.082766048356812e-07,
      "loss": 0.0012,
      "step": 73120
    },
    {
      "epoch": 13296.363636363636,
      "grad_norm": 0.16723209619522095,
      "learning_rate": 6.081630076372821e-07,
      "loss": 0.0012,
      "step": 73130
    },
    {
      "epoch": 13298.181818181818,
      "grad_norm": 0.15997713804244995,
      "learning_rate": 6.080494045814011e-07,
      "loss": 0.0012,
      "step": 73140
    },
    {
      "epoch": 13300.0,
      "grad_norm": 0.1446250081062317,
      "learning_rate": 6.079357956741899e-07,
      "loss": 0.001,
      "step": 73150
    },
    {
      "epoch": 13301.818181818182,
      "grad_norm": 0.14229167997837067,
      "learning_rate": 6.078221809218016e-07,
      "loss": 0.0012,
      "step": 73160
    },
    {
      "epoch": 13303.636363636364,
      "grad_norm": 0.17071017622947693,
      "learning_rate": 6.077085603303882e-07,
      "loss": 0.001,
      "step": 73170
    },
    {
      "epoch": 13305.454545454546,
      "grad_norm": 0.0009312885231338441,
      "learning_rate": 6.075949339061032e-07,
      "loss": 0.0008,
      "step": 73180
    },
    {
      "epoch": 13307.272727272728,
      "grad_norm": 0.010519535280764103,
      "learning_rate": 6.074813016550997e-07,
      "loss": 0.0017,
      "step": 73190
    },
    {
      "epoch": 13309.09090909091,
      "grad_norm": 0.0011993837542831898,
      "learning_rate": 6.073676635835316e-07,
      "loss": 0.0009,
      "step": 73200
    },
    {
      "epoch": 13310.90909090909,
      "grad_norm": 0.0010290471836924553,
      "learning_rate": 6.072540196975527e-07,
      "loss": 0.0012,
      "step": 73210
    },
    {
      "epoch": 13312.727272727272,
      "grad_norm": 0.2663861811161041,
      "learning_rate": 6.071403700033173e-07,
      "loss": 0.0012,
      "step": 73220
    },
    {
      "epoch": 13314.545454545454,
      "grad_norm": 0.1666298508644104,
      "learning_rate": 6.070267145069799e-07,
      "loss": 0.001,
      "step": 73230
    },
    {
      "epoch": 13316.363636363636,
      "grad_norm": 0.21561069786548615,
      "learning_rate": 6.069130532146958e-07,
      "loss": 0.0009,
      "step": 73240
    },
    {
      "epoch": 13318.181818181818,
      "grad_norm": 0.010579314082860947,
      "learning_rate": 6.0679938613262e-07,
      "loss": 0.0013,
      "step": 73250
    },
    {
      "epoch": 13320.0,
      "grad_norm": 0.0011707596713677049,
      "learning_rate": 6.06685713266908e-07,
      "loss": 0.0009,
      "step": 73260
    },
    {
      "epoch": 13321.818181818182,
      "grad_norm": 0.27392876148223877,
      "learning_rate": 6.065720346237156e-07,
      "loss": 0.0012,
      "step": 73270
    },
    {
      "epoch": 13323.636363636364,
      "grad_norm": 0.001169789582490921,
      "learning_rate": 6.064583502091993e-07,
      "loss": 0.0009,
      "step": 73280
    },
    {
      "epoch": 13325.454545454546,
      "grad_norm": 0.0009730614256113768,
      "learning_rate": 6.063446600295153e-07,
      "loss": 0.0012,
      "step": 73290
    },
    {
      "epoch": 13327.272727272728,
      "grad_norm": 0.0011425192933529615,
      "learning_rate": 6.062309640908206e-07,
      "loss": 0.0009,
      "step": 73300
    },
    {
      "epoch": 13329.09090909091,
      "grad_norm": 0.0017326045781373978,
      "learning_rate": 6.061172623992721e-07,
      "loss": 0.0012,
      "step": 73310
    },
    {
      "epoch": 13330.90909090909,
      "grad_norm": 0.002690479625016451,
      "learning_rate": 6.060035549610274e-07,
      "loss": 0.001,
      "step": 73320
    },
    {
      "epoch": 13332.727272727272,
      "grad_norm": 0.21667373180389404,
      "learning_rate": 6.05889841782244e-07,
      "loss": 0.0014,
      "step": 73330
    },
    {
      "epoch": 13334.545454545454,
      "grad_norm": 0.001027564867399633,
      "learning_rate": 6.057761228690802e-07,
      "loss": 0.0008,
      "step": 73340
    },
    {
      "epoch": 13336.363636363636,
      "grad_norm": 0.2661508023738861,
      "learning_rate": 6.056623982276944e-07,
      "loss": 0.0015,
      "step": 73350
    },
    {
      "epoch": 13338.181818181818,
      "grad_norm": 0.2972111701965332,
      "learning_rate": 6.05548667864245e-07,
      "loss": 0.0014,
      "step": 73360
    },
    {
      "epoch": 13340.0,
      "grad_norm": 0.17664723098278046,
      "learning_rate": 6.05434931784891e-07,
      "loss": 0.0008,
      "step": 73370
    },
    {
      "epoch": 13341.818181818182,
      "grad_norm": 0.30125245451927185,
      "learning_rate": 6.05321189995792e-07,
      "loss": 0.0013,
      "step": 73380
    },
    {
      "epoch": 13343.636363636364,
      "grad_norm": 0.0009437522967346013,
      "learning_rate": 6.052074425031074e-07,
      "loss": 0.0013,
      "step": 73390
    },
    {
      "epoch": 13345.454545454546,
      "grad_norm": 0.16840532422065735,
      "learning_rate": 6.05093689312997e-07,
      "loss": 0.001,
      "step": 73400
    },
    {
      "epoch": 13347.272727272728,
      "grad_norm": 0.19380803406238556,
      "learning_rate": 6.049799304316213e-07,
      "loss": 0.0011,
      "step": 73410
    },
    {
      "epoch": 13349.09090909091,
      "grad_norm": 0.0015863789012655616,
      "learning_rate": 6.048661658651404e-07,
      "loss": 0.001,
      "step": 73420
    },
    {
      "epoch": 13350.90909090909,
      "grad_norm": 0.0012981761246919632,
      "learning_rate": 6.047523956197155e-07,
      "loss": 0.001,
      "step": 73430
    },
    {
      "epoch": 13352.727272727272,
      "grad_norm": 0.17301033437252045,
      "learning_rate": 6.046386197015075e-07,
      "loss": 0.0014,
      "step": 73440
    },
    {
      "epoch": 13354.545454545454,
      "grad_norm": 0.2838535010814667,
      "learning_rate": 6.045248381166782e-07,
      "loss": 0.0009,
      "step": 73450
    },
    {
      "epoch": 13356.363636363636,
      "grad_norm": 0.1779554933309555,
      "learning_rate": 6.044110508713888e-07,
      "loss": 0.001,
      "step": 73460
    },
    {
      "epoch": 13358.181818181818,
      "grad_norm": 0.29631882905960083,
      "learning_rate": 6.042972579718018e-07,
      "loss": 0.0014,
      "step": 73470
    },
    {
      "epoch": 13360.0,
      "grad_norm": 0.0022185316774994135,
      "learning_rate": 6.041834594240795e-07,
      "loss": 0.0009,
      "step": 73480
    },
    {
      "epoch": 13361.818181818182,
      "grad_norm": 0.0011671885149553418,
      "learning_rate": 6.040696552343845e-07,
      "loss": 0.0009,
      "step": 73490
    },
    {
      "epoch": 13363.636363636364,
      "grad_norm": 0.0007566357962787151,
      "learning_rate": 6.039558454088795e-07,
      "loss": 0.0014,
      "step": 73500
    },
    {
      "epoch": 13363.636363636364,
      "eval_loss": 4.9549736976623535,
      "eval_runtime": 0.9515,
      "eval_samples_per_second": 10.509,
      "eval_steps_per_second": 5.255,
      "step": 73500
    },
    {
      "epoch": 13365.454545454546,
      "grad_norm": 0.03623214736580849,
      "learning_rate": 6.038420299537284e-07,
      "loss": 0.001,
      "step": 73510
    },
    {
      "epoch": 13367.272727272728,
      "grad_norm": 0.001151455333456397,
      "learning_rate": 6.037282088750944e-07,
      "loss": 0.0008,
      "step": 73520
    },
    {
      "epoch": 13369.09090909091,
      "grad_norm": 0.005768944974988699,
      "learning_rate": 6.036143821791412e-07,
      "loss": 0.0015,
      "step": 73530
    },
    {
      "epoch": 13370.90909090909,
      "grad_norm": 0.21564416587352753,
      "learning_rate": 6.035005498720334e-07,
      "loss": 0.0009,
      "step": 73540
    },
    {
      "epoch": 13372.727272727272,
      "grad_norm": 0.0009275352931581438,
      "learning_rate": 6.033867119599353e-07,
      "loss": 0.001,
      "step": 73550
    },
    {
      "epoch": 13374.545454545454,
      "grad_norm": 0.177666574716568,
      "learning_rate": 6.032728684490117e-07,
      "loss": 0.0015,
      "step": 73560
    },
    {
      "epoch": 13376.363636363636,
      "grad_norm": 0.0006999114993959665,
      "learning_rate": 6.031590193454277e-07,
      "loss": 0.0009,
      "step": 73570
    },
    {
      "epoch": 13378.181818181818,
      "grad_norm": 0.0006983703351579607,
      "learning_rate": 6.030451646553488e-07,
      "loss": 0.0011,
      "step": 73580
    },
    {
      "epoch": 13380.0,
      "grad_norm": 0.18401271104812622,
      "learning_rate": 6.029313043849406e-07,
      "loss": 0.0012,
      "step": 73590
    },
    {
      "epoch": 13381.818181818182,
      "grad_norm": 0.000946068437770009,
      "learning_rate": 6.028174385403692e-07,
      "loss": 0.0009,
      "step": 73600
    },
    {
      "epoch": 13383.636363636364,
      "grad_norm": 0.191239595413208,
      "learning_rate": 6.02703567127801e-07,
      "loss": 0.0013,
      "step": 73610
    },
    {
      "epoch": 13385.454545454546,
      "grad_norm": 0.001338641275651753,
      "learning_rate": 6.025896901534023e-07,
      "loss": 0.0011,
      "step": 73620
    },
    {
      "epoch": 13387.272727272728,
      "grad_norm": 0.0026605220045894384,
      "learning_rate": 6.024758076233403e-07,
      "loss": 0.0009,
      "step": 73630
    },
    {
      "epoch": 13389.09090909091,
      "grad_norm": 0.17320337891578674,
      "learning_rate": 6.023619195437821e-07,
      "loss": 0.0013,
      "step": 73640
    },
    {
      "epoch": 13390.90909090909,
      "grad_norm": 0.2760922312736511,
      "learning_rate": 6.02248025920895e-07,
      "loss": 0.0011,
      "step": 73650
    },
    {
      "epoch": 13392.727272727272,
      "grad_norm": 0.17270609736442566,
      "learning_rate": 6.021341267608475e-07,
      "loss": 0.001,
      "step": 73660
    },
    {
      "epoch": 13394.545454545454,
      "grad_norm": 0.21273311972618103,
      "learning_rate": 6.020202220698071e-07,
      "loss": 0.001,
      "step": 73670
    },
    {
      "epoch": 13396.363636363636,
      "grad_norm": 0.18500421941280365,
      "learning_rate": 6.019063118539424e-07,
      "loss": 0.0013,
      "step": 73680
    },
    {
      "epoch": 13398.181818181818,
      "grad_norm": 0.16360695660114288,
      "learning_rate": 6.01792396119422e-07,
      "loss": 0.0011,
      "step": 73690
    },
    {
      "epoch": 13400.0,
      "grad_norm": 0.21035480499267578,
      "learning_rate": 6.016784748724152e-07,
      "loss": 0.001,
      "step": 73700
    },
    {
      "epoch": 13401.818181818182,
      "grad_norm": 0.2772304117679596,
      "learning_rate": 6.015645481190911e-07,
      "loss": 0.0012,
      "step": 73710
    },
    {
      "epoch": 13403.636363636364,
      "grad_norm": 0.20087474584579468,
      "learning_rate": 6.014506158656194e-07,
      "loss": 0.001,
      "step": 73720
    },
    {
      "epoch": 13405.454545454546,
      "grad_norm": 0.21445737779140472,
      "learning_rate": 6.013366781181699e-07,
      "loss": 0.0011,
      "step": 73730
    },
    {
      "epoch": 13407.272727272728,
      "grad_norm": 0.0006795339868403971,
      "learning_rate": 6.01222734882913e-07,
      "loss": 0.0009,
      "step": 73740
    },
    {
      "epoch": 13409.09090909091,
      "grad_norm": 0.0007291310466825962,
      "learning_rate": 6.01108786166019e-07,
      "loss": 0.0012,
      "step": 73750
    },
    {
      "epoch": 13410.90909090909,
      "grad_norm": 0.006609997246414423,
      "learning_rate": 6.009948319736589e-07,
      "loss": 0.0012,
      "step": 73760
    },
    {
      "epoch": 13412.727272727272,
      "grad_norm": 0.21105539798736572,
      "learning_rate": 6.008808723120034e-07,
      "loss": 0.0007,
      "step": 73770
    },
    {
      "epoch": 13414.545454545454,
      "grad_norm": 0.17527279257774353,
      "learning_rate": 6.007669071872244e-07,
      "loss": 0.0017,
      "step": 73780
    },
    {
      "epoch": 13416.363636363636,
      "grad_norm": 0.16892209649085999,
      "learning_rate": 6.006529366054934e-07,
      "loss": 0.0007,
      "step": 73790
    },
    {
      "epoch": 13418.181818181818,
      "grad_norm": 0.2826990485191345,
      "learning_rate": 6.005389605729824e-07,
      "loss": 0.0014,
      "step": 73800
    },
    {
      "epoch": 13420.0,
      "grad_norm": 0.002264405135065317,
      "learning_rate": 6.004249790958636e-07,
      "loss": 0.0009,
      "step": 73810
    },
    {
      "epoch": 13421.818181818182,
      "grad_norm": 0.2913309335708618,
      "learning_rate": 6.003109921803094e-07,
      "loss": 0.001,
      "step": 73820
    },
    {
      "epoch": 13423.636363636364,
      "grad_norm": 0.18722352385520935,
      "learning_rate": 6.001969998324932e-07,
      "loss": 0.0012,
      "step": 73830
    },
    {
      "epoch": 13425.454545454546,
      "grad_norm": 0.2024628221988678,
      "learning_rate": 6.000830020585876e-07,
      "loss": 0.0009,
      "step": 73840
    },
    {
      "epoch": 13427.272727272728,
      "grad_norm": 0.0008609025971964002,
      "learning_rate": 5.999689988647666e-07,
      "loss": 0.0011,
      "step": 73850
    },
    {
      "epoch": 13429.09090909091,
      "grad_norm": 0.16148163378238678,
      "learning_rate": 5.998549902572034e-07,
      "loss": 0.0012,
      "step": 73860
    },
    {
      "epoch": 13430.90909090909,
      "grad_norm": 0.0007968202698975801,
      "learning_rate": 5.997409762420726e-07,
      "loss": 0.0012,
      "step": 73870
    },
    {
      "epoch": 13432.727272727272,
      "grad_norm": 0.0007490906282328069,
      "learning_rate": 5.99626956825548e-07,
      "loss": 0.001,
      "step": 73880
    },
    {
      "epoch": 13434.545454545454,
      "grad_norm": 0.1747443526983261,
      "learning_rate": 5.995129320138046e-07,
      "loss": 0.001,
      "step": 73890
    },
    {
      "epoch": 13436.363636363636,
      "grad_norm": 0.1704733967781067,
      "learning_rate": 5.993989018130172e-07,
      "loss": 0.0012,
      "step": 73900
    },
    {
      "epoch": 13438.181818181818,
      "grad_norm": 0.0011034186463803053,
      "learning_rate": 5.99284866229361e-07,
      "loss": 0.0009,
      "step": 73910
    },
    {
      "epoch": 13440.0,
      "grad_norm": 0.0009303987608291209,
      "learning_rate": 5.991708252690116e-07,
      "loss": 0.0012,
      "step": 73920
    },
    {
      "epoch": 13441.818181818182,
      "grad_norm": 0.21316125988960266,
      "learning_rate": 5.990567789381446e-07,
      "loss": 0.001,
      "step": 73930
    },
    {
      "epoch": 13443.636363636364,
      "grad_norm": 0.0009716669446788728,
      "learning_rate": 5.989427272429364e-07,
      "loss": 0.0011,
      "step": 73940
    },
    {
      "epoch": 13445.454545454546,
      "grad_norm": 0.0016293218359351158,
      "learning_rate": 5.98828670189563e-07,
      "loss": 0.0012,
      "step": 73950
    },
    {
      "epoch": 13447.272727272728,
      "grad_norm": 0.1698768138885498,
      "learning_rate": 5.987146077842014e-07,
      "loss": 0.001,
      "step": 73960
    },
    {
      "epoch": 13449.09090909091,
      "grad_norm": 0.17091874778270721,
      "learning_rate": 5.986005400330284e-07,
      "loss": 0.001,
      "step": 73970
    },
    {
      "epoch": 13450.90909090909,
      "grad_norm": 0.0006985343643464148,
      "learning_rate": 5.984864669422213e-07,
      "loss": 0.0012,
      "step": 73980
    },
    {
      "epoch": 13452.727272727272,
      "grad_norm": 0.20172907412052155,
      "learning_rate": 5.983723885179575e-07,
      "loss": 0.001,
      "step": 73990
    },
    {
      "epoch": 13454.545454545454,
      "grad_norm": 0.0010850505204871297,
      "learning_rate": 5.98258304766415e-07,
      "loss": 0.001,
      "step": 74000
    },
    {
      "epoch": 13454.545454545454,
      "eval_loss": 4.947445869445801,
      "eval_runtime": 0.955,
      "eval_samples_per_second": 10.471,
      "eval_steps_per_second": 5.236,
      "step": 74000
    },
    {
      "epoch": 13456.363636363636,
      "grad_norm": 0.0009954432025551796,
      "learning_rate": 5.981442156937719e-07,
      "loss": 0.0009,
      "step": 74010
    },
    {
      "epoch": 13458.181818181818,
      "grad_norm": 0.001114496262744069,
      "learning_rate": 5.980301213062066e-07,
      "loss": 0.0012,
      "step": 74020
    },
    {
      "epoch": 13460.0,
      "grad_norm": 0.27086758613586426,
      "learning_rate": 5.979160216098976e-07,
      "loss": 0.0012,
      "step": 74030
    },
    {
      "epoch": 13461.818181818182,
      "grad_norm": 0.0014943116111680865,
      "learning_rate": 5.97801916611024e-07,
      "loss": 0.001,
      "step": 74040
    },
    {
      "epoch": 13463.636363636364,
      "grad_norm": 0.0019649979658424854,
      "learning_rate": 5.976878063157652e-07,
      "loss": 0.0011,
      "step": 74050
    },
    {
      "epoch": 13465.454545454546,
      "grad_norm": 0.20297783613204956,
      "learning_rate": 5.975736907303008e-07,
      "loss": 0.0011,
      "step": 74060
    },
    {
      "epoch": 13467.272727272728,
      "grad_norm": 0.19312377274036407,
      "learning_rate": 5.974595698608102e-07,
      "loss": 0.0015,
      "step": 74070
    },
    {
      "epoch": 13469.09090909091,
      "grad_norm": 0.26588648557662964,
      "learning_rate": 5.973454437134739e-07,
      "loss": 0.0009,
      "step": 74080
    },
    {
      "epoch": 13470.90909090909,
      "grad_norm": 0.2113056480884552,
      "learning_rate": 5.972313122944723e-07,
      "loss": 0.0011,
      "step": 74090
    },
    {
      "epoch": 13472.727272727272,
      "grad_norm": 0.16369885206222534,
      "learning_rate": 5.97117175609986e-07,
      "loss": 0.001,
      "step": 74100
    },
    {
      "epoch": 13474.545454545454,
      "grad_norm": 0.0008028519223444164,
      "learning_rate": 5.97003033666196e-07,
      "loss": 0.0009,
      "step": 74110
    },
    {
      "epoch": 13476.363636363636,
      "grad_norm": 0.2134476602077484,
      "learning_rate": 5.968888864692834e-07,
      "loss": 0.0012,
      "step": 74120
    },
    {
      "epoch": 13478.181818181818,
      "grad_norm": 0.21383735537528992,
      "learning_rate": 5.967747340254302e-07,
      "loss": 0.0012,
      "step": 74130
    },
    {
      "epoch": 13480.0,
      "grad_norm": 0.21391358971595764,
      "learning_rate": 5.966605763408178e-07,
      "loss": 0.001,
      "step": 74140
    },
    {
      "epoch": 13481.818181818182,
      "grad_norm": 0.22540166974067688,
      "learning_rate": 5.965464134216284e-07,
      "loss": 0.0012,
      "step": 74150
    },
    {
      "epoch": 13483.636363636364,
      "grad_norm": 0.2248164415359497,
      "learning_rate": 5.964322452740445e-07,
      "loss": 0.0012,
      "step": 74160
    },
    {
      "epoch": 13485.454545454546,
      "grad_norm": 0.002111904788762331,
      "learning_rate": 5.963180719042489e-07,
      "loss": 0.0011,
      "step": 74170
    },
    {
      "epoch": 13487.272727272728,
      "grad_norm": 0.0014054656494408846,
      "learning_rate": 5.962038933184243e-07,
      "loss": 0.0009,
      "step": 74180
    },
    {
      "epoch": 13489.09090909091,
      "grad_norm": 0.0007485969108529389,
      "learning_rate": 5.96089709522754e-07,
      "loss": 0.0012,
      "step": 74190
    },
    {
      "epoch": 13490.90909090909,
      "grad_norm": 0.16927984356880188,
      "learning_rate": 5.959755205234217e-07,
      "loss": 0.0012,
      "step": 74200
    },
    {
      "epoch": 13492.727272727272,
      "grad_norm": 0.1680610328912735,
      "learning_rate": 5.958613263266112e-07,
      "loss": 0.0012,
      "step": 74210
    },
    {
      "epoch": 13494.545454545454,
      "grad_norm": 0.0010376789141446352,
      "learning_rate": 5.957471269385065e-07,
      "loss": 0.0011,
      "step": 74220
    },
    {
      "epoch": 13496.363636363636,
      "grad_norm": 0.0007930961437523365,
      "learning_rate": 5.956329223652919e-07,
      "loss": 0.0009,
      "step": 74230
    },
    {
      "epoch": 13498.181818181818,
      "grad_norm": 0.2125694304704666,
      "learning_rate": 5.955187126131522e-07,
      "loss": 0.0012,
      "step": 74240
    },
    {
      "epoch": 13500.0,
      "grad_norm": 0.0012241476215422153,
      "learning_rate": 5.954044976882723e-07,
      "loss": 0.0011,
      "step": 74250
    },
    {
      "epoch": 13501.818181818182,
      "grad_norm": 0.0008305463707074523,
      "learning_rate": 5.952902775968376e-07,
      "loss": 0.0012,
      "step": 74260
    },
    {
      "epoch": 13503.636363636364,
      "grad_norm": 0.1428532600402832,
      "learning_rate": 5.951760523450331e-07,
      "loss": 0.0012,
      "step": 74270
    },
    {
      "epoch": 13505.454545454546,
      "grad_norm": 0.0011281808838248253,
      "learning_rate": 5.950618219390451e-07,
      "loss": 0.001,
      "step": 74280
    },
    {
      "epoch": 13507.272727272728,
      "grad_norm": 0.0021433786023408175,
      "learning_rate": 5.949475863850595e-07,
      "loss": 0.001,
      "step": 74290
    },
    {
      "epoch": 13509.09090909091,
      "grad_norm": 0.0008178878924809396,
      "learning_rate": 5.948333456892623e-07,
      "loss": 0.0012,
      "step": 74300
    },
    {
      "epoch": 13510.90909090909,
      "grad_norm": 0.20304393768310547,
      "learning_rate": 5.947190998578406e-07,
      "loss": 0.0012,
      "step": 74310
    },
    {
      "epoch": 13512.727272727272,
      "grad_norm": 0.0008701295591890812,
      "learning_rate": 5.946048488969811e-07,
      "loss": 0.0009,
      "step": 74320
    },
    {
      "epoch": 13514.545454545454,
      "grad_norm": 0.24644218385219574,
      "learning_rate": 5.94490592812871e-07,
      "loss": 0.0013,
      "step": 74330
    },
    {
      "epoch": 13516.363636363636,
      "grad_norm": 0.16541123390197754,
      "learning_rate": 5.943763316116976e-07,
      "loss": 0.001,
      "step": 74340
    },
    {
      "epoch": 13518.181818181818,
      "grad_norm": 0.22444678843021393,
      "learning_rate": 5.942620652996487e-07,
      "loss": 0.001,
      "step": 74350
    },
    {
      "epoch": 13520.0,
      "grad_norm": 0.16827820241451263,
      "learning_rate": 5.941477938829125e-07,
      "loss": 0.0012,
      "step": 74360
    },
    {
      "epoch": 13521.818181818182,
      "grad_norm": 0.0006818413967266679,
      "learning_rate": 5.94033517367677e-07,
      "loss": 0.0011,
      "step": 74370
    },
    {
      "epoch": 13523.636363636364,
      "grad_norm": 0.20056812465190887,
      "learning_rate": 5.939192357601309e-07,
      "loss": 0.0012,
      "step": 74380
    },
    {
      "epoch": 13525.454545454546,
      "grad_norm": 0.16418595612049103,
      "learning_rate": 5.938049490664629e-07,
      "loss": 0.0011,
      "step": 74390
    },
    {
      "epoch": 13527.272727272728,
      "grad_norm": 0.0006611411226913333,
      "learning_rate": 5.936906572928624e-07,
      "loss": 0.0009,
      "step": 74400
    },
    {
      "epoch": 13529.09090909091,
      "grad_norm": 0.371576726436615,
      "learning_rate": 5.935763604455184e-07,
      "loss": 0.0013,
      "step": 74410
    },
    {
      "epoch": 13530.90909090909,
      "grad_norm": 0.0011266974033787847,
      "learning_rate": 5.93462058530621e-07,
      "loss": 0.0009,
      "step": 74420
    },
    {
      "epoch": 13532.727272727272,
      "grad_norm": 0.19849905371665955,
      "learning_rate": 5.933477515543595e-07,
      "loss": 0.001,
      "step": 74430
    },
    {
      "epoch": 13534.545454545454,
      "grad_norm": 0.16627749800682068,
      "learning_rate": 5.932334395229246e-07,
      "loss": 0.0014,
      "step": 74440
    },
    {
      "epoch": 13536.363636363636,
      "grad_norm": 0.2595413029193878,
      "learning_rate": 5.931191224425067e-07,
      "loss": 0.0012,
      "step": 74450
    },
    {
      "epoch": 13538.181818181818,
      "grad_norm": 0.16435588896274567,
      "learning_rate": 5.930048003192964e-07,
      "loss": 0.001,
      "step": 74460
    },
    {
      "epoch": 13540.0,
      "grad_norm": 0.16079799830913544,
      "learning_rate": 5.928904731594849e-07,
      "loss": 0.0012,
      "step": 74470
    },
    {
      "epoch": 13541.818181818182,
      "grad_norm": 0.0007674999651499093,
      "learning_rate": 5.927761409692636e-07,
      "loss": 0.0011,
      "step": 74480
    },
    {
      "epoch": 13543.636363636364,
      "grad_norm": 0.0008320832857862115,
      "learning_rate": 5.926618037548237e-07,
      "loss": 0.0012,
      "step": 74490
    },
    {
      "epoch": 13545.454545454546,
      "grad_norm": 0.0008883526315912604,
      "learning_rate": 5.925474615223572e-07,
      "loss": 0.001,
      "step": 74500
    },
    {
      "epoch": 13545.454545454546,
      "eval_loss": 4.966068267822266,
      "eval_runtime": 0.9497,
      "eval_samples_per_second": 10.529,
      "eval_steps_per_second": 5.265,
      "step": 74500
    },
    {
      "epoch": 13547.272727272728,
      "grad_norm": 0.0011699865572154522,
      "learning_rate": 5.924331142780564e-07,
      "loss": 0.001,
      "step": 74510
    },
    {
      "epoch": 13549.09090909091,
      "grad_norm": 0.0008101015700958669,
      "learning_rate": 5.923187620281135e-07,
      "loss": 0.0012,
      "step": 74520
    },
    {
      "epoch": 13550.90909090909,
      "grad_norm": 0.0017555742524564266,
      "learning_rate": 5.922044047787211e-07,
      "loss": 0.0012,
      "step": 74530
    },
    {
      "epoch": 13552.727272727272,
      "grad_norm": 0.0005563895101659,
      "learning_rate": 5.920900425360722e-07,
      "loss": 0.001,
      "step": 74540
    },
    {
      "epoch": 13554.545454545454,
      "grad_norm": 0.19115285575389862,
      "learning_rate": 5.9197567530636e-07,
      "loss": 0.0012,
      "step": 74550
    },
    {
      "epoch": 13556.363636363636,
      "grad_norm": 0.000939659308642149,
      "learning_rate": 5.918613030957781e-07,
      "loss": 0.0009,
      "step": 74560
    },
    {
      "epoch": 13558.181818181818,
      "grad_norm": 0.0012270257575437427,
      "learning_rate": 5.917469259105201e-07,
      "loss": 0.001,
      "step": 74570
    },
    {
      "epoch": 13560.0,
      "grad_norm": 0.0017463058466091752,
      "learning_rate": 5.916325437567799e-07,
      "loss": 0.0012,
      "step": 74580
    },
    {
      "epoch": 13561.818181818182,
      "grad_norm": 0.2275889664888382,
      "learning_rate": 5.915181566407519e-07,
      "loss": 0.0012,
      "step": 74590
    },
    {
      "epoch": 13563.636363636364,
      "grad_norm": 0.0009172822465188801,
      "learning_rate": 5.914037645686308e-07,
      "loss": 0.0009,
      "step": 74600
    },
    {
      "epoch": 13565.454545454546,
      "grad_norm": 0.0018765759887173772,
      "learning_rate": 5.91289367546611e-07,
      "loss": 0.001,
      "step": 74610
    },
    {
      "epoch": 13567.272727272728,
      "grad_norm": 0.0011059746611863375,
      "learning_rate": 5.911749655808879e-07,
      "loss": 0.0013,
      "step": 74620
    },
    {
      "epoch": 13569.09090909091,
      "grad_norm": 0.20001913607120514,
      "learning_rate": 5.91060558677657e-07,
      "loss": 0.0011,
      "step": 74630
    },
    {
      "epoch": 13570.90909090909,
      "grad_norm": 0.001234452472999692,
      "learning_rate": 5.909461468431134e-07,
      "loss": 0.001,
      "step": 74640
    },
    {
      "epoch": 13572.727272727272,
      "grad_norm": 0.011064053513109684,
      "learning_rate": 5.908317300834534e-07,
      "loss": 0.0012,
      "step": 74650
    },
    {
      "epoch": 13574.545454545454,
      "grad_norm": 0.000777857203502208,
      "learning_rate": 5.907173084048731e-07,
      "loss": 0.0009,
      "step": 74660
    },
    {
      "epoch": 13576.363636363636,
      "grad_norm": 0.20211127400398254,
      "learning_rate": 5.906028818135687e-07,
      "loss": 0.0011,
      "step": 74670
    },
    {
      "epoch": 13578.181818181818,
      "grad_norm": 0.17198221385478973,
      "learning_rate": 5.904884503157371e-07,
      "loss": 0.0012,
      "step": 74680
    },
    {
      "epoch": 13580.0,
      "grad_norm": 0.21553075313568115,
      "learning_rate": 5.903740139175751e-07,
      "loss": 0.0011,
      "step": 74690
    },
    {
      "epoch": 13581.818181818182,
      "grad_norm": 0.2722608149051666,
      "learning_rate": 5.9025957262528e-07,
      "loss": 0.0013,
      "step": 74700
    },
    {
      "epoch": 13583.636363636364,
      "grad_norm": 0.2595730721950531,
      "learning_rate": 5.901451264450492e-07,
      "loss": 0.001,
      "step": 74710
    },
    {
      "epoch": 13585.454545454546,
      "grad_norm": 0.0009681823430582881,
      "learning_rate": 5.900306753830807e-07,
      "loss": 0.0009,
      "step": 74720
    },
    {
      "epoch": 13587.272727272728,
      "grad_norm": 0.19679448008537292,
      "learning_rate": 5.899162194455722e-07,
      "loss": 0.0013,
      "step": 74730
    },
    {
      "epoch": 13589.09090909091,
      "grad_norm": 0.2008107304573059,
      "learning_rate": 5.898017586387219e-07,
      "loss": 0.0011,
      "step": 74740
    },
    {
      "epoch": 13590.90909090909,
      "grad_norm": 0.19295385479927063,
      "learning_rate": 5.896872929687286e-07,
      "loss": 0.0011,
      "step": 74750
    },
    {
      "epoch": 13592.727272727272,
      "grad_norm": 0.22491617500782013,
      "learning_rate": 5.895728224417912e-07,
      "loss": 0.0011,
      "step": 74760
    },
    {
      "epoch": 13594.545454545454,
      "grad_norm": 0.0007859325269237161,
      "learning_rate": 5.894583470641083e-07,
      "loss": 0.0011,
      "step": 74770
    },
    {
      "epoch": 13596.363636363636,
      "grad_norm": 0.0009869328932836652,
      "learning_rate": 5.893438668418797e-07,
      "loss": 0.001,
      "step": 74780
    },
    {
      "epoch": 13598.181818181818,
      "grad_norm": 0.04258108139038086,
      "learning_rate": 5.892293817813046e-07,
      "loss": 0.0013,
      "step": 74790
    },
    {
      "epoch": 13600.0,
      "grad_norm": 0.1627962291240692,
      "learning_rate": 5.891148918885833e-07,
      "loss": 0.0009,
      "step": 74800
    },
    {
      "epoch": 13601.818181818182,
      "grad_norm": 0.0016694599762558937,
      "learning_rate": 5.890003971699152e-07,
      "loss": 0.0012,
      "step": 74810
    },
    {
      "epoch": 13603.636363636364,
      "grad_norm": 0.0007953658350743353,
      "learning_rate": 5.888858976315016e-07,
      "loss": 0.0008,
      "step": 74820
    },
    {
      "epoch": 13605.454545454546,
      "grad_norm": 0.27659523487091064,
      "learning_rate": 5.887713932795423e-07,
      "loss": 0.0015,
      "step": 74830
    },
    {
      "epoch": 13607.272727272728,
      "grad_norm": 0.2681174576282501,
      "learning_rate": 5.886568841202387e-07,
      "loss": 0.001,
      "step": 74840
    },
    {
      "epoch": 13609.09090909091,
      "grad_norm": 0.16929320991039276,
      "learning_rate": 5.885423701597917e-07,
      "loss": 0.001,
      "step": 74850
    },
    {
      "epoch": 13610.90909090909,
      "grad_norm": 0.146249458193779,
      "learning_rate": 5.884278514044027e-07,
      "loss": 0.0012,
      "step": 74860
    },
    {
      "epoch": 13612.727272727272,
      "grad_norm": 0.0007030933047644794,
      "learning_rate": 5.883133278602736e-07,
      "loss": 0.0012,
      "step": 74870
    },
    {
      "epoch": 13614.545454545454,
      "grad_norm": 0.19620470702648163,
      "learning_rate": 5.881987995336062e-07,
      "loss": 0.0009,
      "step": 74880
    },
    {
      "epoch": 13616.363636363636,
      "grad_norm": 0.17543159425258636,
      "learning_rate": 5.880842664306026e-07,
      "loss": 0.0012,
      "step": 74890
    },
    {
      "epoch": 13618.181818181818,
      "grad_norm": 0.1818762719631195,
      "learning_rate": 5.879697285574654e-07,
      "loss": 0.001,
      "step": 74900
    },
    {
      "epoch": 13620.0,
      "grad_norm": 0.212416872382164,
      "learning_rate": 5.878551859203973e-07,
      "loss": 0.0011,
      "step": 74910
    },
    {
      "epoch": 13621.818181818182,
      "grad_norm": 0.2058817595243454,
      "learning_rate": 5.877406385256012e-07,
      "loss": 0.0012,
      "step": 74920
    },
    {
      "epoch": 13623.636363636364,
      "grad_norm": 0.0013869153335690498,
      "learning_rate": 5.876260863792802e-07,
      "loss": 0.0007,
      "step": 74930
    },
    {
      "epoch": 13625.454545454546,
      "grad_norm": 0.000861822918523103,
      "learning_rate": 5.87511529487638e-07,
      "loss": 0.0013,
      "step": 74940
    },
    {
      "epoch": 13627.272727272728,
      "grad_norm": 0.1884334832429886,
      "learning_rate": 5.873969678568783e-07,
      "loss": 0.0011,
      "step": 74950
    },
    {
      "epoch": 13629.09090909091,
      "grad_norm": 0.004243597853928804,
      "learning_rate": 5.872824014932049e-07,
      "loss": 0.001,
      "step": 74960
    },
    {
      "epoch": 13630.90909090909,
      "grad_norm": 0.1658460646867752,
      "learning_rate": 5.871678304028223e-07,
      "loss": 0.0012,
      "step": 74970
    },
    {
      "epoch": 13632.727272727272,
      "grad_norm": 0.17236042022705078,
      "learning_rate": 5.87053254591935e-07,
      "loss": 0.0012,
      "step": 74980
    },
    {
      "epoch": 13634.545454545454,
      "grad_norm": 0.001167463487945497,
      "learning_rate": 5.869386740667477e-07,
      "loss": 0.0009,
      "step": 74990
    },
    {
      "epoch": 13636.363636363636,
      "grad_norm": 0.19977505505084991,
      "learning_rate": 5.868240888334652e-07,
      "loss": 0.0011,
      "step": 75000
    },
    {
      "epoch": 13636.363636363636,
      "eval_loss": 5.041762828826904,
      "eval_runtime": 0.9526,
      "eval_samples_per_second": 10.497,
      "eval_steps_per_second": 5.249,
      "step": 75000
    },
    {
      "epoch": 13638.181818181818,
      "grad_norm": 0.0030470024794340134,
      "learning_rate": 5.86709498898293e-07,
      "loss": 0.001,
      "step": 75010
    },
    {
      "epoch": 13640.0,
      "grad_norm": 0.0048012747429311275,
      "learning_rate": 5.865949042674365e-07,
      "loss": 0.0012,
      "step": 75020
    },
    {
      "epoch": 13641.818181818182,
      "grad_norm": 0.16742156445980072,
      "learning_rate": 5.864803049471019e-07,
      "loss": 0.0012,
      "step": 75030
    },
    {
      "epoch": 13643.636363636364,
      "grad_norm": 0.16752055287361145,
      "learning_rate": 5.863657009434945e-07,
      "loss": 0.001,
      "step": 75040
    },
    {
      "epoch": 13645.454545454546,
      "grad_norm": 0.04342013597488403,
      "learning_rate": 5.862510922628212e-07,
      "loss": 0.0013,
      "step": 75050
    },
    {
      "epoch": 13647.272727272728,
      "grad_norm": 0.24202705919742584,
      "learning_rate": 5.861364789112884e-07,
      "loss": 0.0011,
      "step": 75060
    },
    {
      "epoch": 13649.09090909091,
      "grad_norm": 0.0007640671683475375,
      "learning_rate": 5.860218608951028e-07,
      "loss": 0.0011,
      "step": 75070
    },
    {
      "epoch": 13650.90909090909,
      "grad_norm": 0.20384109020233154,
      "learning_rate": 5.859072382204713e-07,
      "loss": 0.0012,
      "step": 75080
    },
    {
      "epoch": 13652.727272727272,
      "grad_norm": 0.17547959089279175,
      "learning_rate": 5.857926108936014e-07,
      "loss": 0.0012,
      "step": 75090
    },
    {
      "epoch": 13654.545454545454,
      "grad_norm": 0.19845852255821228,
      "learning_rate": 5.856779789207009e-07,
      "loss": 0.001,
      "step": 75100
    },
    {
      "epoch": 13656.363636363636,
      "grad_norm": 0.001122870366089046,
      "learning_rate": 5.855633423079771e-07,
      "loss": 0.0009,
      "step": 75110
    },
    {
      "epoch": 13658.181818181818,
      "grad_norm": 0.19438984990119934,
      "learning_rate": 5.854487010616384e-07,
      "loss": 0.0013,
      "step": 75120
    },
    {
      "epoch": 13660.0,
      "grad_norm": 0.21316687762737274,
      "learning_rate": 5.853340551878928e-07,
      "loss": 0.001,
      "step": 75130
    },
    {
      "epoch": 13661.818181818182,
      "grad_norm": 0.0009957632282748818,
      "learning_rate": 5.852194046929493e-07,
      "loss": 0.0012,
      "step": 75140
    },
    {
      "epoch": 13663.636363636364,
      "grad_norm": 0.017616061493754387,
      "learning_rate": 5.851047495830163e-07,
      "loss": 0.0013,
      "step": 75150
    },
    {
      "epoch": 13665.454545454546,
      "grad_norm": 0.16609728336334229,
      "learning_rate": 5.84990089864303e-07,
      "loss": 0.0009,
      "step": 75160
    },
    {
      "epoch": 13667.272727272728,
      "grad_norm": 0.20109115540981293,
      "learning_rate": 5.848754255430187e-07,
      "loss": 0.0012,
      "step": 75170
    },
    {
      "epoch": 13669.09090909091,
      "grad_norm": 0.0010167955188080668,
      "learning_rate": 5.847607566253732e-07,
      "loss": 0.0011,
      "step": 75180
    },
    {
      "epoch": 13670.90909090909,
      "grad_norm": 0.21004654467105865,
      "learning_rate": 5.846460831175759e-07,
      "loss": 0.0012,
      "step": 75190
    },
    {
      "epoch": 13672.727272727272,
      "grad_norm": 0.21121369302272797,
      "learning_rate": 5.84531405025837e-07,
      "loss": 0.001,
      "step": 75200
    },
    {
      "epoch": 13674.545454545454,
      "grad_norm": 0.0009981341427192092,
      "learning_rate": 5.844167223563668e-07,
      "loss": 0.0012,
      "step": 75210
    },
    {
      "epoch": 13676.363636363636,
      "grad_norm": 0.14578601717948914,
      "learning_rate": 5.843020351153761e-07,
      "loss": 0.0008,
      "step": 75220
    },
    {
      "epoch": 13678.181818181818,
      "grad_norm": 0.00890343077480793,
      "learning_rate": 5.841873433090753e-07,
      "loss": 0.0014,
      "step": 75230
    },
    {
      "epoch": 13680.0,
      "grad_norm": 0.25658777356147766,
      "learning_rate": 5.840726469436757e-07,
      "loss": 0.0009,
      "step": 75240
    },
    {
      "epoch": 13681.818181818182,
      "grad_norm": 0.1976061314344406,
      "learning_rate": 5.839579460253886e-07,
      "loss": 0.001,
      "step": 75250
    },
    {
      "epoch": 13683.636363636364,
      "grad_norm": 0.2655359208583832,
      "learning_rate": 5.838432405604253e-07,
      "loss": 0.0012,
      "step": 75260
    },
    {
      "epoch": 13685.454545454546,
      "grad_norm": 0.1643732637166977,
      "learning_rate": 5.837285305549978e-07,
      "loss": 0.001,
      "step": 75270
    },
    {
      "epoch": 13687.272727272728,
      "grad_norm": 0.0012453445233404636,
      "learning_rate": 5.83613816015318e-07,
      "loss": 0.001,
      "step": 75280
    },
    {
      "epoch": 13689.09090909091,
      "grad_norm": 0.17181266844272614,
      "learning_rate": 5.834990969475983e-07,
      "loss": 0.0012,
      "step": 75290
    },
    {
      "epoch": 13690.90909090909,
      "grad_norm": 0.0013896151212975383,
      "learning_rate": 5.833843733580512e-07,
      "loss": 0.0012,
      "step": 75300
    },
    {
      "epoch": 13692.727272727272,
      "grad_norm": 0.17162635922431946,
      "learning_rate": 5.832696452528893e-07,
      "loss": 0.0009,
      "step": 75310
    },
    {
      "epoch": 13694.545454545454,
      "grad_norm": 0.19744229316711426,
      "learning_rate": 5.831549126383257e-07,
      "loss": 0.0013,
      "step": 75320
    },
    {
      "epoch": 13696.363636363636,
      "grad_norm": 0.17650987207889557,
      "learning_rate": 5.83040175520574e-07,
      "loss": 0.0009,
      "step": 75330
    },
    {
      "epoch": 13698.181818181818,
      "grad_norm": 0.0017531701596453786,
      "learning_rate": 5.82925433905847e-07,
      "loss": 0.0011,
      "step": 75340
    },
    {
      "epoch": 13700.0,
      "grad_norm": 0.00395223218947649,
      "learning_rate": 5.828106878003592e-07,
      "loss": 0.0012,
      "step": 75350
    },
    {
      "epoch": 13701.818181818182,
      "grad_norm": 0.18527114391326904,
      "learning_rate": 5.826959372103238e-07,
      "loss": 0.001,
      "step": 75360
    },
    {
      "epoch": 13703.636363636364,
      "grad_norm": 0.21607056260108948,
      "learning_rate": 5.825811821419557e-07,
      "loss": 0.0009,
      "step": 75370
    },
    {
      "epoch": 13705.454545454546,
      "grad_norm": 0.21302834153175354,
      "learning_rate": 5.824664226014691e-07,
      "loss": 0.0014,
      "step": 75380
    },
    {
      "epoch": 13707.272727272728,
      "grad_norm": 0.16113249957561493,
      "learning_rate": 5.823516585950787e-07,
      "loss": 0.001,
      "step": 75390
    },
    {
      "epoch": 13709.09090909091,
      "grad_norm": 0.001142750377766788,
      "learning_rate": 5.822368901289993e-07,
      "loss": 0.0011,
      "step": 75400
    },
    {
      "epoch": 13710.90909090909,
      "grad_norm": 0.2044503390789032,
      "learning_rate": 5.821221172094466e-07,
      "loss": 0.001,
      "step": 75410
    },
    {
      "epoch": 13712.727272727272,
      "grad_norm": 0.0007250273483805358,
      "learning_rate": 5.820073398426355e-07,
      "loss": 0.0011,
      "step": 75420
    },
    {
      "epoch": 13714.545454545454,
      "grad_norm": 0.2664516866207123,
      "learning_rate": 5.818925580347819e-07,
      "loss": 0.0011,
      "step": 75430
    },
    {
      "epoch": 13716.363636363636,
      "grad_norm": 0.0006815044907853007,
      "learning_rate": 5.817777717921017e-07,
      "loss": 0.0013,
      "step": 75440
    },
    {
      "epoch": 13718.181818181818,
      "grad_norm": 0.0009837150573730469,
      "learning_rate": 5.816629811208111e-07,
      "loss": 0.0008,
      "step": 75450
    },
    {
      "epoch": 13720.0,
      "grad_norm": 0.17231391370296478,
      "learning_rate": 5.815481860271264e-07,
      "loss": 0.0012,
      "step": 75460
    },
    {
      "epoch": 13721.818181818182,
      "grad_norm": 0.28290584683418274,
      "learning_rate": 5.814333865172643e-07,
      "loss": 0.001,
      "step": 75470
    },
    {
      "epoch": 13723.636363636364,
      "grad_norm": 0.0010650664335116744,
      "learning_rate": 5.813185825974418e-07,
      "loss": 0.001,
      "step": 75480
    },
    {
      "epoch": 13725.454545454546,
      "grad_norm": 0.1649392992258072,
      "learning_rate": 5.812037742738758e-07,
      "loss": 0.0014,
      "step": 75490
    },
    {
      "epoch": 13727.272727272728,
      "grad_norm": 0.15582434833049774,
      "learning_rate": 5.810889615527838e-07,
      "loss": 0.001,
      "step": 75500
    },
    {
      "epoch": 13727.272727272728,
      "eval_loss": 4.91804313659668,
      "eval_runtime": 0.9522,
      "eval_samples_per_second": 10.502,
      "eval_steps_per_second": 5.251,
      "step": 75500
    },
    {
      "epoch": 13729.09090909091,
      "grad_norm": 0.16406014561653137,
      "learning_rate": 5.809741444403831e-07,
      "loss": 0.0012,
      "step": 75510
    },
    {
      "epoch": 13730.90909090909,
      "grad_norm": 0.2718520164489746,
      "learning_rate": 5.808593229428919e-07,
      "loss": 0.0009,
      "step": 75520
    },
    {
      "epoch": 13732.727272727272,
      "grad_norm": 0.011518956162035465,
      "learning_rate": 5.807444970665282e-07,
      "loss": 0.0012,
      "step": 75530
    },
    {
      "epoch": 13734.545454545454,
      "grad_norm": 0.1629316210746765,
      "learning_rate": 5.806296668175104e-07,
      "loss": 0.0011,
      "step": 75540
    },
    {
      "epoch": 13736.363636363636,
      "grad_norm": 0.19333167374134064,
      "learning_rate": 5.805148322020564e-07,
      "loss": 0.0012,
      "step": 75550
    },
    {
      "epoch": 13738.181818181818,
      "grad_norm": 0.2059951275587082,
      "learning_rate": 5.803999932263859e-07,
      "loss": 0.0009,
      "step": 75560
    },
    {
      "epoch": 13740.0,
      "grad_norm": 0.16104820370674133,
      "learning_rate": 5.802851498967171e-07,
      "loss": 0.001,
      "step": 75570
    },
    {
      "epoch": 13741.818181818182,
      "grad_norm": 0.21423648297786713,
      "learning_rate": 5.8017030221927e-07,
      "loss": 0.0012,
      "step": 75580
    },
    {
      "epoch": 13743.636363636364,
      "grad_norm": 0.27174243330955505,
      "learning_rate": 5.800554502002634e-07,
      "loss": 0.0013,
      "step": 75590
    },
    {
      "epoch": 13745.454545454546,
      "grad_norm": 0.0007203982095234096,
      "learning_rate": 5.799405938459174e-07,
      "loss": 0.0009,
      "step": 75600
    },
    {
      "epoch": 13747.272727272728,
      "grad_norm": 0.0017596305115148425,
      "learning_rate": 5.798257331624518e-07,
      "loss": 0.0011,
      "step": 75610
    },
    {
      "epoch": 13749.09090909091,
      "grad_norm": 0.1601964384317398,
      "learning_rate": 5.797108681560872e-07,
      "loss": 0.0014,
      "step": 75620
    },
    {
      "epoch": 13750.90909090909,
      "grad_norm": 0.002606745110824704,
      "learning_rate": 5.795959988330434e-07,
      "loss": 0.0012,
      "step": 75630
    },
    {
      "epoch": 13752.727272727272,
      "grad_norm": 0.0024896084796637297,
      "learning_rate": 5.794811251995413e-07,
      "loss": 0.0007,
      "step": 75640
    },
    {
      "epoch": 13754.545454545454,
      "grad_norm": 0.21637162566184998,
      "learning_rate": 5.79366247261802e-07,
      "loss": 0.0013,
      "step": 75650
    },
    {
      "epoch": 13756.363636363636,
      "grad_norm": 0.1573696732521057,
      "learning_rate": 5.792513650260465e-07,
      "loss": 0.0012,
      "step": 75660
    },
    {
      "epoch": 13758.181818181818,
      "grad_norm": 0.1625581979751587,
      "learning_rate": 5.791364784984959e-07,
      "loss": 0.001,
      "step": 75670
    },
    {
      "epoch": 13760.0,
      "grad_norm": 0.001047360128723085,
      "learning_rate": 5.79021587685372e-07,
      "loss": 0.0011,
      "step": 75680
    },
    {
      "epoch": 13761.818181818182,
      "grad_norm": 0.2154054343700409,
      "learning_rate": 5.789066925928969e-07,
      "loss": 0.0012,
      "step": 75690
    },
    {
      "epoch": 13763.636363636364,
      "grad_norm": 0.15823926031589508,
      "learning_rate": 5.787917932272921e-07,
      "loss": 0.0011,
      "step": 75700
    },
    {
      "epoch": 13765.454545454546,
      "grad_norm": 0.0010339501313865185,
      "learning_rate": 5.786768895947804e-07,
      "loss": 0.0011,
      "step": 75710
    },
    {
      "epoch": 13767.272727272728,
      "grad_norm": 0.21091653406620026,
      "learning_rate": 5.785619817015838e-07,
      "loss": 0.0009,
      "step": 75720
    },
    {
      "epoch": 13769.09090909091,
      "grad_norm": 0.43241947889328003,
      "learning_rate": 5.784470695539257e-07,
      "loss": 0.0015,
      "step": 75730
    },
    {
      "epoch": 13770.90909090909,
      "grad_norm": 0.1623712033033371,
      "learning_rate": 5.783321531580283e-07,
      "loss": 0.0009,
      "step": 75740
    },
    {
      "epoch": 13772.727272727272,
      "grad_norm": 0.19675233960151672,
      "learning_rate": 5.782172325201155e-07,
      "loss": 0.0009,
      "step": 75750
    },
    {
      "epoch": 13774.545454545454,
      "grad_norm": 0.0007532232557423413,
      "learning_rate": 5.781023076464103e-07,
      "loss": 0.0009,
      "step": 75760
    },
    {
      "epoch": 13776.363636363636,
      "grad_norm": 0.15625658631324768,
      "learning_rate": 5.779873785431367e-07,
      "loss": 0.0015,
      "step": 75770
    },
    {
      "epoch": 13778.181818181818,
      "grad_norm": 0.15642739832401276,
      "learning_rate": 5.77872445216518e-07,
      "loss": 0.0011,
      "step": 75780
    },
    {
      "epoch": 13780.0,
      "grad_norm": 0.15632712841033936,
      "learning_rate": 5.77757507672779e-07,
      "loss": 0.0012,
      "step": 75790
    },
    {
      "epoch": 13781.818181818182,
      "grad_norm": 0.154831200838089,
      "learning_rate": 5.776425659181437e-07,
      "loss": 0.0012,
      "step": 75800
    },
    {
      "epoch": 13783.636363636364,
      "grad_norm": 0.0015296307392418385,
      "learning_rate": 5.77527619958837e-07,
      "loss": 0.001,
      "step": 75810
    },
    {
      "epoch": 13785.454545454546,
      "grad_norm": 0.0013496277388185263,
      "learning_rate": 5.774126698010832e-07,
      "loss": 0.0011,
      "step": 75820
    },
    {
      "epoch": 13787.272727272728,
      "grad_norm": 0.15700069069862366,
      "learning_rate": 5.772977154511077e-07,
      "loss": 0.0012,
      "step": 75830
    },
    {
      "epoch": 13789.09090909091,
      "grad_norm": 0.000931017566472292,
      "learning_rate": 5.771827569151356e-07,
      "loss": 0.001,
      "step": 75840
    },
    {
      "epoch": 13790.90909090909,
      "grad_norm": 0.2781074643135071,
      "learning_rate": 5.770677941993924e-07,
      "loss": 0.001,
      "step": 75850
    },
    {
      "epoch": 13792.727272727272,
      "grad_norm": 0.20183132588863373,
      "learning_rate": 5.76952827310104e-07,
      "loss": 0.0013,
      "step": 75860
    },
    {
      "epoch": 13794.545454545454,
      "grad_norm": 0.19630791246891022,
      "learning_rate": 5.768378562534961e-07,
      "loss": 0.001,
      "step": 75870
    },
    {
      "epoch": 13796.363636363636,
      "grad_norm": 0.15768742561340332,
      "learning_rate": 5.767228810357951e-07,
      "loss": 0.001,
      "step": 75880
    },
    {
      "epoch": 13798.181818181818,
      "grad_norm": 0.0008246711804531515,
      "learning_rate": 5.766079016632271e-07,
      "loss": 0.0011,
      "step": 75890
    },
    {
      "epoch": 13800.0,
      "grad_norm": 0.017817432060837746,
      "learning_rate": 5.764929181420191e-07,
      "loss": 0.0012,
      "step": 75900
    },
    {
      "epoch": 13801.818181818182,
      "grad_norm": 0.0006829238845966756,
      "learning_rate": 5.763779304783975e-07,
      "loss": 0.0012,
      "step": 75910
    },
    {
      "epoch": 13803.636363636364,
      "grad_norm": 0.0011521328706294298,
      "learning_rate": 5.762629386785898e-07,
      "loss": 0.0009,
      "step": 75920
    },
    {
      "epoch": 13805.454545454546,
      "grad_norm": 0.21265125274658203,
      "learning_rate": 5.761479427488228e-07,
      "loss": 0.0014,
      "step": 75930
    },
    {
      "epoch": 13807.272727272728,
      "grad_norm": 0.000903745589312166,
      "learning_rate": 5.760329426953245e-07,
      "loss": 0.0008,
      "step": 75940
    },
    {
      "epoch": 13809.09090909091,
      "grad_norm": 0.2013280987739563,
      "learning_rate": 5.759179385243223e-07,
      "loss": 0.0014,
      "step": 75950
    },
    {
      "epoch": 13810.90909090909,
      "grad_norm": 0.25954103469848633,
      "learning_rate": 5.758029302420445e-07,
      "loss": 0.0012,
      "step": 75960
    },
    {
      "epoch": 13812.727272727272,
      "grad_norm": 0.1607663929462433,
      "learning_rate": 5.75687917854719e-07,
      "loss": 0.0012,
      "step": 75970
    },
    {
      "epoch": 13814.545454545454,
      "grad_norm": 0.0017512632766738534,
      "learning_rate": 5.755729013685741e-07,
      "loss": 0.0009,
      "step": 75980
    },
    {
      "epoch": 13816.363636363636,
      "grad_norm": 0.20239433646202087,
      "learning_rate": 5.754578807898387e-07,
      "loss": 0.0013,
      "step": 75990
    },
    {
      "epoch": 13818.181818181818,
      "grad_norm": 0.16158732771873474,
      "learning_rate": 5.753428561247415e-07,
      "loss": 0.0012,
      "step": 76000
    },
    {
      "epoch": 13818.181818181818,
      "eval_loss": 4.921827793121338,
      "eval_runtime": 0.9511,
      "eval_samples_per_second": 10.514,
      "eval_steps_per_second": 5.257,
      "step": 76000
    },
    {
      "epoch": 13820.0,
      "grad_norm": 0.0008422691025771201,
      "learning_rate": 5.752278273795117e-07,
      "loss": 0.0011,
      "step": 76010
    },
    {
      "epoch": 13821.818181818182,
      "grad_norm": 0.0009877892443910241,
      "learning_rate": 5.751127945603785e-07,
      "loss": 0.0012,
      "step": 76020
    },
    {
      "epoch": 13823.636363636364,
      "grad_norm": 0.261731892824173,
      "learning_rate": 5.749977576735715e-07,
      "loss": 0.0012,
      "step": 76030
    },
    {
      "epoch": 13825.454545454546,
      "grad_norm": 0.19724668562412262,
      "learning_rate": 5.748827167253202e-07,
      "loss": 0.0009,
      "step": 76040
    },
    {
      "epoch": 13827.272727272728,
      "grad_norm": 0.16227076947689056,
      "learning_rate": 5.747676717218548e-07,
      "loss": 0.0011,
      "step": 76050
    },
    {
      "epoch": 13829.09090909091,
      "grad_norm": 0.000977689865976572,
      "learning_rate": 5.746526226694053e-07,
      "loss": 0.001,
      "step": 76060
    },
    {
      "epoch": 13830.90909090909,
      "grad_norm": 0.27473127841949463,
      "learning_rate": 5.745375695742023e-07,
      "loss": 0.0011,
      "step": 76070
    },
    {
      "epoch": 13832.727272727272,
      "grad_norm": 0.1979718804359436,
      "learning_rate": 5.744225124424761e-07,
      "loss": 0.0012,
      "step": 76080
    },
    {
      "epoch": 13834.545454545454,
      "grad_norm": 0.0008985110907815397,
      "learning_rate": 5.743074512804578e-07,
      "loss": 0.0011,
      "step": 76090
    },
    {
      "epoch": 13836.363636363636,
      "grad_norm": 0.2639574408531189,
      "learning_rate": 5.741923860943783e-07,
      "loss": 0.0013,
      "step": 76100
    },
    {
      "epoch": 13838.181818181818,
      "grad_norm": 0.15653853118419647,
      "learning_rate": 5.74077316890469e-07,
      "loss": 0.0011,
      "step": 76110
    },
    {
      "epoch": 13840.0,
      "grad_norm": 0.0009320609387941658,
      "learning_rate": 5.739622436749611e-07,
      "loss": 0.0011,
      "step": 76120
    },
    {
      "epoch": 13841.818181818182,
      "grad_norm": 0.24045825004577637,
      "learning_rate": 5.738471664540867e-07,
      "loss": 0.001,
      "step": 76130
    },
    {
      "epoch": 13843.636363636364,
      "grad_norm": 0.000923957210034132,
      "learning_rate": 5.737320852340775e-07,
      "loss": 0.0011,
      "step": 76140
    },
    {
      "epoch": 13845.454545454546,
      "grad_norm": 0.000878187594935298,
      "learning_rate": 5.736170000211655e-07,
      "loss": 0.0012,
      "step": 76150
    },
    {
      "epoch": 13847.272727272728,
      "grad_norm": 0.16609999537467957,
      "learning_rate": 5.735019108215832e-07,
      "loss": 0.0015,
      "step": 76160
    },
    {
      "epoch": 13849.09090909091,
      "grad_norm": 0.36912351846694946,
      "learning_rate": 5.733868176415633e-07,
      "loss": 0.001,
      "step": 76170
    },
    {
      "epoch": 13850.90909090909,
      "grad_norm": 0.2487669736146927,
      "learning_rate": 5.732717204873383e-07,
      "loss": 0.0011,
      "step": 76180
    },
    {
      "epoch": 13852.727272727272,
      "grad_norm": 0.0008230208186432719,
      "learning_rate": 5.731566193651414e-07,
      "loss": 0.001,
      "step": 76190
    },
    {
      "epoch": 13854.545454545454,
      "grad_norm": 0.1599857658147812,
      "learning_rate": 5.730415142812058e-07,
      "loss": 0.001,
      "step": 76200
    },
    {
      "epoch": 13856.363636363636,
      "grad_norm": 0.1561695784330368,
      "learning_rate": 5.729264052417647e-07,
      "loss": 0.0012,
      "step": 76210
    },
    {
      "epoch": 13858.181818181818,
      "grad_norm": 0.15459918975830078,
      "learning_rate": 5.728112922530522e-07,
      "loss": 0.0012,
      "step": 76220
    },
    {
      "epoch": 13860.0,
      "grad_norm": 0.17474432289600372,
      "learning_rate": 5.726961753213015e-07,
      "loss": 0.0011,
      "step": 76230
    },
    {
      "epoch": 13861.818181818182,
      "grad_norm": 0.1803579330444336,
      "learning_rate": 5.725810544527472e-07,
      "loss": 0.0011,
      "step": 76240
    },
    {
      "epoch": 13863.636363636364,
      "grad_norm": 0.0008494488429278135,
      "learning_rate": 5.724659296536232e-07,
      "loss": 0.0011,
      "step": 76250
    },
    {
      "epoch": 13865.454545454546,
      "grad_norm": 0.0008632788667455316,
      "learning_rate": 5.723508009301645e-07,
      "loss": 0.0012,
      "step": 76260
    },
    {
      "epoch": 13867.272727272728,
      "grad_norm": 0.0005737972678616643,
      "learning_rate": 5.722356682886054e-07,
      "loss": 0.001,
      "step": 76270
    },
    {
      "epoch": 13869.09090909091,
      "grad_norm": 0.1567194014787674,
      "learning_rate": 5.721205317351809e-07,
      "loss": 0.0014,
      "step": 76280
    },
    {
      "epoch": 13870.90909090909,
      "grad_norm": 0.20834743976593018,
      "learning_rate": 5.72005391276126e-07,
      "loss": 0.001,
      "step": 76290
    },
    {
      "epoch": 13872.727272727272,
      "grad_norm": 0.0011166424956172705,
      "learning_rate": 5.718902469176764e-07,
      "loss": 0.0009,
      "step": 76300
    },
    {
      "epoch": 13874.545454545454,
      "grad_norm": 0.2698899805545807,
      "learning_rate": 5.717750986660673e-07,
      "loss": 0.0014,
      "step": 76310
    },
    {
      "epoch": 13876.363636363636,
      "grad_norm": 0.2541009187698364,
      "learning_rate": 5.716599465275347e-07,
      "loss": 0.0011,
      "step": 76320
    },
    {
      "epoch": 13878.181818181818,
      "grad_norm": 0.19621586799621582,
      "learning_rate": 5.715447905083143e-07,
      "loss": 0.0009,
      "step": 76330
    },
    {
      "epoch": 13880.0,
      "grad_norm": 0.16149507462978363,
      "learning_rate": 5.714296306146426e-07,
      "loss": 0.0011,
      "step": 76340
    },
    {
      "epoch": 13881.818181818182,
      "grad_norm": 0.1650092452764511,
      "learning_rate": 5.713144668527558e-07,
      "loss": 0.0012,
      "step": 76350
    },
    {
      "epoch": 13883.636363636364,
      "grad_norm": 0.0014578780392184854,
      "learning_rate": 5.711992992288906e-07,
      "loss": 0.0009,
      "step": 76360
    },
    {
      "epoch": 13885.454545454546,
      "grad_norm": 0.0008872090838849545,
      "learning_rate": 5.710841277492836e-07,
      "loss": 0.001,
      "step": 76370
    },
    {
      "epoch": 13887.272727272728,
      "grad_norm": 0.20059455931186676,
      "learning_rate": 5.709689524201722e-07,
      "loss": 0.0012,
      "step": 76380
    },
    {
      "epoch": 13889.09090909091,
      "grad_norm": 0.16428159177303314,
      "learning_rate": 5.708537732477933e-07,
      "loss": 0.0012,
      "step": 76390
    },
    {
      "epoch": 13890.90909090909,
      "grad_norm": 0.18221190571784973,
      "learning_rate": 5.707385902383844e-07,
      "loss": 0.001,
      "step": 76400
    },
    {
      "epoch": 13892.727272727272,
      "grad_norm": 0.15420001745224,
      "learning_rate": 5.706234033981834e-07,
      "loss": 0.0011,
      "step": 76410
    },
    {
      "epoch": 13894.545454545454,
      "grad_norm": 0.18916070461273193,
      "learning_rate": 5.705082127334276e-07,
      "loss": 0.001,
      "step": 76420
    },
    {
      "epoch": 13896.363636363636,
      "grad_norm": 0.14856866002082825,
      "learning_rate": 5.703930182503558e-07,
      "loss": 0.0014,
      "step": 76430
    },
    {
      "epoch": 13898.181818181818,
      "grad_norm": 0.28783243894577026,
      "learning_rate": 5.702778199552054e-07,
      "loss": 0.0014,
      "step": 76440
    },
    {
      "epoch": 13900.0,
      "grad_norm": 0.23576416075229645,
      "learning_rate": 5.701626178542157e-07,
      "loss": 0.0009,
      "step": 76450
    },
    {
      "epoch": 13901.818181818182,
      "grad_norm": 0.0011617150157690048,
      "learning_rate": 5.700474119536249e-07,
      "loss": 0.001,
      "step": 76460
    },
    {
      "epoch": 13903.636363636364,
      "grad_norm": 0.03468301147222519,
      "learning_rate": 5.699322022596721e-07,
      "loss": 0.001,
      "step": 76470
    },
    {
      "epoch": 13905.454545454546,
      "grad_norm": 0.19992883503437042,
      "learning_rate": 5.698169887785961e-07,
      "loss": 0.0014,
      "step": 76480
    },
    {
      "epoch": 13907.272727272728,
      "grad_norm": 0.002184659941121936,
      "learning_rate": 5.697017715166365e-07,
      "loss": 0.0007,
      "step": 76490
    },
    {
      "epoch": 13909.09090909091,
      "grad_norm": 0.001104055787436664,
      "learning_rate": 5.695865504800327e-07,
      "loss": 0.0012,
      "step": 76500
    },
    {
      "epoch": 13909.09090909091,
      "eval_loss": 4.963104248046875,
      "eval_runtime": 0.9508,
      "eval_samples_per_second": 10.517,
      "eval_steps_per_second": 5.259,
      "step": 76500
    },
    {
      "epoch": 13910.90909090909,
      "grad_norm": 0.0008968577021732926,
      "learning_rate": 5.694713256750245e-07,
      "loss": 0.001,
      "step": 76510
    },
    {
      "epoch": 13912.727272727272,
      "grad_norm": 0.0007088002166710794,
      "learning_rate": 5.693560971078514e-07,
      "loss": 0.0009,
      "step": 76520
    },
    {
      "epoch": 13914.545454545454,
      "grad_norm": 0.19615548849105835,
      "learning_rate": 5.692408647847541e-07,
      "loss": 0.0013,
      "step": 76530
    },
    {
      "epoch": 13916.363636363636,
      "grad_norm": 0.2734159231185913,
      "learning_rate": 5.691256287119725e-07,
      "loss": 0.0016,
      "step": 76540
    },
    {
      "epoch": 13918.181818181818,
      "grad_norm": 0.16333840787410736,
      "learning_rate": 5.690103888957473e-07,
      "loss": 0.0009,
      "step": 76550
    },
    {
      "epoch": 13920.0,
      "grad_norm": 0.0012512399116531014,
      "learning_rate": 5.68895145342319e-07,
      "loss": 0.0011,
      "step": 76560
    },
    {
      "epoch": 13921.818181818182,
      "grad_norm": 0.17317160964012146,
      "learning_rate": 5.687798980579288e-07,
      "loss": 0.0011,
      "step": 76570
    },
    {
      "epoch": 13923.636363636364,
      "grad_norm": 0.15264031291007996,
      "learning_rate": 5.686646470488178e-07,
      "loss": 0.001,
      "step": 76580
    },
    {
      "epoch": 13925.454545454546,
      "grad_norm": 0.0011124813463538885,
      "learning_rate": 5.685493923212272e-07,
      "loss": 0.0009,
      "step": 76590
    },
    {
      "epoch": 13927.272727272728,
      "grad_norm": 0.0008974187076091766,
      "learning_rate": 5.684341338813985e-07,
      "loss": 0.0012,
      "step": 76600
    },
    {
      "epoch": 13929.09090909091,
      "grad_norm": 0.15803830325603485,
      "learning_rate": 5.683188717355734e-07,
      "loss": 0.0013,
      "step": 76610
    },
    {
      "epoch": 13930.90909090909,
      "grad_norm": 0.21092645823955536,
      "learning_rate": 5.682036058899942e-07,
      "loss": 0.0009,
      "step": 76620
    },
    {
      "epoch": 13932.727272727272,
      "grad_norm": 0.2645306885242462,
      "learning_rate": 5.680883363509025e-07,
      "loss": 0.0012,
      "step": 76630
    },
    {
      "epoch": 13934.545454545454,
      "grad_norm": 0.0014770685229450464,
      "learning_rate": 5.679730631245411e-07,
      "loss": 0.0009,
      "step": 76640
    },
    {
      "epoch": 13936.363636363636,
      "grad_norm": 0.0017005272675305605,
      "learning_rate": 5.678577862171522e-07,
      "loss": 0.0011,
      "step": 76650
    },
    {
      "epoch": 13938.181818181818,
      "grad_norm": 0.0006365998415276408,
      "learning_rate": 5.677425056349787e-07,
      "loss": 0.0011,
      "step": 76660
    },
    {
      "epoch": 13940.0,
      "grad_norm": 0.01865607500076294,
      "learning_rate": 5.676272213842633e-07,
      "loss": 0.0012,
      "step": 76670
    },
    {
      "epoch": 13941.818181818182,
      "grad_norm": 0.2966309189796448,
      "learning_rate": 5.675119334712496e-07,
      "loss": 0.0012,
      "step": 76680
    },
    {
      "epoch": 13943.636363636364,
      "grad_norm": 0.002420363249257207,
      "learning_rate": 5.673966419021805e-07,
      "loss": 0.001,
      "step": 76690
    },
    {
      "epoch": 13945.454545454546,
      "grad_norm": 0.12953275442123413,
      "learning_rate": 5.672813466832997e-07,
      "loss": 0.001,
      "step": 76700
    },
    {
      "epoch": 13947.272727272728,
      "grad_norm": 0.0008976695244200528,
      "learning_rate": 5.671660478208507e-07,
      "loss": 0.0009,
      "step": 76710
    },
    {
      "epoch": 13949.09090909091,
      "grad_norm": 0.1328430026769638,
      "learning_rate": 5.670507453210778e-07,
      "loss": 0.0013,
      "step": 76720
    },
    {
      "epoch": 13950.90909090909,
      "grad_norm": 0.19609200954437256,
      "learning_rate": 5.669354391902248e-07,
      "loss": 0.0009,
      "step": 76730
    },
    {
      "epoch": 13952.727272727272,
      "grad_norm": 0.1556360274553299,
      "learning_rate": 5.668201294345362e-07,
      "loss": 0.0012,
      "step": 76740
    },
    {
      "epoch": 13954.545454545454,
      "grad_norm": 0.1609591543674469,
      "learning_rate": 5.667048160602563e-07,
      "loss": 0.0012,
      "step": 76750
    },
    {
      "epoch": 13956.363636363636,
      "grad_norm": 0.0011722593335434794,
      "learning_rate": 5.665894990736301e-07,
      "loss": 0.0011,
      "step": 76760
    },
    {
      "epoch": 13958.181818181818,
      "grad_norm": 0.0018399788532406092,
      "learning_rate": 5.664741784809021e-07,
      "loss": 0.0009,
      "step": 76770
    },
    {
      "epoch": 13960.0,
      "grad_norm": 0.0007847603410482407,
      "learning_rate": 5.66358854288318e-07,
      "loss": 0.0012,
      "step": 76780
    },
    {
      "epoch": 13961.818181818182,
      "grad_norm": 0.0011955717345699668,
      "learning_rate": 5.662435265021224e-07,
      "loss": 0.0012,
      "step": 76790
    },
    {
      "epoch": 13963.636363636364,
      "grad_norm": 0.20296786725521088,
      "learning_rate": 5.661281951285612e-07,
      "loss": 0.001,
      "step": 76800
    },
    {
      "epoch": 13965.454545454546,
      "grad_norm": 0.26662349700927734,
      "learning_rate": 5.660128601738801e-07,
      "loss": 0.0014,
      "step": 76810
    },
    {
      "epoch": 13967.272727272728,
      "grad_norm": 0.21710342168807983,
      "learning_rate": 5.658975216443247e-07,
      "loss": 0.0008,
      "step": 76820
    },
    {
      "epoch": 13969.09090909091,
      "grad_norm": 0.0009883197490125895,
      "learning_rate": 5.657821795461413e-07,
      "loss": 0.001,
      "step": 76830
    },
    {
      "epoch": 13970.90909090909,
      "grad_norm": 0.0009478771244175732,
      "learning_rate": 5.65666833885576e-07,
      "loss": 0.0012,
      "step": 76840
    },
    {
      "epoch": 13972.727272727272,
      "grad_norm": 0.17349985241889954,
      "learning_rate": 5.655514846688755e-07,
      "loss": 0.0012,
      "step": 76850
    },
    {
      "epoch": 13974.545454545454,
      "grad_norm": 0.010110048577189445,
      "learning_rate": 5.654361319022861e-07,
      "loss": 0.0009,
      "step": 76860
    },
    {
      "epoch": 13976.363636363636,
      "grad_norm": 0.0022685038857162,
      "learning_rate": 5.65320775592055e-07,
      "loss": 0.0012,
      "step": 76870
    },
    {
      "epoch": 13978.181818181818,
      "grad_norm": 0.0006209097919054329,
      "learning_rate": 5.652054157444288e-07,
      "loss": 0.001,
      "step": 76880
    },
    {
      "epoch": 13980.0,
      "grad_norm": 0.0022548723500221968,
      "learning_rate": 5.650900523656552e-07,
      "loss": 0.0012,
      "step": 76890
    },
    {
      "epoch": 13981.818181818182,
      "grad_norm": 0.0005656955181621015,
      "learning_rate": 5.649746854619814e-07,
      "loss": 0.0009,
      "step": 76900
    },
    {
      "epoch": 13983.636363636364,
      "grad_norm": 0.0013607674045488238,
      "learning_rate": 5.648593150396548e-07,
      "loss": 0.0013,
      "step": 76910
    },
    {
      "epoch": 13985.454545454546,
      "grad_norm": 0.0007555408519692719,
      "learning_rate": 5.647439411049234e-07,
      "loss": 0.0008,
      "step": 76920
    },
    {
      "epoch": 13987.272727272728,
      "grad_norm": 0.009102833457291126,
      "learning_rate": 5.646285636640353e-07,
      "loss": 0.0015,
      "step": 76930
    },
    {
      "epoch": 13989.09090909091,
      "grad_norm": 0.001996302977204323,
      "learning_rate": 5.645131827232386e-07,
      "loss": 0.0009,
      "step": 76940
    },
    {
      "epoch": 13990.90909090909,
      "grad_norm": 0.25589290261268616,
      "learning_rate": 5.643977982887814e-07,
      "loss": 0.001,
      "step": 76950
    },
    {
      "epoch": 13992.727272727272,
      "grad_norm": 0.00140691630076617,
      "learning_rate": 5.642824103669125e-07,
      "loss": 0.0011,
      "step": 76960
    },
    {
      "epoch": 13994.545454545454,
      "grad_norm": 0.16407375037670135,
      "learning_rate": 5.641670189638808e-07,
      "loss": 0.0016,
      "step": 76970
    },
    {
      "epoch": 13996.363636363636,
      "grad_norm": 0.22804026305675507,
      "learning_rate": 5.640516240859348e-07,
      "loss": 0.0011,
      "step": 76980
    },
    {
      "epoch": 13998.181818181818,
      "grad_norm": 0.00110316660720855,
      "learning_rate": 5.639362257393238e-07,
      "loss": 0.0008,
      "step": 76990
    },
    {
      "epoch": 14000.0,
      "grad_norm": 0.0022748627234250307,
      "learning_rate": 5.638208239302974e-07,
      "loss": 0.0012,
      "step": 77000
    },
    {
      "epoch": 14000.0,
      "eval_loss": 4.926701545715332,
      "eval_runtime": 0.9534,
      "eval_samples_per_second": 10.488,
      "eval_steps_per_second": 5.244,
      "step": 77000
    },
    {
      "epoch": 14001.818181818182,
      "grad_norm": 0.15960800647735596,
      "learning_rate": 5.637054186651047e-07,
      "loss": 0.0012,
      "step": 77010
    },
    {
      "epoch": 14003.636363636364,
      "grad_norm": 0.15415988862514496,
      "learning_rate": 5.635900099499954e-07,
      "loss": 0.0012,
      "step": 77020
    },
    {
      "epoch": 14005.454545454546,
      "grad_norm": 0.0009871699148789048,
      "learning_rate": 5.634745977912197e-07,
      "loss": 0.0009,
      "step": 77030
    },
    {
      "epoch": 14007.272727272728,
      "grad_norm": 0.017678113654255867,
      "learning_rate": 5.633591821950274e-07,
      "loss": 0.0013,
      "step": 77040
    },
    {
      "epoch": 14009.09090909091,
      "grad_norm": 0.1718522608280182,
      "learning_rate": 5.632437631676687e-07,
      "loss": 0.0009,
      "step": 77050
    },
    {
      "epoch": 14010.90909090909,
      "grad_norm": 0.21001781523227692,
      "learning_rate": 5.631283407153941e-07,
      "loss": 0.0012,
      "step": 77060
    },
    {
      "epoch": 14012.727272727272,
      "grad_norm": 0.2040315866470337,
      "learning_rate": 5.630129148444543e-07,
      "loss": 0.001,
      "step": 77070
    },
    {
      "epoch": 14014.545454545454,
      "grad_norm": 0.206385537981987,
      "learning_rate": 5.628974855611001e-07,
      "loss": 0.0011,
      "step": 77080
    },
    {
      "epoch": 14016.363636363636,
      "grad_norm": 0.2133970856666565,
      "learning_rate": 5.627820528715823e-07,
      "loss": 0.0012,
      "step": 77090
    },
    {
      "epoch": 14018.181818181818,
      "grad_norm": 0.278968870639801,
      "learning_rate": 5.626666167821521e-07,
      "loss": 0.0012,
      "step": 77100
    },
    {
      "epoch": 14020.0,
      "grad_norm": 0.278900146484375,
      "learning_rate": 5.62551177299061e-07,
      "loss": 0.0009,
      "step": 77110
    },
    {
      "epoch": 14021.818181818182,
      "grad_norm": 0.2647244930267334,
      "learning_rate": 5.624357344285605e-07,
      "loss": 0.0012,
      "step": 77120
    },
    {
      "epoch": 14023.636363636364,
      "grad_norm": 0.1893678456544876,
      "learning_rate": 5.623202881769022e-07,
      "loss": 0.0012,
      "step": 77130
    },
    {
      "epoch": 14025.454545454546,
      "grad_norm": 0.002948926528915763,
      "learning_rate": 5.62204838550338e-07,
      "loss": 0.0009,
      "step": 77140
    },
    {
      "epoch": 14027.272727272728,
      "grad_norm": 0.21589355170726776,
      "learning_rate": 5.620893855551202e-07,
      "loss": 0.0012,
      "step": 77150
    },
    {
      "epoch": 14029.09090909091,
      "grad_norm": 0.17654384672641754,
      "learning_rate": 5.619739291975008e-07,
      "loss": 0.0012,
      "step": 77160
    },
    {
      "epoch": 14030.90909090909,
      "grad_norm": 0.0012120433384552598,
      "learning_rate": 5.618584694837325e-07,
      "loss": 0.0009,
      "step": 77170
    },
    {
      "epoch": 14032.727272727272,
      "grad_norm": 0.0008244971977546811,
      "learning_rate": 5.617430064200677e-07,
      "loss": 0.0013,
      "step": 77180
    },
    {
      "epoch": 14034.545454545454,
      "grad_norm": 0.24611833691596985,
      "learning_rate": 5.616275400127594e-07,
      "loss": 0.001,
      "step": 77190
    },
    {
      "epoch": 14036.363636363636,
      "grad_norm": 0.15711507201194763,
      "learning_rate": 5.615120702680604e-07,
      "loss": 0.0012,
      "step": 77200
    },
    {
      "epoch": 14038.181818181818,
      "grad_norm": 0.0008583538583479822,
      "learning_rate": 5.613965971922239e-07,
      "loss": 0.0009,
      "step": 77210
    },
    {
      "epoch": 14040.0,
      "grad_norm": 0.2736634612083435,
      "learning_rate": 5.612811207915033e-07,
      "loss": 0.0013,
      "step": 77220
    },
    {
      "epoch": 14041.818181818182,
      "grad_norm": 0.0007435083971358836,
      "learning_rate": 5.611656410721525e-07,
      "loss": 0.0012,
      "step": 77230
    },
    {
      "epoch": 14043.636363636364,
      "grad_norm": 0.14974525570869446,
      "learning_rate": 5.610501580404246e-07,
      "loss": 0.0009,
      "step": 77240
    },
    {
      "epoch": 14045.454545454546,
      "grad_norm": 0.00113992381375283,
      "learning_rate": 5.609346717025737e-07,
      "loss": 0.0011,
      "step": 77250
    },
    {
      "epoch": 14047.272727272728,
      "grad_norm": 0.0013643158599734306,
      "learning_rate": 5.60819182064854e-07,
      "loss": 0.0011,
      "step": 77260
    },
    {
      "epoch": 14049.09090909091,
      "grad_norm": 0.0014608512865379453,
      "learning_rate": 5.607036891335198e-07,
      "loss": 0.0012,
      "step": 77270
    },
    {
      "epoch": 14050.90909090909,
      "grad_norm": 0.0008030852186493576,
      "learning_rate": 5.605881929148253e-07,
      "loss": 0.001,
      "step": 77280
    },
    {
      "epoch": 14052.727272727272,
      "grad_norm": 0.21481862664222717,
      "learning_rate": 5.604726934150252e-07,
      "loss": 0.0013,
      "step": 77290
    },
    {
      "epoch": 14054.545454545454,
      "grad_norm": 0.15710057318210602,
      "learning_rate": 5.603571906403744e-07,
      "loss": 0.0009,
      "step": 77300
    },
    {
      "epoch": 14056.363636363636,
      "grad_norm": 0.0009306963183917105,
      "learning_rate": 5.602416845971276e-07,
      "loss": 0.0012,
      "step": 77310
    },
    {
      "epoch": 14058.181818181818,
      "grad_norm": 0.27254071831703186,
      "learning_rate": 5.601261752915403e-07,
      "loss": 0.0012,
      "step": 77320
    },
    {
      "epoch": 14060.0,
      "grad_norm": 0.2145235687494278,
      "learning_rate": 5.600106627298674e-07,
      "loss": 0.0009,
      "step": 77330
    },
    {
      "epoch": 14061.818181818182,
      "grad_norm": 0.00187077431473881,
      "learning_rate": 5.598951469183648e-07,
      "loss": 0.0012,
      "step": 77340
    },
    {
      "epoch": 14063.636363636364,
      "grad_norm": 0.2925626039505005,
      "learning_rate": 5.597796278632879e-07,
      "loss": 0.0011,
      "step": 77350
    },
    {
      "epoch": 14065.454545454546,
      "grad_norm": 0.2771577537059784,
      "learning_rate": 5.596641055708925e-07,
      "loss": 0.0012,
      "step": 77360
    },
    {
      "epoch": 14067.272727272728,
      "grad_norm": 0.27840080857276917,
      "learning_rate": 5.595485800474348e-07,
      "loss": 0.0011,
      "step": 77370
    },
    {
      "epoch": 14069.09090909091,
      "grad_norm": 0.000765273638535291,
      "learning_rate": 5.594330512991711e-07,
      "loss": 0.0009,
      "step": 77380
    },
    {
      "epoch": 14070.90909090909,
      "grad_norm": 0.000747120357118547,
      "learning_rate": 5.593175193323574e-07,
      "loss": 0.0012,
      "step": 77390
    },
    {
      "epoch": 14072.727272727272,
      "grad_norm": 0.0008549860212951899,
      "learning_rate": 5.592019841532506e-07,
      "loss": 0.0011,
      "step": 77400
    },
    {
      "epoch": 14074.545454545454,
      "grad_norm": 0.2583038806915283,
      "learning_rate": 5.59086445768107e-07,
      "loss": 0.001,
      "step": 77410
    },
    {
      "epoch": 14076.363636363636,
      "grad_norm": 0.14224790036678314,
      "learning_rate": 5.58970904183184e-07,
      "loss": 0.0013,
      "step": 77420
    },
    {
      "epoch": 14078.181818181818,
      "grad_norm": 0.0014612206723541021,
      "learning_rate": 5.588553594047382e-07,
      "loss": 0.0007,
      "step": 77430
    },
    {
      "epoch": 14080.0,
      "grad_norm": 0.009477338753640652,
      "learning_rate": 5.587398114390272e-07,
      "loss": 0.0012,
      "step": 77440
    },
    {
      "epoch": 14081.818181818182,
      "grad_norm": 0.0007422516937367618,
      "learning_rate": 5.586242602923081e-07,
      "loss": 0.0009,
      "step": 77450
    },
    {
      "epoch": 14083.636363636364,
      "grad_norm": 0.0008345206151716411,
      "learning_rate": 5.585087059708387e-07,
      "loss": 0.0015,
      "step": 77460
    },
    {
      "epoch": 14085.454545454546,
      "grad_norm": 0.01779627799987793,
      "learning_rate": 5.583931484808767e-07,
      "loss": 0.0011,
      "step": 77470
    },
    {
      "epoch": 14087.272727272728,
      "grad_norm": 0.0022058088798075914,
      "learning_rate": 5.582775878286801e-07,
      "loss": 0.0009,
      "step": 77480
    },
    {
      "epoch": 14089.09090909091,
      "grad_norm": 0.18960043787956238,
      "learning_rate": 5.581620240205067e-07,
      "loss": 0.001,
      "step": 77490
    },
    {
      "epoch": 14090.90909090909,
      "grad_norm": 0.001322565134614706,
      "learning_rate": 5.580464570626151e-07,
      "loss": 0.0009,
      "step": 77500
    },
    {
      "epoch": 14090.90909090909,
      "eval_loss": 5.005852222442627,
      "eval_runtime": 0.9505,
      "eval_samples_per_second": 10.521,
      "eval_steps_per_second": 5.261,
      "step": 77500
    },
    {
      "epoch": 14092.727272727272,
      "grad_norm": 0.190395787358284,
      "learning_rate": 5.579308869612636e-07,
      "loss": 0.0014,
      "step": 77510
    },
    {
      "epoch": 14094.545454545454,
      "grad_norm": 0.2055021971464157,
      "learning_rate": 5.578153137227109e-07,
      "loss": 0.0012,
      "step": 77520
    },
    {
      "epoch": 14096.363636363636,
      "grad_norm": 0.001703716698102653,
      "learning_rate": 5.576997373532156e-07,
      "loss": 0.0009,
      "step": 77530
    },
    {
      "epoch": 14098.181818181818,
      "grad_norm": 0.0014346395619213581,
      "learning_rate": 5.575841578590367e-07,
      "loss": 0.0012,
      "step": 77540
    },
    {
      "epoch": 14100.0,
      "grad_norm": 0.0011785035021603107,
      "learning_rate": 5.574685752464333e-07,
      "loss": 0.0011,
      "step": 77550
    },
    {
      "epoch": 14101.818181818182,
      "grad_norm": 0.04170721769332886,
      "learning_rate": 5.573529895216647e-07,
      "loss": 0.0012,
      "step": 77560
    },
    {
      "epoch": 14103.636363636364,
      "grad_norm": 0.2154463827610016,
      "learning_rate": 5.572374006909907e-07,
      "loss": 0.0012,
      "step": 77570
    },
    {
      "epoch": 14105.454545454546,
      "grad_norm": 0.26766055822372437,
      "learning_rate": 5.571218087606704e-07,
      "loss": 0.0013,
      "step": 77580
    },
    {
      "epoch": 14107.272727272728,
      "grad_norm": 0.0013936626492068172,
      "learning_rate": 5.570062137369639e-07,
      "loss": 0.0008,
      "step": 77590
    },
    {
      "epoch": 14109.09090909091,
      "grad_norm": 0.0020783590152859688,
      "learning_rate": 5.568906156261308e-07,
      "loss": 0.0011,
      "step": 77600
    },
    {
      "epoch": 14110.90909090909,
      "grad_norm": 0.15469859540462494,
      "learning_rate": 5.567750144344318e-07,
      "loss": 0.0009,
      "step": 77610
    },
    {
      "epoch": 14112.727272727272,
      "grad_norm": 0.0011997815454378724,
      "learning_rate": 5.566594101681267e-07,
      "loss": 0.0013,
      "step": 77620
    },
    {
      "epoch": 14114.545454545454,
      "grad_norm": 0.20367056131362915,
      "learning_rate": 5.565438028334762e-07,
      "loss": 0.0009,
      "step": 77630
    },
    {
      "epoch": 14116.363636363636,
      "grad_norm": 0.1901962161064148,
      "learning_rate": 5.564281924367407e-07,
      "loss": 0.0013,
      "step": 77640
    },
    {
      "epoch": 14118.181818181818,
      "grad_norm": 0.26987361907958984,
      "learning_rate": 5.563125789841814e-07,
      "loss": 0.0012,
      "step": 77650
    },
    {
      "epoch": 14120.0,
      "grad_norm": 0.15961842238903046,
      "learning_rate": 5.561969624820589e-07,
      "loss": 0.0009,
      "step": 77660
    },
    {
      "epoch": 14121.818181818182,
      "grad_norm": 0.2246820628643036,
      "learning_rate": 5.560813429366345e-07,
      "loss": 0.0012,
      "step": 77670
    },
    {
      "epoch": 14123.636363636364,
      "grad_norm": 0.0011858745710924268,
      "learning_rate": 5.559657203541692e-07,
      "loss": 0.0009,
      "step": 77680
    },
    {
      "epoch": 14125.454545454546,
      "grad_norm": 0.001202672254294157,
      "learning_rate": 5.558500947409248e-07,
      "loss": 0.0012,
      "step": 77690
    },
    {
      "epoch": 14127.272727272728,
      "grad_norm": 0.20660491287708282,
      "learning_rate": 5.557344661031627e-07,
      "loss": 0.0013,
      "step": 77700
    },
    {
      "epoch": 14129.09090909091,
      "grad_norm": 0.0009054467082023621,
      "learning_rate": 5.556188344471448e-07,
      "loss": 0.0011,
      "step": 77710
    },
    {
      "epoch": 14130.90909090909,
      "grad_norm": 0.0007255783420987427,
      "learning_rate": 5.55503199779133e-07,
      "loss": 0.001,
      "step": 77720
    },
    {
      "epoch": 14132.727272727272,
      "grad_norm": 0.2963438034057617,
      "learning_rate": 5.553875621053893e-07,
      "loss": 0.0014,
      "step": 77730
    },
    {
      "epoch": 14134.545454545454,
      "grad_norm": 0.0008019965025596321,
      "learning_rate": 5.552719214321761e-07,
      "loss": 0.0009,
      "step": 77740
    },
    {
      "epoch": 14136.363636363636,
      "grad_norm": 0.0009968633530661464,
      "learning_rate": 5.551562777657559e-07,
      "loss": 0.0012,
      "step": 77750
    },
    {
      "epoch": 14138.181818181818,
      "grad_norm": 0.26921117305755615,
      "learning_rate": 5.550406311123911e-07,
      "loss": 0.0012,
      "step": 77760
    },
    {
      "epoch": 14140.0,
      "grad_norm": 0.0009565061191096902,
      "learning_rate": 5.549249814783445e-07,
      "loss": 0.001,
      "step": 77770
    },
    {
      "epoch": 14141.818181818182,
      "grad_norm": 0.0032262876629829407,
      "learning_rate": 5.54809328869879e-07,
      "loss": 0.0011,
      "step": 77780
    },
    {
      "epoch": 14143.636363636364,
      "grad_norm": 0.0007191715994849801,
      "learning_rate": 5.546936732932578e-07,
      "loss": 0.0012,
      "step": 77790
    },
    {
      "epoch": 14145.454545454546,
      "grad_norm": 0.17163024842739105,
      "learning_rate": 5.54578014754744e-07,
      "loss": 0.001,
      "step": 77800
    },
    {
      "epoch": 14147.272727272728,
      "grad_norm": 0.0017921577673405409,
      "learning_rate": 5.544623532606011e-07,
      "loss": 0.0013,
      "step": 77810
    },
    {
      "epoch": 14149.09090909091,
      "grad_norm": 0.1435801088809967,
      "learning_rate": 5.543466888170925e-07,
      "loss": 0.001,
      "step": 77820
    },
    {
      "epoch": 14150.90909090909,
      "grad_norm": 0.0009591531124897301,
      "learning_rate": 5.542310214304822e-07,
      "loss": 0.0011,
      "step": 77830
    },
    {
      "epoch": 14152.727272727272,
      "grad_norm": 0.20446892082691193,
      "learning_rate": 5.541153511070337e-07,
      "loss": 0.001,
      "step": 77840
    },
    {
      "epoch": 14154.545454545454,
      "grad_norm": 0.0016855336725711823,
      "learning_rate": 5.539996778530114e-07,
      "loss": 0.001,
      "step": 77850
    },
    {
      "epoch": 14156.363636363636,
      "grad_norm": 0.17136524617671967,
      "learning_rate": 5.538840016746793e-07,
      "loss": 0.0013,
      "step": 77860
    },
    {
      "epoch": 14158.181818181818,
      "grad_norm": 0.17584110796451569,
      "learning_rate": 5.537683225783016e-07,
      "loss": 0.001,
      "step": 77870
    },
    {
      "epoch": 14160.0,
      "grad_norm": 0.0008424845873378217,
      "learning_rate": 5.536526405701433e-07,
      "loss": 0.001,
      "step": 77880
    },
    {
      "epoch": 14161.818181818182,
      "grad_norm": 0.0011043482227250934,
      "learning_rate": 5.535369556564686e-07,
      "loss": 0.0012,
      "step": 77890
    },
    {
      "epoch": 14163.636363636364,
      "grad_norm": 0.0011051835026592016,
      "learning_rate": 5.534212678435426e-07,
      "loss": 0.0006,
      "step": 77900
    },
    {
      "epoch": 14165.454545454546,
      "grad_norm": 0.011437888257205486,
      "learning_rate": 5.533055771376299e-07,
      "loss": 0.0015,
      "step": 77910
    },
    {
      "epoch": 14167.272727272728,
      "grad_norm": 0.20006458461284637,
      "learning_rate": 5.53189883544996e-07,
      "loss": 0.0011,
      "step": 77920
    },
    {
      "epoch": 14169.09090909091,
      "grad_norm": 0.15203732252120972,
      "learning_rate": 5.530741870719063e-07,
      "loss": 0.001,
      "step": 77930
    },
    {
      "epoch": 14170.90909090909,
      "grad_norm": 0.16182072460651398,
      "learning_rate": 5.529584877246259e-07,
      "loss": 0.001,
      "step": 77940
    },
    {
      "epoch": 14172.727272727272,
      "grad_norm": 0.0006477374699898064,
      "learning_rate": 5.528427855094206e-07,
      "loss": 0.0012,
      "step": 77950
    },
    {
      "epoch": 14174.545454545454,
      "grad_norm": 0.0016950155841186643,
      "learning_rate": 5.52727080432556e-07,
      "loss": 0.001,
      "step": 77960
    },
    {
      "epoch": 14176.363636363636,
      "grad_norm": 0.17436440289020538,
      "learning_rate": 5.526113725002983e-07,
      "loss": 0.0012,
      "step": 77970
    },
    {
      "epoch": 14178.181818181818,
      "grad_norm": 0.0007506871479563415,
      "learning_rate": 5.524956617189132e-07,
      "loss": 0.0009,
      "step": 77980
    },
    {
      "epoch": 14180.0,
      "grad_norm": 0.1970512568950653,
      "learning_rate": 5.523799480946673e-07,
      "loss": 0.0012,
      "step": 77990
    },
    {
      "epoch": 14181.818181818182,
      "grad_norm": 0.16504600644111633,
      "learning_rate": 5.522642316338268e-07,
      "loss": 0.001,
      "step": 78000
    },
    {
      "epoch": 14181.818181818182,
      "eval_loss": 4.952549934387207,
      "eval_runtime": 0.9546,
      "eval_samples_per_second": 10.475,
      "eval_steps_per_second": 5.238,
      "step": 78000
    },
    {
      "epoch": 14183.636363636364,
      "grad_norm": 0.0022792990785092115,
      "learning_rate": 5.521485123426582e-07,
      "loss": 0.0012,
      "step": 78010
    },
    {
      "epoch": 14185.454545454546,
      "grad_norm": 0.000962361169513315,
      "learning_rate": 5.520327902274283e-07,
      "loss": 0.0009,
      "step": 78020
    },
    {
      "epoch": 14187.272727272728,
      "grad_norm": 0.16533263027668,
      "learning_rate": 5.519170652944036e-07,
      "loss": 0.0016,
      "step": 78030
    },
    {
      "epoch": 14189.09090909091,
      "grad_norm": 0.16470454633235931,
      "learning_rate": 5.518013375498516e-07,
      "loss": 0.001,
      "step": 78040
    },
    {
      "epoch": 14190.90909090909,
      "grad_norm": 0.20973797142505646,
      "learning_rate": 5.516856070000393e-07,
      "loss": 0.0011,
      "step": 78050
    },
    {
      "epoch": 14192.727272727272,
      "grad_norm": 0.0018529753433540463,
      "learning_rate": 5.515698736512337e-07,
      "loss": 0.0012,
      "step": 78060
    },
    {
      "epoch": 14194.545454545454,
      "grad_norm": 0.0009062079479917884,
      "learning_rate": 5.514541375097025e-07,
      "loss": 0.001,
      "step": 78070
    },
    {
      "epoch": 14196.363636363636,
      "grad_norm": 0.0006349325412884355,
      "learning_rate": 5.513383985817133e-07,
      "loss": 0.0009,
      "step": 78080
    },
    {
      "epoch": 14198.181818181818,
      "grad_norm": 0.0007742607849650085,
      "learning_rate": 5.512226568735338e-07,
      "loss": 0.001,
      "step": 78090
    },
    {
      "epoch": 14200.0,
      "grad_norm": 0.0006321226828731596,
      "learning_rate": 5.511069123914318e-07,
      "loss": 0.0012,
      "step": 78100
    },
    {
      "epoch": 14201.818181818182,
      "grad_norm": 0.0007879313779994845,
      "learning_rate": 5.509911651416755e-07,
      "loss": 0.0009,
      "step": 78110
    },
    {
      "epoch": 14203.636363636364,
      "grad_norm": 0.0006997517193667591,
      "learning_rate": 5.508754151305331e-07,
      "loss": 0.0011,
      "step": 78120
    },
    {
      "epoch": 14205.454545454546,
      "grad_norm": 0.002725201891735196,
      "learning_rate": 5.507596623642729e-07,
      "loss": 0.0013,
      "step": 78130
    },
    {
      "epoch": 14207.272727272728,
      "grad_norm": 0.17775899171829224,
      "learning_rate": 5.506439068491633e-07,
      "loss": 0.0012,
      "step": 78140
    },
    {
      "epoch": 14209.09090909091,
      "grad_norm": 0.0008906811126507819,
      "learning_rate": 5.505281485914732e-07,
      "loss": 0.0009,
      "step": 78150
    },
    {
      "epoch": 14210.90909090909,
      "grad_norm": 0.001667256117798388,
      "learning_rate": 5.504123875974712e-07,
      "loss": 0.0009,
      "step": 78160
    },
    {
      "epoch": 14212.727272727272,
      "grad_norm": 0.17274194955825806,
      "learning_rate": 5.502966238734263e-07,
      "loss": 0.0011,
      "step": 78170
    },
    {
      "epoch": 14214.545454545454,
      "grad_norm": 0.0007922740769572556,
      "learning_rate": 5.501808574256074e-07,
      "loss": 0.0013,
      "step": 78180
    },
    {
      "epoch": 14216.363636363636,
      "grad_norm": 0.1963900774717331,
      "learning_rate": 5.50065088260284e-07,
      "loss": 0.0011,
      "step": 78190
    },
    {
      "epoch": 14218.181818181818,
      "grad_norm": 0.19125409424304962,
      "learning_rate": 5.499493163837257e-07,
      "loss": 0.0012,
      "step": 78200
    },
    {
      "epoch": 14220.0,
      "grad_norm": 0.0016342547023668885,
      "learning_rate": 5.498335418022014e-07,
      "loss": 0.001,
      "step": 78210
    },
    {
      "epoch": 14221.818181818182,
      "grad_norm": 0.16351379454135895,
      "learning_rate": 5.497177645219813e-07,
      "loss": 0.0012,
      "step": 78220
    },
    {
      "epoch": 14223.636363636364,
      "grad_norm": 0.0008150081848725677,
      "learning_rate": 5.49601984549335e-07,
      "loss": 0.0009,
      "step": 78230
    },
    {
      "epoch": 14225.454545454546,
      "grad_norm": 0.000966879480984062,
      "learning_rate": 5.494862018905326e-07,
      "loss": 0.001,
      "step": 78240
    },
    {
      "epoch": 14227.272727272728,
      "grad_norm": 0.0007928065024316311,
      "learning_rate": 5.493704165518439e-07,
      "loss": 0.0011,
      "step": 78250
    },
    {
      "epoch": 14229.09090909091,
      "grad_norm": 0.17914430797100067,
      "learning_rate": 5.492546285395396e-07,
      "loss": 0.0012,
      "step": 78260
    },
    {
      "epoch": 14230.90909090909,
      "grad_norm": 0.0011209446238353848,
      "learning_rate": 5.491388378598899e-07,
      "loss": 0.0012,
      "step": 78270
    },
    {
      "epoch": 14232.727272727272,
      "grad_norm": 0.17986942827701569,
      "learning_rate": 5.490230445191653e-07,
      "loss": 0.0012,
      "step": 78280
    },
    {
      "epoch": 14234.545454545454,
      "grad_norm": 0.16861934959888458,
      "learning_rate": 5.489072485236367e-07,
      "loss": 0.0009,
      "step": 78290
    },
    {
      "epoch": 14236.363636363636,
      "grad_norm": 0.19748900830745697,
      "learning_rate": 5.487914498795747e-07,
      "loss": 0.0012,
      "step": 78300
    },
    {
      "epoch": 14238.181818181818,
      "grad_norm": 0.0007388325757347047,
      "learning_rate": 5.486756485932504e-07,
      "loss": 0.0009,
      "step": 78310
    },
    {
      "epoch": 14240.0,
      "grad_norm": 0.0010442871134728193,
      "learning_rate": 5.48559844670935e-07,
      "loss": 0.0012,
      "step": 78320
    },
    {
      "epoch": 14241.818181818182,
      "grad_norm": 0.2708214223384857,
      "learning_rate": 5.484440381188996e-07,
      "loss": 0.0012,
      "step": 78330
    },
    {
      "epoch": 14243.636363636364,
      "grad_norm": 0.28604236245155334,
      "learning_rate": 5.483282289434157e-07,
      "loss": 0.0012,
      "step": 78340
    },
    {
      "epoch": 14245.454545454546,
      "grad_norm": 0.0014076442457735538,
      "learning_rate": 5.48212417150755e-07,
      "loss": 0.0007,
      "step": 78350
    },
    {
      "epoch": 14247.272727272728,
      "grad_norm": 0.0009370492771267891,
      "learning_rate": 5.480966027471888e-07,
      "loss": 0.001,
      "step": 78360
    },
    {
      "epoch": 14249.09090909091,
      "grad_norm": 0.0009946796344593167,
      "learning_rate": 5.479807857389894e-07,
      "loss": 0.0012,
      "step": 78370
    },
    {
      "epoch": 14250.90909090909,
      "grad_norm": 0.0011977464891970158,
      "learning_rate": 5.478649661324283e-07,
      "loss": 0.0012,
      "step": 78380
    },
    {
      "epoch": 14252.727272727272,
      "grad_norm": 0.0014257528819143772,
      "learning_rate": 5.477491439337782e-07,
      "loss": 0.0009,
      "step": 78390
    },
    {
      "epoch": 14254.545454545454,
      "grad_norm": 0.2826014459133148,
      "learning_rate": 5.476333191493107e-07,
      "loss": 0.0016,
      "step": 78400
    },
    {
      "epoch": 14256.363636363636,
      "grad_norm": 0.2643132209777832,
      "learning_rate": 5.475174917852988e-07,
      "loss": 0.0009,
      "step": 78410
    },
    {
      "epoch": 14258.181818181818,
      "grad_norm": 0.15384000539779663,
      "learning_rate": 5.474016618480146e-07,
      "loss": 0.001,
      "step": 78420
    },
    {
      "epoch": 14260.0,
      "grad_norm": 0.001465233275666833,
      "learning_rate": 5.472858293437311e-07,
      "loss": 0.0011,
      "step": 78430
    },
    {
      "epoch": 14261.818181818182,
      "grad_norm": 0.0011673959670588374,
      "learning_rate": 5.47169994278721e-07,
      "loss": 0.001,
      "step": 78440
    },
    {
      "epoch": 14263.636363636364,
      "grad_norm": 0.0009504014160484076,
      "learning_rate": 5.470541566592572e-07,
      "loss": 0.0012,
      "step": 78450
    },
    {
      "epoch": 14265.454545454546,
      "grad_norm": 0.001188275869935751,
      "learning_rate": 5.469383164916127e-07,
      "loss": 0.0009,
      "step": 78460
    },
    {
      "epoch": 14267.272727272728,
      "grad_norm": 0.005072193220257759,
      "learning_rate": 5.468224737820611e-07,
      "loss": 0.0012,
      "step": 78470
    },
    {
      "epoch": 14269.09090909091,
      "grad_norm": 0.23222240805625916,
      "learning_rate": 5.467066285368753e-07,
      "loss": 0.0012,
      "step": 78480
    },
    {
      "epoch": 14270.90909090909,
      "grad_norm": 0.16965386271476746,
      "learning_rate": 5.46590780762329e-07,
      "loss": 0.0012,
      "step": 78490
    },
    {
      "epoch": 14272.727272727272,
      "grad_norm": 0.0013307937188073993,
      "learning_rate": 5.464749304646961e-07,
      "loss": 0.001,
      "step": 78500
    },
    {
      "epoch": 14272.727272727272,
      "eval_loss": 5.037296295166016,
      "eval_runtime": 0.9558,
      "eval_samples_per_second": 10.462,
      "eval_steps_per_second": 5.231,
      "step": 78500
    },
    {
      "epoch": 14274.545454545454,
      "grad_norm": 0.21233506500720978,
      "learning_rate": 5.4635907765025e-07,
      "loss": 0.0013,
      "step": 78510
    },
    {
      "epoch": 14276.363636363636,
      "grad_norm": 0.0013796972343698144,
      "learning_rate": 5.46243222325265e-07,
      "loss": 0.0009,
      "step": 78520
    },
    {
      "epoch": 14278.181818181818,
      "grad_norm": 0.002742297016084194,
      "learning_rate": 5.461273644960146e-07,
      "loss": 0.0011,
      "step": 78530
    },
    {
      "epoch": 14280.0,
      "grad_norm": 0.04466450959444046,
      "learning_rate": 5.460115041687736e-07,
      "loss": 0.0012,
      "step": 78540
    },
    {
      "epoch": 14281.818181818182,
      "grad_norm": 0.21108520030975342,
      "learning_rate": 5.458956413498159e-07,
      "loss": 0.0012,
      "step": 78550
    },
    {
      "epoch": 14283.636363636364,
      "grad_norm": 0.0011896620271727443,
      "learning_rate": 5.457797760454163e-07,
      "loss": 0.001,
      "step": 78560
    },
    {
      "epoch": 14285.454545454546,
      "grad_norm": 0.17889603972434998,
      "learning_rate": 5.456639082618489e-07,
      "loss": 0.0009,
      "step": 78570
    },
    {
      "epoch": 14287.272727272728,
      "grad_norm": 0.2841435372829437,
      "learning_rate": 5.455480380053888e-07,
      "loss": 0.0013,
      "step": 78580
    },
    {
      "epoch": 14289.09090909091,
      "grad_norm": 0.21326321363449097,
      "learning_rate": 5.45432165282311e-07,
      "loss": 0.0009,
      "step": 78590
    },
    {
      "epoch": 14290.90909090909,
      "grad_norm": 0.21007077395915985,
      "learning_rate": 5.453162900988901e-07,
      "loss": 0.0012,
      "step": 78600
    },
    {
      "epoch": 14292.727272727272,
      "grad_norm": 0.21990413963794708,
      "learning_rate": 5.452004124614014e-07,
      "loss": 0.0009,
      "step": 78610
    },
    {
      "epoch": 14294.545454545454,
      "grad_norm": 0.0015041496371850371,
      "learning_rate": 5.450845323761203e-07,
      "loss": 0.001,
      "step": 78620
    },
    {
      "epoch": 14296.363636363636,
      "grad_norm": 0.2102072536945343,
      "learning_rate": 5.449686498493219e-07,
      "loss": 0.0015,
      "step": 78630
    },
    {
      "epoch": 14298.181818181818,
      "grad_norm": 0.002967199543491006,
      "learning_rate": 5.448527648872822e-07,
      "loss": 0.0009,
      "step": 78640
    },
    {
      "epoch": 14300.0,
      "grad_norm": 0.0010025539668276906,
      "learning_rate": 5.447368774962762e-07,
      "loss": 0.0012,
      "step": 78650
    },
    {
      "epoch": 14301.818181818182,
      "grad_norm": 0.21299707889556885,
      "learning_rate": 5.446209876825803e-07,
      "loss": 0.0012,
      "step": 78660
    },
    {
      "epoch": 14303.636363636364,
      "grad_norm": 0.0008674230775795877,
      "learning_rate": 5.4450509545247e-07,
      "loss": 0.0012,
      "step": 78670
    },
    {
      "epoch": 14305.454545454546,
      "grad_norm": 0.18904830515384674,
      "learning_rate": 5.443892008122216e-07,
      "loss": 0.0007,
      "step": 78680
    },
    {
      "epoch": 14307.272727272728,
      "grad_norm": 0.00129309028852731,
      "learning_rate": 5.442733037681111e-07,
      "loss": 0.0011,
      "step": 78690
    },
    {
      "epoch": 14309.09090909091,
      "grad_norm": 0.17549054324626923,
      "learning_rate": 5.44157404326415e-07,
      "loss": 0.0012,
      "step": 78700
    },
    {
      "epoch": 14310.90909090909,
      "grad_norm": 0.20002135634422302,
      "learning_rate": 5.440415024934097e-07,
      "loss": 0.001,
      "step": 78710
    },
    {
      "epoch": 14312.727272727272,
      "grad_norm": 0.16625802218914032,
      "learning_rate": 5.439255982753717e-07,
      "loss": 0.001,
      "step": 78720
    },
    {
      "epoch": 14314.545454545454,
      "grad_norm": 0.17099963128566742,
      "learning_rate": 5.438096916785776e-07,
      "loss": 0.0012,
      "step": 78730
    },
    {
      "epoch": 14316.363636363636,
      "grad_norm": 0.2242555171251297,
      "learning_rate": 5.436937827093044e-07,
      "loss": 0.0012,
      "step": 78740
    },
    {
      "epoch": 14318.181818181818,
      "grad_norm": 0.2078646868467331,
      "learning_rate": 5.435778713738292e-07,
      "loss": 0.0015,
      "step": 78750
    },
    {
      "epoch": 14320.0,
      "grad_norm": 0.0008850264712236822,
      "learning_rate": 5.434619576784287e-07,
      "loss": 0.0007,
      "step": 78760
    },
    {
      "epoch": 14321.818181818182,
      "grad_norm": 0.23705998063087463,
      "learning_rate": 5.433460416293804e-07,
      "loss": 0.0012,
      "step": 78770
    },
    {
      "epoch": 14323.636363636364,
      "grad_norm": 0.00544478464871645,
      "learning_rate": 5.432301232329614e-07,
      "loss": 0.0012,
      "step": 78780
    },
    {
      "epoch": 14325.454545454546,
      "grad_norm": 0.16827906668186188,
      "learning_rate": 5.431142024954496e-07,
      "loss": 0.0009,
      "step": 78790
    },
    {
      "epoch": 14327.272727272728,
      "grad_norm": 0.0027638168539851904,
      "learning_rate": 5.42998279423122e-07,
      "loss": 0.0009,
      "step": 78800
    },
    {
      "epoch": 14329.09090909091,
      "grad_norm": 0.0020322471391409636,
      "learning_rate": 5.428823540222569e-07,
      "loss": 0.0012,
      "step": 78810
    },
    {
      "epoch": 14330.90909090909,
      "grad_norm": 0.0008525904850102961,
      "learning_rate": 5.427664262991318e-07,
      "loss": 0.0011,
      "step": 78820
    },
    {
      "epoch": 14332.727272727272,
      "grad_norm": 0.27391859889030457,
      "learning_rate": 5.426504962600249e-07,
      "loss": 0.0013,
      "step": 78830
    },
    {
      "epoch": 14334.545454545454,
      "grad_norm": 0.0010151730384677649,
      "learning_rate": 5.42534563911214e-07,
      "loss": 0.0007,
      "step": 78840
    },
    {
      "epoch": 14336.363636363636,
      "grad_norm": 0.0025634104385972023,
      "learning_rate": 5.424186292589775e-07,
      "loss": 0.0012,
      "step": 78850
    },
    {
      "epoch": 14338.181818181818,
      "grad_norm": 0.26345542073249817,
      "learning_rate": 5.423026923095938e-07,
      "loss": 0.0013,
      "step": 78860
    },
    {
      "epoch": 14340.0,
      "grad_norm": 0.0014664737973362207,
      "learning_rate": 5.421867530693414e-07,
      "loss": 0.0009,
      "step": 78870
    },
    {
      "epoch": 14341.818181818182,
      "grad_norm": 0.1443907916545868,
      "learning_rate": 5.420708115444988e-07,
      "loss": 0.0011,
      "step": 78880
    },
    {
      "epoch": 14343.636363636364,
      "grad_norm": 0.0005888976156711578,
      "learning_rate": 5.419548677413444e-07,
      "loss": 0.0012,
      "step": 78890
    },
    {
      "epoch": 14345.454545454546,
      "grad_norm": 0.20963449776172638,
      "learning_rate": 5.418389216661578e-07,
      "loss": 0.0011,
      "step": 78900
    },
    {
      "epoch": 14347.272727272728,
      "grad_norm": 0.1565794199705124,
      "learning_rate": 5.417229733252173e-07,
      "loss": 0.001,
      "step": 78910
    },
    {
      "epoch": 14349.09090909091,
      "grad_norm": 0.0007742504240013659,
      "learning_rate": 5.416070227248023e-07,
      "loss": 0.001,
      "step": 78920
    },
    {
      "epoch": 14350.90909090909,
      "grad_norm": 0.24866050481796265,
      "learning_rate": 5.414910698711919e-07,
      "loss": 0.0012,
      "step": 78930
    },
    {
      "epoch": 14352.727272727272,
      "grad_norm": 0.0024586350191384554,
      "learning_rate": 5.413751147706657e-07,
      "loss": 0.0012,
      "step": 78940
    },
    {
      "epoch": 14354.545454545454,
      "grad_norm": 0.1995602697134018,
      "learning_rate": 5.412591574295026e-07,
      "loss": 0.0011,
      "step": 78950
    },
    {
      "epoch": 14356.363636363636,
      "grad_norm": 0.011322441510856152,
      "learning_rate": 5.411431978539827e-07,
      "loss": 0.0012,
      "step": 78960
    },
    {
      "epoch": 14358.181818181818,
      "grad_norm": 0.14240020513534546,
      "learning_rate": 5.410272360503855e-07,
      "loss": 0.001,
      "step": 78970
    },
    {
      "epoch": 14360.0,
      "grad_norm": 0.0007182583794929087,
      "learning_rate": 5.40911272024991e-07,
      "loss": 0.0011,
      "step": 78980
    },
    {
      "epoch": 14361.818181818182,
      "grad_norm": 0.001056576264090836,
      "learning_rate": 5.407953057840788e-07,
      "loss": 0.001,
      "step": 78990
    },
    {
      "epoch": 14363.636363636364,
      "grad_norm": 0.014821650460362434,
      "learning_rate": 5.406793373339292e-07,
      "loss": 0.0013,
      "step": 79000
    },
    {
      "epoch": 14363.636363636364,
      "eval_loss": 4.951694488525391,
      "eval_runtime": 0.9515,
      "eval_samples_per_second": 10.509,
      "eval_steps_per_second": 5.255,
      "step": 79000
    },
    {
      "epoch": 14365.454545454546,
      "grad_norm": 0.21100175380706787,
      "learning_rate": 5.405633666808222e-07,
      "loss": 0.0009,
      "step": 79010
    },
    {
      "epoch": 14367.272727272728,
      "grad_norm": 0.19769398868083954,
      "learning_rate": 5.404473938310383e-07,
      "loss": 0.001,
      "step": 79020
    },
    {
      "epoch": 14369.09090909091,
      "grad_norm": 0.28904998302459717,
      "learning_rate": 5.403314187908579e-07,
      "loss": 0.0012,
      "step": 79030
    },
    {
      "epoch": 14370.90909090909,
      "grad_norm": 0.22107534110546112,
      "learning_rate": 5.402154415665613e-07,
      "loss": 0.0011,
      "step": 79040
    },
    {
      "epoch": 14372.727272727272,
      "grad_norm": 0.005736035760492086,
      "learning_rate": 5.400994621644294e-07,
      "loss": 0.001,
      "step": 79050
    },
    {
      "epoch": 14374.545454545454,
      "grad_norm": 0.000751866027712822,
      "learning_rate": 5.39983480590743e-07,
      "loss": 0.001,
      "step": 79060
    },
    {
      "epoch": 14376.363636363636,
      "grad_norm": 0.0018889072816818953,
      "learning_rate": 5.398674968517827e-07,
      "loss": 0.0012,
      "step": 79070
    },
    {
      "epoch": 14378.181818181818,
      "grad_norm": 0.0012907456839457154,
      "learning_rate": 5.397515109538299e-07,
      "loss": 0.001,
      "step": 79080
    },
    {
      "epoch": 14380.0,
      "grad_norm": 0.0008975254604592919,
      "learning_rate": 5.396355229031655e-07,
      "loss": 0.0012,
      "step": 79090
    },
    {
      "epoch": 14381.818181818182,
      "grad_norm": 0.0005805667024105787,
      "learning_rate": 5.395195327060707e-07,
      "loss": 0.001,
      "step": 79100
    },
    {
      "epoch": 14383.636363636364,
      "grad_norm": 0.2058580070734024,
      "learning_rate": 5.394035403688267e-07,
      "loss": 0.0011,
      "step": 79110
    },
    {
      "epoch": 14385.454545454546,
      "grad_norm": 0.0007550322916358709,
      "learning_rate": 5.392875458977154e-07,
      "loss": 0.001,
      "step": 79120
    },
    {
      "epoch": 14387.272727272728,
      "grad_norm": 0.21016085147857666,
      "learning_rate": 5.391715492990183e-07,
      "loss": 0.0012,
      "step": 79130
    },
    {
      "epoch": 14389.09090909091,
      "grad_norm": 0.0008817383204586804,
      "learning_rate": 5.390555505790168e-07,
      "loss": 0.001,
      "step": 79140
    },
    {
      "epoch": 14390.90909090909,
      "grad_norm": 0.22289371490478516,
      "learning_rate": 5.389395497439929e-07,
      "loss": 0.0011,
      "step": 79150
    },
    {
      "epoch": 14392.727272727272,
      "grad_norm": 0.2260061353445053,
      "learning_rate": 5.388235468002286e-07,
      "loss": 0.0012,
      "step": 79160
    },
    {
      "epoch": 14394.545454545454,
      "grad_norm": 0.1718839854001999,
      "learning_rate": 5.387075417540059e-07,
      "loss": 0.0012,
      "step": 79170
    },
    {
      "epoch": 14396.363636363636,
      "grad_norm": 0.13742473721504211,
      "learning_rate": 5.385915346116069e-07,
      "loss": 0.0012,
      "step": 79180
    },
    {
      "epoch": 14398.181818181818,
      "grad_norm": 0.0006495525594800711,
      "learning_rate": 5.384755253793138e-07,
      "loss": 0.0011,
      "step": 79190
    },
    {
      "epoch": 14400.0,
      "grad_norm": 0.2537402808666229,
      "learning_rate": 5.383595140634093e-07,
      "loss": 0.0012,
      "step": 79200
    },
    {
      "epoch": 14401.818181818182,
      "grad_norm": 0.0016332975355908275,
      "learning_rate": 5.382435006701757e-07,
      "loss": 0.0012,
      "step": 79210
    },
    {
      "epoch": 14403.636363636364,
      "grad_norm": 0.0006959924940019846,
      "learning_rate": 5.381274852058955e-07,
      "loss": 0.0009,
      "step": 79220
    },
    {
      "epoch": 14405.454545454546,
      "grad_norm": 0.20900708436965942,
      "learning_rate": 5.380114676768516e-07,
      "loss": 0.0015,
      "step": 79230
    },
    {
      "epoch": 14407.272727272728,
      "grad_norm": 0.0009625870734453201,
      "learning_rate": 5.378954480893268e-07,
      "loss": 0.0009,
      "step": 79240
    },
    {
      "epoch": 14409.09090909091,
      "grad_norm": 0.0008712216513231397,
      "learning_rate": 5.377794264496041e-07,
      "loss": 0.001,
      "step": 79250
    },
    {
      "epoch": 14410.90909090909,
      "grad_norm": 0.21097293496131897,
      "learning_rate": 5.376634027639664e-07,
      "loss": 0.001,
      "step": 79260
    },
    {
      "epoch": 14412.727272727272,
      "grad_norm": 0.25897982716560364,
      "learning_rate": 5.375473770386969e-07,
      "loss": 0.0012,
      "step": 79270
    },
    {
      "epoch": 14414.545454545454,
      "grad_norm": 0.20022746920585632,
      "learning_rate": 5.37431349280079e-07,
      "loss": 0.0012,
      "step": 79280
    },
    {
      "epoch": 14416.363636363636,
      "grad_norm": 0.1966775357723236,
      "learning_rate": 5.373153194943961e-07,
      "loss": 0.0009,
      "step": 79290
    },
    {
      "epoch": 14418.181818181818,
      "grad_norm": 0.0008315655868500471,
      "learning_rate": 5.371992876879318e-07,
      "loss": 0.001,
      "step": 79300
    },
    {
      "epoch": 14420.0,
      "grad_norm": 0.2624441683292389,
      "learning_rate": 5.370832538669692e-07,
      "loss": 0.0012,
      "step": 79310
    },
    {
      "epoch": 14421.818181818182,
      "grad_norm": 0.26767733693122864,
      "learning_rate": 5.369672180377926e-07,
      "loss": 0.0012,
      "step": 79320
    },
    {
      "epoch": 14423.636363636364,
      "grad_norm": 0.20951732993125916,
      "learning_rate": 5.368511802066854e-07,
      "loss": 0.0011,
      "step": 79330
    },
    {
      "epoch": 14425.454545454546,
      "grad_norm": 0.27299773693084717,
      "learning_rate": 5.367351403799319e-07,
      "loss": 0.0012,
      "step": 79340
    },
    {
      "epoch": 14427.272727272728,
      "grad_norm": 0.284341961145401,
      "learning_rate": 5.366190985638158e-07,
      "loss": 0.0014,
      "step": 79350
    },
    {
      "epoch": 14429.09090909091,
      "grad_norm": 0.0010494623566046357,
      "learning_rate": 5.365030547646217e-07,
      "loss": 0.0006,
      "step": 79360
    },
    {
      "epoch": 14430.90909090909,
      "grad_norm": 0.0020427764393389225,
      "learning_rate": 5.363870089886333e-07,
      "loss": 0.0012,
      "step": 79370
    },
    {
      "epoch": 14432.727272727272,
      "grad_norm": 0.0007710179197601974,
      "learning_rate": 5.362709612421354e-07,
      "loss": 0.0009,
      "step": 79380
    },
    {
      "epoch": 14434.545454545454,
      "grad_norm": 0.18411605060100555,
      "learning_rate": 5.361549115314122e-07,
      "loss": 0.001,
      "step": 79390
    },
    {
      "epoch": 14436.363636363636,
      "grad_norm": 0.0011395385954529047,
      "learning_rate": 5.360388598627487e-07,
      "loss": 0.0012,
      "step": 79400
    },
    {
      "epoch": 14438.181818181818,
      "grad_norm": 0.19086535274982452,
      "learning_rate": 5.359228062424292e-07,
      "loss": 0.0012,
      "step": 79410
    },
    {
      "epoch": 14440.0,
      "grad_norm": 0.2738317847251892,
      "learning_rate": 5.358067506767384e-07,
      "loss": 0.001,
      "step": 79420
    },
    {
      "epoch": 14441.818181818182,
      "grad_norm": 0.17009511590003967,
      "learning_rate": 5.356906931719615e-07,
      "loss": 0.0012,
      "step": 79430
    },
    {
      "epoch": 14443.636363636364,
      "grad_norm": 0.19882912933826447,
      "learning_rate": 5.355746337343835e-07,
      "loss": 0.0011,
      "step": 79440
    },
    {
      "epoch": 14445.454545454546,
      "grad_norm": 0.0012010011123493314,
      "learning_rate": 5.354585723702892e-07,
      "loss": 0.0007,
      "step": 79450
    },
    {
      "epoch": 14447.272727272728,
      "grad_norm": 0.21318332850933075,
      "learning_rate": 5.353425090859641e-07,
      "loss": 0.0016,
      "step": 79460
    },
    {
      "epoch": 14449.09090909091,
      "grad_norm": 0.0013827853836119175,
      "learning_rate": 5.352264438876935e-07,
      "loss": 0.0007,
      "step": 79470
    },
    {
      "epoch": 14450.90909090909,
      "grad_norm": 0.005386178381741047,
      "learning_rate": 5.351103767817626e-07,
      "loss": 0.0012,
      "step": 79480
    },
    {
      "epoch": 14452.727272727272,
      "grad_norm": 0.0016141602536663413,
      "learning_rate": 5.349943077744572e-07,
      "loss": 0.0006,
      "step": 79490
    },
    {
      "epoch": 14454.545454545454,
      "grad_norm": 0.26902908086776733,
      "learning_rate": 5.348782368720625e-07,
      "loss": 0.0016,
      "step": 79500
    },
    {
      "epoch": 14454.545454545454,
      "eval_loss": 4.981856346130371,
      "eval_runtime": 0.9583,
      "eval_samples_per_second": 10.435,
      "eval_steps_per_second": 5.218,
      "step": 79500
    },
    {
      "epoch": 14456.363636363636,
      "grad_norm": 0.0006651869625784457,
      "learning_rate": 5.34762164080865e-07,
      "loss": 0.0009,
      "step": 79510
    },
    {
      "epoch": 14458.181818181818,
      "grad_norm": 0.14805001020431519,
      "learning_rate": 5.346460894071496e-07,
      "loss": 0.0012,
      "step": 79520
    },
    {
      "epoch": 14460.0,
      "grad_norm": 0.0005933944485150278,
      "learning_rate": 5.34530012857203e-07,
      "loss": 0.001,
      "step": 79530
    },
    {
      "epoch": 14461.818181818182,
      "grad_norm": 0.15506578981876373,
      "learning_rate": 5.344139344373107e-07,
      "loss": 0.001,
      "step": 79540
    },
    {
      "epoch": 14463.636363636364,
      "grad_norm": 0.15144279599189758,
      "learning_rate": 5.342978541537591e-07,
      "loss": 0.0014,
      "step": 79550
    },
    {
      "epoch": 14465.454545454546,
      "grad_norm": 0.19966919720172882,
      "learning_rate": 5.341817720128343e-07,
      "loss": 0.0008,
      "step": 79560
    },
    {
      "epoch": 14467.272727272728,
      "grad_norm": 0.0008906801231205463,
      "learning_rate": 5.34065688020823e-07,
      "loss": 0.001,
      "step": 79570
    },
    {
      "epoch": 14469.09090909091,
      "grad_norm": 0.2034144401550293,
      "learning_rate": 5.339496021840109e-07,
      "loss": 0.0012,
      "step": 79580
    },
    {
      "epoch": 14470.90909090909,
      "grad_norm": 0.0020279581658542156,
      "learning_rate": 5.338335145086854e-07,
      "loss": 0.0012,
      "step": 79590
    },
    {
      "epoch": 14472.727272727272,
      "grad_norm": 0.14830534160137177,
      "learning_rate": 5.337174250011326e-07,
      "loss": 0.0012,
      "step": 79600
    },
    {
      "epoch": 14474.545454545454,
      "grad_norm": 0.15671579539775848,
      "learning_rate": 5.336013336676393e-07,
      "loss": 0.001,
      "step": 79610
    },
    {
      "epoch": 14476.363636363636,
      "grad_norm": 0.20976965129375458,
      "learning_rate": 5.334852405144925e-07,
      "loss": 0.001,
      "step": 79620
    },
    {
      "epoch": 14478.181818181818,
      "grad_norm": 0.0008053833153098822,
      "learning_rate": 5.33369145547979e-07,
      "loss": 0.0009,
      "step": 79630
    },
    {
      "epoch": 14480.0,
      "grad_norm": 0.0008487161830998957,
      "learning_rate": 5.332530487743857e-07,
      "loss": 0.0012,
      "step": 79640
    },
    {
      "epoch": 14481.818181818182,
      "grad_norm": 0.0009013674571178854,
      "learning_rate": 5.331369502000002e-07,
      "loss": 0.0012,
      "step": 79650
    },
    {
      "epoch": 14483.636363636364,
      "grad_norm": 0.0006144929793663323,
      "learning_rate": 5.330208498311092e-07,
      "loss": 0.0012,
      "step": 79660
    },
    {
      "epoch": 14485.454545454546,
      "grad_norm": 0.0009090168168768287,
      "learning_rate": 5.329047476740002e-07,
      "loss": 0.0009,
      "step": 79670
    },
    {
      "epoch": 14487.272727272728,
      "grad_norm": 0.010280506685376167,
      "learning_rate": 5.327886437349608e-07,
      "loss": 0.0014,
      "step": 79680
    },
    {
      "epoch": 14489.09090909091,
      "grad_norm": 0.0009316412033513188,
      "learning_rate": 5.326725380202782e-07,
      "loss": 0.0007,
      "step": 79690
    },
    {
      "epoch": 14490.90909090909,
      "grad_norm": 0.2131025642156601,
      "learning_rate": 5.325564305362404e-07,
      "loss": 0.0012,
      "step": 79700
    },
    {
      "epoch": 14492.727272727272,
      "grad_norm": 0.19637812674045563,
      "learning_rate": 5.324403212891347e-07,
      "loss": 0.0011,
      "step": 79710
    },
    {
      "epoch": 14494.545454545454,
      "grad_norm": 0.0015874464297667146,
      "learning_rate": 5.323242102852493e-07,
      "loss": 0.0009,
      "step": 79720
    },
    {
      "epoch": 14496.363636363636,
      "grad_norm": 0.15878643095493317,
      "learning_rate": 5.322080975308717e-07,
      "loss": 0.0013,
      "step": 79730
    },
    {
      "epoch": 14498.181818181818,
      "grad_norm": 0.017589479684829712,
      "learning_rate": 5.320919830322902e-07,
      "loss": 0.0012,
      "step": 79740
    },
    {
      "epoch": 14500.0,
      "grad_norm": 0.2834039628505707,
      "learning_rate": 5.319758667957927e-07,
      "loss": 0.0011,
      "step": 79750
    },
    {
      "epoch": 14501.818181818182,
      "grad_norm": 0.01052453275769949,
      "learning_rate": 5.318597488276677e-07,
      "loss": 0.0012,
      "step": 79760
    },
    {
      "epoch": 14503.636363636364,
      "grad_norm": 0.2113533467054367,
      "learning_rate": 5.317436291342031e-07,
      "loss": 0.0009,
      "step": 79770
    },
    {
      "epoch": 14505.454545454546,
      "grad_norm": 0.15835615992546082,
      "learning_rate": 5.316275077216874e-07,
      "loss": 0.0013,
      "step": 79780
    },
    {
      "epoch": 14507.272727272728,
      "grad_norm": 0.17454306781291962,
      "learning_rate": 5.315113845964089e-07,
      "loss": 0.0009,
      "step": 79790
    },
    {
      "epoch": 14509.09090909091,
      "grad_norm": 0.16011813282966614,
      "learning_rate": 5.313952597646567e-07,
      "loss": 0.0012,
      "step": 79800
    },
    {
      "epoch": 14510.90909090909,
      "grad_norm": 0.1619807481765747,
      "learning_rate": 5.312791332327189e-07,
      "loss": 0.001,
      "step": 79810
    },
    {
      "epoch": 14512.727272727272,
      "grad_norm": 0.0008769836276769638,
      "learning_rate": 5.311630050068844e-07,
      "loss": 0.0011,
      "step": 79820
    },
    {
      "epoch": 14514.545454545454,
      "grad_norm": 0.001288781757466495,
      "learning_rate": 5.31046875093442e-07,
      "loss": 0.001,
      "step": 79830
    },
    {
      "epoch": 14516.363636363636,
      "grad_norm": 0.006365660112351179,
      "learning_rate": 5.309307434986809e-07,
      "loss": 0.0012,
      "step": 79840
    },
    {
      "epoch": 14518.181818181818,
      "grad_norm": 0.16478992998600006,
      "learning_rate": 5.308146102288897e-07,
      "loss": 0.0011,
      "step": 79850
    },
    {
      "epoch": 14520.0,
      "grad_norm": 0.23053449392318726,
      "learning_rate": 5.306984752903577e-07,
      "loss": 0.001,
      "step": 79860
    },
    {
      "epoch": 14521.818181818182,
      "grad_norm": 0.001596010522916913,
      "learning_rate": 5.305823386893742e-07,
      "loss": 0.0009,
      "step": 79870
    },
    {
      "epoch": 14523.636363636364,
      "grad_norm": 0.16513358056545258,
      "learning_rate": 5.304662004322284e-07,
      "loss": 0.001,
      "step": 79880
    },
    {
      "epoch": 14525.454545454546,
      "grad_norm": 0.0007683287840336561,
      "learning_rate": 5.303500605252095e-07,
      "loss": 0.0013,
      "step": 79890
    },
    {
      "epoch": 14527.272727272728,
      "grad_norm": 0.2378978431224823,
      "learning_rate": 5.302339189746071e-07,
      "loss": 0.0015,
      "step": 79900
    },
    {
      "epoch": 14529.09090909091,
      "grad_norm": 0.0010240046540275216,
      "learning_rate": 5.301177757867109e-07,
      "loss": 0.0007,
      "step": 79910
    },
    {
      "epoch": 14530.90909090909,
      "grad_norm": 0.2842627465724945,
      "learning_rate": 5.300016309678104e-07,
      "loss": 0.001,
      "step": 79920
    },
    {
      "epoch": 14532.727272727272,
      "grad_norm": 0.001044184435158968,
      "learning_rate": 5.298854845241952e-07,
      "loss": 0.0009,
      "step": 79930
    },
    {
      "epoch": 14534.545454545454,
      "grad_norm": 0.2134435772895813,
      "learning_rate": 5.297693364621554e-07,
      "loss": 0.0014,
      "step": 79940
    },
    {
      "epoch": 14536.363636363636,
      "grad_norm": 0.00099223293364048,
      "learning_rate": 5.296531867879809e-07,
      "loss": 0.001,
      "step": 79950
    },
    {
      "epoch": 14538.181818181818,
      "grad_norm": 0.0007869174587540329,
      "learning_rate": 5.295370355079614e-07,
      "loss": 0.001,
      "step": 79960
    },
    {
      "epoch": 14540.0,
      "grad_norm": 0.0007224135915748775,
      "learning_rate": 5.294208826283871e-07,
      "loss": 0.0012,
      "step": 79970
    },
    {
      "epoch": 14541.818181818182,
      "grad_norm": 0.15928055346012115,
      "learning_rate": 5.293047281555481e-07,
      "loss": 0.001,
      "step": 79980
    },
    {
      "epoch": 14543.636363636364,
      "grad_norm": 0.21421656012535095,
      "learning_rate": 5.29188572095735e-07,
      "loss": 0.0012,
      "step": 79990
    },
    {
      "epoch": 14545.454545454546,
      "grad_norm": 0.17496606707572937,
      "learning_rate": 5.290724144552379e-07,
      "loss": 0.0009,
      "step": 80000
    },
    {
      "epoch": 14545.454545454546,
      "eval_loss": 4.979773998260498,
      "eval_runtime": 0.9551,
      "eval_samples_per_second": 10.47,
      "eval_steps_per_second": 5.235,
      "step": 80000
    },
    {
      "epoch": 14547.272727272728,
      "grad_norm": 0.0007798640290275216,
      "learning_rate": 5.289562552403471e-07,
      "loss": 0.0011,
      "step": 80010
    },
    {
      "epoch": 14549.09090909091,
      "grad_norm": 0.0028042211197316647,
      "learning_rate": 5.288400944573533e-07,
      "loss": 0.0012,
      "step": 80020
    },
    {
      "epoch": 14550.90909090909,
      "grad_norm": 0.0011027883738279343,
      "learning_rate": 5.287239321125472e-07,
      "loss": 0.0012,
      "step": 80030
    },
    {
      "epoch": 14552.727272727272,
      "grad_norm": 0.001278018462471664,
      "learning_rate": 5.28607768212219e-07,
      "loss": 0.001,
      "step": 80040
    },
    {
      "epoch": 14554.545454545454,
      "grad_norm": 0.0014697284204885364,
      "learning_rate": 5.284916027626599e-07,
      "loss": 0.001,
      "step": 80050
    },
    {
      "epoch": 14556.363636363636,
      "grad_norm": 0.03594696894288063,
      "learning_rate": 5.283754357701608e-07,
      "loss": 0.0014,
      "step": 80060
    },
    {
      "epoch": 14558.181818181818,
      "grad_norm": 0.0010332128731533885,
      "learning_rate": 5.282592672410124e-07,
      "loss": 0.0009,
      "step": 80070
    },
    {
      "epoch": 14560.0,
      "grad_norm": 0.006506405770778656,
      "learning_rate": 5.281430971815055e-07,
      "loss": 0.0012,
      "step": 80080
    },
    {
      "epoch": 14561.818181818182,
      "grad_norm": 0.28553667664527893,
      "learning_rate": 5.280269255979317e-07,
      "loss": 0.0012,
      "step": 80090
    },
    {
      "epoch": 14563.636363636364,
      "grad_norm": 0.1803666055202484,
      "learning_rate": 5.27910752496582e-07,
      "loss": 0.001,
      "step": 80100
    },
    {
      "epoch": 14565.454545454546,
      "grad_norm": 0.21682202816009521,
      "learning_rate": 5.277945778837475e-07,
      "loss": 0.0009,
      "step": 80110
    },
    {
      "epoch": 14567.272727272728,
      "grad_norm": 0.1754959374666214,
      "learning_rate": 5.276784017657195e-07,
      "loss": 0.0012,
      "step": 80120
    },
    {
      "epoch": 14569.09090909091,
      "grad_norm": 0.16944855451583862,
      "learning_rate": 5.275622241487898e-07,
      "loss": 0.0013,
      "step": 80130
    },
    {
      "epoch": 14570.90909090909,
      "grad_norm": 0.0006825111340731382,
      "learning_rate": 5.274460450392497e-07,
      "loss": 0.0008,
      "step": 80140
    },
    {
      "epoch": 14572.727272727272,
      "grad_norm": 0.1965102255344391,
      "learning_rate": 5.273298644433907e-07,
      "loss": 0.001,
      "step": 80150
    },
    {
      "epoch": 14574.545454545454,
      "grad_norm": 0.160354882478714,
      "learning_rate": 5.272136823675045e-07,
      "loss": 0.0015,
      "step": 80160
    },
    {
      "epoch": 14576.363636363636,
      "grad_norm": 0.1608167588710785,
      "learning_rate": 5.270974988178829e-07,
      "loss": 0.0009,
      "step": 80170
    },
    {
      "epoch": 14578.181818181818,
      "grad_norm": 0.21062645316123962,
      "learning_rate": 5.269813138008178e-07,
      "loss": 0.0011,
      "step": 80180
    },
    {
      "epoch": 14580.0,
      "grad_norm": 0.16183871030807495,
      "learning_rate": 5.26865127322601e-07,
      "loss": 0.0012,
      "step": 80190
    },
    {
      "epoch": 14581.818181818182,
      "grad_norm": 0.2565593123435974,
      "learning_rate": 5.267489393895246e-07,
      "loss": 0.001,
      "step": 80200
    },
    {
      "epoch": 14583.636363636364,
      "grad_norm": 0.18931931257247925,
      "learning_rate": 5.266327500078806e-07,
      "loss": 0.0011,
      "step": 80210
    },
    {
      "epoch": 14585.454545454546,
      "grad_norm": 0.01186345610767603,
      "learning_rate": 5.265165591839608e-07,
      "loss": 0.0013,
      "step": 80220
    },
    {
      "epoch": 14587.272727272728,
      "grad_norm": 0.0004730129148811102,
      "learning_rate": 5.264003669240581e-07,
      "loss": 0.0007,
      "step": 80230
    },
    {
      "epoch": 14589.09090909091,
      "grad_norm": 0.15941952168941498,
      "learning_rate": 5.262841732344642e-07,
      "loss": 0.0014,
      "step": 80240
    },
    {
      "epoch": 14590.90909090909,
      "grad_norm": 0.0007601632387377322,
      "learning_rate": 5.26167978121472e-07,
      "loss": 0.001,
      "step": 80250
    },
    {
      "epoch": 14592.727272727272,
      "grad_norm": 0.15277142822742462,
      "learning_rate": 5.260517815913734e-07,
      "loss": 0.0012,
      "step": 80260
    },
    {
      "epoch": 14594.545454545454,
      "grad_norm": 0.2113371193408966,
      "learning_rate": 5.259355836504614e-07,
      "loss": 0.0008,
      "step": 80270
    },
    {
      "epoch": 14596.363636363636,
      "grad_norm": 0.0057952976785600185,
      "learning_rate": 5.258193843050282e-07,
      "loss": 0.0013,
      "step": 80280
    },
    {
      "epoch": 14598.181818181818,
      "grad_norm": 0.001060053938999772,
      "learning_rate": 5.25703183561367e-07,
      "loss": 0.0009,
      "step": 80290
    },
    {
      "epoch": 14600.0,
      "grad_norm": 0.20863448083400726,
      "learning_rate": 5.255869814257701e-07,
      "loss": 0.0012,
      "step": 80300
    },
    {
      "epoch": 14601.818181818182,
      "grad_norm": 0.19484607875347137,
      "learning_rate": 5.254707779045305e-07,
      "loss": 0.0012,
      "step": 80310
    },
    {
      "epoch": 14603.636363636364,
      "grad_norm": 0.1590549647808075,
      "learning_rate": 5.25354573003941e-07,
      "loss": 0.0009,
      "step": 80320
    },
    {
      "epoch": 14605.454545454546,
      "grad_norm": 0.1632089763879776,
      "learning_rate": 5.252383667302948e-07,
      "loss": 0.0012,
      "step": 80330
    },
    {
      "epoch": 14607.272727272728,
      "grad_norm": 0.0022177109494805336,
      "learning_rate": 5.251221590898848e-07,
      "loss": 0.0012,
      "step": 80340
    },
    {
      "epoch": 14609.09090909091,
      "grad_norm": 0.0006611160933971405,
      "learning_rate": 5.250059500890041e-07,
      "loss": 0.0009,
      "step": 80350
    },
    {
      "epoch": 14610.90909090909,
      "grad_norm": 0.15901780128479004,
      "learning_rate": 5.248897397339461e-07,
      "loss": 0.0012,
      "step": 80360
    },
    {
      "epoch": 14612.727272727272,
      "grad_norm": 0.0011184896575286984,
      "learning_rate": 5.247735280310041e-07,
      "loss": 0.001,
      "step": 80370
    },
    {
      "epoch": 14614.545454545454,
      "grad_norm": 0.2628375291824341,
      "learning_rate": 5.246573149864711e-07,
      "loss": 0.0011,
      "step": 80380
    },
    {
      "epoch": 14616.363636363636,
      "grad_norm": 0.2023797631263733,
      "learning_rate": 5.245411006066407e-07,
      "loss": 0.001,
      "step": 80390
    },
    {
      "epoch": 14618.181818181818,
      "grad_norm": 0.17570404708385468,
      "learning_rate": 5.244248848978067e-07,
      "loss": 0.0012,
      "step": 80400
    },
    {
      "epoch": 14620.0,
      "grad_norm": 0.19427461922168732,
      "learning_rate": 5.243086678662622e-07,
      "loss": 0.0011,
      "step": 80410
    },
    {
      "epoch": 14621.818181818182,
      "grad_norm": 0.21312233805656433,
      "learning_rate": 5.241924495183011e-07,
      "loss": 0.0012,
      "step": 80420
    },
    {
      "epoch": 14623.636363636364,
      "grad_norm": 0.21284766495227814,
      "learning_rate": 5.240762298602171e-07,
      "loss": 0.001,
      "step": 80430
    },
    {
      "epoch": 14625.454545454546,
      "grad_norm": 0.011172443628311157,
      "learning_rate": 5.239600088983039e-07,
      "loss": 0.0012,
      "step": 80440
    },
    {
      "epoch": 14627.272727272728,
      "grad_norm": 0.16875027120113373,
      "learning_rate": 5.238437866388554e-07,
      "loss": 0.0009,
      "step": 80450
    },
    {
      "epoch": 14629.09090909091,
      "grad_norm": 0.0009926529601216316,
      "learning_rate": 5.237275630881656e-07,
      "loss": 0.001,
      "step": 80460
    },
    {
      "epoch": 14630.90909090909,
      "grad_norm": 0.0009987287921831012,
      "learning_rate": 5.236113382525283e-07,
      "loss": 0.0012,
      "step": 80470
    },
    {
      "epoch": 14632.727272727272,
      "grad_norm": 0.0013209183234721422,
      "learning_rate": 5.234951121382378e-07,
      "loss": 0.001,
      "step": 80480
    },
    {
      "epoch": 14634.545454545454,
      "grad_norm": 0.1648194044828415,
      "learning_rate": 5.233788847515881e-07,
      "loss": 0.0012,
      "step": 80490
    },
    {
      "epoch": 14636.363636363636,
      "grad_norm": 0.0014490471221506596,
      "learning_rate": 5.232626560988734e-07,
      "loss": 0.001,
      "step": 80500
    },
    {
      "epoch": 14636.363636363636,
      "eval_loss": 4.9819111824035645,
      "eval_runtime": 0.9477,
      "eval_samples_per_second": 10.552,
      "eval_steps_per_second": 5.276,
      "step": 80500
    },
    {
      "epoch": 14638.181818181818,
      "grad_norm": 0.21628043055534363,
      "learning_rate": 5.231464261863878e-07,
      "loss": 0.0011,
      "step": 80510
    },
    {
      "epoch": 14640.0,
      "grad_norm": 0.0010093152523040771,
      "learning_rate": 5.230301950204262e-07,
      "loss": 0.001,
      "step": 80520
    },
    {
      "epoch": 14641.818181818182,
      "grad_norm": 0.15802188217639923,
      "learning_rate": 5.229139626072824e-07,
      "loss": 0.001,
      "step": 80530
    },
    {
      "epoch": 14643.636363636364,
      "grad_norm": 0.0014074635691940784,
      "learning_rate": 5.227977289532511e-07,
      "loss": 0.0009,
      "step": 80540
    },
    {
      "epoch": 14645.454545454546,
      "grad_norm": 0.005896682385355234,
      "learning_rate": 5.226814940646268e-07,
      "loss": 0.0015,
      "step": 80550
    },
    {
      "epoch": 14647.272727272728,
      "grad_norm": 0.20459121465682983,
      "learning_rate": 5.225652579477043e-07,
      "loss": 0.0011,
      "step": 80560
    },
    {
      "epoch": 14649.09090909091,
      "grad_norm": 0.17034310102462769,
      "learning_rate": 5.224490206087782e-07,
      "loss": 0.001,
      "step": 80570
    },
    {
      "epoch": 14650.90909090909,
      "grad_norm": 0.001466061221435666,
      "learning_rate": 5.223327820541431e-07,
      "loss": 0.001,
      "step": 80580
    },
    {
      "epoch": 14652.727272727272,
      "grad_norm": 0.22310586273670197,
      "learning_rate": 5.222165422900938e-07,
      "loss": 0.0012,
      "step": 80590
    },
    {
      "epoch": 14654.545454545454,
      "grad_norm": 0.0005358414491638541,
      "learning_rate": 5.221003013229253e-07,
      "loss": 0.0007,
      "step": 80600
    },
    {
      "epoch": 14656.363636363636,
      "grad_norm": 0.001380380243062973,
      "learning_rate": 5.219840591589324e-07,
      "loss": 0.0012,
      "step": 80610
    },
    {
      "epoch": 14658.181818181818,
      "grad_norm": 0.0014957443345338106,
      "learning_rate": 5.218678158044104e-07,
      "loss": 0.001,
      "step": 80620
    },
    {
      "epoch": 14660.0,
      "grad_norm": 0.0012018128763884306,
      "learning_rate": 5.217515712656541e-07,
      "loss": 0.0012,
      "step": 80630
    },
    {
      "epoch": 14661.818181818182,
      "grad_norm": 0.20992311835289001,
      "learning_rate": 5.216353255489585e-07,
      "loss": 0.001,
      "step": 80640
    },
    {
      "epoch": 14663.636363636364,
      "grad_norm": 0.0007907523540779948,
      "learning_rate": 5.215190786606192e-07,
      "loss": 0.001,
      "step": 80650
    },
    {
      "epoch": 14665.454545454546,
      "grad_norm": 0.000754653534386307,
      "learning_rate": 5.21402830606931e-07,
      "loss": 0.001,
      "step": 80660
    },
    {
      "epoch": 14667.272727272728,
      "grad_norm": 0.002241997979581356,
      "learning_rate": 5.212865813941898e-07,
      "loss": 0.001,
      "step": 80670
    },
    {
      "epoch": 14669.09090909091,
      "grad_norm": 0.37767094373703003,
      "learning_rate": 5.211703310286904e-07,
      "loss": 0.0015,
      "step": 80680
    },
    {
      "epoch": 14670.90909090909,
      "grad_norm": 0.0010912495199590921,
      "learning_rate": 5.210540795167287e-07,
      "loss": 0.001,
      "step": 80690
    },
    {
      "epoch": 14672.727272727272,
      "grad_norm": 0.21209558844566345,
      "learning_rate": 5.209378268645997e-07,
      "loss": 0.0009,
      "step": 80700
    },
    {
      "epoch": 14674.545454545454,
      "grad_norm": 0.0013610805617645383,
      "learning_rate": 5.208215730785995e-07,
      "loss": 0.0015,
      "step": 80710
    },
    {
      "epoch": 14676.363636363636,
      "grad_norm": 0.01723134145140648,
      "learning_rate": 5.207053181650235e-07,
      "loss": 0.0012,
      "step": 80720
    },
    {
      "epoch": 14678.181818181818,
      "grad_norm": 0.0008916502702049911,
      "learning_rate": 5.205890621301675e-07,
      "loss": 0.0008,
      "step": 80730
    },
    {
      "epoch": 14680.0,
      "grad_norm": 0.1982652246952057,
      "learning_rate": 5.204728049803269e-07,
      "loss": 0.0012,
      "step": 80740
    },
    {
      "epoch": 14681.818181818182,
      "grad_norm": 0.1535634696483612,
      "learning_rate": 5.20356546721798e-07,
      "loss": 0.001,
      "step": 80750
    },
    {
      "epoch": 14683.636363636364,
      "grad_norm": 0.20057769119739532,
      "learning_rate": 5.202402873608762e-07,
      "loss": 0.0012,
      "step": 80760
    },
    {
      "epoch": 14685.454545454546,
      "grad_norm": 0.2239372879266739,
      "learning_rate": 5.20124026903858e-07,
      "loss": 0.0009,
      "step": 80770
    },
    {
      "epoch": 14687.272727272728,
      "grad_norm": 0.15726682543754578,
      "learning_rate": 5.200077653570387e-07,
      "loss": 0.0012,
      "step": 80780
    },
    {
      "epoch": 14689.09090909091,
      "grad_norm": 0.0009693059255369008,
      "learning_rate": 5.198915027267149e-07,
      "loss": 0.001,
      "step": 80790
    },
    {
      "epoch": 14690.90909090909,
      "grad_norm": 0.19411490857601166,
      "learning_rate": 5.197752390191826e-07,
      "loss": 0.0012,
      "step": 80800
    },
    {
      "epoch": 14692.727272727272,
      "grad_norm": 0.0010366096394136548,
      "learning_rate": 5.196589742407378e-07,
      "loss": 0.001,
      "step": 80810
    },
    {
      "epoch": 14694.545454545454,
      "grad_norm": 0.0007208961760625243,
      "learning_rate": 5.195427083976768e-07,
      "loss": 0.0013,
      "step": 80820
    },
    {
      "epoch": 14696.363636363636,
      "grad_norm": 0.017467226833105087,
      "learning_rate": 5.194264414962959e-07,
      "loss": 0.001,
      "step": 80830
    },
    {
      "epoch": 14698.181818181818,
      "grad_norm": 0.1574564427137375,
      "learning_rate": 5.193101735428915e-07,
      "loss": 0.0009,
      "step": 80840
    },
    {
      "epoch": 14700.0,
      "grad_norm": 0.0013041510246694088,
      "learning_rate": 5.1919390454376e-07,
      "loss": 0.001,
      "step": 80850
    },
    {
      "epoch": 14701.818181818182,
      "grad_norm": 0.0008431584574282169,
      "learning_rate": 5.190776345051977e-07,
      "loss": 0.0012,
      "step": 80860
    },
    {
      "epoch": 14703.636363636364,
      "grad_norm": 0.27339643239974976,
      "learning_rate": 5.189613634335012e-07,
      "loss": 0.0012,
      "step": 80870
    },
    {
      "epoch": 14705.454545454546,
      "grad_norm": 0.2536253333091736,
      "learning_rate": 5.188450913349673e-07,
      "loss": 0.001,
      "step": 80880
    },
    {
      "epoch": 14707.272727272728,
      "grad_norm": 0.0007894659065641463,
      "learning_rate": 5.187288182158923e-07,
      "loss": 0.001,
      "step": 80890
    },
    {
      "epoch": 14709.09090909091,
      "grad_norm": 0.0004948286223225296,
      "learning_rate": 5.18612544082573e-07,
      "loss": 0.0011,
      "step": 80900
    },
    {
      "epoch": 14710.90909090909,
      "grad_norm": 0.21045373380184174,
      "learning_rate": 5.184962689413059e-07,
      "loss": 0.0011,
      "step": 80910
    },
    {
      "epoch": 14712.727272727272,
      "grad_norm": 0.15296989679336548,
      "learning_rate": 5.183799927983883e-07,
      "loss": 0.0012,
      "step": 80920
    },
    {
      "epoch": 14714.545454545454,
      "grad_norm": 0.19835367798805237,
      "learning_rate": 5.182637156601167e-07,
      "loss": 0.0012,
      "step": 80930
    },
    {
      "epoch": 14716.363636363636,
      "grad_norm": 0.0010928799165412784,
      "learning_rate": 5.181474375327878e-07,
      "loss": 0.0012,
      "step": 80940
    },
    {
      "epoch": 14718.181818181818,
      "grad_norm": 0.2658534348011017,
      "learning_rate": 5.180311584226991e-07,
      "loss": 0.0011,
      "step": 80950
    },
    {
      "epoch": 14720.0,
      "grad_norm": 0.010929860174655914,
      "learning_rate": 5.179148783361473e-07,
      "loss": 0.0009,
      "step": 80960
    },
    {
      "epoch": 14721.818181818182,
      "grad_norm": 0.1601894348859787,
      "learning_rate": 5.177985972794292e-07,
      "loss": 0.001,
      "step": 80970
    },
    {
      "epoch": 14723.636363636364,
      "grad_norm": 0.0008591159130446613,
      "learning_rate": 5.176823152588423e-07,
      "loss": 0.001,
      "step": 80980
    },
    {
      "epoch": 14725.454545454546,
      "grad_norm": 0.26794907450675964,
      "learning_rate": 5.175660322806837e-07,
      "loss": 0.0015,
      "step": 80990
    },
    {
      "epoch": 14727.272727272728,
      "grad_norm": 0.2111983746290207,
      "learning_rate": 5.174497483512505e-07,
      "loss": 0.0009,
      "step": 81000
    },
    {
      "epoch": 14727.272727272728,
      "eval_loss": 4.931218147277832,
      "eval_runtime": 0.9568,
      "eval_samples_per_second": 10.452,
      "eval_steps_per_second": 5.226,
      "step": 81000
    },
    {
      "epoch": 14729.09090909091,
      "grad_norm": 0.16377992928028107,
      "learning_rate": 5.173334634768399e-07,
      "loss": 0.001,
      "step": 81010
    },
    {
      "epoch": 14730.90909090909,
      "grad_norm": 0.1668989062309265,
      "learning_rate": 5.172171776637494e-07,
      "loss": 0.0012,
      "step": 81020
    },
    {
      "epoch": 14732.727272727272,
      "grad_norm": 0.0007904917001724243,
      "learning_rate": 5.171008909182765e-07,
      "loss": 0.001,
      "step": 81030
    },
    {
      "epoch": 14734.545454545454,
      "grad_norm": 0.0006501565803773701,
      "learning_rate": 5.169846032467181e-07,
      "loss": 0.0009,
      "step": 81040
    },
    {
      "epoch": 14736.363636363636,
      "grad_norm": 0.2127690464258194,
      "learning_rate": 5.16868314655372e-07,
      "loss": 0.0014,
      "step": 81050
    },
    {
      "epoch": 14738.181818181818,
      "grad_norm": 0.0008135826792567968,
      "learning_rate": 5.167520251505358e-07,
      "loss": 0.0009,
      "step": 81060
    },
    {
      "epoch": 14740.0,
      "grad_norm": 0.0007209563627839088,
      "learning_rate": 5.166357347385068e-07,
      "loss": 0.0012,
      "step": 81070
    },
    {
      "epoch": 14741.818181818182,
      "grad_norm": 0.002014127327129245,
      "learning_rate": 5.16519443425583e-07,
      "loss": 0.0009,
      "step": 81080
    },
    {
      "epoch": 14743.636363636364,
      "grad_norm": 0.0009080609888769686,
      "learning_rate": 5.164031512180616e-07,
      "loss": 0.0011,
      "step": 81090
    },
    {
      "epoch": 14745.454545454546,
      "grad_norm": 0.19367554783821106,
      "learning_rate": 5.162868581222406e-07,
      "loss": 0.0014,
      "step": 81100
    },
    {
      "epoch": 14747.272727272728,
      "grad_norm": 0.2683189809322357,
      "learning_rate": 5.161705641444177e-07,
      "loss": 0.0014,
      "step": 81110
    },
    {
      "epoch": 14749.09090909091,
      "grad_norm": 0.010637247003614902,
      "learning_rate": 5.160542692908907e-07,
      "loss": 0.0009,
      "step": 81120
    },
    {
      "epoch": 14750.90909090909,
      "grad_norm": 0.0009424651507288218,
      "learning_rate": 5.159379735679575e-07,
      "loss": 0.001,
      "step": 81130
    },
    {
      "epoch": 14752.727272727272,
      "grad_norm": 0.2110028862953186,
      "learning_rate": 5.15821676981916e-07,
      "loss": 0.001,
      "step": 81140
    },
    {
      "epoch": 14754.545454545454,
      "grad_norm": 0.0007933159940876067,
      "learning_rate": 5.157053795390641e-07,
      "loss": 0.001,
      "step": 81150
    },
    {
      "epoch": 14756.363636363636,
      "grad_norm": 0.0005847193533554673,
      "learning_rate": 5.155890812456999e-07,
      "loss": 0.0012,
      "step": 81160
    },
    {
      "epoch": 14758.181818181818,
      "grad_norm": 0.0007963637472130358,
      "learning_rate": 5.154727821081211e-07,
      "loss": 0.0012,
      "step": 81170
    },
    {
      "epoch": 14760.0,
      "grad_norm": 0.2672925889492035,
      "learning_rate": 5.153564821326264e-07,
      "loss": 0.0012,
      "step": 81180
    },
    {
      "epoch": 14761.818181818182,
      "grad_norm": 0.24992279708385468,
      "learning_rate": 5.152401813255133e-07,
      "loss": 0.0012,
      "step": 81190
    },
    {
      "epoch": 14763.636363636364,
      "grad_norm": 0.22247377038002014,
      "learning_rate": 5.151238796930803e-07,
      "loss": 0.001,
      "step": 81200
    },
    {
      "epoch": 14765.454545454546,
      "grad_norm": 0.0008365794783458114,
      "learning_rate": 5.150075772416256e-07,
      "loss": 0.0009,
      "step": 81210
    },
    {
      "epoch": 14767.272727272728,
      "grad_norm": 0.0008860243833623827,
      "learning_rate": 5.148912739774476e-07,
      "loss": 0.001,
      "step": 81220
    },
    {
      "epoch": 14769.09090909091,
      "grad_norm": 0.210751473903656,
      "learning_rate": 5.147749699068442e-07,
      "loss": 0.0012,
      "step": 81230
    },
    {
      "epoch": 14770.90909090909,
      "grad_norm": 0.0019396576099097729,
      "learning_rate": 5.146586650361142e-07,
      "loss": 0.0012,
      "step": 81240
    },
    {
      "epoch": 14772.727272727272,
      "grad_norm": 0.0015942432219162583,
      "learning_rate": 5.145423593715557e-07,
      "loss": 0.0008,
      "step": 81250
    },
    {
      "epoch": 14774.545454545454,
      "grad_norm": 0.0008595380350016057,
      "learning_rate": 5.144260529194672e-07,
      "loss": 0.0012,
      "step": 81260
    },
    {
      "epoch": 14776.363636363636,
      "grad_norm": 0.0015483340248465538,
      "learning_rate": 5.143097456861474e-07,
      "loss": 0.0012,
      "step": 81270
    },
    {
      "epoch": 14778.181818181818,
      "grad_norm": 0.001097285421565175,
      "learning_rate": 5.141934376778946e-07,
      "loss": 0.0011,
      "step": 81280
    },
    {
      "epoch": 14780.0,
      "grad_norm": 0.0008415259653702378,
      "learning_rate": 5.140771289010073e-07,
      "loss": 0.0012,
      "step": 81290
    },
    {
      "epoch": 14781.818181818182,
      "grad_norm": 0.001353247556835413,
      "learning_rate": 5.139608193617844e-07,
      "loss": 0.001,
      "step": 81300
    },
    {
      "epoch": 14783.636363636364,
      "grad_norm": 0.19964341819286346,
      "learning_rate": 5.138445090665243e-07,
      "loss": 0.0009,
      "step": 81310
    },
    {
      "epoch": 14785.454545454546,
      "grad_norm": 0.0009451322839595377,
      "learning_rate": 5.137281980215259e-07,
      "loss": 0.0012,
      "step": 81320
    },
    {
      "epoch": 14787.272727272728,
      "grad_norm": 0.0006039052386768162,
      "learning_rate": 5.136118862330875e-07,
      "loss": 0.001,
      "step": 81330
    },
    {
      "epoch": 14789.09090909091,
      "grad_norm": 0.20818135142326355,
      "learning_rate": 5.134955737075086e-07,
      "loss": 0.0012,
      "step": 81340
    },
    {
      "epoch": 14790.90909090909,
      "grad_norm": 0.0018597209127619863,
      "learning_rate": 5.133792604510874e-07,
      "loss": 0.0012,
      "step": 81350
    },
    {
      "epoch": 14792.727272727272,
      "grad_norm": 0.18968001008033752,
      "learning_rate": 5.13262946470123e-07,
      "loss": 0.0009,
      "step": 81360
    },
    {
      "epoch": 14794.545454545454,
      "grad_norm": 0.0034641327802091837,
      "learning_rate": 5.131466317709142e-07,
      "loss": 0.0011,
      "step": 81370
    },
    {
      "epoch": 14796.363636363636,
      "grad_norm": 0.19297727942466736,
      "learning_rate": 5.130303163597601e-07,
      "loss": 0.0012,
      "step": 81380
    },
    {
      "epoch": 14798.181818181818,
      "grad_norm": 0.017359260469675064,
      "learning_rate": 5.129140002429594e-07,
      "loss": 0.0014,
      "step": 81390
    },
    {
      "epoch": 14800.0,
      "grad_norm": 0.2556725740432739,
      "learning_rate": 5.127976834268111e-07,
      "loss": 0.001,
      "step": 81400
    },
    {
      "epoch": 14801.818181818182,
      "grad_norm": 0.14668498933315277,
      "learning_rate": 5.126813659176147e-07,
      "loss": 0.0012,
      "step": 81410
    },
    {
      "epoch": 14803.636363636364,
      "grad_norm": 0.21919997036457062,
      "learning_rate": 5.125650477216687e-07,
      "loss": 0.001,
      "step": 81420
    },
    {
      "epoch": 14805.454545454546,
      "grad_norm": 0.0007026871899142861,
      "learning_rate": 5.124487288452728e-07,
      "loss": 0.0012,
      "step": 81430
    },
    {
      "epoch": 14807.272727272728,
      "grad_norm": 0.04102422297000885,
      "learning_rate": 5.123324092947255e-07,
      "loss": 0.001,
      "step": 81440
    },
    {
      "epoch": 14809.09090909091,
      "grad_norm": 0.001167413080111146,
      "learning_rate": 5.122160890763266e-07,
      "loss": 0.0009,
      "step": 81450
    },
    {
      "epoch": 14810.90909090909,
      "grad_norm": 0.17984120547771454,
      "learning_rate": 5.12099768196375e-07,
      "loss": 0.0013,
      "step": 81460
    },
    {
      "epoch": 14812.727272727272,
      "grad_norm": 0.1553208976984024,
      "learning_rate": 5.119834466611701e-07,
      "loss": 0.0014,
      "step": 81470
    },
    {
      "epoch": 14814.545454545454,
      "grad_norm": 0.0007693291991017759,
      "learning_rate": 5.118671244770109e-07,
      "loss": 0.0008,
      "step": 81480
    },
    {
      "epoch": 14816.363636363636,
      "grad_norm": 0.0010757875861600041,
      "learning_rate": 5.117508016501973e-07,
      "loss": 0.0011,
      "step": 81490
    },
    {
      "epoch": 14818.181818181818,
      "grad_norm": 0.13068681955337524,
      "learning_rate": 5.116344781870281e-07,
      "loss": 0.0014,
      "step": 81500
    },
    {
      "epoch": 14818.181818181818,
      "eval_loss": 4.960814476013184,
      "eval_runtime": 0.9478,
      "eval_samples_per_second": 10.551,
      "eval_steps_per_second": 5.276,
      "step": 81500
    },
    {
      "epoch": 14820.0,
      "grad_norm": 0.000790530233643949,
      "learning_rate": 5.115181540938032e-07,
      "loss": 0.001,
      "step": 81510
    },
    {
      "epoch": 14821.818181818182,
      "grad_norm": 0.2487388700246811,
      "learning_rate": 5.114018293768215e-07,
      "loss": 0.0012,
      "step": 81520
    },
    {
      "epoch": 14823.636363636364,
      "grad_norm": 0.0007552449824288487,
      "learning_rate": 5.11285504042383e-07,
      "loss": 0.0007,
      "step": 81530
    },
    {
      "epoch": 14825.454545454546,
      "grad_norm": 0.20970365405082703,
      "learning_rate": 5.111691780967868e-07,
      "loss": 0.0015,
      "step": 81540
    },
    {
      "epoch": 14827.272727272728,
      "grad_norm": 0.21108196675777435,
      "learning_rate": 5.110528515463328e-07,
      "loss": 0.0012,
      "step": 81550
    },
    {
      "epoch": 14829.09090909091,
      "grad_norm": 0.21903805434703827,
      "learning_rate": 5.109365243973203e-07,
      "loss": 0.0009,
      "step": 81560
    },
    {
      "epoch": 14830.90909090909,
      "grad_norm": 0.01650615781545639,
      "learning_rate": 5.108201966560489e-07,
      "loss": 0.001,
      "step": 81570
    },
    {
      "epoch": 14832.727272727272,
      "grad_norm": 0.1630755215883255,
      "learning_rate": 5.107038683288184e-07,
      "loss": 0.0009,
      "step": 81580
    },
    {
      "epoch": 14834.545454545454,
      "grad_norm": 0.16381396353244781,
      "learning_rate": 5.105875394219282e-07,
      "loss": 0.0012,
      "step": 81590
    },
    {
      "epoch": 14836.363636363636,
      "grad_norm": 0.0005435417406260967,
      "learning_rate": 5.104712099416785e-07,
      "loss": 0.0009,
      "step": 81600
    },
    {
      "epoch": 14838.181818181818,
      "grad_norm": 0.15347450971603394,
      "learning_rate": 5.103548798943686e-07,
      "loss": 0.0013,
      "step": 81610
    },
    {
      "epoch": 14840.0,
      "grad_norm": 0.0009968222584575415,
      "learning_rate": 5.102385492862984e-07,
      "loss": 0.0011,
      "step": 81620
    },
    {
      "epoch": 14841.818181818182,
      "grad_norm": 0.0011335843009874225,
      "learning_rate": 5.101222181237675e-07,
      "loss": 0.0011,
      "step": 81630
    },
    {
      "epoch": 14843.636363636364,
      "grad_norm": 0.0048028589226305485,
      "learning_rate": 5.100058864130761e-07,
      "loss": 0.0012,
      "step": 81640
    },
    {
      "epoch": 14845.454545454546,
      "grad_norm": 0.0008549895719625056,
      "learning_rate": 5.098895541605239e-07,
      "loss": 0.0009,
      "step": 81650
    },
    {
      "epoch": 14847.272727272728,
      "grad_norm": 0.0007373880944214761,
      "learning_rate": 5.097732213724107e-07,
      "loss": 0.0012,
      "step": 81660
    },
    {
      "epoch": 14849.09090909091,
      "grad_norm": 0.16282321512699127,
      "learning_rate": 5.096568880550361e-07,
      "loss": 0.0012,
      "step": 81670
    },
    {
      "epoch": 14850.90909090909,
      "grad_norm": 0.18168161809444427,
      "learning_rate": 5.095405542147006e-07,
      "loss": 0.0009,
      "step": 81680
    },
    {
      "epoch": 14852.727272727272,
      "grad_norm": 0.000851504853926599,
      "learning_rate": 5.094242198577041e-07,
      "loss": 0.0015,
      "step": 81690
    },
    {
      "epoch": 14854.545454545454,
      "grad_norm": 0.0067682405933737755,
      "learning_rate": 5.093078849903464e-07,
      "loss": 0.0009,
      "step": 81700
    },
    {
      "epoch": 14856.363636363636,
      "grad_norm": 0.011220458894968033,
      "learning_rate": 5.091915496189274e-07,
      "loss": 0.0014,
      "step": 81710
    },
    {
      "epoch": 14858.181818181818,
      "grad_norm": 0.22121843695640564,
      "learning_rate": 5.090752137497474e-07,
      "loss": 0.0011,
      "step": 81720
    },
    {
      "epoch": 14860.0,
      "grad_norm": 0.0006565890507772565,
      "learning_rate": 5.089588773891063e-07,
      "loss": 0.001,
      "step": 81730
    },
    {
      "epoch": 14861.818181818182,
      "grad_norm": 0.0006627507973462343,
      "learning_rate": 5.088425405433046e-07,
      "loss": 0.0012,
      "step": 81740
    },
    {
      "epoch": 14863.636363636364,
      "grad_norm": 0.1875326782464981,
      "learning_rate": 5.087262032186418e-07,
      "loss": 0.001,
      "step": 81750
    },
    {
      "epoch": 14865.454545454546,
      "grad_norm": 0.19652293622493744,
      "learning_rate": 5.086098654214184e-07,
      "loss": 0.0011,
      "step": 81760
    },
    {
      "epoch": 14867.272727272728,
      "grad_norm": 0.0010005936492234468,
      "learning_rate": 5.084935271579347e-07,
      "loss": 0.0009,
      "step": 81770
    },
    {
      "epoch": 14869.09090909091,
      "grad_norm": 0.0009847275214269757,
      "learning_rate": 5.083771884344908e-07,
      "loss": 0.0012,
      "step": 81780
    },
    {
      "epoch": 14870.90909090909,
      "grad_norm": 0.19742164015769958,
      "learning_rate": 5.082608492573867e-07,
      "loss": 0.0012,
      "step": 81790
    },
    {
      "epoch": 14872.727272727272,
      "grad_norm": 0.017146827653050423,
      "learning_rate": 5.081445096329229e-07,
      "loss": 0.0012,
      "step": 81800
    },
    {
      "epoch": 14874.545454545454,
      "grad_norm": 0.0007462233770638704,
      "learning_rate": 5.080281695673998e-07,
      "loss": 0.0007,
      "step": 81810
    },
    {
      "epoch": 14876.363636363636,
      "grad_norm": 0.19226182997226715,
      "learning_rate": 5.079118290671174e-07,
      "loss": 0.0013,
      "step": 81820
    },
    {
      "epoch": 14878.181818181818,
      "grad_norm": 0.17977535724639893,
      "learning_rate": 5.077954881383761e-07,
      "loss": 0.0011,
      "step": 81830
    },
    {
      "epoch": 14880.0,
      "grad_norm": 0.2742667496204376,
      "learning_rate": 5.076791467874764e-07,
      "loss": 0.001,
      "step": 81840
    },
    {
      "epoch": 14881.818181818182,
      "grad_norm": 0.22102858126163483,
      "learning_rate": 5.075628050207187e-07,
      "loss": 0.0012,
      "step": 81850
    },
    {
      "epoch": 14883.636363636364,
      "grad_norm": 0.002503846539184451,
      "learning_rate": 5.074464628444032e-07,
      "loss": 0.0009,
      "step": 81860
    },
    {
      "epoch": 14885.454545454546,
      "grad_norm": 0.0008748683030717075,
      "learning_rate": 5.073301202648303e-07,
      "loss": 0.001,
      "step": 81870
    },
    {
      "epoch": 14887.272727272728,
      "grad_norm": 0.0008707421366125345,
      "learning_rate": 5.072137772883006e-07,
      "loss": 0.0012,
      "step": 81880
    },
    {
      "epoch": 14889.09090909091,
      "grad_norm": 0.0008420714875683188,
      "learning_rate": 5.070974339211148e-07,
      "loss": 0.0011,
      "step": 81890
    },
    {
      "epoch": 14890.90909090909,
      "grad_norm": 0.0005983221926726401,
      "learning_rate": 5.069810901695727e-07,
      "loss": 0.0012,
      "step": 81900
    },
    {
      "epoch": 14892.727272727272,
      "grad_norm": 0.19835205376148224,
      "learning_rate": 5.068647460399752e-07,
      "loss": 0.0009,
      "step": 81910
    },
    {
      "epoch": 14894.545454545454,
      "grad_norm": 0.2908555567264557,
      "learning_rate": 5.067484015386229e-07,
      "loss": 0.0012,
      "step": 81920
    },
    {
      "epoch": 14896.363636363636,
      "grad_norm": 0.01104077510535717,
      "learning_rate": 5.066320566718164e-07,
      "loss": 0.0012,
      "step": 81930
    },
    {
      "epoch": 14898.181818181818,
      "grad_norm": 0.1817854940891266,
      "learning_rate": 5.06515711445856e-07,
      "loss": 0.001,
      "step": 81940
    },
    {
      "epoch": 14900.0,
      "grad_norm": 0.2085677981376648,
      "learning_rate": 5.063993658670425e-07,
      "loss": 0.0011,
      "step": 81950
    },
    {
      "epoch": 14901.818181818182,
      "grad_norm": 0.20680151879787445,
      "learning_rate": 5.062830199416763e-07,
      "loss": 0.0011,
      "step": 81960
    },
    {
      "epoch": 14903.636363636364,
      "grad_norm": 0.0010607597650960088,
      "learning_rate": 5.061666736760582e-07,
      "loss": 0.0012,
      "step": 81970
    },
    {
      "epoch": 14905.454545454546,
      "grad_norm": 0.17666082084178925,
      "learning_rate": 5.060503270764888e-07,
      "loss": 0.001,
      "step": 81980
    },
    {
      "epoch": 14907.272727272728,
      "grad_norm": 0.04225972667336464,
      "learning_rate": 5.059339801492686e-07,
      "loss": 0.0013,
      "step": 81990
    },
    {
      "epoch": 14909.09090909091,
      "grad_norm": 0.21926777064800262,
      "learning_rate": 5.058176329006985e-07,
      "loss": 0.0009,
      "step": 82000
    },
    {
      "epoch": 14909.09090909091,
      "eval_loss": 5.022387504577637,
      "eval_runtime": 0.9532,
      "eval_samples_per_second": 10.491,
      "eval_steps_per_second": 5.245,
      "step": 82000
    },
    {
      "epoch": 14910.90909090909,
      "grad_norm": 0.0014528225874528289,
      "learning_rate": 5.057012853370791e-07,
      "loss": 0.0012,
      "step": 82010
    },
    {
      "epoch": 14912.727272727272,
      "grad_norm": 0.19871892035007477,
      "learning_rate": 5.05584937464711e-07,
      "loss": 0.001,
      "step": 82020
    },
    {
      "epoch": 14914.545454545454,
      "grad_norm": 0.20609575510025024,
      "learning_rate": 5.054685892898952e-07,
      "loss": 0.0012,
      "step": 82030
    },
    {
      "epoch": 14916.363636363636,
      "grad_norm": 0.20575666427612305,
      "learning_rate": 5.053522408189322e-07,
      "loss": 0.0011,
      "step": 82040
    },
    {
      "epoch": 14918.181818181818,
      "grad_norm": 0.20004355907440186,
      "learning_rate": 5.052358920581229e-07,
      "loss": 0.0012,
      "step": 82050
    },
    {
      "epoch": 14920.0,
      "grad_norm": 0.18194273114204407,
      "learning_rate": 5.051195430137679e-07,
      "loss": 0.0011,
      "step": 82060
    },
    {
      "epoch": 14921.818181818182,
      "grad_norm": 0.0006977735320106149,
      "learning_rate": 5.050031936921682e-07,
      "loss": 0.001,
      "step": 82070
    },
    {
      "epoch": 14923.636363636364,
      "grad_norm": 0.000732187763787806,
      "learning_rate": 5.048868440996246e-07,
      "loss": 0.0012,
      "step": 82080
    },
    {
      "epoch": 14925.454545454546,
      "grad_norm": 0.17462709546089172,
      "learning_rate": 5.047704942424377e-07,
      "loss": 0.001,
      "step": 82090
    },
    {
      "epoch": 14927.272727272728,
      "grad_norm": 0.0005729282274842262,
      "learning_rate": 5.046541441269084e-07,
      "loss": 0.0011,
      "step": 82100
    },
    {
      "epoch": 14929.09090909091,
      "grad_norm": 0.26523447036743164,
      "learning_rate": 5.045377937593376e-07,
      "loss": 0.0012,
      "step": 82110
    },
    {
      "epoch": 14930.90909090909,
      "grad_norm": 0.20812955498695374,
      "learning_rate": 5.044214431460264e-07,
      "loss": 0.0012,
      "step": 82120
    },
    {
      "epoch": 14932.727272727272,
      "grad_norm": 0.1736176759004593,
      "learning_rate": 5.043050922932752e-07,
      "loss": 0.001,
      "step": 82130
    },
    {
      "epoch": 14934.545454545454,
      "grad_norm": 0.16820651292800903,
      "learning_rate": 5.041887412073853e-07,
      "loss": 0.001,
      "step": 82140
    },
    {
      "epoch": 14936.363636363636,
      "grad_norm": 0.2590695321559906,
      "learning_rate": 5.040723898946573e-07,
      "loss": 0.0012,
      "step": 82150
    },
    {
      "epoch": 14938.181818181818,
      "grad_norm": 0.20955654978752136,
      "learning_rate": 5.039560383613925e-07,
      "loss": 0.0012,
      "step": 82160
    },
    {
      "epoch": 14940.0,
      "grad_norm": 0.0012796580558642745,
      "learning_rate": 5.038396866138914e-07,
      "loss": 0.0011,
      "step": 82170
    },
    {
      "epoch": 14941.818181818182,
      "grad_norm": 0.19578784704208374,
      "learning_rate": 5.03723334658455e-07,
      "loss": 0.0012,
      "step": 82180
    },
    {
      "epoch": 14943.636363636364,
      "grad_norm": 0.15848903357982635,
      "learning_rate": 5.036069825013846e-07,
      "loss": 0.001,
      "step": 82190
    },
    {
      "epoch": 14945.454545454546,
      "grad_norm": 0.001150605850853026,
      "learning_rate": 5.034906301489807e-07,
      "loss": 0.0011,
      "step": 82200
    },
    {
      "epoch": 14947.272727272728,
      "grad_norm": 0.1568262130022049,
      "learning_rate": 5.033742776075446e-07,
      "loss": 0.0011,
      "step": 82210
    },
    {
      "epoch": 14949.09090909091,
      "grad_norm": 0.0010715835960581899,
      "learning_rate": 5.032579248833772e-07,
      "loss": 0.001,
      "step": 82220
    },
    {
      "epoch": 14950.90909090909,
      "grad_norm": 0.15845607221126556,
      "learning_rate": 5.031415719827795e-07,
      "loss": 0.0012,
      "step": 82230
    },
    {
      "epoch": 14952.727272727272,
      "grad_norm": 0.16011668741703033,
      "learning_rate": 5.030252189120525e-07,
      "loss": 0.0012,
      "step": 82240
    },
    {
      "epoch": 14954.545454545454,
      "grad_norm": 0.20911726355552673,
      "learning_rate": 5.029088656774969e-07,
      "loss": 0.0009,
      "step": 82250
    },
    {
      "epoch": 14956.363636363636,
      "grad_norm": 0.2544476091861725,
      "learning_rate": 5.027925122854141e-07,
      "loss": 0.0013,
      "step": 82260
    },
    {
      "epoch": 14958.181818181818,
      "grad_norm": 0.0017547965981066227,
      "learning_rate": 5.026761587421051e-07,
      "loss": 0.0008,
      "step": 82270
    },
    {
      "epoch": 14960.0,
      "grad_norm": 0.20978498458862305,
      "learning_rate": 5.025598050538707e-07,
      "loss": 0.0012,
      "step": 82280
    },
    {
      "epoch": 14961.818181818182,
      "grad_norm": 0.000822066271211952,
      "learning_rate": 5.024434512270123e-07,
      "loss": 0.0012,
      "step": 82290
    },
    {
      "epoch": 14963.636363636364,
      "grad_norm": 0.19118955731391907,
      "learning_rate": 5.023270972678306e-07,
      "loss": 0.0011,
      "step": 82300
    },
    {
      "epoch": 14965.454545454546,
      "grad_norm": 0.20893864333629608,
      "learning_rate": 5.022107431826268e-07,
      "loss": 0.0011,
      "step": 82310
    },
    {
      "epoch": 14967.272727272728,
      "grad_norm": 0.254590779542923,
      "learning_rate": 5.02094388977702e-07,
      "loss": 0.0013,
      "step": 82320
    },
    {
      "epoch": 14969.09090909091,
      "grad_norm": 0.1891946941614151,
      "learning_rate": 5.019780346593571e-07,
      "loss": 0.0007,
      "step": 82330
    },
    {
      "epoch": 14970.90909090909,
      "grad_norm": 0.0006929843220859766,
      "learning_rate": 5.018616802338935e-07,
      "loss": 0.001,
      "step": 82340
    },
    {
      "epoch": 14972.727272727272,
      "grad_norm": 0.22064465284347534,
      "learning_rate": 5.017453257076118e-07,
      "loss": 0.0012,
      "step": 82350
    },
    {
      "epoch": 14974.545454545454,
      "grad_norm": 0.0006965500651858747,
      "learning_rate": 5.016289710868137e-07,
      "loss": 0.0012,
      "step": 82360
    },
    {
      "epoch": 14976.363636363636,
      "grad_norm": 0.16043990850448608,
      "learning_rate": 5.015126163777996e-07,
      "loss": 0.001,
      "step": 82370
    },
    {
      "epoch": 14978.181818181818,
      "grad_norm": 0.0007601403049193323,
      "learning_rate": 5.013962615868713e-07,
      "loss": 0.001,
      "step": 82380
    },
    {
      "epoch": 14980.0,
      "grad_norm": 0.0007466554525308311,
      "learning_rate": 5.012799067203293e-07,
      "loss": 0.0012,
      "step": 82390
    },
    {
      "epoch": 14981.818181818182,
      "grad_norm": 0.0012330594472587109,
      "learning_rate": 5.011635517844752e-07,
      "loss": 0.0012,
      "step": 82400
    },
    {
      "epoch": 14983.636363636364,
      "grad_norm": 0.2683112621307373,
      "learning_rate": 5.010471967856096e-07,
      "loss": 0.0011,
      "step": 82410
    },
    {
      "epoch": 14985.454545454546,
      "grad_norm": 0.25617220997810364,
      "learning_rate": 5.009308417300342e-07,
      "loss": 0.0011,
      "step": 82420
    },
    {
      "epoch": 14987.272727272728,
      "grad_norm": 0.2101108431816101,
      "learning_rate": 5.008144866240496e-07,
      "loss": 0.0011,
      "step": 82430
    },
    {
      "epoch": 14989.09090909091,
      "grad_norm": 0.0007303519523702562,
      "learning_rate": 5.006981314739572e-07,
      "loss": 0.0009,
      "step": 82440
    },
    {
      "epoch": 14990.90909090909,
      "grad_norm": 0.1645369976758957,
      "learning_rate": 5.005817762860579e-07,
      "loss": 0.0011,
      "step": 82450
    },
    {
      "epoch": 14992.727272727272,
      "grad_norm": 0.001295511145144701,
      "learning_rate": 5.004654210666532e-07,
      "loss": 0.001,
      "step": 82460
    },
    {
      "epoch": 14994.545454545454,
      "grad_norm": 0.0007791407406330109,
      "learning_rate": 5.003490658220437e-07,
      "loss": 0.0013,
      "step": 82470
    },
    {
      "epoch": 14996.363636363636,
      "grad_norm": 0.15266819298267365,
      "learning_rate": 5.002327105585312e-07,
      "loss": 0.001,
      "step": 82480
    },
    {
      "epoch": 14998.181818181818,
      "grad_norm": 0.0007662443094886839,
      "learning_rate": 5.001163552824162e-07,
      "loss": 0.001,
      "step": 82490
    },
    {
      "epoch": 15000.0,
      "grad_norm": 0.0006725515122525394,
      "learning_rate": 5e-07,
      "loss": 0.0012,
      "step": 82500
    },
    {
      "epoch": 15000.0,
      "eval_loss": 5.008402347564697,
      "eval_runtime": 0.9475,
      "eval_samples_per_second": 10.555,
      "eval_steps_per_second": 5.277,
      "step": 82500
    },
    {
      "epoch": 15001.818181818182,
      "grad_norm": 0.0009018937707878649,
      "learning_rate": 4.998836447175839e-07,
      "loss": 0.001,
      "step": 82510
    },
    {
      "epoch": 15003.636363636364,
      "grad_norm": 0.13744375109672546,
      "learning_rate": 4.99767289441469e-07,
      "loss": 0.0013,
      "step": 82520
    },
    {
      "epoch": 15005.454545454546,
      "grad_norm": 0.21188963949680328,
      "learning_rate": 4.996509341779563e-07,
      "loss": 0.0008,
      "step": 82530
    },
    {
      "epoch": 15007.272727272728,
      "grad_norm": 0.2602009177207947,
      "learning_rate": 4.995345789333468e-07,
      "loss": 0.0013,
      "step": 82540
    },
    {
      "epoch": 15009.09090909091,
      "grad_norm": 0.1582898497581482,
      "learning_rate": 4.994182237139421e-07,
      "loss": 0.0011,
      "step": 82550
    },
    {
      "epoch": 15010.90909090909,
      "grad_norm": 0.2066228836774826,
      "learning_rate": 4.993018685260427e-07,
      "loss": 0.0011,
      "step": 82560
    },
    {
      "epoch": 15012.727272727272,
      "grad_norm": 0.0007794075645506382,
      "learning_rate": 4.991855133759506e-07,
      "loss": 0.0009,
      "step": 82570
    },
    {
      "epoch": 15014.545454545454,
      "grad_norm": 0.20786377787590027,
      "learning_rate": 4.990691582699659e-07,
      "loss": 0.0014,
      "step": 82580
    },
    {
      "epoch": 15016.363636363636,
      "grad_norm": 0.25396454334259033,
      "learning_rate": 4.989528032143903e-07,
      "loss": 0.001,
      "step": 82590
    },
    {
      "epoch": 15018.181818181818,
      "grad_norm": 0.19703103601932526,
      "learning_rate": 4.988364482155249e-07,
      "loss": 0.0011,
      "step": 82600
    },
    {
      "epoch": 15020.0,
      "grad_norm": 0.0009028157219290733,
      "learning_rate": 4.987200932796707e-07,
      "loss": 0.001,
      "step": 82610
    },
    {
      "epoch": 15021.818181818182,
      "grad_norm": 0.0008704171050339937,
      "learning_rate": 4.986037384131288e-07,
      "loss": 0.001,
      "step": 82620
    },
    {
      "epoch": 15023.636363636364,
      "grad_norm": 0.0009805853478610516,
      "learning_rate": 4.984873836222004e-07,
      "loss": 0.0011,
      "step": 82630
    },
    {
      "epoch": 15025.454545454546,
      "grad_norm": 0.165574848651886,
      "learning_rate": 4.983710289131863e-07,
      "loss": 0.0014,
      "step": 82640
    },
    {
      "epoch": 15027.272727272728,
      "grad_norm": 0.1714317798614502,
      "learning_rate": 4.982546742923882e-07,
      "loss": 0.0011,
      "step": 82650
    },
    {
      "epoch": 15029.09090909091,
      "grad_norm": 0.20962058007717133,
      "learning_rate": 4.981383197661066e-07,
      "loss": 0.0011,
      "step": 82660
    },
    {
      "epoch": 15030.90909090909,
      "grad_norm": 0.006343596614897251,
      "learning_rate": 4.980219653406429e-07,
      "loss": 0.0012,
      "step": 82670
    },
    {
      "epoch": 15032.727272727272,
      "grad_norm": 0.18995848298072815,
      "learning_rate": 4.979056110222981e-07,
      "loss": 0.0011,
      "step": 82680
    },
    {
      "epoch": 15034.545454545454,
      "grad_norm": 0.26709839701652527,
      "learning_rate": 4.977892568173733e-07,
      "loss": 0.0012,
      "step": 82690
    },
    {
      "epoch": 15036.363636363636,
      "grad_norm": 0.0021947496570646763,
      "learning_rate": 4.976729027321694e-07,
      "loss": 0.0008,
      "step": 82700
    },
    {
      "epoch": 15038.181818181818,
      "grad_norm": 0.0007260811980813742,
      "learning_rate": 4.975565487729878e-07,
      "loss": 0.0012,
      "step": 82710
    },
    {
      "epoch": 15040.0,
      "grad_norm": 0.0006581680499948561,
      "learning_rate": 4.974401949461292e-07,
      "loss": 0.0012,
      "step": 82720
    },
    {
      "epoch": 15041.818181818182,
      "grad_norm": 0.006002162117511034,
      "learning_rate": 4.97323841257895e-07,
      "loss": 0.0012,
      "step": 82730
    },
    {
      "epoch": 15043.636363636364,
      "grad_norm": 0.0009320946410298347,
      "learning_rate": 4.972074877145859e-07,
      "loss": 0.0012,
      "step": 82740
    },
    {
      "epoch": 15045.454545454546,
      "grad_norm": 0.15989042818546295,
      "learning_rate": 4.97091134322503e-07,
      "loss": 0.001,
      "step": 82750
    },
    {
      "epoch": 15047.272727272728,
      "grad_norm": 0.0007044129888527095,
      "learning_rate": 4.969747810879478e-07,
      "loss": 0.0009,
      "step": 82760
    },
    {
      "epoch": 15049.09090909091,
      "grad_norm": 0.16383109986782074,
      "learning_rate": 4.968584280172205e-07,
      "loss": 0.0013,
      "step": 82770
    },
    {
      "epoch": 15050.90909090909,
      "grad_norm": 0.19717636704444885,
      "learning_rate": 4.967420751166227e-07,
      "loss": 0.0011,
      "step": 82780
    },
    {
      "epoch": 15052.727272727272,
      "grad_norm": 0.19876520335674286,
      "learning_rate": 4.966257223924554e-07,
      "loss": 0.0012,
      "step": 82790
    },
    {
      "epoch": 15054.545454545454,
      "grad_norm": 0.15819333493709564,
      "learning_rate": 4.965093698510192e-07,
      "loss": 0.0011,
      "step": 82800
    },
    {
      "epoch": 15056.363636363636,
      "grad_norm": 0.0008797727641649544,
      "learning_rate": 4.963930174986154e-07,
      "loss": 0.0009,
      "step": 82810
    },
    {
      "epoch": 15058.181818181818,
      "grad_norm": 0.26036447286605835,
      "learning_rate": 4.96276665341545e-07,
      "loss": 0.0015,
      "step": 82820
    },
    {
      "epoch": 15060.0,
      "grad_norm": 0.00114361010491848,
      "learning_rate": 4.961603133861086e-07,
      "loss": 0.0009,
      "step": 82830
    },
    {
      "epoch": 15061.818181818182,
      "grad_norm": 0.0007605034625157714,
      "learning_rate": 4.960439616386077e-07,
      "loss": 0.001,
      "step": 82840
    },
    {
      "epoch": 15063.636363636364,
      "grad_norm": 0.0011636388953775167,
      "learning_rate": 4.959276101053426e-07,
      "loss": 0.0012,
      "step": 82850
    },
    {
      "epoch": 15065.454545454546,
      "grad_norm": 0.2098134458065033,
      "learning_rate": 4.958112587926146e-07,
      "loss": 0.001,
      "step": 82860
    },
    {
      "epoch": 15067.272727272728,
      "grad_norm": 0.253918319940567,
      "learning_rate": 4.956949077067248e-07,
      "loss": 0.0013,
      "step": 82870
    },
    {
      "epoch": 15069.09090909091,
      "grad_norm": 0.001570950960740447,
      "learning_rate": 4.955785568539736e-07,
      "loss": 0.0007,
      "step": 82880
    },
    {
      "epoch": 15070.90909090909,
      "grad_norm": 0.0006623842054978013,
      "learning_rate": 4.954622062406623e-07,
      "loss": 0.0012,
      "step": 82890
    },
    {
      "epoch": 15072.727272727272,
      "grad_norm": 0.0006396819371730089,
      "learning_rate": 4.953458558730916e-07,
      "loss": 0.0012,
      "step": 82900
    },
    {
      "epoch": 15074.545454545454,
      "grad_norm": 0.2006659358739853,
      "learning_rate": 4.952295057575624e-07,
      "loss": 0.0013,
      "step": 82910
    },
    {
      "epoch": 15076.363636363636,
      "grad_norm": 0.0009523123735561967,
      "learning_rate": 4.951131559003756e-07,
      "loss": 0.0007,
      "step": 82920
    },
    {
      "epoch": 15078.181818181818,
      "grad_norm": 0.2073250263929367,
      "learning_rate": 4.949968063078318e-07,
      "loss": 0.0012,
      "step": 82930
    },
    {
      "epoch": 15080.0,
      "grad_norm": 0.13987785577774048,
      "learning_rate": 4.94880456986232e-07,
      "loss": 0.001,
      "step": 82940
    },
    {
      "epoch": 15081.818181818182,
      "grad_norm": 0.0018632755381986499,
      "learning_rate": 4.947641079418773e-07,
      "loss": 0.0011,
      "step": 82950
    },
    {
      "epoch": 15083.636363636364,
      "grad_norm": 0.15562906861305237,
      "learning_rate": 4.946477591810677e-07,
      "loss": 0.0012,
      "step": 82960
    },
    {
      "epoch": 15085.454545454546,
      "grad_norm": 0.000607692520134151,
      "learning_rate": 4.945314107101048e-07,
      "loss": 0.0007,
      "step": 82970
    },
    {
      "epoch": 15087.272727272728,
      "grad_norm": 0.0010865357471629977,
      "learning_rate": 4.944150625352889e-07,
      "loss": 0.0012,
      "step": 82980
    },
    {
      "epoch": 15089.09090909091,
      "grad_norm": 0.20725466310977936,
      "learning_rate": 4.942987146629209e-07,
      "loss": 0.0012,
      "step": 82990
    },
    {
      "epoch": 15090.90909090909,
      "grad_norm": 0.18695951998233795,
      "learning_rate": 4.941823670993015e-07,
      "loss": 0.0012,
      "step": 83000
    },
    {
      "epoch": 15090.90909090909,
      "eval_loss": 4.927631378173828,
      "eval_runtime": 0.9507,
      "eval_samples_per_second": 10.519,
      "eval_steps_per_second": 5.259,
      "step": 83000
    },
    {
      "epoch": 15092.727272727272,
      "grad_norm": 0.1543445885181427,
      "learning_rate": 4.940660198507315e-07,
      "loss": 0.0011,
      "step": 83010
    },
    {
      "epoch": 15094.545454545454,
      "grad_norm": 0.15304149687290192,
      "learning_rate": 4.939496729235112e-07,
      "loss": 0.0014,
      "step": 83020
    },
    {
      "epoch": 15096.363636363636,
      "grad_norm": 0.1509910672903061,
      "learning_rate": 4.938333263239419e-07,
      "loss": 0.001,
      "step": 83030
    },
    {
      "epoch": 15098.181818181818,
      "grad_norm": 0.15948210656642914,
      "learning_rate": 4.937169800583236e-07,
      "loss": 0.0009,
      "step": 83040
    },
    {
      "epoch": 15100.0,
      "grad_norm": 0.1581699252128601,
      "learning_rate": 4.936006341329575e-07,
      "loss": 0.0011,
      "step": 83050
    },
    {
      "epoch": 15101.818181818182,
      "grad_norm": 0.20703521370887756,
      "learning_rate": 4.93484288554144e-07,
      "loss": 0.0012,
      "step": 83060
    },
    {
      "epoch": 15103.636363636364,
      "grad_norm": 0.19908304512500763,
      "learning_rate": 4.933679433281836e-07,
      "loss": 0.0012,
      "step": 83070
    },
    {
      "epoch": 15105.454545454546,
      "grad_norm": 0.25562408566474915,
      "learning_rate": 4.93251598461377e-07,
      "loss": 0.001,
      "step": 83080
    },
    {
      "epoch": 15107.272727272728,
      "grad_norm": 0.2656862139701843,
      "learning_rate": 4.931352539600248e-07,
      "loss": 0.0012,
      "step": 83090
    },
    {
      "epoch": 15109.09090909091,
      "grad_norm": 0.0006340421969071031,
      "learning_rate": 4.930189098304274e-07,
      "loss": 0.0009,
      "step": 83100
    },
    {
      "epoch": 15110.90909090909,
      "grad_norm": 0.27886202931404114,
      "learning_rate": 4.929025660788855e-07,
      "loss": 0.0012,
      "step": 83110
    },
    {
      "epoch": 15112.727272727272,
      "grad_norm": 0.19902995228767395,
      "learning_rate": 4.927862227116994e-07,
      "loss": 0.0012,
      "step": 83120
    },
    {
      "epoch": 15114.545454545454,
      "grad_norm": 0.22378353774547577,
      "learning_rate": 4.926698797351696e-07,
      "loss": 0.0012,
      "step": 83130
    },
    {
      "epoch": 15116.363636363636,
      "grad_norm": 0.16421471536159515,
      "learning_rate": 4.92553537155597e-07,
      "loss": 0.0012,
      "step": 83140
    },
    {
      "epoch": 15118.181818181818,
      "grad_norm": 0.2573586404323578,
      "learning_rate": 4.924371949792813e-07,
      "loss": 0.001,
      "step": 83150
    },
    {
      "epoch": 15120.0,
      "grad_norm": 0.0007816836005076766,
      "learning_rate": 4.923208532125235e-07,
      "loss": 0.0009,
      "step": 83160
    },
    {
      "epoch": 15121.818181818182,
      "grad_norm": 0.0007852624403312802,
      "learning_rate": 4.922045118616239e-07,
      "loss": 0.0012,
      "step": 83170
    },
    {
      "epoch": 15123.636363636364,
      "grad_norm": 0.0006205583922564983,
      "learning_rate": 4.920881709328826e-07,
      "loss": 0.0008,
      "step": 83180
    },
    {
      "epoch": 15125.454545454546,
      "grad_norm": 0.0006169383996166289,
      "learning_rate": 4.919718304326004e-07,
      "loss": 0.0011,
      "step": 83190
    },
    {
      "epoch": 15127.272727272728,
      "grad_norm": 0.18070602416992188,
      "learning_rate": 4.918554903670771e-07,
      "loss": 0.0011,
      "step": 83200
    },
    {
      "epoch": 15129.09090909091,
      "grad_norm": 0.22089143097400665,
      "learning_rate": 4.917391507426133e-07,
      "loss": 0.0011,
      "step": 83210
    },
    {
      "epoch": 15130.90909090909,
      "grad_norm": 0.0008592863450758159,
      "learning_rate": 4.916228115655094e-07,
      "loss": 0.001,
      "step": 83220
    },
    {
      "epoch": 15132.727272727272,
      "grad_norm": 0.0004552661848720163,
      "learning_rate": 4.915064728420653e-07,
      "loss": 0.0011,
      "step": 83230
    },
    {
      "epoch": 15134.545454545454,
      "grad_norm": 0.0013310880167409778,
      "learning_rate": 4.913901345785815e-07,
      "loss": 0.0012,
      "step": 83240
    },
    {
      "epoch": 15136.363636363636,
      "grad_norm": 0.1870725154876709,
      "learning_rate": 4.912737967813582e-07,
      "loss": 0.0009,
      "step": 83250
    },
    {
      "epoch": 15138.181818181818,
      "grad_norm": 0.0010724663734436035,
      "learning_rate": 4.911574594566955e-07,
      "loss": 0.001,
      "step": 83260
    },
    {
      "epoch": 15140.0,
      "grad_norm": 0.25937771797180176,
      "learning_rate": 4.910411226108937e-07,
      "loss": 0.0012,
      "step": 83270
    },
    {
      "epoch": 15141.818181818182,
      "grad_norm": 0.013417722657322884,
      "learning_rate": 4.909247862502527e-07,
      "loss": 0.0011,
      "step": 83280
    },
    {
      "epoch": 15143.636363636364,
      "grad_norm": 0.2102293223142624,
      "learning_rate": 4.908084503810726e-07,
      "loss": 0.0009,
      "step": 83290
    },
    {
      "epoch": 15145.454545454546,
      "grad_norm": 0.0008122075814753771,
      "learning_rate": 4.906921150096537e-07,
      "loss": 0.001,
      "step": 83300
    },
    {
      "epoch": 15147.272727272728,
      "grad_norm": 0.17139165103435516,
      "learning_rate": 4.90575780142296e-07,
      "loss": 0.0012,
      "step": 83310
    },
    {
      "epoch": 15149.09090909091,
      "grad_norm": 0.16653510928153992,
      "learning_rate": 4.904594457852992e-07,
      "loss": 0.0012,
      "step": 83320
    },
    {
      "epoch": 15150.90909090909,
      "grad_norm": 0.20764635503292084,
      "learning_rate": 4.903431119449639e-07,
      "loss": 0.0011,
      "step": 83330
    },
    {
      "epoch": 15152.727272727272,
      "grad_norm": 0.0014133318327367306,
      "learning_rate": 4.902267786275894e-07,
      "loss": 0.0012,
      "step": 83340
    },
    {
      "epoch": 15154.545454545454,
      "grad_norm": 0.1742456704378128,
      "learning_rate": 4.901104458394763e-07,
      "loss": 0.001,
      "step": 83350
    },
    {
      "epoch": 15156.363636363636,
      "grad_norm": 0.20647549629211426,
      "learning_rate": 4.899941135869238e-07,
      "loss": 0.0012,
      "step": 83360
    },
    {
      "epoch": 15158.181818181818,
      "grad_norm": 0.0012791391927748919,
      "learning_rate": 4.898777818762324e-07,
      "loss": 0.0008,
      "step": 83370
    },
    {
      "epoch": 15160.0,
      "grad_norm": 0.0011791393626481295,
      "learning_rate": 4.897614507137017e-07,
      "loss": 0.0012,
      "step": 83380
    },
    {
      "epoch": 15161.818181818182,
      "grad_norm": 0.0010572135215625167,
      "learning_rate": 4.896451201056315e-07,
      "loss": 0.0012,
      "step": 83390
    },
    {
      "epoch": 15163.636363636364,
      "grad_norm": 0.0006498515140265226,
      "learning_rate": 4.895287900583216e-07,
      "loss": 0.0009,
      "step": 83400
    },
    {
      "epoch": 15165.454545454546,
      "grad_norm": 0.16771361231803894,
      "learning_rate": 4.894124605780717e-07,
      "loss": 0.0014,
      "step": 83410
    },
    {
      "epoch": 15167.272727272728,
      "grad_norm": 0.0013587191933766007,
      "learning_rate": 4.892961316711816e-07,
      "loss": 0.0009,
      "step": 83420
    },
    {
      "epoch": 15169.09090909091,
      "grad_norm": 0.0007247380563057959,
      "learning_rate": 4.891798033439511e-07,
      "loss": 0.0012,
      "step": 83430
    },
    {
      "epoch": 15170.90909090909,
      "grad_norm": 0.2799631357192993,
      "learning_rate": 4.890634756026798e-07,
      "loss": 0.0011,
      "step": 83440
    },
    {
      "epoch": 15172.727272727272,
      "grad_norm": 0.2606971561908722,
      "learning_rate": 4.889471484536672e-07,
      "loss": 0.0014,
      "step": 83450
    },
    {
      "epoch": 15174.545454545454,
      "grad_norm": 0.18066050112247467,
      "learning_rate": 4.888308219032131e-07,
      "loss": 0.001,
      "step": 83460
    },
    {
      "epoch": 15176.363636363636,
      "grad_norm": 0.000803326372988522,
      "learning_rate": 4.887144959576171e-07,
      "loss": 0.0009,
      "step": 83470
    },
    {
      "epoch": 15178.181818181818,
      "grad_norm": 0.000755917513743043,
      "learning_rate": 4.885981706231784e-07,
      "loss": 0.0011,
      "step": 83480
    },
    {
      "epoch": 15180.0,
      "grad_norm": 0.18253399431705475,
      "learning_rate": 4.88481845906197e-07,
      "loss": 0.0012,
      "step": 83490
    },
    {
      "epoch": 15181.818181818182,
      "grad_norm": 0.2069355696439743,
      "learning_rate": 4.883655218129719e-07,
      "loss": 0.001,
      "step": 83500
    },
    {
      "epoch": 15181.818181818182,
      "eval_loss": 5.021727561950684,
      "eval_runtime": 0.9468,
      "eval_samples_per_second": 10.562,
      "eval_steps_per_second": 5.281,
      "step": 83500
    },
    {
      "epoch": 15183.636363636364,
      "grad_norm": 0.2659749686717987,
      "learning_rate": 4.882491983498026e-07,
      "loss": 0.0014,
      "step": 83510
    },
    {
      "epoch": 15185.454545454546,
      "grad_norm": 0.1699732393026352,
      "learning_rate": 4.881328755229891e-07,
      "loss": 0.001,
      "step": 83520
    },
    {
      "epoch": 15187.272727272728,
      "grad_norm": 0.0010694105876609683,
      "learning_rate": 4.8801655333883e-07,
      "loss": 0.001,
      "step": 83530
    },
    {
      "epoch": 15189.09090909091,
      "grad_norm": 0.04512013494968414,
      "learning_rate": 4.879002318036252e-07,
      "loss": 0.0013,
      "step": 83540
    },
    {
      "epoch": 15190.90909090909,
      "grad_norm": 0.23404431343078613,
      "learning_rate": 4.877839109236734e-07,
      "loss": 0.0011,
      "step": 83550
    },
    {
      "epoch": 15192.727272727272,
      "grad_norm": 0.207325279712677,
      "learning_rate": 4.876675907052744e-07,
      "loss": 0.0011,
      "step": 83560
    },
    {
      "epoch": 15194.545454545454,
      "grad_norm": 0.20585165917873383,
      "learning_rate": 4.875512711547273e-07,
      "loss": 0.0013,
      "step": 83570
    },
    {
      "epoch": 15196.363636363636,
      "grad_norm": 0.2680993378162384,
      "learning_rate": 4.874349522783312e-07,
      "loss": 0.0011,
      "step": 83580
    },
    {
      "epoch": 15198.181818181818,
      "grad_norm": 0.0007922379299998283,
      "learning_rate": 4.873186340823854e-07,
      "loss": 0.0007,
      "step": 83590
    },
    {
      "epoch": 15200.0,
      "grad_norm": 0.016585972160100937,
      "learning_rate": 4.872023165731889e-07,
      "loss": 0.0012,
      "step": 83600
    },
    {
      "epoch": 15201.818181818182,
      "grad_norm": 0.20814312994480133,
      "learning_rate": 4.870859997570407e-07,
      "loss": 0.0012,
      "step": 83610
    },
    {
      "epoch": 15203.636363636364,
      "grad_norm": 0.016377640888094902,
      "learning_rate": 4.869696836402401e-07,
      "loss": 0.001,
      "step": 83620
    },
    {
      "epoch": 15205.454545454546,
      "grad_norm": 0.001622533891350031,
      "learning_rate": 4.868533682290858e-07,
      "loss": 0.0012,
      "step": 83630
    },
    {
      "epoch": 15207.272727272728,
      "grad_norm": 0.0006791484775021672,
      "learning_rate": 4.867370535298769e-07,
      "loss": 0.0009,
      "step": 83640
    },
    {
      "epoch": 15209.09090909091,
      "grad_norm": 0.4432070553302765,
      "learning_rate": 4.866207395489126e-07,
      "loss": 0.0014,
      "step": 83650
    },
    {
      "epoch": 15210.90909090909,
      "grad_norm": 0.16594748198986053,
      "learning_rate": 4.865044262924914e-07,
      "loss": 0.0011,
      "step": 83660
    },
    {
      "epoch": 15212.727272727272,
      "grad_norm": 0.18543243408203125,
      "learning_rate": 4.863881137669122e-07,
      "loss": 0.0013,
      "step": 83670
    },
    {
      "epoch": 15214.545454545454,
      "grad_norm": 0.18350332975387573,
      "learning_rate": 4.862718019784741e-07,
      "loss": 0.0009,
      "step": 83680
    },
    {
      "epoch": 15216.363636363636,
      "grad_norm": 0.17344793677330017,
      "learning_rate": 4.861554909334756e-07,
      "loss": 0.0012,
      "step": 83690
    },
    {
      "epoch": 15218.181818181818,
      "grad_norm": 0.14271198213100433,
      "learning_rate": 4.860391806382156e-07,
      "loss": 0.001,
      "step": 83700
    },
    {
      "epoch": 15220.0,
      "grad_norm": 0.001811324036680162,
      "learning_rate": 4.859228710989927e-07,
      "loss": 0.0011,
      "step": 83710
    },
    {
      "epoch": 15221.818181818182,
      "grad_norm": 0.1901908814907074,
      "learning_rate": 4.858065623221053e-07,
      "loss": 0.0012,
      "step": 83720
    },
    {
      "epoch": 15223.636363636364,
      "grad_norm": 0.0007181934779509902,
      "learning_rate": 4.856902543138527e-07,
      "loss": 0.0009,
      "step": 83730
    },
    {
      "epoch": 15225.454545454546,
      "grad_norm": 0.2530909478664398,
      "learning_rate": 4.855739470805327e-07,
      "loss": 0.0012,
      "step": 83740
    },
    {
      "epoch": 15227.272727272728,
      "grad_norm": 0.19094187021255493,
      "learning_rate": 4.854576406284443e-07,
      "loss": 0.0012,
      "step": 83750
    },
    {
      "epoch": 15229.09090909091,
      "grad_norm": 0.0016783657483756542,
      "learning_rate": 4.853413349638859e-07,
      "loss": 0.0009,
      "step": 83760
    },
    {
      "epoch": 15230.90909090909,
      "grad_norm": 0.16329021751880646,
      "learning_rate": 4.852250300931557e-07,
      "loss": 0.0012,
      "step": 83770
    },
    {
      "epoch": 15232.727272727272,
      "grad_norm": 0.0013851820258423686,
      "learning_rate": 4.851087260225524e-07,
      "loss": 0.001,
      "step": 83780
    },
    {
      "epoch": 15234.545454545454,
      "grad_norm": 0.0008861559326760471,
      "learning_rate": 4.849924227583744e-07,
      "loss": 0.0009,
      "step": 83790
    },
    {
      "epoch": 15236.363636363636,
      "grad_norm": 0.26708436012268066,
      "learning_rate": 4.848761203069196e-07,
      "loss": 0.0013,
      "step": 83800
    },
    {
      "epoch": 15238.181818181818,
      "grad_norm": 0.0006065715570002794,
      "learning_rate": 4.847598186744868e-07,
      "loss": 0.0009,
      "step": 83810
    },
    {
      "epoch": 15240.0,
      "grad_norm": 0.1838838905096054,
      "learning_rate": 4.846435178673736e-07,
      "loss": 0.0012,
      "step": 83820
    },
    {
      "epoch": 15241.818181818182,
      "grad_norm": 0.0009425670723430812,
      "learning_rate": 4.845272178918787e-07,
      "loss": 0.0012,
      "step": 83830
    },
    {
      "epoch": 15243.636363636364,
      "grad_norm": 0.0006529766251333058,
      "learning_rate": 4.844109187543002e-07,
      "loss": 0.0011,
      "step": 83840
    },
    {
      "epoch": 15245.454545454546,
      "grad_norm": 0.0009121571201831102,
      "learning_rate": 4.842946204609359e-07,
      "loss": 0.001,
      "step": 83850
    },
    {
      "epoch": 15247.272727272728,
      "grad_norm": 0.2604825496673584,
      "learning_rate": 4.841783230180839e-07,
      "loss": 0.0013,
      "step": 83860
    },
    {
      "epoch": 15249.09090909091,
      "grad_norm": 0.2056380808353424,
      "learning_rate": 4.840620264320424e-07,
      "loss": 0.001,
      "step": 83870
    },
    {
      "epoch": 15250.90909090909,
      "grad_norm": 0.000983650446869433,
      "learning_rate": 4.839457307091092e-07,
      "loss": 0.0012,
      "step": 83880
    },
    {
      "epoch": 15252.727272727272,
      "grad_norm": 0.19811415672302246,
      "learning_rate": 4.838294358555823e-07,
      "loss": 0.0009,
      "step": 83890
    },
    {
      "epoch": 15254.545454545454,
      "grad_norm": 0.0007389599923044443,
      "learning_rate": 4.837131418777594e-07,
      "loss": 0.001,
      "step": 83900
    },
    {
      "epoch": 15256.363636363636,
      "grad_norm": 0.0021219749469310045,
      "learning_rate": 4.835968487819384e-07,
      "loss": 0.0011,
      "step": 83910
    },
    {
      "epoch": 15258.181818181818,
      "grad_norm": 0.15495970845222473,
      "learning_rate": 4.834805565744172e-07,
      "loss": 0.0015,
      "step": 83920
    },
    {
      "epoch": 15260.0,
      "grad_norm": 0.040290091186761856,
      "learning_rate": 4.833642652614931e-07,
      "loss": 0.0009,
      "step": 83930
    },
    {
      "epoch": 15261.818181818182,
      "grad_norm": 0.000792943756096065,
      "learning_rate": 4.832479748494642e-07,
      "loss": 0.0008,
      "step": 83940
    },
    {
      "epoch": 15263.636363636364,
      "grad_norm": 0.001294008456170559,
      "learning_rate": 4.83131685344628e-07,
      "loss": 0.0015,
      "step": 83950
    },
    {
      "epoch": 15265.454545454546,
      "grad_norm": 0.0013564630644395947,
      "learning_rate": 4.83015396753282e-07,
      "loss": 0.0008,
      "step": 83960
    },
    {
      "epoch": 15267.272727272728,
      "grad_norm": 0.0006203624070622027,
      "learning_rate": 4.828991090817237e-07,
      "loss": 0.001,
      "step": 83970
    },
    {
      "epoch": 15269.09090909091,
      "grad_norm": 0.0009623560472391546,
      "learning_rate": 4.827828223362506e-07,
      "loss": 0.0012,
      "step": 83980
    },
    {
      "epoch": 15270.90909090909,
      "grad_norm": 0.20552104711532593,
      "learning_rate": 4.826665365231601e-07,
      "loss": 0.001,
      "step": 83990
    },
    {
      "epoch": 15272.727272727272,
      "grad_norm": 0.003779221558943391,
      "learning_rate": 4.825502516487496e-07,
      "loss": 0.0009,
      "step": 84000
    },
    {
      "epoch": 15272.727272727272,
      "eval_loss": 5.066316604614258,
      "eval_runtime": 0.9541,
      "eval_samples_per_second": 10.482,
      "eval_steps_per_second": 5.241,
      "step": 84000
    },
    {
      "epoch": 15274.545454545454,
      "grad_norm": 0.00098419189453125,
      "learning_rate": 4.824339677193163e-07,
      "loss": 0.0013,
      "step": 84010
    },
    {
      "epoch": 15276.363636363636,
      "grad_norm": 0.0021651084534823895,
      "learning_rate": 4.823176847411575e-07,
      "loss": 0.0011,
      "step": 84020
    },
    {
      "epoch": 15278.181818181818,
      "grad_norm": 0.18058472871780396,
      "learning_rate": 4.822014027205707e-07,
      "loss": 0.0012,
      "step": 84030
    },
    {
      "epoch": 15280.0,
      "grad_norm": 0.28673210740089417,
      "learning_rate": 4.820851216638528e-07,
      "loss": 0.0012,
      "step": 84040
    },
    {
      "epoch": 15281.818181818182,
      "grad_norm": 0.20893482863903046,
      "learning_rate": 4.819688415773009e-07,
      "loss": 0.0011,
      "step": 84050
    },
    {
      "epoch": 15283.636363636364,
      "grad_norm": 0.0007979548536241055,
      "learning_rate": 4.818525624672121e-07,
      "loss": 0.0011,
      "step": 84060
    },
    {
      "epoch": 15285.454545454546,
      "grad_norm": 0.0008198958821594715,
      "learning_rate": 4.817362843398834e-07,
      "loss": 0.0009,
      "step": 84070
    },
    {
      "epoch": 15287.272727272728,
      "grad_norm": 0.2682495713233948,
      "learning_rate": 4.816200072016118e-07,
      "loss": 0.0014,
      "step": 84080
    },
    {
      "epoch": 15289.09090909091,
      "grad_norm": 0.0014046295545995235,
      "learning_rate": 4.81503731058694e-07,
      "loss": 0.0009,
      "step": 84090
    },
    {
      "epoch": 15290.90909090909,
      "grad_norm": 0.21778947114944458,
      "learning_rate": 4.81387455917427e-07,
      "loss": 0.0012,
      "step": 84100
    },
    {
      "epoch": 15292.727272727272,
      "grad_norm": 0.18053314089775085,
      "learning_rate": 4.812711817841078e-07,
      "loss": 0.001,
      "step": 84110
    },
    {
      "epoch": 15294.545454545454,
      "grad_norm": 0.00102317298296839,
      "learning_rate": 4.811549086650326e-07,
      "loss": 0.0009,
      "step": 84120
    },
    {
      "epoch": 15296.363636363636,
      "grad_norm": 0.176065593957901,
      "learning_rate": 4.810386365664986e-07,
      "loss": 0.0012,
      "step": 84130
    },
    {
      "epoch": 15298.181818181818,
      "grad_norm": 0.17313751578330994,
      "learning_rate": 4.809223654948023e-07,
      "loss": 0.0012,
      "step": 84140
    },
    {
      "epoch": 15300.0,
      "grad_norm": 0.0007754148682579398,
      "learning_rate": 4.8080609545624e-07,
      "loss": 0.001,
      "step": 84150
    },
    {
      "epoch": 15301.818181818182,
      "grad_norm": 0.2910078167915344,
      "learning_rate": 4.806898264571086e-07,
      "loss": 0.0012,
      "step": 84160
    },
    {
      "epoch": 15303.636363636364,
      "grad_norm": 0.0008853332255966961,
      "learning_rate": 4.805735585037041e-07,
      "loss": 0.0011,
      "step": 84170
    },
    {
      "epoch": 15305.454545454546,
      "grad_norm": 0.0006158465403132141,
      "learning_rate": 4.804572916023232e-07,
      "loss": 0.0012,
      "step": 84180
    },
    {
      "epoch": 15307.272727272728,
      "grad_norm": 0.0006576782907359302,
      "learning_rate": 4.803410257592624e-07,
      "loss": 0.0011,
      "step": 84190
    },
    {
      "epoch": 15309.09090909091,
      "grad_norm": 0.19074104726314545,
      "learning_rate": 4.802247609808175e-07,
      "loss": 0.0011,
      "step": 84200
    },
    {
      "epoch": 15310.90909090909,
      "grad_norm": 0.0010215258225798607,
      "learning_rate": 4.80108497273285e-07,
      "loss": 0.0012,
      "step": 84210
    },
    {
      "epoch": 15312.727272727272,
      "grad_norm": 0.0006586886593140662,
      "learning_rate": 4.799922346429613e-07,
      "loss": 0.0009,
      "step": 84220
    },
    {
      "epoch": 15314.545454545454,
      "grad_norm": 0.0014025602722540498,
      "learning_rate": 4.798759730961421e-07,
      "loss": 0.001,
      "step": 84230
    },
    {
      "epoch": 15316.363636363636,
      "grad_norm": 0.19799408316612244,
      "learning_rate": 4.797597126391238e-07,
      "loss": 0.0016,
      "step": 84240
    },
    {
      "epoch": 15318.181818181818,
      "grad_norm": 0.0006323633715510368,
      "learning_rate": 4.796434532782022e-07,
      "loss": 0.0006,
      "step": 84250
    },
    {
      "epoch": 15320.0,
      "grad_norm": 0.0010294383391737938,
      "learning_rate": 4.795271950196731e-07,
      "loss": 0.0012,
      "step": 84260
    },
    {
      "epoch": 15321.818181818182,
      "grad_norm": 0.0005109604680910707,
      "learning_rate": 4.794109378698326e-07,
      "loss": 0.001,
      "step": 84270
    },
    {
      "epoch": 15323.636363636364,
      "grad_norm": 0.16996750235557556,
      "learning_rate": 4.792946818349765e-07,
      "loss": 0.0014,
      "step": 84280
    },
    {
      "epoch": 15325.454545454546,
      "grad_norm": 0.0007862170459702611,
      "learning_rate": 4.791784269214003e-07,
      "loss": 0.0008,
      "step": 84290
    },
    {
      "epoch": 15327.272727272728,
      "grad_norm": 0.0005869463202543557,
      "learning_rate": 4.790621731354002e-07,
      "loss": 0.001,
      "step": 84300
    },
    {
      "epoch": 15329.09090909091,
      "grad_norm": 0.17444312572479248,
      "learning_rate": 4.789459204832713e-07,
      "loss": 0.0012,
      "step": 84310
    },
    {
      "epoch": 15330.90909090909,
      "grad_norm": 0.0056578307412564754,
      "learning_rate": 4.788296689713097e-07,
      "loss": 0.0011,
      "step": 84320
    },
    {
      "epoch": 15332.727272727272,
      "grad_norm": 0.0007336607668548822,
      "learning_rate": 4.787134186058102e-07,
      "loss": 0.001,
      "step": 84330
    },
    {
      "epoch": 15334.545454545454,
      "grad_norm": 0.2666773796081543,
      "learning_rate": 4.785971693930688e-07,
      "loss": 0.0012,
      "step": 84340
    },
    {
      "epoch": 15336.363636363636,
      "grad_norm": 0.20871660113334656,
      "learning_rate": 4.784809213393809e-07,
      "loss": 0.0011,
      "step": 84350
    },
    {
      "epoch": 15338.181818181818,
      "grad_norm": 0.26642876863479614,
      "learning_rate": 4.783646744510415e-07,
      "loss": 0.0013,
      "step": 84360
    },
    {
      "epoch": 15340.0,
      "grad_norm": 0.17576490342617035,
      "learning_rate": 4.78248428734346e-07,
      "loss": 0.0009,
      "step": 84370
    },
    {
      "epoch": 15341.818181818182,
      "grad_norm": 0.2059885412454605,
      "learning_rate": 4.781321841955897e-07,
      "loss": 0.0012,
      "step": 84380
    },
    {
      "epoch": 15343.636363636364,
      "grad_norm": 0.0008012980106286705,
      "learning_rate": 4.780159408410676e-07,
      "loss": 0.0009,
      "step": 84390
    },
    {
      "epoch": 15345.454545454546,
      "grad_norm": 0.000877576123457402,
      "learning_rate": 4.778996986770746e-07,
      "loss": 0.0011,
      "step": 84400
    },
    {
      "epoch": 15347.272727272728,
      "grad_norm": 0.000994631671346724,
      "learning_rate": 4.777834577099063e-07,
      "loss": 0.0012,
      "step": 84410
    },
    {
      "epoch": 15349.09090909091,
      "grad_norm": 0.0006525524077005684,
      "learning_rate": 4.77667217945857e-07,
      "loss": 0.0012,
      "step": 84420
    },
    {
      "epoch": 15350.90909090909,
      "grad_norm": 0.0013855865690857172,
      "learning_rate": 4.775509793912219e-07,
      "loss": 0.001,
      "step": 84430
    },
    {
      "epoch": 15352.727272727272,
      "grad_norm": 0.18509088456630707,
      "learning_rate": 4.774347420522957e-07,
      "loss": 0.0015,
      "step": 84440
    },
    {
      "epoch": 15354.545454545454,
      "grad_norm": 0.0005577810225076973,
      "learning_rate": 4.77318505935373e-07,
      "loss": 0.0008,
      "step": 84450
    },
    {
      "epoch": 15356.363636363636,
      "grad_norm": 0.1609347015619278,
      "learning_rate": 4.77202271046749e-07,
      "loss": 0.0012,
      "step": 84460
    },
    {
      "epoch": 15358.181818181818,
      "grad_norm": 0.004703935235738754,
      "learning_rate": 4.770860373927177e-07,
      "loss": 0.001,
      "step": 84470
    },
    {
      "epoch": 15360.0,
      "grad_norm": 0.0004925350076518953,
      "learning_rate": 4.769698049795738e-07,
      "loss": 0.0012,
      "step": 84480
    },
    {
      "epoch": 15361.818181818182,
      "grad_norm": 0.0007332157110795379,
      "learning_rate": 4.768535738136122e-07,
      "loss": 0.0011,
      "step": 84490
    },
    {
      "epoch": 15363.636363636364,
      "grad_norm": 0.19220823049545288,
      "learning_rate": 4.7673734390112666e-07,
      "loss": 0.0011,
      "step": 84500
    },
    {
      "epoch": 15363.636363636364,
      "eval_loss": 5.008403778076172,
      "eval_runtime": 0.9491,
      "eval_samples_per_second": 10.537,
      "eval_steps_per_second": 5.268,
      "step": 84500
    },
    {
      "epoch": 15365.454545454546,
      "grad_norm": 0.0018401462584733963,
      "learning_rate": 4.7662111524841206e-07,
      "loss": 0.0011,
      "step": 84510
    },
    {
      "epoch": 15367.272727272728,
      "grad_norm": 0.0059222448617219925,
      "learning_rate": 4.765048878617622e-07,
      "loss": 0.0012,
      "step": 84520
    },
    {
      "epoch": 15369.09090909091,
      "grad_norm": 0.0023397975601255894,
      "learning_rate": 4.7638866174747164e-07,
      "loss": 0.0009,
      "step": 84530
    },
    {
      "epoch": 15370.90909090909,
      "grad_norm": 0.0006306784343905747,
      "learning_rate": 4.7627243691183447e-07,
      "loss": 0.0012,
      "step": 84540
    },
    {
      "epoch": 15372.727272727272,
      "grad_norm": 0.2782457172870636,
      "learning_rate": 4.761562133611446e-07,
      "loss": 0.0012,
      "step": 84550
    },
    {
      "epoch": 15374.545454545454,
      "grad_norm": 0.0008017005166038871,
      "learning_rate": 4.760399911016961e-07,
      "loss": 0.001,
      "step": 84560
    },
    {
      "epoch": 15376.363636363636,
      "grad_norm": 0.20570756494998932,
      "learning_rate": 4.75923770139783e-07,
      "loss": 0.0012,
      "step": 84570
    },
    {
      "epoch": 15378.181818181818,
      "grad_norm": 0.0007241661078296602,
      "learning_rate": 4.75807550481699e-07,
      "loss": 0.0009,
      "step": 84580
    },
    {
      "epoch": 15380.0,
      "grad_norm": 0.001408421783708036,
      "learning_rate": 4.7569133213373793e-07,
      "loss": 0.0012,
      "step": 84590
    },
    {
      "epoch": 15381.818181818182,
      "grad_norm": 0.0005646198987960815,
      "learning_rate": 4.7557511510219335e-07,
      "loss": 0.001,
      "step": 84600
    },
    {
      "epoch": 15383.636363636364,
      "grad_norm": 0.0019708878826349974,
      "learning_rate": 4.7545889939335916e-07,
      "loss": 0.001,
      "step": 84610
    },
    {
      "epoch": 15385.454545454546,
      "grad_norm": 0.2578069269657135,
      "learning_rate": 4.75342685013529e-07,
      "loss": 0.0013,
      "step": 84620
    },
    {
      "epoch": 15387.272727272728,
      "grad_norm": 0.0007777756545692682,
      "learning_rate": 4.75226471968996e-07,
      "loss": 0.0009,
      "step": 84630
    },
    {
      "epoch": 15389.09090909091,
      "grad_norm": 0.0028010080568492413,
      "learning_rate": 4.7511026026605386e-07,
      "loss": 0.0011,
      "step": 84640
    },
    {
      "epoch": 15390.90909090909,
      "grad_norm": 0.20555832982063293,
      "learning_rate": 4.749940499109959e-07,
      "loss": 0.0012,
      "step": 84650
    },
    {
      "epoch": 15392.727272727272,
      "grad_norm": 0.0006127129890955985,
      "learning_rate": 4.748778409101152e-07,
      "loss": 0.0011,
      "step": 84660
    },
    {
      "epoch": 15394.545454545454,
      "grad_norm": 0.0006259978981688619,
      "learning_rate": 4.7476163326970513e-07,
      "loss": 0.0012,
      "step": 84670
    },
    {
      "epoch": 15396.363636363636,
      "grad_norm": 0.0010570192243903875,
      "learning_rate": 4.746454269960591e-07,
      "loss": 0.0009,
      "step": 84680
    },
    {
      "epoch": 15398.181818181818,
      "grad_norm": 0.1767391413450241,
      "learning_rate": 4.7452922209546954e-07,
      "loss": 0.0012,
      "step": 84690
    },
    {
      "epoch": 15400.0,
      "grad_norm": 0.2579319477081299,
      "learning_rate": 4.744130185742301e-07,
      "loss": 0.0011,
      "step": 84700
    },
    {
      "epoch": 15401.818181818182,
      "grad_norm": 0.0008185408660210669,
      "learning_rate": 4.7429681643863304e-07,
      "loss": 0.0012,
      "step": 84710
    },
    {
      "epoch": 15403.636363636364,
      "grad_norm": 0.19458015263080597,
      "learning_rate": 4.741806156949717e-07,
      "loss": 0.0009,
      "step": 84720
    },
    {
      "epoch": 15405.454545454546,
      "grad_norm": 0.0007650292245671153,
      "learning_rate": 4.740644163495387e-07,
      "loss": 0.001,
      "step": 84730
    },
    {
      "epoch": 15407.272727272728,
      "grad_norm": 0.1651875376701355,
      "learning_rate": 4.739482184086266e-07,
      "loss": 0.0013,
      "step": 84740
    },
    {
      "epoch": 15409.09090909091,
      "grad_norm": 0.45505112409591675,
      "learning_rate": 4.7383202187852804e-07,
      "loss": 0.0012,
      "step": 84750
    },
    {
      "epoch": 15410.90909090909,
      "grad_norm": 0.0007133473409339786,
      "learning_rate": 4.7371582676553576e-07,
      "loss": 0.0007,
      "step": 84760
    },
    {
      "epoch": 15412.727272727272,
      "grad_norm": 0.0006323483539745212,
      "learning_rate": 4.7359963307594195e-07,
      "loss": 0.001,
      "step": 84770
    },
    {
      "epoch": 15414.545454545454,
      "grad_norm": 0.20851299166679382,
      "learning_rate": 4.734834408160392e-07,
      "loss": 0.0011,
      "step": 84780
    },
    {
      "epoch": 15416.363636363636,
      "grad_norm": 0.19442275166511536,
      "learning_rate": 4.7336724999211946e-07,
      "loss": 0.0013,
      "step": 84790
    },
    {
      "epoch": 15418.181818181818,
      "grad_norm": 0.0009998002788051963,
      "learning_rate": 4.732510606104754e-07,
      "loss": 0.0009,
      "step": 84800
    },
    {
      "epoch": 15420.0,
      "grad_norm": 0.17152033746242523,
      "learning_rate": 4.731348726773989e-07,
      "loss": 0.0012,
      "step": 84810
    },
    {
      "epoch": 15421.818181818182,
      "grad_norm": 0.2625284194946289,
      "learning_rate": 4.730186861991821e-07,
      "loss": 0.001,
      "step": 84820
    },
    {
      "epoch": 15423.636363636364,
      "grad_norm": 0.20765270292758942,
      "learning_rate": 4.7290250118211693e-07,
      "loss": 0.0014,
      "step": 84830
    },
    {
      "epoch": 15425.454545454546,
      "grad_norm": 0.0017990025226026773,
      "learning_rate": 4.727863176324955e-07,
      "loss": 0.0007,
      "step": 84840
    },
    {
      "epoch": 15427.272727272728,
      "grad_norm": 0.0008552224026061594,
      "learning_rate": 4.7267013555660935e-07,
      "loss": 0.0015,
      "step": 84850
    },
    {
      "epoch": 15429.09090909091,
      "grad_norm": 0.0017577760154381394,
      "learning_rate": 4.725539549607504e-07,
      "loss": 0.0009,
      "step": 84860
    },
    {
      "epoch": 15430.90909090909,
      "grad_norm": 0.0007596868090331554,
      "learning_rate": 4.7243777585121024e-07,
      "loss": 0.0012,
      "step": 84870
    },
    {
      "epoch": 15432.727272727272,
      "grad_norm": 0.0007971563609316945,
      "learning_rate": 4.723215982342803e-07,
      "loss": 0.001,
      "step": 84880
    },
    {
      "epoch": 15434.545454545454,
      "grad_norm": 0.0010061714565381408,
      "learning_rate": 4.722054221162527e-07,
      "loss": 0.0012,
      "step": 84890
    },
    {
      "epoch": 15436.363636363636,
      "grad_norm": 0.0008156507392413914,
      "learning_rate": 4.7208924750341805e-07,
      "loss": 0.0009,
      "step": 84900
    },
    {
      "epoch": 15438.181818181818,
      "grad_norm": 0.16018818318843842,
      "learning_rate": 4.719730744020682e-07,
      "loss": 0.0013,
      "step": 84910
    },
    {
      "epoch": 15440.0,
      "grad_norm": 0.001858470612205565,
      "learning_rate": 4.718569028184945e-07,
      "loss": 0.0011,
      "step": 84920
    },
    {
      "epoch": 15441.818181818182,
      "grad_norm": 0.2597494423389435,
      "learning_rate": 4.7174073275898776e-07,
      "loss": 0.001,
      "step": 84930
    },
    {
      "epoch": 15443.636363636364,
      "grad_norm": 0.0013123814715072513,
      "learning_rate": 4.7162456422983934e-07,
      "loss": 0.0013,
      "step": 84940
    },
    {
      "epoch": 15445.454545454546,
      "grad_norm": 0.0008557478431612253,
      "learning_rate": 4.7150839723734003e-07,
      "loss": 0.001,
      "step": 84950
    },
    {
      "epoch": 15447.272727272728,
      "grad_norm": 0.0012010745704174042,
      "learning_rate": 4.713922317877809e-07,
      "loss": 0.0011,
      "step": 84960
    },
    {
      "epoch": 15449.09090909091,
      "grad_norm": 0.1954164206981659,
      "learning_rate": 4.71276067887453e-07,
      "loss": 0.001,
      "step": 84970
    },
    {
      "epoch": 15450.90909090909,
      "grad_norm": 0.16405969858169556,
      "learning_rate": 4.711599055426467e-07,
      "loss": 0.001,
      "step": 84980
    },
    {
      "epoch": 15452.727272727272,
      "grad_norm": 0.16643720865249634,
      "learning_rate": 4.7104374475965275e-07,
      "loss": 0.0012,
      "step": 84990
    },
    {
      "epoch": 15454.545454545454,
      "grad_norm": 0.0009166021482087672,
      "learning_rate": 4.7092758554476206e-07,
      "loss": 0.0009,
      "step": 85000
    },
    {
      "epoch": 15454.545454545454,
      "eval_loss": 5.095431327819824,
      "eval_runtime": 0.953,
      "eval_samples_per_second": 10.494,
      "eval_steps_per_second": 5.247,
      "step": 85000
    },
    {
      "epoch": 15456.363636363636,
      "grad_norm": 0.17891095578670502,
      "learning_rate": 4.7081142790426485e-07,
      "loss": 0.0013,
      "step": 85010
    },
    {
      "epoch": 15458.181818181818,
      "grad_norm": 0.1978980004787445,
      "learning_rate": 4.7069527184445166e-07,
      "loss": 0.0011,
      "step": 85020
    },
    {
      "epoch": 15460.0,
      "grad_norm": 0.034838367253541946,
      "learning_rate": 4.7057911737161294e-07,
      "loss": 0.0011,
      "step": 85030
    },
    {
      "epoch": 15461.818181818182,
      "grad_norm": 0.2075691819190979,
      "learning_rate": 4.7046296449203863e-07,
      "loss": 0.001,
      "step": 85040
    },
    {
      "epoch": 15463.636363636364,
      "grad_norm": 0.0009598791948519647,
      "learning_rate": 4.7034681321201924e-07,
      "loss": 0.001,
      "step": 85050
    },
    {
      "epoch": 15465.454545454546,
      "grad_norm": 0.14874853193759918,
      "learning_rate": 4.7023066353784455e-07,
      "loss": 0.0014,
      "step": 85060
    },
    {
      "epoch": 15467.272727272728,
      "grad_norm": 0.2637732923030853,
      "learning_rate": 4.701145154758046e-07,
      "loss": 0.001,
      "step": 85070
    },
    {
      "epoch": 15469.09090909091,
      "grad_norm": 0.0008990548667497933,
      "learning_rate": 4.6999836903218975e-07,
      "loss": 0.0009,
      "step": 85080
    },
    {
      "epoch": 15470.90909090909,
      "grad_norm": 0.0019090153509750962,
      "learning_rate": 4.6988222421328903e-07,
      "loss": 0.0012,
      "step": 85090
    },
    {
      "epoch": 15472.727272727272,
      "grad_norm": 0.19783905148506165,
      "learning_rate": 4.697660810253928e-07,
      "loss": 0.001,
      "step": 85100
    },
    {
      "epoch": 15474.545454545454,
      "grad_norm": 0.0008615778060629964,
      "learning_rate": 4.696499394747905e-07,
      "loss": 0.0008,
      "step": 85110
    },
    {
      "epoch": 15476.363636363636,
      "grad_norm": 0.24821875989437103,
      "learning_rate": 4.6953379956777165e-07,
      "loss": 0.0016,
      "step": 85120
    },
    {
      "epoch": 15478.181818181818,
      "grad_norm": 0.0005521419807337224,
      "learning_rate": 4.694176613106259e-07,
      "loss": 0.0009,
      "step": 85130
    },
    {
      "epoch": 15480.0,
      "grad_norm": 0.0008094316581264138,
      "learning_rate": 4.6930152470964227e-07,
      "loss": 0.0012,
      "step": 85140
    },
    {
      "epoch": 15481.818181818182,
      "grad_norm": 0.0011646512430161238,
      "learning_rate": 4.6918538977111035e-07,
      "loss": 0.0008,
      "step": 85150
    },
    {
      "epoch": 15483.636363636364,
      "grad_norm": 0.259746253490448,
      "learning_rate": 4.690692565013192e-07,
      "loss": 0.0016,
      "step": 85160
    },
    {
      "epoch": 15485.454545454546,
      "grad_norm": 0.14113666117191315,
      "learning_rate": 4.6895312490655795e-07,
      "loss": 0.001,
      "step": 85170
    },
    {
      "epoch": 15487.272727272728,
      "grad_norm": 0.005519120488315821,
      "learning_rate": 4.6883699499311554e-07,
      "loss": 0.0012,
      "step": 85180
    },
    {
      "epoch": 15489.09090909091,
      "grad_norm": 0.0006492373067885637,
      "learning_rate": 4.6872086676728114e-07,
      "loss": 0.0009,
      "step": 85190
    },
    {
      "epoch": 15490.90909090909,
      "grad_norm": 0.0007534486358053982,
      "learning_rate": 4.686047402353433e-07,
      "loss": 0.0012,
      "step": 85200
    },
    {
      "epoch": 15492.727272727272,
      "grad_norm": 0.21196173131465912,
      "learning_rate": 4.6848861540359096e-07,
      "loss": 0.0012,
      "step": 85210
    },
    {
      "epoch": 15494.545454545454,
      "grad_norm": 0.0005205455236136913,
      "learning_rate": 4.683724922783127e-07,
      "loss": 0.001,
      "step": 85220
    },
    {
      "epoch": 15496.363636363636,
      "grad_norm": 0.27468425035476685,
      "learning_rate": 4.6825637086579695e-07,
      "loss": 0.0013,
      "step": 85230
    },
    {
      "epoch": 15498.181818181818,
      "grad_norm": 0.14486759901046753,
      "learning_rate": 4.681402511723324e-07,
      "loss": 0.0008,
      "step": 85240
    },
    {
      "epoch": 15500.0,
      "grad_norm": 0.16080686450004578,
      "learning_rate": 4.680241332042072e-07,
      "loss": 0.001,
      "step": 85250
    },
    {
      "epoch": 15501.818181818182,
      "grad_norm": 0.15529963374137878,
      "learning_rate": 4.6790801696770964e-07,
      "loss": 0.0012,
      "step": 85260
    },
    {
      "epoch": 15503.636363636364,
      "grad_norm": 0.004929497838020325,
      "learning_rate": 4.6779190246912835e-07,
      "loss": 0.0009,
      "step": 85270
    },
    {
      "epoch": 15505.454545454546,
      "grad_norm": 0.17758825421333313,
      "learning_rate": 4.6767578971475074e-07,
      "loss": 0.0015,
      "step": 85280
    },
    {
      "epoch": 15507.272727272728,
      "grad_norm": 0.0018867304315790534,
      "learning_rate": 4.6755967871086524e-07,
      "loss": 0.0007,
      "step": 85290
    },
    {
      "epoch": 15509.09090909091,
      "grad_norm": 0.19746434688568115,
      "learning_rate": 4.674435694637597e-07,
      "loss": 0.0012,
      "step": 85300
    },
    {
      "epoch": 15510.90909090909,
      "grad_norm": 0.21307726204395294,
      "learning_rate": 4.673274619797217e-07,
      "loss": 0.001,
      "step": 85310
    },
    {
      "epoch": 15512.727272727272,
      "grad_norm": 0.0014541751006618142,
      "learning_rate": 4.672113562650393e-07,
      "loss": 0.0012,
      "step": 85320
    },
    {
      "epoch": 15514.545454545454,
      "grad_norm": 0.16158361732959747,
      "learning_rate": 4.670952523259998e-07,
      "loss": 0.0011,
      "step": 85330
    },
    {
      "epoch": 15516.363636363636,
      "grad_norm": 0.0014744822401553392,
      "learning_rate": 4.669791501688909e-07,
      "loss": 0.0009,
      "step": 85340
    },
    {
      "epoch": 15518.181818181818,
      "grad_norm": 0.001638092682696879,
      "learning_rate": 4.668630498e-07,
      "loss": 0.0012,
      "step": 85350
    },
    {
      "epoch": 15520.0,
      "grad_norm": 0.0009753108024597168,
      "learning_rate": 4.667469512256143e-07,
      "loss": 0.0012,
      "step": 85360
    },
    {
      "epoch": 15521.818181818182,
      "grad_norm": 0.0012516092974692583,
      "learning_rate": 4.66630854452021e-07,
      "loss": 0.0011,
      "step": 85370
    },
    {
      "epoch": 15523.636363636364,
      "grad_norm": 0.2107802927494049,
      "learning_rate": 4.6651475948550755e-07,
      "loss": 0.0012,
      "step": 85380
    },
    {
      "epoch": 15525.454545454546,
      "grad_norm": 0.20126873254776,
      "learning_rate": 4.663986663323606e-07,
      "loss": 0.0012,
      "step": 85390
    },
    {
      "epoch": 15527.272727272728,
      "grad_norm": 0.16920705139636993,
      "learning_rate": 4.6628257499886745e-07,
      "loss": 0.001,
      "step": 85400
    },
    {
      "epoch": 15529.09090909091,
      "grad_norm": 0.1580599546432495,
      "learning_rate": 4.6616648549131464e-07,
      "loss": 0.001,
      "step": 85410
    },
    {
      "epoch": 15530.90909090909,
      "grad_norm": 0.0005876100622117519,
      "learning_rate": 4.660503978159889e-07,
      "loss": 0.001,
      "step": 85420
    },
    {
      "epoch": 15532.727272727272,
      "grad_norm": 0.1598879098892212,
      "learning_rate": 4.659343119791772e-07,
      "loss": 0.001,
      "step": 85430
    },
    {
      "epoch": 15534.545454545454,
      "grad_norm": 0.0007708783377893269,
      "learning_rate": 4.658182279871657e-07,
      "loss": 0.0009,
      "step": 85440
    },
    {
      "epoch": 15536.363636363636,
      "grad_norm": 0.1652512401342392,
      "learning_rate": 4.657021458462408e-07,
      "loss": 0.0013,
      "step": 85450
    },
    {
      "epoch": 15538.181818181818,
      "grad_norm": 0.0008345146779902279,
      "learning_rate": 4.6558606556268943e-07,
      "loss": 0.0009,
      "step": 85460
    },
    {
      "epoch": 15540.0,
      "grad_norm": 0.2501120865345001,
      "learning_rate": 4.6546998714279705e-07,
      "loss": 0.0012,
      "step": 85470
    },
    {
      "epoch": 15541.818181818182,
      "grad_norm": 0.2603915333747864,
      "learning_rate": 4.6535391059285045e-07,
      "loss": 0.0012,
      "step": 85480
    },
    {
      "epoch": 15543.636363636364,
      "grad_norm": 0.2559800446033478,
      "learning_rate": 4.6523783591913513e-07,
      "loss": 0.0009,
      "step": 85490
    },
    {
      "epoch": 15545.454545454546,
      "grad_norm": 0.1626463532447815,
      "learning_rate": 4.6512176312793735e-07,
      "loss": 0.001,
      "step": 85500
    },
    {
      "epoch": 15545.454545454546,
      "eval_loss": 5.0533294677734375,
      "eval_runtime": 0.9533,
      "eval_samples_per_second": 10.49,
      "eval_steps_per_second": 5.245,
      "step": 85500
    },
    {
      "epoch": 15547.272727272728,
      "grad_norm": 0.002184199169278145,
      "learning_rate": 4.650056922255429e-07,
      "loss": 0.0014,
      "step": 85510
    },
    {
      "epoch": 15549.09090909091,
      "grad_norm": 0.16870209574699402,
      "learning_rate": 4.648896232182374e-07,
      "loss": 0.0009,
      "step": 85520
    },
    {
      "epoch": 15550.90909090909,
      "grad_norm": 0.2780035734176636,
      "learning_rate": 4.647735561123065e-07,
      "loss": 0.0012,
      "step": 85530
    },
    {
      "epoch": 15552.727272727272,
      "grad_norm": 0.0007818973390385509,
      "learning_rate": 4.6465749091403593e-07,
      "loss": 0.0009,
      "step": 85540
    },
    {
      "epoch": 15554.545454545454,
      "grad_norm": 0.0011550505878403783,
      "learning_rate": 4.645414276297108e-07,
      "loss": 0.0014,
      "step": 85550
    },
    {
      "epoch": 15556.363636363636,
      "grad_norm": 0.19028426706790924,
      "learning_rate": 4.644253662656167e-07,
      "loss": 0.0011,
      "step": 85560
    },
    {
      "epoch": 15558.181818181818,
      "grad_norm": 0.14187130331993103,
      "learning_rate": 4.6430930682803853e-07,
      "loss": 0.0012,
      "step": 85570
    },
    {
      "epoch": 15560.0,
      "grad_norm": 0.15966933965682983,
      "learning_rate": 4.641932493232615e-07,
      "loss": 0.001,
      "step": 85580
    },
    {
      "epoch": 15561.818181818182,
      "grad_norm": 0.16255711019039154,
      "learning_rate": 4.640771937575709e-07,
      "loss": 0.001,
      "step": 85590
    },
    {
      "epoch": 15563.636363636364,
      "grad_norm": 0.15871404111385345,
      "learning_rate": 4.639611401372513e-07,
      "loss": 0.0012,
      "step": 85600
    },
    {
      "epoch": 15565.454545454546,
      "grad_norm": 0.0007254843367263675,
      "learning_rate": 4.638450884685876e-07,
      "loss": 0.0009,
      "step": 85610
    },
    {
      "epoch": 15567.272727272728,
      "grad_norm": 0.0014965400332584977,
      "learning_rate": 4.637290387578646e-07,
      "loss": 0.0011,
      "step": 85620
    },
    {
      "epoch": 15569.09090909091,
      "grad_norm": 0.000693913025315851,
      "learning_rate": 4.636129910113667e-07,
      "loss": 0.0011,
      "step": 85630
    },
    {
      "epoch": 15570.90909090909,
      "grad_norm": 0.0009513827972114086,
      "learning_rate": 4.634969452353783e-07,
      "loss": 0.0012,
      "step": 85640
    },
    {
      "epoch": 15572.727272727272,
      "grad_norm": 0.0006796640227548778,
      "learning_rate": 4.6338090143618427e-07,
      "loss": 0.001,
      "step": 85650
    },
    {
      "epoch": 15574.545454545454,
      "grad_norm": 0.0009659905917942524,
      "learning_rate": 4.632648596200681e-07,
      "loss": 0.0009,
      "step": 85660
    },
    {
      "epoch": 15576.363636363636,
      "grad_norm": 0.1917201429605484,
      "learning_rate": 4.631488197933147e-07,
      "loss": 0.0015,
      "step": 85670
    },
    {
      "epoch": 15578.181818181818,
      "grad_norm": 0.20894205570220947,
      "learning_rate": 4.630327819622075e-07,
      "loss": 0.0011,
      "step": 85680
    },
    {
      "epoch": 15580.0,
      "grad_norm": 0.0007682388531975448,
      "learning_rate": 4.629167461330308e-07,
      "loss": 0.001,
      "step": 85690
    },
    {
      "epoch": 15581.818181818182,
      "grad_norm": 0.002417981857433915,
      "learning_rate": 4.628007123120684e-07,
      "loss": 0.0008,
      "step": 85700
    },
    {
      "epoch": 15583.636363636364,
      "grad_norm": 0.27285489439964294,
      "learning_rate": 4.6268468050560386e-07,
      "loss": 0.0013,
      "step": 85710
    },
    {
      "epoch": 15585.454545454546,
      "grad_norm": 0.26548030972480774,
      "learning_rate": 4.625686507199209e-07,
      "loss": 0.0013,
      "step": 85720
    },
    {
      "epoch": 15587.272727272728,
      "grad_norm": 0.0011674085399135947,
      "learning_rate": 4.624526229613031e-07,
      "loss": 0.0007,
      "step": 85730
    },
    {
      "epoch": 15589.09090909091,
      "grad_norm": 0.19729290902614594,
      "learning_rate": 4.623365972360337e-07,
      "loss": 0.0013,
      "step": 85740
    },
    {
      "epoch": 15590.90909090909,
      "grad_norm": 0.17024894058704376,
      "learning_rate": 4.6222057355039606e-07,
      "loss": 0.0011,
      "step": 85750
    },
    {
      "epoch": 15592.727272727272,
      "grad_norm": 0.0006240705261006951,
      "learning_rate": 4.6210455191067327e-07,
      "loss": 0.0009,
      "step": 85760
    },
    {
      "epoch": 15594.545454545454,
      "grad_norm": 0.0014825682155787945,
      "learning_rate": 4.619885323231483e-07,
      "loss": 0.0014,
      "step": 85770
    },
    {
      "epoch": 15596.363636363636,
      "grad_norm": 0.20025645196437836,
      "learning_rate": 4.618725147941045e-07,
      "loss": 0.0012,
      "step": 85780
    },
    {
      "epoch": 15598.181818181818,
      "grad_norm": 0.18529783189296722,
      "learning_rate": 4.617564993298243e-07,
      "loss": 0.0012,
      "step": 85790
    },
    {
      "epoch": 15600.0,
      "grad_norm": 0.22369734942913055,
      "learning_rate": 4.6164048593659065e-07,
      "loss": 0.001,
      "step": 85800
    },
    {
      "epoch": 15601.818181818182,
      "grad_norm": 0.17200574278831482,
      "learning_rate": 4.6152447462068615e-07,
      "loss": 0.0012,
      "step": 85810
    },
    {
      "epoch": 15603.636363636364,
      "grad_norm": 0.0007838070159777999,
      "learning_rate": 4.614084653883932e-07,
      "loss": 0.0007,
      "step": 85820
    },
    {
      "epoch": 15605.454545454546,
      "grad_norm": 0.17932544648647308,
      "learning_rate": 4.6129245824599424e-07,
      "loss": 0.0013,
      "step": 85830
    },
    {
      "epoch": 15607.272727272728,
      "grad_norm": 0.17097492516040802,
      "learning_rate": 4.611764531997715e-07,
      "loss": 0.001,
      "step": 85840
    },
    {
      "epoch": 15609.09090909091,
      "grad_norm": 0.0009995991131290793,
      "learning_rate": 4.6106045025600704e-07,
      "loss": 0.0011,
      "step": 85850
    },
    {
      "epoch": 15610.90909090909,
      "grad_norm": 0.2725999355316162,
      "learning_rate": 4.609444494209833e-07,
      "loss": 0.0011,
      "step": 85860
    },
    {
      "epoch": 15612.727272727272,
      "grad_norm": 0.28990310430526733,
      "learning_rate": 4.608284507009818e-07,
      "loss": 0.0013,
      "step": 85870
    },
    {
      "epoch": 15614.545454545454,
      "grad_norm": 0.16555652022361755,
      "learning_rate": 4.607124541022845e-07,
      "loss": 0.0009,
      "step": 85880
    },
    {
      "epoch": 15616.363636363636,
      "grad_norm": 0.20861726999282837,
      "learning_rate": 4.6059645963117327e-07,
      "loss": 0.0012,
      "step": 85890
    },
    {
      "epoch": 15618.181818181818,
      "grad_norm": 0.2090245485305786,
      "learning_rate": 4.6048046729392944e-07,
      "loss": 0.001,
      "step": 85900
    },
    {
      "epoch": 15620.0,
      "grad_norm": 0.007412430830299854,
      "learning_rate": 4.6036447709683457e-07,
      "loss": 0.001,
      "step": 85910
    },
    {
      "epoch": 15621.818181818182,
      "grad_norm": 0.1607123166322708,
      "learning_rate": 4.6024848904617016e-07,
      "loss": 0.0009,
      "step": 85920
    },
    {
      "epoch": 15623.636363636364,
      "grad_norm": 0.0013497209874913096,
      "learning_rate": 4.601325031482172e-07,
      "loss": 0.0012,
      "step": 85930
    },
    {
      "epoch": 15625.454545454546,
      "grad_norm": 0.0421457476913929,
      "learning_rate": 4.6001651940925714e-07,
      "loss": 0.0012,
      "step": 85940
    },
    {
      "epoch": 15627.272727272728,
      "grad_norm": 0.0007862562779337168,
      "learning_rate": 4.599005378355706e-07,
      "loss": 0.001,
      "step": 85950
    },
    {
      "epoch": 15629.09090909091,
      "grad_norm": 0.0007978730136528611,
      "learning_rate": 4.597845584334386e-07,
      "loss": 0.0011,
      "step": 85960
    },
    {
      "epoch": 15630.90909090909,
      "grad_norm": 0.17179034650325775,
      "learning_rate": 4.5966858120914215e-07,
      "loss": 0.001,
      "step": 85970
    },
    {
      "epoch": 15632.727272727272,
      "grad_norm": 0.19667281210422516,
      "learning_rate": 4.595526061689616e-07,
      "loss": 0.0012,
      "step": 85980
    },
    {
      "epoch": 15634.545454545454,
      "grad_norm": 0.1737595796585083,
      "learning_rate": 4.594366333191777e-07,
      "loss": 0.001,
      "step": 85990
    },
    {
      "epoch": 15636.363636363636,
      "grad_norm": 0.0010600893292576075,
      "learning_rate": 4.593206626660709e-07,
      "loss": 0.001,
      "step": 86000
    },
    {
      "epoch": 15636.363636363636,
      "eval_loss": 5.06578254699707,
      "eval_runtime": 0.9504,
      "eval_samples_per_second": 10.522,
      "eval_steps_per_second": 5.261,
      "step": 86000
    },
    {
      "epoch": 15638.181818181818,
      "grad_norm": 0.0012183886719867587,
      "learning_rate": 4.592046942159212e-07,
      "loss": 0.001,
      "step": 86010
    },
    {
      "epoch": 15640.0,
      "grad_norm": 0.0009462644811719656,
      "learning_rate": 4.5908872797500915e-07,
      "loss": 0.0012,
      "step": 86020
    },
    {
      "epoch": 15641.818181818182,
      "grad_norm": 0.1712203472852707,
      "learning_rate": 4.589727639496144e-07,
      "loss": 0.001,
      "step": 86030
    },
    {
      "epoch": 15643.636363636364,
      "grad_norm": 0.20799307525157928,
      "learning_rate": 4.588568021460171e-07,
      "loss": 0.0014,
      "step": 86040
    },
    {
      "epoch": 15645.454545454546,
      "grad_norm": 0.21898312866687775,
      "learning_rate": 4.5874084257049736e-07,
      "loss": 0.0011,
      "step": 86050
    },
    {
      "epoch": 15647.272727272728,
      "grad_norm": 0.16261136531829834,
      "learning_rate": 4.5862488522933433e-07,
      "loss": 0.001,
      "step": 86060
    },
    {
      "epoch": 15649.09090909091,
      "grad_norm": 0.19690877199172974,
      "learning_rate": 4.5850893012880796e-07,
      "loss": 0.001,
      "step": 86070
    },
    {
      "epoch": 15650.90909090909,
      "grad_norm": 0.0006444408791139722,
      "learning_rate": 4.583929772751977e-07,
      "loss": 0.0012,
      "step": 86080
    },
    {
      "epoch": 15652.727272727272,
      "grad_norm": 0.19343402981758118,
      "learning_rate": 4.5827702667478267e-07,
      "loss": 0.001,
      "step": 86090
    },
    {
      "epoch": 15654.545454545454,
      "grad_norm": 0.0005790380528196692,
      "learning_rate": 4.5816107833384233e-07,
      "loss": 0.0012,
      "step": 86100
    },
    {
      "epoch": 15656.363636363636,
      "grad_norm": 0.2045355588197708,
      "learning_rate": 4.580451322586555e-07,
      "loss": 0.0012,
      "step": 86110
    },
    {
      "epoch": 15658.181818181818,
      "grad_norm": 0.0005881036631762981,
      "learning_rate": 4.5792918845550133e-07,
      "loss": 0.0009,
      "step": 86120
    },
    {
      "epoch": 15660.0,
      "grad_norm": 0.1655241996049881,
      "learning_rate": 4.5781324693065873e-07,
      "loss": 0.0012,
      "step": 86130
    },
    {
      "epoch": 15661.818181818182,
      "grad_norm": 0.2861805856227875,
      "learning_rate": 4.5769730769040623e-07,
      "loss": 0.0012,
      "step": 86140
    },
    {
      "epoch": 15663.636363636364,
      "grad_norm": 0.20919936895370483,
      "learning_rate": 4.575813707410223e-07,
      "loss": 0.001,
      "step": 86150
    },
    {
      "epoch": 15665.454545454546,
      "grad_norm": 0.0007908041588962078,
      "learning_rate": 4.5746543608878593e-07,
      "loss": 0.0009,
      "step": 86160
    },
    {
      "epoch": 15667.272727272728,
      "grad_norm": 0.000836806430015713,
      "learning_rate": 4.5734950373997513e-07,
      "loss": 0.001,
      "step": 86170
    },
    {
      "epoch": 15669.09090909091,
      "grad_norm": 0.2622644305229187,
      "learning_rate": 4.5723357370086816e-07,
      "loss": 0.0012,
      "step": 86180
    },
    {
      "epoch": 15670.90909090909,
      "grad_norm": 0.15476486086845398,
      "learning_rate": 4.5711764597774304e-07,
      "loss": 0.0012,
      "step": 86190
    },
    {
      "epoch": 15672.727272727272,
      "grad_norm": 0.19283780455589294,
      "learning_rate": 4.5700172057687785e-07,
      "loss": 0.0012,
      "step": 86200
    },
    {
      "epoch": 15674.545454545454,
      "grad_norm": 0.0013908448163419962,
      "learning_rate": 4.5688579750455054e-07,
      "loss": 0.0011,
      "step": 86210
    },
    {
      "epoch": 15676.363636363636,
      "grad_norm": 0.0007004640065133572,
      "learning_rate": 4.567698767670386e-07,
      "loss": 0.0011,
      "step": 86220
    },
    {
      "epoch": 15678.181818181818,
      "grad_norm": 0.18647050857543945,
      "learning_rate": 4.566539583706196e-07,
      "loss": 0.0012,
      "step": 86230
    },
    {
      "epoch": 15680.0,
      "grad_norm": 0.0007177497609518468,
      "learning_rate": 4.5653804232157143e-07,
      "loss": 0.001,
      "step": 86240
    },
    {
      "epoch": 15681.818181818182,
      "grad_norm": 0.16166914999485016,
      "learning_rate": 4.5642212862617085e-07,
      "loss": 0.0011,
      "step": 86250
    },
    {
      "epoch": 15683.636363636364,
      "grad_norm": 0.0008149201166816056,
      "learning_rate": 4.5630621729069554e-07,
      "loss": 0.0009,
      "step": 86260
    },
    {
      "epoch": 15685.454545454546,
      "grad_norm": 0.0011724175419658422,
      "learning_rate": 4.561903083214224e-07,
      "loss": 0.001,
      "step": 86270
    },
    {
      "epoch": 15687.272727272728,
      "grad_norm": 0.19651365280151367,
      "learning_rate": 4.560744017246284e-07,
      "loss": 0.0014,
      "step": 86280
    },
    {
      "epoch": 15689.09090909091,
      "grad_norm": 0.20569349825382233,
      "learning_rate": 4.559584975065904e-07,
      "loss": 0.001,
      "step": 86290
    },
    {
      "epoch": 15690.90909090909,
      "grad_norm": 0.1586337387561798,
      "learning_rate": 4.5584259567358503e-07,
      "loss": 0.001,
      "step": 86300
    },
    {
      "epoch": 15692.727272727272,
      "grad_norm": 0.0006557663436979055,
      "learning_rate": 4.5572669623188887e-07,
      "loss": 0.0009,
      "step": 86310
    },
    {
      "epoch": 15694.545454545454,
      "grad_norm": 0.1809086799621582,
      "learning_rate": 4.5561079918777855e-07,
      "loss": 0.0014,
      "step": 86320
    },
    {
      "epoch": 15696.363636363636,
      "grad_norm": 0.15750007331371307,
      "learning_rate": 4.554949045475301e-07,
      "loss": 0.0011,
      "step": 86330
    },
    {
      "epoch": 15698.181818181818,
      "grad_norm": 0.2519005239009857,
      "learning_rate": 4.5537901231741973e-07,
      "loss": 0.0013,
      "step": 86340
    },
    {
      "epoch": 15700.0,
      "grad_norm": 0.20861658453941345,
      "learning_rate": 4.552631225037239e-07,
      "loss": 0.0008,
      "step": 86350
    },
    {
      "epoch": 15701.818181818182,
      "grad_norm": 0.1917775720357895,
      "learning_rate": 4.5514723511271793e-07,
      "loss": 0.0012,
      "step": 86360
    },
    {
      "epoch": 15703.636363636364,
      "grad_norm": 0.006112138275057077,
      "learning_rate": 4.5503135015067805e-07,
      "loss": 0.0012,
      "step": 86370
    },
    {
      "epoch": 15705.454545454546,
      "grad_norm": 0.15545794367790222,
      "learning_rate": 4.549154676238797e-07,
      "loss": 0.0008,
      "step": 86380
    },
    {
      "epoch": 15707.272727272728,
      "grad_norm": 0.01688198186457157,
      "learning_rate": 4.5479958753859856e-07,
      "loss": 0.0013,
      "step": 86390
    },
    {
      "epoch": 15709.09090909091,
      "grad_norm": 0.0007744482718408108,
      "learning_rate": 4.5468370990110997e-07,
      "loss": 0.0009,
      "step": 86400
    },
    {
      "epoch": 15710.90909090909,
      "grad_norm": 0.2585170269012451,
      "learning_rate": 4.545678347176891e-07,
      "loss": 0.0011,
      "step": 86410
    },
    {
      "epoch": 15712.727272727272,
      "grad_norm": 0.16805973649024963,
      "learning_rate": 4.54451961994611e-07,
      "loss": 0.0012,
      "step": 86420
    },
    {
      "epoch": 15714.545454545454,
      "grad_norm": 0.16298194229602814,
      "learning_rate": 4.543360917381512e-07,
      "loss": 0.0013,
      "step": 86430
    },
    {
      "epoch": 15716.363636363636,
      "grad_norm": 0.0010934831807389855,
      "learning_rate": 4.5422022395458375e-07,
      "loss": 0.0006,
      "step": 86440
    },
    {
      "epoch": 15718.181818181818,
      "grad_norm": 0.000649193418212235,
      "learning_rate": 4.541043586501842e-07,
      "loss": 0.0011,
      "step": 86450
    },
    {
      "epoch": 15720.0,
      "grad_norm": 0.2666066884994507,
      "learning_rate": 4.5398849583122646e-07,
      "loss": 0.0012,
      "step": 86460
    },
    {
      "epoch": 15721.818181818182,
      "grad_norm": 0.19887860119342804,
      "learning_rate": 4.538726355039853e-07,
      "loss": 0.0011,
      "step": 86470
    },
    {
      "epoch": 15723.636363636364,
      "grad_norm": 0.19403311610221863,
      "learning_rate": 4.5375677767473513e-07,
      "loss": 0.001,
      "step": 86480
    },
    {
      "epoch": 15725.454545454546,
      "grad_norm": 0.0013230516342446208,
      "learning_rate": 4.5364092234974995e-07,
      "loss": 0.001,
      "step": 86490
    },
    {
      "epoch": 15727.272727272728,
      "grad_norm": 0.20150402188301086,
      "learning_rate": 4.535250695353039e-07,
      "loss": 0.0013,
      "step": 86500
    },
    {
      "epoch": 15727.272727272728,
      "eval_loss": 4.966455459594727,
      "eval_runtime": 0.9507,
      "eval_samples_per_second": 10.518,
      "eval_steps_per_second": 5.259,
      "step": 86500
    },
    {
      "epoch": 15729.09090909091,
      "grad_norm": 0.19502495229244232,
      "learning_rate": 4.5340921923767094e-07,
      "loss": 0.0012,
      "step": 86510
    },
    {
      "epoch": 15730.90909090909,
      "grad_norm": 0.19895438849925995,
      "learning_rate": 4.5329337146312474e-07,
      "loss": 0.0009,
      "step": 86520
    },
    {
      "epoch": 15732.727272727272,
      "grad_norm": 0.25995272397994995,
      "learning_rate": 4.531775262179389e-07,
      "loss": 0.0013,
      "step": 86530
    },
    {
      "epoch": 15734.545454545454,
      "grad_norm": 0.19565734267234802,
      "learning_rate": 4.5306168350838735e-07,
      "loss": 0.0009,
      "step": 86540
    },
    {
      "epoch": 15736.363636363636,
      "grad_norm": 0.19488514959812164,
      "learning_rate": 4.529458433407428e-07,
      "loss": 0.0013,
      "step": 86550
    },
    {
      "epoch": 15738.181818181818,
      "grad_norm": 0.19433832168579102,
      "learning_rate": 4.52830005721279e-07,
      "loss": 0.0011,
      "step": 86560
    },
    {
      "epoch": 15740.0,
      "grad_norm": 0.007910137996077538,
      "learning_rate": 4.527141706562688e-07,
      "loss": 0.001,
      "step": 86570
    },
    {
      "epoch": 15741.818181818182,
      "grad_norm": 0.16456210613250732,
      "learning_rate": 4.5259833815198526e-07,
      "loss": 0.0011,
      "step": 86580
    },
    {
      "epoch": 15743.636363636364,
      "grad_norm": 0.17732389271259308,
      "learning_rate": 4.5248250821470123e-07,
      "loss": 0.0012,
      "step": 86590
    },
    {
      "epoch": 15745.454545454546,
      "grad_norm": 0.1951044499874115,
      "learning_rate": 4.5236668085068926e-07,
      "loss": 0.0012,
      "step": 86600
    },
    {
      "epoch": 15747.272727272728,
      "grad_norm": 0.19667670130729675,
      "learning_rate": 4.522508560662218e-07,
      "loss": 0.0009,
      "step": 86610
    },
    {
      "epoch": 15749.09090909091,
      "grad_norm": 0.20654639601707458,
      "learning_rate": 4.5213503386757175e-07,
      "loss": 0.0012,
      "step": 86620
    },
    {
      "epoch": 15750.90909090909,
      "grad_norm": 0.1945764273405075,
      "learning_rate": 4.5201921426101066e-07,
      "loss": 0.0011,
      "step": 86630
    },
    {
      "epoch": 15752.727272727272,
      "grad_norm": 0.18295086920261383,
      "learning_rate": 4.5190339725281133e-07,
      "loss": 0.001,
      "step": 86640
    },
    {
      "epoch": 15754.545454545454,
      "grad_norm": 0.0012678606435656548,
      "learning_rate": 4.517875828492451e-07,
      "loss": 0.001,
      "step": 86650
    },
    {
      "epoch": 15756.363636363636,
      "grad_norm": 0.0010566985001787543,
      "learning_rate": 4.516717710565842e-07,
      "loss": 0.0009,
      "step": 86660
    },
    {
      "epoch": 15758.181818181818,
      "grad_norm": 0.20748721063137054,
      "learning_rate": 4.5155596188110047e-07,
      "loss": 0.0013,
      "step": 86670
    },
    {
      "epoch": 15760.0,
      "grad_norm": 0.000525997718796134,
      "learning_rate": 4.51440155329065e-07,
      "loss": 0.001,
      "step": 86680
    },
    {
      "epoch": 15761.818181818182,
      "grad_norm": 0.0007843004423193634,
      "learning_rate": 4.513243514067495e-07,
      "loss": 0.0011,
      "step": 86690
    },
    {
      "epoch": 15763.636363636364,
      "grad_norm": 0.0007369223749265075,
      "learning_rate": 4.512085501204253e-07,
      "loss": 0.0015,
      "step": 86700
    },
    {
      "epoch": 15765.454545454546,
      "grad_norm": 0.0005796722834929824,
      "learning_rate": 4.510927514763633e-07,
      "loss": 0.0009,
      "step": 86710
    },
    {
      "epoch": 15767.272727272728,
      "grad_norm": 0.008335391990840435,
      "learning_rate": 4.509769554808347e-07,
      "loss": 0.0011,
      "step": 86720
    },
    {
      "epoch": 15769.09090909091,
      "grad_norm": 0.22018089890480042,
      "learning_rate": 4.5086116214011014e-07,
      "loss": 0.0012,
      "step": 86730
    },
    {
      "epoch": 15770.90909090909,
      "grad_norm": 0.19970038533210754,
      "learning_rate": 4.507453714604603e-07,
      "loss": 0.001,
      "step": 86740
    },
    {
      "epoch": 15772.727272727272,
      "grad_norm": 0.0009440717403776944,
      "learning_rate": 4.5062958344815606e-07,
      "loss": 0.0012,
      "step": 86750
    },
    {
      "epoch": 15774.545454545454,
      "grad_norm": 0.0029910660814493895,
      "learning_rate": 4.505137981094675e-07,
      "loss": 0.0008,
      "step": 86760
    },
    {
      "epoch": 15776.363636363636,
      "grad_norm": 0.0006598430336453021,
      "learning_rate": 4.50398015450665e-07,
      "loss": 0.0012,
      "step": 86770
    },
    {
      "epoch": 15778.181818181818,
      "grad_norm": 0.20876623690128326,
      "learning_rate": 4.502822354780188e-07,
      "loss": 0.0011,
      "step": 86780
    },
    {
      "epoch": 15780.0,
      "grad_norm": 0.1944461166858673,
      "learning_rate": 4.501664581977986e-07,
      "loss": 0.001,
      "step": 86790
    },
    {
      "epoch": 15781.818181818182,
      "grad_norm": 0.0008765468955971301,
      "learning_rate": 4.500506836162745e-07,
      "loss": 0.001,
      "step": 86800
    },
    {
      "epoch": 15783.636363636364,
      "grad_norm": 0.0006897146813571453,
      "learning_rate": 4.499349117397159e-07,
      "loss": 0.001,
      "step": 86810
    },
    {
      "epoch": 15785.454545454546,
      "grad_norm": 0.000662234437186271,
      "learning_rate": 4.4981914257439247e-07,
      "loss": 0.0012,
      "step": 86820
    },
    {
      "epoch": 15787.272727272728,
      "grad_norm": 0.1803172379732132,
      "learning_rate": 4.497033761265739e-07,
      "loss": 0.0015,
      "step": 86830
    },
    {
      "epoch": 15789.09090909091,
      "grad_norm": 0.0011034804629161954,
      "learning_rate": 4.4958761240252886e-07,
      "loss": 0.0009,
      "step": 86840
    },
    {
      "epoch": 15790.90909090909,
      "grad_norm": 0.22300341725349426,
      "learning_rate": 4.494718514085268e-07,
      "loss": 0.0012,
      "step": 86850
    },
    {
      "epoch": 15792.727272727272,
      "grad_norm": 0.0011037512449547648,
      "learning_rate": 4.4935609315083667e-07,
      "loss": 0.0012,
      "step": 86860
    },
    {
      "epoch": 15794.545454545454,
      "grad_norm": 0.000878529972396791,
      "learning_rate": 4.492403376357271e-07,
      "loss": 0.0009,
      "step": 86870
    },
    {
      "epoch": 15796.363636363636,
      "grad_norm": 0.001005010213702917,
      "learning_rate": 4.4912458486946687e-07,
      "loss": 0.0012,
      "step": 86880
    },
    {
      "epoch": 15798.181818181818,
      "grad_norm": 0.2203114628791809,
      "learning_rate": 4.490088348583245e-07,
      "loss": 0.001,
      "step": 86890
    },
    {
      "epoch": 15800.0,
      "grad_norm": 0.27582022547721863,
      "learning_rate": 4.488930876085682e-07,
      "loss": 0.001,
      "step": 86900
    },
    {
      "epoch": 15801.818181818182,
      "grad_norm": 0.15366420149803162,
      "learning_rate": 4.487773431264664e-07,
      "loss": 0.0011,
      "step": 86910
    },
    {
      "epoch": 15803.636363636364,
      "grad_norm": 0.0005843125400133431,
      "learning_rate": 4.486616014182868e-07,
      "loss": 0.0013,
      "step": 86920
    },
    {
      "epoch": 15805.454545454546,
      "grad_norm": 0.0007928971317596734,
      "learning_rate": 4.485458624902974e-07,
      "loss": 0.0011,
      "step": 86930
    },
    {
      "epoch": 15807.272727272728,
      "grad_norm": 0.20725534856319427,
      "learning_rate": 4.4843012634876643e-07,
      "loss": 0.0011,
      "step": 86940
    },
    {
      "epoch": 15809.09090909091,
      "grad_norm": 0.16423280537128448,
      "learning_rate": 4.4831439299996074e-07,
      "loss": 0.0012,
      "step": 86950
    },
    {
      "epoch": 15810.90909090909,
      "grad_norm": 0.15778303146362305,
      "learning_rate": 4.481986624501483e-07,
      "loss": 0.001,
      "step": 86960
    },
    {
      "epoch": 15812.727272727272,
      "grad_norm": 0.15814317762851715,
      "learning_rate": 4.4808293470559637e-07,
      "loss": 0.0011,
      "step": 86970
    },
    {
      "epoch": 15814.545454545454,
      "grad_norm": 0.19470715522766113,
      "learning_rate": 4.4796720977257186e-07,
      "loss": 0.0009,
      "step": 86980
    },
    {
      "epoch": 15816.363636363636,
      "grad_norm": 0.0010526446858420968,
      "learning_rate": 4.4785148765734195e-07,
      "loss": 0.0014,
      "step": 86990
    },
    {
      "epoch": 15818.181818181818,
      "grad_norm": 0.000629943038802594,
      "learning_rate": 4.477357683661733e-07,
      "loss": 0.0009,
      "step": 87000
    },
    {
      "epoch": 15818.181818181818,
      "eval_loss": 5.036818504333496,
      "eval_runtime": 0.9509,
      "eval_samples_per_second": 10.517,
      "eval_steps_per_second": 5.258,
      "step": 87000
    },
    {
      "epoch": 15820.0,
      "grad_norm": 0.0006380246486514807,
      "learning_rate": 4.4762005190533264e-07,
      "loss": 0.0012,
      "step": 87010
    },
    {
      "epoch": 15821.818181818182,
      "grad_norm": 0.20783735811710358,
      "learning_rate": 4.4750433828108684e-07,
      "loss": 0.001,
      "step": 87020
    },
    {
      "epoch": 15823.636363636364,
      "grad_norm": 0.25869789719581604,
      "learning_rate": 4.473886274997017e-07,
      "loss": 0.0012,
      "step": 87030
    },
    {
      "epoch": 15825.454545454546,
      "grad_norm": 0.20491120219230652,
      "learning_rate": 4.4727291956744395e-07,
      "loss": 0.0013,
      "step": 87040
    },
    {
      "epoch": 15827.272727272728,
      "grad_norm": 0.19201846420764923,
      "learning_rate": 4.4715721449057945e-07,
      "loss": 0.001,
      "step": 87050
    },
    {
      "epoch": 15829.09090909091,
      "grad_norm": 0.1903335452079773,
      "learning_rate": 4.470415122753741e-07,
      "loss": 0.0012,
      "step": 87060
    },
    {
      "epoch": 15830.90909090909,
      "grad_norm": 0.0005333927110768855,
      "learning_rate": 4.469258129280938e-07,
      "loss": 0.001,
      "step": 87070
    },
    {
      "epoch": 15832.727272727272,
      "grad_norm": 0.25991755723953247,
      "learning_rate": 4.4681011645500393e-07,
      "loss": 0.001,
      "step": 87080
    },
    {
      "epoch": 15834.545454545454,
      "grad_norm": 0.1652100682258606,
      "learning_rate": 4.4669442286237e-07,
      "loss": 0.0011,
      "step": 87090
    },
    {
      "epoch": 15836.363636363636,
      "grad_norm": 0.2529571056365967,
      "learning_rate": 4.465787321564576e-07,
      "loss": 0.0012,
      "step": 87100
    },
    {
      "epoch": 15838.181818181818,
      "grad_norm": 0.005634129047393799,
      "learning_rate": 4.4646304434353144e-07,
      "loss": 0.001,
      "step": 87110
    },
    {
      "epoch": 15840.0,
      "grad_norm": 0.0006751147448085248,
      "learning_rate": 4.4634735942985665e-07,
      "loss": 0.0009,
      "step": 87120
    },
    {
      "epoch": 15841.818181818182,
      "grad_norm": 0.20647597312927246,
      "learning_rate": 4.462316774216983e-07,
      "loss": 0.001,
      "step": 87130
    },
    {
      "epoch": 15843.636363636364,
      "grad_norm": 0.20444142818450928,
      "learning_rate": 4.4611599832532065e-07,
      "loss": 0.001,
      "step": 87140
    },
    {
      "epoch": 15845.454545454546,
      "grad_norm": 0.264212965965271,
      "learning_rate": 4.4600032214698854e-07,
      "loss": 0.0014,
      "step": 87150
    },
    {
      "epoch": 15847.272727272728,
      "grad_norm": 0.0011507999151945114,
      "learning_rate": 4.458846488929662e-07,
      "loss": 0.0009,
      "step": 87160
    },
    {
      "epoch": 15849.09090909091,
      "grad_norm": 0.000614117830991745,
      "learning_rate": 4.4576897856951785e-07,
      "loss": 0.0012,
      "step": 87170
    },
    {
      "epoch": 15850.90909090909,
      "grad_norm": 0.19258283078670502,
      "learning_rate": 4.456533111829075e-07,
      "loss": 0.0012,
      "step": 87180
    },
    {
      "epoch": 15852.727272727272,
      "grad_norm": 0.000829501252155751,
      "learning_rate": 4.45537646739399e-07,
      "loss": 0.001,
      "step": 87190
    },
    {
      "epoch": 15854.545454545454,
      "grad_norm": 0.2094281017780304,
      "learning_rate": 4.4542198524525593e-07,
      "loss": 0.0012,
      "step": 87200
    },
    {
      "epoch": 15856.363636363636,
      "grad_norm": 0.0006640040664933622,
      "learning_rate": 4.4530632670674234e-07,
      "loss": 0.0009,
      "step": 87210
    },
    {
      "epoch": 15858.181818181818,
      "grad_norm": 0.0006528649246320128,
      "learning_rate": 4.4519067113012097e-07,
      "loss": 0.0011,
      "step": 87220
    },
    {
      "epoch": 15860.0,
      "grad_norm": 0.16787120699882507,
      "learning_rate": 4.4507501852165545e-07,
      "loss": 0.0012,
      "step": 87230
    },
    {
      "epoch": 15861.818181818182,
      "grad_norm": 0.16965612769126892,
      "learning_rate": 4.4495936888760897e-07,
      "loss": 0.001,
      "step": 87240
    },
    {
      "epoch": 15863.636363636364,
      "grad_norm": 0.0007556092459708452,
      "learning_rate": 4.448437222342441e-07,
      "loss": 0.0011,
      "step": 87250
    },
    {
      "epoch": 15865.454545454546,
      "grad_norm": 0.0006539634196087718,
      "learning_rate": 4.447280785678239e-07,
      "loss": 0.0011,
      "step": 87260
    },
    {
      "epoch": 15867.272727272728,
      "grad_norm": 0.2453436404466629,
      "learning_rate": 4.446124378946107e-07,
      "loss": 0.0014,
      "step": 87270
    },
    {
      "epoch": 15869.09090909091,
      "grad_norm": 0.1637643575668335,
      "learning_rate": 4.44496800220867e-07,
      "loss": 0.001,
      "step": 87280
    },
    {
      "epoch": 15870.90909090909,
      "grad_norm": 0.28437376022338867,
      "learning_rate": 4.443811655528553e-07,
      "loss": 0.001,
      "step": 87290
    },
    {
      "epoch": 15872.727272727272,
      "grad_norm": 0.0012078676372766495,
      "learning_rate": 4.442655338968373e-07,
      "loss": 0.0012,
      "step": 87300
    },
    {
      "epoch": 15874.545454545454,
      "grad_norm": 0.20491573214530945,
      "learning_rate": 4.441499052590751e-07,
      "loss": 0.0012,
      "step": 87310
    },
    {
      "epoch": 15876.363636363636,
      "grad_norm": 0.28061965107917786,
      "learning_rate": 4.4403427964583086e-07,
      "loss": 0.0011,
      "step": 87320
    },
    {
      "epoch": 15878.181818181818,
      "grad_norm": 0.0007088736747391522,
      "learning_rate": 4.439186570633655e-07,
      "loss": 0.0009,
      "step": 87330
    },
    {
      "epoch": 15880.0,
      "grad_norm": 0.20815594494342804,
      "learning_rate": 4.438030375179411e-07,
      "loss": 0.0012,
      "step": 87340
    },
    {
      "epoch": 15881.818181818182,
      "grad_norm": 0.0005671959952451289,
      "learning_rate": 4.436874210158186e-07,
      "loss": 0.0011,
      "step": 87350
    },
    {
      "epoch": 15883.636363636364,
      "grad_norm": 0.20744459331035614,
      "learning_rate": 4.435718075632591e-07,
      "loss": 0.0012,
      "step": 87360
    },
    {
      "epoch": 15885.454545454546,
      "grad_norm": 0.16336441040039062,
      "learning_rate": 4.434561971665238e-07,
      "loss": 0.0011,
      "step": 87370
    },
    {
      "epoch": 15887.272727272728,
      "grad_norm": 0.15778981149196625,
      "learning_rate": 4.433405898318733e-07,
      "loss": 0.0012,
      "step": 87380
    },
    {
      "epoch": 15889.09090909091,
      "grad_norm": 0.19729329645633698,
      "learning_rate": 4.432249855655681e-07,
      "loss": 0.001,
      "step": 87390
    },
    {
      "epoch": 15890.90909090909,
      "grad_norm": 0.15420043468475342,
      "learning_rate": 4.4310938437386916e-07,
      "loss": 0.001,
      "step": 87400
    },
    {
      "epoch": 15892.727272727272,
      "grad_norm": 0.0007729685166850686,
      "learning_rate": 4.429937862630361e-07,
      "loss": 0.001,
      "step": 87410
    },
    {
      "epoch": 15894.545454545454,
      "grad_norm": 0.24063152074813843,
      "learning_rate": 4.428781912393298e-07,
      "loss": 0.001,
      "step": 87420
    },
    {
      "epoch": 15896.363636363636,
      "grad_norm": 0.000596787896938622,
      "learning_rate": 4.4276259930900935e-07,
      "loss": 0.0009,
      "step": 87430
    },
    {
      "epoch": 15898.181818181818,
      "grad_norm": 0.0010022976202890277,
      "learning_rate": 4.4264701047833514e-07,
      "loss": 0.0012,
      "step": 87440
    },
    {
      "epoch": 15900.0,
      "grad_norm": 0.0005432976176962256,
      "learning_rate": 4.4253142475356674e-07,
      "loss": 0.0012,
      "step": 87450
    },
    {
      "epoch": 15901.818181818182,
      "grad_norm": 0.0006987904780544341,
      "learning_rate": 4.424158421409634e-07,
      "loss": 0.0012,
      "step": 87460
    },
    {
      "epoch": 15903.636363636364,
      "grad_norm": 0.20936468243598938,
      "learning_rate": 4.423002626467845e-07,
      "loss": 0.001,
      "step": 87470
    },
    {
      "epoch": 15905.454545454546,
      "grad_norm": 0.19285516440868378,
      "learning_rate": 4.4218468627728927e-07,
      "loss": 0.001,
      "step": 87480
    },
    {
      "epoch": 15907.272727272728,
      "grad_norm": 0.20881645381450653,
      "learning_rate": 4.4206911303873646e-07,
      "loss": 0.0012,
      "step": 87490
    },
    {
      "epoch": 15909.09090909091,
      "grad_norm": 0.0007256991812027991,
      "learning_rate": 4.419535429373848e-07,
      "loss": 0.001,
      "step": 87500
    },
    {
      "epoch": 15909.09090909091,
      "eval_loss": 4.999173641204834,
      "eval_runtime": 0.9548,
      "eval_samples_per_second": 10.474,
      "eval_steps_per_second": 5.237,
      "step": 87500
    },
    {
      "epoch": 15910.90909090909,
      "grad_norm": 0.15532855689525604,
      "learning_rate": 4.4183797597949334e-07,
      "loss": 0.001,
      "step": 87510
    },
    {
      "epoch": 15912.727272727272,
      "grad_norm": 0.19601601362228394,
      "learning_rate": 4.417224121713199e-07,
      "loss": 0.0012,
      "step": 87520
    },
    {
      "epoch": 15914.545454545454,
      "grad_norm": 0.148941308259964,
      "learning_rate": 4.416068515191234e-07,
      "loss": 0.0012,
      "step": 87530
    },
    {
      "epoch": 15916.363636363636,
      "grad_norm": 0.19494174420833588,
      "learning_rate": 4.414912940291613e-07,
      "loss": 0.0012,
      "step": 87540
    },
    {
      "epoch": 15918.181818181818,
      "grad_norm": 0.000835293554700911,
      "learning_rate": 4.413757397076919e-07,
      "loss": 0.001,
      "step": 87550
    },
    {
      "epoch": 15920.0,
      "grad_norm": 0.000846803595777601,
      "learning_rate": 4.412601885609729e-07,
      "loss": 0.0012,
      "step": 87560
    },
    {
      "epoch": 15921.818181818182,
      "grad_norm": 0.0009754933998920023,
      "learning_rate": 4.411446405952618e-07,
      "loss": 0.0012,
      "step": 87570
    },
    {
      "epoch": 15923.636363636364,
      "grad_norm": 0.17327606678009033,
      "learning_rate": 4.410290958168161e-07,
      "loss": 0.001,
      "step": 87580
    },
    {
      "epoch": 15925.454545454546,
      "grad_norm": 0.004867246374487877,
      "learning_rate": 4.4091355423189303e-07,
      "loss": 0.001,
      "step": 87590
    },
    {
      "epoch": 15927.272727272728,
      "grad_norm": 0.18627573549747467,
      "learning_rate": 4.407980158467495e-07,
      "loss": 0.0012,
      "step": 87600
    },
    {
      "epoch": 15929.09090909091,
      "grad_norm": 0.0009612494031898677,
      "learning_rate": 4.4068248066764275e-07,
      "loss": 0.0009,
      "step": 87610
    },
    {
      "epoch": 15930.90909090909,
      "grad_norm": 0.001624242402613163,
      "learning_rate": 4.4056694870082895e-07,
      "loss": 0.0012,
      "step": 87620
    },
    {
      "epoch": 15932.727272727272,
      "grad_norm": 0.01061030849814415,
      "learning_rate": 4.4045141995256506e-07,
      "loss": 0.0011,
      "step": 87630
    },
    {
      "epoch": 15934.545454545454,
      "grad_norm": 0.0012191194109618664,
      "learning_rate": 4.403358944291075e-07,
      "loss": 0.0009,
      "step": 87640
    },
    {
      "epoch": 15936.363636363636,
      "grad_norm": 0.010470655746757984,
      "learning_rate": 4.4022037213671216e-07,
      "loss": 0.0015,
      "step": 87650
    },
    {
      "epoch": 15938.181818181818,
      "grad_norm": 0.1645514816045761,
      "learning_rate": 4.401048530816352e-07,
      "loss": 0.0009,
      "step": 87660
    },
    {
      "epoch": 15940.0,
      "grad_norm": 0.2059665024280548,
      "learning_rate": 4.399893372701326e-07,
      "loss": 0.001,
      "step": 87670
    },
    {
      "epoch": 15941.818181818182,
      "grad_norm": 0.0012162335915490985,
      "learning_rate": 4.398738247084598e-07,
      "loss": 0.0012,
      "step": 87680
    },
    {
      "epoch": 15943.636363636364,
      "grad_norm": 0.16471458971500397,
      "learning_rate": 4.3975831540287247e-07,
      "loss": 0.0012,
      "step": 87690
    },
    {
      "epoch": 15945.454545454546,
      "grad_norm": 0.0005961177521385252,
      "learning_rate": 4.3964280935962574e-07,
      "loss": 0.0009,
      "step": 87700
    },
    {
      "epoch": 15947.272727272728,
      "grad_norm": 0.000809829100035131,
      "learning_rate": 4.395273065849747e-07,
      "loss": 0.0014,
      "step": 87710
    },
    {
      "epoch": 15949.09090909091,
      "grad_norm": 0.0004906096728518605,
      "learning_rate": 4.3941180708517486e-07,
      "loss": 0.0009,
      "step": 87720
    },
    {
      "epoch": 15950.90909090909,
      "grad_norm": 0.002544351853430271,
      "learning_rate": 4.392963108664802e-07,
      "loss": 0.0012,
      "step": 87730
    },
    {
      "epoch": 15952.727272727272,
      "grad_norm": 0.17371408641338348,
      "learning_rate": 4.391808179351459e-07,
      "loss": 0.001,
      "step": 87740
    },
    {
      "epoch": 15954.545454545454,
      "grad_norm": 0.0011736140586435795,
      "learning_rate": 4.390653282974263e-07,
      "loss": 0.0009,
      "step": 87750
    },
    {
      "epoch": 15956.363636363636,
      "grad_norm": 0.0005265149520710111,
      "learning_rate": 4.3894984195957546e-07,
      "loss": 0.0012,
      "step": 87760
    },
    {
      "epoch": 15958.181818181818,
      "grad_norm": 0.19175855815410614,
      "learning_rate": 4.388343589278476e-07,
      "loss": 0.0012,
      "step": 87770
    },
    {
      "epoch": 15960.0,
      "grad_norm": 0.20819209516048431,
      "learning_rate": 4.387188792084966e-07,
      "loss": 0.0011,
      "step": 87780
    },
    {
      "epoch": 15961.818181818182,
      "grad_norm": 0.0007975640473887324,
      "learning_rate": 4.3860340280777596e-07,
      "loss": 0.0011,
      "step": 87790
    },
    {
      "epoch": 15963.636363636364,
      "grad_norm": 0.011021889746189117,
      "learning_rate": 4.3848792973193976e-07,
      "loss": 0.001,
      "step": 87800
    },
    {
      "epoch": 15965.454545454546,
      "grad_norm": 0.2516426146030426,
      "learning_rate": 4.3837245998724067e-07,
      "loss": 0.0013,
      "step": 87810
    },
    {
      "epoch": 15967.272727272728,
      "grad_norm": 0.16070601344108582,
      "learning_rate": 4.3825699357993223e-07,
      "loss": 0.0009,
      "step": 87820
    },
    {
      "epoch": 15969.09090909091,
      "grad_norm": 0.18743513524532318,
      "learning_rate": 4.381415305162675e-07,
      "loss": 0.0012,
      "step": 87830
    },
    {
      "epoch": 15970.90909090909,
      "grad_norm": 0.1904114931821823,
      "learning_rate": 4.380260708024991e-07,
      "loss": 0.0009,
      "step": 87840
    },
    {
      "epoch": 15972.727272727272,
      "grad_norm": 0.0007270019850693643,
      "learning_rate": 4.3791061444487974e-07,
      "loss": 0.0012,
      "step": 87850
    },
    {
      "epoch": 15974.545454545454,
      "grad_norm": 0.19094455242156982,
      "learning_rate": 4.37795161449662e-07,
      "loss": 0.001,
      "step": 87860
    },
    {
      "epoch": 15976.363636363636,
      "grad_norm": 0.001203435007482767,
      "learning_rate": 4.376797118230978e-07,
      "loss": 0.0012,
      "step": 87870
    },
    {
      "epoch": 15978.181818181818,
      "grad_norm": 0.0013694651424884796,
      "learning_rate": 4.3756426557143963e-07,
      "loss": 0.0009,
      "step": 87880
    },
    {
      "epoch": 15980.0,
      "grad_norm": 0.27672240138053894,
      "learning_rate": 4.3744882270093904e-07,
      "loss": 0.0012,
      "step": 87890
    },
    {
      "epoch": 15981.818181818182,
      "grad_norm": 0.000591510790400207,
      "learning_rate": 4.3733338321784777e-07,
      "loss": 0.0012,
      "step": 87900
    },
    {
      "epoch": 15983.636363636364,
      "grad_norm": 0.005585798062384129,
      "learning_rate": 4.372179471284179e-07,
      "loss": 0.0009,
      "step": 87910
    },
    {
      "epoch": 15985.454545454546,
      "grad_norm": 0.16712822020053864,
      "learning_rate": 4.3710251443889995e-07,
      "loss": 0.0012,
      "step": 87920
    },
    {
      "epoch": 15987.272727272728,
      "grad_norm": 0.0016235673101618886,
      "learning_rate": 4.369870851555456e-07,
      "loss": 0.0009,
      "step": 87930
    },
    {
      "epoch": 15989.09090909091,
      "grad_norm": 0.18188484013080597,
      "learning_rate": 4.368716592846059e-07,
      "loss": 0.0012,
      "step": 87940
    },
    {
      "epoch": 15990.90909090909,
      "grad_norm": 0.18937717378139496,
      "learning_rate": 4.367562368323313e-07,
      "loss": 0.0012,
      "step": 87950
    },
    {
      "epoch": 15992.727272727272,
      "grad_norm": 0.0008825971162877977,
      "learning_rate": 4.3664081780497276e-07,
      "loss": 0.0009,
      "step": 87960
    },
    {
      "epoch": 15994.545454545454,
      "grad_norm": 0.000820219749584794,
      "learning_rate": 4.3652540220878044e-07,
      "loss": 0.0013,
      "step": 87970
    },
    {
      "epoch": 15996.363636363636,
      "grad_norm": 0.1690887212753296,
      "learning_rate": 4.364099900500045e-07,
      "loss": 0.0011,
      "step": 87980
    },
    {
      "epoch": 15998.181818181818,
      "grad_norm": 0.1962108016014099,
      "learning_rate": 4.362945813348955e-07,
      "loss": 0.0011,
      "step": 87990
    },
    {
      "epoch": 16000.0,
      "grad_norm": 0.002016117563471198,
      "learning_rate": 4.3617917606970267e-07,
      "loss": 0.001,
      "step": 88000
    },
    {
      "epoch": 16000.0,
      "eval_loss": 5.078011512756348,
      "eval_runtime": 0.951,
      "eval_samples_per_second": 10.515,
      "eval_steps_per_second": 5.258,
      "step": 88000
    },
    {
      "epoch": 16001.818181818182,
      "grad_norm": 0.20658200979232788,
      "learning_rate": 4.3606377426067606e-07,
      "loss": 0.001,
      "step": 88010
    },
    {
      "epoch": 16003.636363636364,
      "grad_norm": 0.1975783407688141,
      "learning_rate": 4.359483759140653e-07,
      "loss": 0.0012,
      "step": 88020
    },
    {
      "epoch": 16005.454545454546,
      "grad_norm": 0.0008040858083404601,
      "learning_rate": 4.358329810361193e-07,
      "loss": 0.001,
      "step": 88030
    },
    {
      "epoch": 16007.272727272728,
      "grad_norm": 0.16881854832172394,
      "learning_rate": 4.3571758963308754e-07,
      "loss": 0.001,
      "step": 88040
    },
    {
      "epoch": 16009.09090909091,
      "grad_norm": 0.0006232666200958192,
      "learning_rate": 4.356022017112186e-07,
      "loss": 0.001,
      "step": 88050
    },
    {
      "epoch": 16010.90909090909,
      "grad_norm": 0.0029282053001224995,
      "learning_rate": 4.354868172767615e-07,
      "loss": 0.001,
      "step": 88060
    },
    {
      "epoch": 16012.727272727272,
      "grad_norm": 0.19433213770389557,
      "learning_rate": 4.3537143633596475e-07,
      "loss": 0.0012,
      "step": 88070
    },
    {
      "epoch": 16014.545454545454,
      "grad_norm": 0.0004540827649179846,
      "learning_rate": 4.3525605889507654e-07,
      "loss": 0.0009,
      "step": 88080
    },
    {
      "epoch": 16016.363636363636,
      "grad_norm": 0.1536920666694641,
      "learning_rate": 4.351406849603451e-07,
      "loss": 0.0016,
      "step": 88090
    },
    {
      "epoch": 16018.181818181818,
      "grad_norm": 0.15760943293571472,
      "learning_rate": 4.350253145380188e-07,
      "loss": 0.0009,
      "step": 88100
    },
    {
      "epoch": 16020.0,
      "grad_norm": 0.0013580725062638521,
      "learning_rate": 4.3490994763434476e-07,
      "loss": 0.0011,
      "step": 88110
    },
    {
      "epoch": 16021.818181818182,
      "grad_norm": 0.161426842212677,
      "learning_rate": 4.3479458425557107e-07,
      "loss": 0.0012,
      "step": 88120
    },
    {
      "epoch": 16023.636363636364,
      "grad_norm": 0.0011509753530845046,
      "learning_rate": 4.346792244079451e-07,
      "loss": 0.0009,
      "step": 88130
    },
    {
      "epoch": 16025.454545454546,
      "grad_norm": 0.001030971179716289,
      "learning_rate": 4.345638680977139e-07,
      "loss": 0.001,
      "step": 88140
    },
    {
      "epoch": 16027.272727272728,
      "grad_norm": 0.2610798478126526,
      "learning_rate": 4.3444851533112465e-07,
      "loss": 0.0013,
      "step": 88150
    },
    {
      "epoch": 16029.09090909091,
      "grad_norm": 0.24687959253787994,
      "learning_rate": 4.3433316611442404e-07,
      "loss": 0.0012,
      "step": 88160
    },
    {
      "epoch": 16030.90909090909,
      "grad_norm": 0.210321307182312,
      "learning_rate": 4.3421782045385874e-07,
      "loss": 0.001,
      "step": 88170
    },
    {
      "epoch": 16032.727272727272,
      "grad_norm": 0.0006296960054896772,
      "learning_rate": 4.341024783556754e-07,
      "loss": 0.0008,
      "step": 88180
    },
    {
      "epoch": 16034.545454545454,
      "grad_norm": 0.21905292570590973,
      "learning_rate": 4.3398713982611995e-07,
      "loss": 0.0012,
      "step": 88190
    },
    {
      "epoch": 16036.363636363636,
      "grad_norm": 0.20627182722091675,
      "learning_rate": 4.338718048714387e-07,
      "loss": 0.0012,
      "step": 88200
    },
    {
      "epoch": 16038.181818181818,
      "grad_norm": 0.20719225704669952,
      "learning_rate": 4.337564734978776e-07,
      "loss": 0.0013,
      "step": 88210
    },
    {
      "epoch": 16040.0,
      "grad_norm": 0.1516905277967453,
      "learning_rate": 4.336411457116821e-07,
      "loss": 0.0011,
      "step": 88220
    },
    {
      "epoch": 16041.818181818182,
      "grad_norm": 0.0007063029333949089,
      "learning_rate": 4.335258215190978e-07,
      "loss": 0.0013,
      "step": 88230
    },
    {
      "epoch": 16043.636363636364,
      "grad_norm": 0.19898183643817902,
      "learning_rate": 4.3341050092636995e-07,
      "loss": 0.001,
      "step": 88240
    },
    {
      "epoch": 16045.454545454546,
      "grad_norm": 0.0007419785251840949,
      "learning_rate": 4.332951839397436e-07,
      "loss": 0.001,
      "step": 88250
    },
    {
      "epoch": 16047.272727272728,
      "grad_norm": 0.20667819678783417,
      "learning_rate": 4.331798705654639e-07,
      "loss": 0.001,
      "step": 88260
    },
    {
      "epoch": 16049.09090909091,
      "grad_norm": 0.01726158894598484,
      "learning_rate": 4.330645608097752e-07,
      "loss": 0.0014,
      "step": 88270
    },
    {
      "epoch": 16050.90909090909,
      "grad_norm": 0.001591301872394979,
      "learning_rate": 4.329492546789221e-07,
      "loss": 0.0009,
      "step": 88280
    },
    {
      "epoch": 16052.727272727272,
      "grad_norm": 0.15029722452163696,
      "learning_rate": 4.328339521791493e-07,
      "loss": 0.001,
      "step": 88290
    },
    {
      "epoch": 16054.545454545454,
      "grad_norm": 0.000641880847979337,
      "learning_rate": 4.3271865331670034e-07,
      "loss": 0.0007,
      "step": 88300
    },
    {
      "epoch": 16056.363636363636,
      "grad_norm": 0.20938536524772644,
      "learning_rate": 4.3260335809781965e-07,
      "loss": 0.0015,
      "step": 88310
    },
    {
      "epoch": 16058.181818181818,
      "grad_norm": 0.000706057995557785,
      "learning_rate": 4.324880665287504e-07,
      "loss": 0.0009,
      "step": 88320
    },
    {
      "epoch": 16060.0,
      "grad_norm": 0.15350453555583954,
      "learning_rate": 4.323727786157366e-07,
      "loss": 0.0012,
      "step": 88330
    },
    {
      "epoch": 16061.818181818182,
      "grad_norm": 0.0007350258529186249,
      "learning_rate": 4.322574943650214e-07,
      "loss": 0.001,
      "step": 88340
    },
    {
      "epoch": 16063.636363636364,
      "grad_norm": 0.20921209454536438,
      "learning_rate": 4.3214221378284785e-07,
      "loss": 0.0013,
      "step": 88350
    },
    {
      "epoch": 16065.454545454546,
      "grad_norm": 0.0006434570532292128,
      "learning_rate": 4.3202693687545894e-07,
      "loss": 0.0009,
      "step": 88360
    },
    {
      "epoch": 16067.272727272728,
      "grad_norm": 0.0008552034851163626,
      "learning_rate": 4.3191166364909755e-07,
      "loss": 0.0009,
      "step": 88370
    },
    {
      "epoch": 16069.09090909091,
      "grad_norm": 0.0006289908778853714,
      "learning_rate": 4.317963941100058e-07,
      "loss": 0.0012,
      "step": 88380
    },
    {
      "epoch": 16070.90909090909,
      "grad_norm": 0.0008299534674733877,
      "learning_rate": 4.3168112826442645e-07,
      "loss": 0.0012,
      "step": 88390
    },
    {
      "epoch": 16072.727272727272,
      "grad_norm": 0.22326606512069702,
      "learning_rate": 4.315658661186016e-07,
      "loss": 0.001,
      "step": 88400
    },
    {
      "epoch": 16074.545454545454,
      "grad_norm": 0.0008198524592444301,
      "learning_rate": 4.3145060767877286e-07,
      "loss": 0.0012,
      "step": 88410
    },
    {
      "epoch": 16076.363636363636,
      "grad_norm": 0.19625407457351685,
      "learning_rate": 4.313353529511823e-07,
      "loss": 0.0011,
      "step": 88420
    },
    {
      "epoch": 16078.181818181818,
      "grad_norm": 0.0006504658958874643,
      "learning_rate": 4.3122010194207117e-07,
      "loss": 0.001,
      "step": 88430
    },
    {
      "epoch": 16080.0,
      "grad_norm": 0.21969576179981232,
      "learning_rate": 4.31104854657681e-07,
      "loss": 0.0012,
      "step": 88440
    },
    {
      "epoch": 16081.818181818182,
      "grad_norm": 0.0008631522068753839,
      "learning_rate": 4.3098961110425286e-07,
      "loss": 0.001,
      "step": 88450
    },
    {
      "epoch": 16083.636363636364,
      "grad_norm": 0.20843975245952606,
      "learning_rate": 4.3087437128802754e-07,
      "loss": 0.0011,
      "step": 88460
    },
    {
      "epoch": 16085.454545454546,
      "grad_norm": 0.011476376093924046,
      "learning_rate": 4.3075913521524585e-07,
      "loss": 0.0015,
      "step": 88470
    },
    {
      "epoch": 16087.272727272728,
      "grad_norm": 0.16404956579208374,
      "learning_rate": 4.3064390289214864e-07,
      "loss": 0.0008,
      "step": 88480
    },
    {
      "epoch": 16089.09090909091,
      "grad_norm": 0.2803925573825836,
      "learning_rate": 4.305286743249755e-07,
      "loss": 0.0011,
      "step": 88490
    },
    {
      "epoch": 16090.90909090909,
      "grad_norm": 0.21877482533454895,
      "learning_rate": 4.304134495199674e-07,
      "loss": 0.0012,
      "step": 88500
    },
    {
      "epoch": 16090.90909090909,
      "eval_loss": 5.056342601776123,
      "eval_runtime": 0.954,
      "eval_samples_per_second": 10.482,
      "eval_steps_per_second": 5.241,
      "step": 88500
    },
    {
      "epoch": 16092.727272727272,
      "grad_norm": 0.19298605620861053,
      "learning_rate": 4.3029822848336344e-07,
      "loss": 0.0012,
      "step": 88510
    },
    {
      "epoch": 16094.545454545454,
      "grad_norm": 0.20725508034229279,
      "learning_rate": 4.3018301122140385e-07,
      "loss": 0.0007,
      "step": 88520
    },
    {
      "epoch": 16096.363636363636,
      "grad_norm": 0.0011102319695055485,
      "learning_rate": 4.30067797740328e-07,
      "loss": 0.001,
      "step": 88530
    },
    {
      "epoch": 16098.181818181818,
      "grad_norm": 0.0005976995453238487,
      "learning_rate": 4.2995258804637513e-07,
      "loss": 0.0012,
      "step": 88540
    },
    {
      "epoch": 16100.0,
      "grad_norm": 0.20688609778881073,
      "learning_rate": 4.298373821457843e-07,
      "loss": 0.0012,
      "step": 88550
    },
    {
      "epoch": 16101.818181818182,
      "grad_norm": 0.0009079693118110299,
      "learning_rate": 4.297221800447945e-07,
      "loss": 0.001,
      "step": 88560
    },
    {
      "epoch": 16103.636363636364,
      "grad_norm": 0.0012149799149483442,
      "learning_rate": 4.296069817496443e-07,
      "loss": 0.001,
      "step": 88570
    },
    {
      "epoch": 16105.454545454546,
      "grad_norm": 0.16896237432956696,
      "learning_rate": 4.294917872665724e-07,
      "loss": 0.0015,
      "step": 88580
    },
    {
      "epoch": 16107.272727272728,
      "grad_norm": 0.1568215936422348,
      "learning_rate": 4.293765966018167e-07,
      "loss": 0.001,
      "step": 88590
    },
    {
      "epoch": 16109.09090909091,
      "grad_norm": 0.17004698514938354,
      "learning_rate": 4.292614097616155e-07,
      "loss": 0.0011,
      "step": 88600
    },
    {
      "epoch": 16110.90909090909,
      "grad_norm": 0.001261577708646655,
      "learning_rate": 4.2914622675220673e-07,
      "loss": 0.0009,
      "step": 88610
    },
    {
      "epoch": 16112.727272727272,
      "grad_norm": 0.001109267701394856,
      "learning_rate": 4.290310475798278e-07,
      "loss": 0.0012,
      "step": 88620
    },
    {
      "epoch": 16114.545454545454,
      "grad_norm": 0.1400585025548935,
      "learning_rate": 4.289158722507163e-07,
      "loss": 0.0012,
      "step": 88630
    },
    {
      "epoch": 16116.363636363636,
      "grad_norm": 0.0005263796774670482,
      "learning_rate": 4.288007007711095e-07,
      "loss": 0.0008,
      "step": 88640
    },
    {
      "epoch": 16118.181818181818,
      "grad_norm": 0.16220982372760773,
      "learning_rate": 4.286855331472442e-07,
      "loss": 0.0013,
      "step": 88650
    },
    {
      "epoch": 16120.0,
      "grad_norm": 0.001052501262165606,
      "learning_rate": 4.2857036938535753e-07,
      "loss": 0.001,
      "step": 88660
    },
    {
      "epoch": 16121.818181818182,
      "grad_norm": 0.0012068687938153744,
      "learning_rate": 4.2845520949168574e-07,
      "loss": 0.0012,
      "step": 88670
    },
    {
      "epoch": 16123.636363636364,
      "grad_norm": 0.1596730798482895,
      "learning_rate": 4.2834005347246525e-07,
      "loss": 0.0009,
      "step": 88680
    },
    {
      "epoch": 16125.454545454546,
      "grad_norm": 0.15948046743869781,
      "learning_rate": 4.282249013339328e-07,
      "loss": 0.0011,
      "step": 88690
    },
    {
      "epoch": 16127.272727272728,
      "grad_norm": 0.0005246172659099102,
      "learning_rate": 4.281097530823236e-07,
      "loss": 0.001,
      "step": 88700
    },
    {
      "epoch": 16129.09090909091,
      "grad_norm": 0.00071966543328017,
      "learning_rate": 4.279946087238739e-07,
      "loss": 0.0011,
      "step": 88710
    },
    {
      "epoch": 16130.90909090909,
      "grad_norm": 0.16083745658397675,
      "learning_rate": 4.278794682648192e-07,
      "loss": 0.001,
      "step": 88720
    },
    {
      "epoch": 16132.727272727272,
      "grad_norm": 0.0008441430400125682,
      "learning_rate": 4.277643317113946e-07,
      "loss": 0.0013,
      "step": 88730
    },
    {
      "epoch": 16134.545454545454,
      "grad_norm": 0.1653209775686264,
      "learning_rate": 4.2764919906983543e-07,
      "loss": 0.0009,
      "step": 88740
    },
    {
      "epoch": 16136.363636363636,
      "grad_norm": 0.20728427171707153,
      "learning_rate": 4.2753407034637666e-07,
      "loss": 0.0011,
      "step": 88750
    },
    {
      "epoch": 16138.181818181818,
      "grad_norm": 0.0061592827551066875,
      "learning_rate": 4.2741894554725283e-07,
      "loss": 0.0013,
      "step": 88760
    },
    {
      "epoch": 16140.0,
      "grad_norm": 0.0007394631975330412,
      "learning_rate": 4.2730382467869857e-07,
      "loss": 0.0009,
      "step": 88770
    },
    {
      "epoch": 16141.818181818182,
      "grad_norm": 0.19205456972122192,
      "learning_rate": 4.271887077469479e-07,
      "loss": 0.0012,
      "step": 88780
    },
    {
      "epoch": 16143.636363636364,
      "grad_norm": 0.0006383652216754854,
      "learning_rate": 4.2707359475823513e-07,
      "loss": 0.0009,
      "step": 88790
    },
    {
      "epoch": 16145.454545454546,
      "grad_norm": 0.0008561749127693474,
      "learning_rate": 4.2695848571879424e-07,
      "loss": 0.0009,
      "step": 88800
    },
    {
      "epoch": 16147.272727272728,
      "grad_norm": 0.002820502733811736,
      "learning_rate": 4.268433806348585e-07,
      "loss": 0.0013,
      "step": 88810
    },
    {
      "epoch": 16149.09090909091,
      "grad_norm": 0.001024765195325017,
      "learning_rate": 4.2672827951266157e-07,
      "loss": 0.0011,
      "step": 88820
    },
    {
      "epoch": 16150.90909090909,
      "grad_norm": 0.15670965611934662,
      "learning_rate": 4.266131823584367e-07,
      "loss": 0.0012,
      "step": 88830
    },
    {
      "epoch": 16152.727272727272,
      "grad_norm": 0.0016638269880786538,
      "learning_rate": 4.2649808917841665e-07,
      "loss": 0.0011,
      "step": 88840
    },
    {
      "epoch": 16154.545454545454,
      "grad_norm": 0.159839928150177,
      "learning_rate": 4.263829999788345e-07,
      "loss": 0.001,
      "step": 88850
    },
    {
      "epoch": 16156.363636363636,
      "grad_norm": 0.0015325449639931321,
      "learning_rate": 4.2626791476592263e-07,
      "loss": 0.0009,
      "step": 88860
    },
    {
      "epoch": 16158.181818181818,
      "grad_norm": 0.000971584115177393,
      "learning_rate": 4.261528335459132e-07,
      "loss": 0.0012,
      "step": 88870
    },
    {
      "epoch": 16160.0,
      "grad_norm": 0.157167449593544,
      "learning_rate": 4.260377563250389e-07,
      "loss": 0.0012,
      "step": 88880
    },
    {
      "epoch": 16161.818181818182,
      "grad_norm": 0.0004967519780620933,
      "learning_rate": 4.25922683109531e-07,
      "loss": 0.0011,
      "step": 88890
    },
    {
      "epoch": 16163.636363636364,
      "grad_norm": 0.15795522928237915,
      "learning_rate": 4.2580761390562163e-07,
      "loss": 0.0012,
      "step": 88900
    },
    {
      "epoch": 16165.454545454546,
      "grad_norm": 0.19318647682666779,
      "learning_rate": 4.2569254871954223e-07,
      "loss": 0.0009,
      "step": 88910
    },
    {
      "epoch": 16167.272727272728,
      "grad_norm": 0.0006715984782204032,
      "learning_rate": 4.255774875575239e-07,
      "loss": 0.0013,
      "step": 88920
    },
    {
      "epoch": 16169.09090909091,
      "grad_norm": 0.24943262338638306,
      "learning_rate": 4.254624304257979e-07,
      "loss": 0.0009,
      "step": 88930
    },
    {
      "epoch": 16170.90909090909,
      "grad_norm": 0.26006776094436646,
      "learning_rate": 4.2534737733059473e-07,
      "loss": 0.0012,
      "step": 88940
    },
    {
      "epoch": 16172.727272727272,
      "grad_norm": 0.1588175892829895,
      "learning_rate": 4.2523232827814527e-07,
      "loss": 0.0012,
      "step": 88950
    },
    {
      "epoch": 16174.545454545454,
      "grad_norm": 0.0011183182941749692,
      "learning_rate": 4.251172832746799e-07,
      "loss": 0.0009,
      "step": 88960
    },
    {
      "epoch": 16176.363636363636,
      "grad_norm": 0.15662889182567596,
      "learning_rate": 4.250022423264285e-07,
      "loss": 0.001,
      "step": 88970
    },
    {
      "epoch": 16178.181818181818,
      "grad_norm": 0.0009482671157456934,
      "learning_rate": 4.2488720543962144e-07,
      "loss": 0.001,
      "step": 88980
    },
    {
      "epoch": 16180.0,
      "grad_norm": 0.15853427350521088,
      "learning_rate": 4.2477217262048827e-07,
      "loss": 0.0012,
      "step": 88990
    },
    {
      "epoch": 16181.818181818182,
      "grad_norm": 0.010834069922566414,
      "learning_rate": 4.246571438752584e-07,
      "loss": 0.001,
      "step": 89000
    },
    {
      "epoch": 16181.818181818182,
      "eval_loss": 5.076421737670898,
      "eval_runtime": 0.9524,
      "eval_samples_per_second": 10.5,
      "eval_steps_per_second": 5.25,
      "step": 89000
    },
    {
      "epoch": 16183.636363636364,
      "grad_norm": 0.15099482238292694,
      "learning_rate": 4.2454211921016123e-07,
      "loss": 0.0011,
      "step": 89010
    },
    {
      "epoch": 16185.454545454546,
      "grad_norm": 0.25930020213127136,
      "learning_rate": 4.2442709863142596e-07,
      "loss": 0.0013,
      "step": 89020
    },
    {
      "epoch": 16187.272727272728,
      "grad_norm": 0.000761040486395359,
      "learning_rate": 4.2431208214528113e-07,
      "loss": 0.0011,
      "step": 89030
    },
    {
      "epoch": 16189.09090909091,
      "grad_norm": 0.2092021107673645,
      "learning_rate": 4.2419706975795564e-07,
      "loss": 0.001,
      "step": 89040
    },
    {
      "epoch": 16190.90909090909,
      "grad_norm": 0.1520388424396515,
      "learning_rate": 4.2408206147567764e-07,
      "loss": 0.0012,
      "step": 89050
    },
    {
      "epoch": 16192.727272727272,
      "grad_norm": 0.20729446411132812,
      "learning_rate": 4.239670573046754e-07,
      "loss": 0.0011,
      "step": 89060
    },
    {
      "epoch": 16194.545454545454,
      "grad_norm": 0.0006995254661887884,
      "learning_rate": 4.238520572511772e-07,
      "loss": 0.001,
      "step": 89070
    },
    {
      "epoch": 16196.363636363636,
      "grad_norm": 0.0007839473546482623,
      "learning_rate": 4.237370613214103e-07,
      "loss": 0.0009,
      "step": 89080
    },
    {
      "epoch": 16198.181818181818,
      "grad_norm": 0.0005723962094634771,
      "learning_rate": 4.236220695216024e-07,
      "loss": 0.0012,
      "step": 89090
    },
    {
      "epoch": 16200.0,
      "grad_norm": 0.26036155223846436,
      "learning_rate": 4.23507081857981e-07,
      "loss": 0.0012,
      "step": 89100
    },
    {
      "epoch": 16201.818181818182,
      "grad_norm": 0.001177306054159999,
      "learning_rate": 4.233920983367728e-07,
      "loss": 0.0009,
      "step": 89110
    },
    {
      "epoch": 16203.636363636364,
      "grad_norm": 0.1732206642627716,
      "learning_rate": 4.2327711896420496e-07,
      "loss": 0.0014,
      "step": 89120
    },
    {
      "epoch": 16205.454545454546,
      "grad_norm": 0.15287910401821136,
      "learning_rate": 4.231621437465039e-07,
      "loss": 0.0012,
      "step": 89130
    },
    {
      "epoch": 16207.272727272728,
      "grad_norm": 0.14925222098827362,
      "learning_rate": 4.23047172689896e-07,
      "loss": 0.001,
      "step": 89140
    },
    {
      "epoch": 16209.09090909091,
      "grad_norm": 0.01022597961127758,
      "learning_rate": 4.2293220580060765e-07,
      "loss": 0.0012,
      "step": 89150
    },
    {
      "epoch": 16210.90909090909,
      "grad_norm": 0.000856171827763319,
      "learning_rate": 4.2281724308486436e-07,
      "loss": 0.001,
      "step": 89160
    },
    {
      "epoch": 16212.727272727272,
      "grad_norm": 0.21082058548927307,
      "learning_rate": 4.227022845488923e-07,
      "loss": 0.0011,
      "step": 89170
    },
    {
      "epoch": 16214.545454545454,
      "grad_norm": 0.26740244030952454,
      "learning_rate": 4.225873301989168e-07,
      "loss": 0.0012,
      "step": 89180
    },
    {
      "epoch": 16216.363636363636,
      "grad_norm": 0.20872417092323303,
      "learning_rate": 4.224723800411631e-07,
      "loss": 0.0009,
      "step": 89190
    },
    {
      "epoch": 16218.181818181818,
      "grad_norm": 0.0009184821392409503,
      "learning_rate": 4.2235743408185625e-07,
      "loss": 0.001,
      "step": 89200
    },
    {
      "epoch": 16220.0,
      "grad_norm": 0.000618976482655853,
      "learning_rate": 4.22242492327221e-07,
      "loss": 0.0012,
      "step": 89210
    },
    {
      "epoch": 16221.818181818182,
      "grad_norm": 0.1940533071756363,
      "learning_rate": 4.221275547834819e-07,
      "loss": 0.0012,
      "step": 89220
    },
    {
      "epoch": 16223.636363636364,
      "grad_norm": 0.19382886588573456,
      "learning_rate": 4.2201262145686356e-07,
      "loss": 0.0012,
      "step": 89230
    },
    {
      "epoch": 16225.454545454546,
      "grad_norm": 0.0006148937973193824,
      "learning_rate": 4.2189769235358975e-07,
      "loss": 0.0009,
      "step": 89240
    },
    {
      "epoch": 16227.272727272728,
      "grad_norm": 0.0006998032331466675,
      "learning_rate": 4.2178276747988444e-07,
      "loss": 0.0011,
      "step": 89250
    },
    {
      "epoch": 16229.09090909091,
      "grad_norm": 0.0006261391681618989,
      "learning_rate": 4.216678468419718e-07,
      "loss": 0.0012,
      "step": 89260
    },
    {
      "epoch": 16230.90909090909,
      "grad_norm": 0.0017636517295613885,
      "learning_rate": 4.2155293044607444e-07,
      "loss": 0.0011,
      "step": 89270
    },
    {
      "epoch": 16232.727272727272,
      "grad_norm": 0.0021757271606475115,
      "learning_rate": 4.214380182984163e-07,
      "loss": 0.0009,
      "step": 89280
    },
    {
      "epoch": 16234.545454545454,
      "grad_norm": 0.0011945662554353476,
      "learning_rate": 4.213231104052197e-07,
      "loss": 0.0014,
      "step": 89290
    },
    {
      "epoch": 16236.363636363636,
      "grad_norm": 0.20938773453235626,
      "learning_rate": 4.2120820677270787e-07,
      "loss": 0.0009,
      "step": 89300
    },
    {
      "epoch": 16238.181818181818,
      "grad_norm": 0.0006324892165139318,
      "learning_rate": 4.2109330740710325e-07,
      "loss": 0.0011,
      "step": 89310
    },
    {
      "epoch": 16240.0,
      "grad_norm": 0.26260727643966675,
      "learning_rate": 4.2097841231462796e-07,
      "loss": 0.0012,
      "step": 89320
    },
    {
      "epoch": 16241.818181818182,
      "grad_norm": 0.0005760930362157524,
      "learning_rate": 4.2086352150150416e-07,
      "loss": 0.0011,
      "step": 89330
    },
    {
      "epoch": 16243.636363636364,
      "grad_norm": 0.1590977907180786,
      "learning_rate": 4.2074863497395377e-07,
      "loss": 0.0013,
      "step": 89340
    },
    {
      "epoch": 16245.454545454546,
      "grad_norm": 0.0006767650484107435,
      "learning_rate": 4.206337527381981e-07,
      "loss": 0.0008,
      "step": 89350
    },
    {
      "epoch": 16247.272727272728,
      "grad_norm": 0.0008816471672616899,
      "learning_rate": 4.205188748004586e-07,
      "loss": 0.0012,
      "step": 89360
    },
    {
      "epoch": 16249.09090909091,
      "grad_norm": 0.0011177631095051765,
      "learning_rate": 4.2040400116695664e-07,
      "loss": 0.0012,
      "step": 89370
    },
    {
      "epoch": 16250.90909090909,
      "grad_norm": 0.0006751670152880251,
      "learning_rate": 4.202891318439129e-07,
      "loss": 0.001,
      "step": 89380
    },
    {
      "epoch": 16252.727272727272,
      "grad_norm": 0.0011905067367479205,
      "learning_rate": 4.201742668375481e-07,
      "loss": 0.0014,
      "step": 89390
    },
    {
      "epoch": 16254.545454545454,
      "grad_norm": 0.0008810973959043622,
      "learning_rate": 4.200594061540826e-07,
      "loss": 0.0009,
      "step": 89400
    },
    {
      "epoch": 16256.363636363636,
      "grad_norm": 0.1640392541885376,
      "learning_rate": 4.199445497997366e-07,
      "loss": 0.0012,
      "step": 89410
    },
    {
      "epoch": 16258.181818181818,
      "grad_norm": 0.0005928692407906055,
      "learning_rate": 4.1982969778073015e-07,
      "loss": 0.0009,
      "step": 89420
    },
    {
      "epoch": 16260.0,
      "grad_norm": 0.15791164338588715,
      "learning_rate": 4.197148501032828e-07,
      "loss": 0.0012,
      "step": 89430
    },
    {
      "epoch": 16261.818181818182,
      "grad_norm": 0.0005959700210951269,
      "learning_rate": 4.196000067736141e-07,
      "loss": 0.0012,
      "step": 89440
    },
    {
      "epoch": 16263.636363636364,
      "grad_norm": 0.0005918499664403498,
      "learning_rate": 4.194851677979436e-07,
      "loss": 0.001,
      "step": 89450
    },
    {
      "epoch": 16265.454545454546,
      "grad_norm": 0.20596779882907867,
      "learning_rate": 4.193703331824897e-07,
      "loss": 0.0012,
      "step": 89460
    },
    {
      "epoch": 16267.272727272728,
      "grad_norm": 0.0006769152241759002,
      "learning_rate": 4.192555029334719e-07,
      "loss": 0.0009,
      "step": 89470
    },
    {
      "epoch": 16269.09090909091,
      "grad_norm": 0.0005012553301639855,
      "learning_rate": 4.1914067705710797e-07,
      "loss": 0.001,
      "step": 89480
    },
    {
      "epoch": 16270.90909090909,
      "grad_norm": 0.16286596655845642,
      "learning_rate": 4.190258555596168e-07,
      "loss": 0.0011,
      "step": 89490
    },
    {
      "epoch": 16272.727272727272,
      "grad_norm": 0.192588210105896,
      "learning_rate": 4.1891103844721634e-07,
      "loss": 0.0013,
      "step": 89500
    },
    {
      "epoch": 16272.727272727272,
      "eval_loss": 5.0195770263671875,
      "eval_runtime": 0.9522,
      "eval_samples_per_second": 10.502,
      "eval_steps_per_second": 5.251,
      "step": 89500
    },
    {
      "epoch": 16274.545454545454,
      "grad_norm": 0.25496160984039307,
      "learning_rate": 4.187962257261242e-07,
      "loss": 0.0009,
      "step": 89510
    },
    {
      "epoch": 16276.363636363636,
      "grad_norm": 0.16966307163238525,
      "learning_rate": 4.1868141740255817e-07,
      "loss": 0.0011,
      "step": 89520
    },
    {
      "epoch": 16278.181818181818,
      "grad_norm": 0.16365432739257812,
      "learning_rate": 4.185666134827357e-07,
      "loss": 0.001,
      "step": 89530
    },
    {
      "epoch": 16280.0,
      "grad_norm": 0.014230436645448208,
      "learning_rate": 4.1845181397287357e-07,
      "loss": 0.001,
      "step": 89540
    },
    {
      "epoch": 16281.818181818182,
      "grad_norm": 0.0006261842790991068,
      "learning_rate": 4.1833701887918897e-07,
      "loss": 0.0012,
      "step": 89550
    },
    {
      "epoch": 16283.636363636364,
      "grad_norm": 0.0015283938264474273,
      "learning_rate": 4.1822222820789823e-07,
      "loss": 0.0009,
      "step": 89560
    },
    {
      "epoch": 16285.454545454546,
      "grad_norm": 0.001396985724568367,
      "learning_rate": 4.1810744196521804e-07,
      "loss": 0.0012,
      "step": 89570
    },
    {
      "epoch": 16287.272727272728,
      "grad_norm": 0.0010501932119950652,
      "learning_rate": 4.179926601573645e-07,
      "loss": 0.0009,
      "step": 89580
    },
    {
      "epoch": 16289.09090909091,
      "grad_norm": 0.1800968199968338,
      "learning_rate": 4.178778827905534e-07,
      "loss": 0.0013,
      "step": 89590
    },
    {
      "epoch": 16290.90909090909,
      "grad_norm": 0.18226554989814758,
      "learning_rate": 4.177631098710005e-07,
      "loss": 0.0011,
      "step": 89600
    },
    {
      "epoch": 16292.727272727272,
      "grad_norm": 0.26959532499313354,
      "learning_rate": 4.1764834140492135e-07,
      "loss": 0.0009,
      "step": 89610
    },
    {
      "epoch": 16294.545454545454,
      "grad_norm": 0.16076739132404327,
      "learning_rate": 4.175335773985309e-07,
      "loss": 0.001,
      "step": 89620
    },
    {
      "epoch": 16296.363636363636,
      "grad_norm": 0.0034962655045092106,
      "learning_rate": 4.174188178580441e-07,
      "loss": 0.0011,
      "step": 89630
    },
    {
      "epoch": 16298.181818181818,
      "grad_norm": 0.15801842510700226,
      "learning_rate": 4.173040627896762e-07,
      "loss": 0.0015,
      "step": 89640
    },
    {
      "epoch": 16300.0,
      "grad_norm": 0.0029638810083270073,
      "learning_rate": 4.171893121996409e-07,
      "loss": 0.0011,
      "step": 89650
    },
    {
      "epoch": 16301.818181818182,
      "grad_norm": 0.2724873125553131,
      "learning_rate": 4.17074566094153e-07,
      "loss": 0.0012,
      "step": 89660
    },
    {
      "epoch": 16303.636363636364,
      "grad_norm": 0.17574042081832886,
      "learning_rate": 4.169598244794261e-07,
      "loss": 0.0011,
      "step": 89670
    },
    {
      "epoch": 16305.454545454546,
      "grad_norm": 0.0008407311397604644,
      "learning_rate": 4.168450873616741e-07,
      "loss": 0.0009,
      "step": 89680
    },
    {
      "epoch": 16307.272727272728,
      "grad_norm": 0.20694999396800995,
      "learning_rate": 4.167303547471107e-07,
      "loss": 0.0012,
      "step": 89690
    },
    {
      "epoch": 16309.09090909091,
      "grad_norm": 0.0006530089303851128,
      "learning_rate": 4.166156266419489e-07,
      "loss": 0.0011,
      "step": 89700
    },
    {
      "epoch": 16310.90909090909,
      "grad_norm": 0.18492715060710907,
      "learning_rate": 4.165009030524017e-07,
      "loss": 0.0012,
      "step": 89710
    },
    {
      "epoch": 16312.727272727272,
      "grad_norm": 0.168452188372612,
      "learning_rate": 4.1638618398468206e-07,
      "loss": 0.0012,
      "step": 89720
    },
    {
      "epoch": 16314.545454545454,
      "grad_norm": 0.017333557829260826,
      "learning_rate": 4.1627146944500225e-07,
      "loss": 0.001,
      "step": 89730
    },
    {
      "epoch": 16316.363636363636,
      "grad_norm": 0.0016199556412175298,
      "learning_rate": 4.1615675943957483e-07,
      "loss": 0.0008,
      "step": 89740
    },
    {
      "epoch": 16318.181818181818,
      "grad_norm": 0.16241902112960815,
      "learning_rate": 4.1604205397461146e-07,
      "loss": 0.0014,
      "step": 89750
    },
    {
      "epoch": 16320.0,
      "grad_norm": 0.2629607617855072,
      "learning_rate": 4.159273530563242e-07,
      "loss": 0.001,
      "step": 89760
    },
    {
      "epoch": 16321.818181818182,
      "grad_norm": 0.27376362681388855,
      "learning_rate": 4.1581265669092463e-07,
      "loss": 0.0012,
      "step": 89770
    },
    {
      "epoch": 16323.636363636364,
      "grad_norm": 0.0007603734266012907,
      "learning_rate": 4.1569796488462387e-07,
      "loss": 0.0007,
      "step": 89780
    },
    {
      "epoch": 16325.454545454546,
      "grad_norm": 0.2174350619316101,
      "learning_rate": 4.1558327764363307e-07,
      "loss": 0.0014,
      "step": 89790
    },
    {
      "epoch": 16327.272727272728,
      "grad_norm": 0.26859116554260254,
      "learning_rate": 4.15468594974163e-07,
      "loss": 0.0013,
      "step": 89800
    },
    {
      "epoch": 16329.09090909091,
      "grad_norm": 0.0005172149976715446,
      "learning_rate": 4.1535391688242414e-07,
      "loss": 0.0009,
      "step": 89810
    },
    {
      "epoch": 16330.90909090909,
      "grad_norm": 0.19724297523498535,
      "learning_rate": 4.1523924337462696e-07,
      "loss": 0.001,
      "step": 89820
    },
    {
      "epoch": 16332.727272727272,
      "grad_norm": 0.0007958560599945486,
      "learning_rate": 4.1512457445698126e-07,
      "loss": 0.0007,
      "step": 89830
    },
    {
      "epoch": 16334.545454545454,
      "grad_norm": 0.0007342628668993711,
      "learning_rate": 4.1500991013569685e-07,
      "loss": 0.0015,
      "step": 89840
    },
    {
      "epoch": 16336.363636363636,
      "grad_norm": 0.001531754620373249,
      "learning_rate": 4.148952504169838e-07,
      "loss": 0.0012,
      "step": 89850
    },
    {
      "epoch": 16338.181818181818,
      "grad_norm": 0.15536490082740784,
      "learning_rate": 4.147805953070507e-07,
      "loss": 0.0013,
      "step": 89860
    },
    {
      "epoch": 16340.0,
      "grad_norm": 0.20757602155208588,
      "learning_rate": 4.146659448121071e-07,
      "loss": 0.0009,
      "step": 89870
    },
    {
      "epoch": 16341.818181818182,
      "grad_norm": 0.0009914591209962964,
      "learning_rate": 4.145512989383617e-07,
      "loss": 0.001,
      "step": 89880
    },
    {
      "epoch": 16343.636363636364,
      "grad_norm": 0.0032460300717502832,
      "learning_rate": 4.144366576920229e-07,
      "loss": 0.001,
      "step": 89890
    },
    {
      "epoch": 16345.454545454546,
      "grad_norm": 0.19788767397403717,
      "learning_rate": 4.1432202107929927e-07,
      "loss": 0.0012,
      "step": 89900
    },
    {
      "epoch": 16347.272727272728,
      "grad_norm": 0.15744978189468384,
      "learning_rate": 4.142073891063985e-07,
      "loss": 0.0013,
      "step": 89910
    },
    {
      "epoch": 16349.09090909091,
      "grad_norm": 0.15497653186321259,
      "learning_rate": 4.140927617795287e-07,
      "loss": 0.0012,
      "step": 89920
    },
    {
      "epoch": 16350.90909090909,
      "grad_norm": 0.20730380713939667,
      "learning_rate": 4.1397813910489744e-07,
      "loss": 0.001,
      "step": 89930
    },
    {
      "epoch": 16352.727272727272,
      "grad_norm": 0.005795012228190899,
      "learning_rate": 4.138635210887117e-07,
      "loss": 0.001,
      "step": 89940
    },
    {
      "epoch": 16354.545454545454,
      "grad_norm": 0.1513262540102005,
      "learning_rate": 4.1374890773717866e-07,
      "loss": 0.0012,
      "step": 89950
    },
    {
      "epoch": 16356.363636363636,
      "grad_norm": 0.0006065450143069029,
      "learning_rate": 4.136342990565054e-07,
      "loss": 0.0007,
      "step": 89960
    },
    {
      "epoch": 16358.181818181818,
      "grad_norm": 0.0005319483461789787,
      "learning_rate": 4.135196950528982e-07,
      "loss": 0.0012,
      "step": 89970
    },
    {
      "epoch": 16360.0,
      "grad_norm": 0.000594629324041307,
      "learning_rate": 4.134050957325633e-07,
      "loss": 0.0012,
      "step": 89980
    },
    {
      "epoch": 16361.818181818182,
      "grad_norm": 0.005566486157476902,
      "learning_rate": 4.1329050110170704e-07,
      "loss": 0.001,
      "step": 89990
    },
    {
      "epoch": 16363.636363636364,
      "grad_norm": 0.1636168360710144,
      "learning_rate": 4.131759111665348e-07,
      "loss": 0.0012,
      "step": 90000
    },
    {
      "epoch": 16363.636363636364,
      "eval_loss": 5.002274513244629,
      "eval_runtime": 0.9507,
      "eval_samples_per_second": 10.519,
      "eval_steps_per_second": 5.259,
      "step": 90000
    },
    {
      "epoch": 16365.454545454546,
      "grad_norm": 0.13911129534244537,
      "learning_rate": 4.130613259332525e-07,
      "loss": 0.001,
      "step": 90010
    },
    {
      "epoch": 16367.272727272728,
      "grad_norm": 0.23952719569206238,
      "learning_rate": 4.1294674540806506e-07,
      "loss": 0.0014,
      "step": 90020
    },
    {
      "epoch": 16369.09090909091,
      "grad_norm": 0.16090072691440582,
      "learning_rate": 4.128321695971775e-07,
      "loss": 0.0007,
      "step": 90030
    },
    {
      "epoch": 16370.90909090909,
      "grad_norm": 0.016679227352142334,
      "learning_rate": 4.127175985067951e-07,
      "loss": 0.001,
      "step": 90040
    },
    {
      "epoch": 16372.727272727272,
      "grad_norm": 0.001213408657349646,
      "learning_rate": 4.126030321431217e-07,
      "loss": 0.0012,
      "step": 90050
    },
    {
      "epoch": 16374.545454545454,
      "grad_norm": 0.0005192314856685698,
      "learning_rate": 4.124884705123619e-07,
      "loss": 0.0009,
      "step": 90060
    },
    {
      "epoch": 16376.363636363636,
      "grad_norm": 0.2089289277791977,
      "learning_rate": 4.123739136207198e-07,
      "loss": 0.0013,
      "step": 90070
    },
    {
      "epoch": 16378.181818181818,
      "grad_norm": 0.0006411589565686882,
      "learning_rate": 4.122593614743989e-07,
      "loss": 0.0009,
      "step": 90080
    },
    {
      "epoch": 16380.0,
      "grad_norm": 0.0007148505537770689,
      "learning_rate": 4.1214481407960276e-07,
      "loss": 0.0012,
      "step": 90090
    },
    {
      "epoch": 16381.818181818182,
      "grad_norm": 0.1932629495859146,
      "learning_rate": 4.120302714425346e-07,
      "loss": 0.0009,
      "step": 90100
    },
    {
      "epoch": 16383.636363636364,
      "grad_norm": 0.0012192714493721724,
      "learning_rate": 4.1191573356939734e-07,
      "loss": 0.0014,
      "step": 90110
    },
    {
      "epoch": 16385.454545454544,
      "grad_norm": 0.17096859216690063,
      "learning_rate": 4.118012004663939e-07,
      "loss": 0.0012,
      "step": 90120
    },
    {
      "epoch": 16387.272727272728,
      "grad_norm": 0.1499767005443573,
      "learning_rate": 4.1168667213972645e-07,
      "loss": 0.001,
      "step": 90130
    },
    {
      "epoch": 16389.090909090908,
      "grad_norm": 0.0009441446745768189,
      "learning_rate": 4.115721485955972e-07,
      "loss": 0.001,
      "step": 90140
    },
    {
      "epoch": 16390.909090909092,
      "grad_norm": 0.2091989368200302,
      "learning_rate": 4.114576298402084e-07,
      "loss": 0.0011,
      "step": 90150
    },
    {
      "epoch": 16392.727272727272,
      "grad_norm": 0.15452730655670166,
      "learning_rate": 4.1134311587976135e-07,
      "loss": 0.0013,
      "step": 90160
    },
    {
      "epoch": 16394.545454545456,
      "grad_norm": 0.1332869678735733,
      "learning_rate": 4.112286067204577e-07,
      "loss": 0.001,
      "step": 90170
    },
    {
      "epoch": 16396.363636363636,
      "grad_norm": 0.0007615803624503314,
      "learning_rate": 4.1111410236849854e-07,
      "loss": 0.0009,
      "step": 90180
    },
    {
      "epoch": 16398.18181818182,
      "grad_norm": 0.005828092806041241,
      "learning_rate": 4.109996028300846e-07,
      "loss": 0.0014,
      "step": 90190
    },
    {
      "epoch": 16400.0,
      "grad_norm": 0.000845787231810391,
      "learning_rate": 4.1088510811141685e-07,
      "loss": 0.0009,
      "step": 90200
    },
    {
      "epoch": 16401.81818181818,
      "grad_norm": 0.22357232868671417,
      "learning_rate": 4.1077061821869537e-07,
      "loss": 0.001,
      "step": 90210
    },
    {
      "epoch": 16403.636363636364,
      "grad_norm": 0.16773509979248047,
      "learning_rate": 4.106561331581202e-07,
      "loss": 0.0011,
      "step": 90220
    },
    {
      "epoch": 16405.454545454544,
      "grad_norm": 0.001892753760330379,
      "learning_rate": 4.105416529358917e-07,
      "loss": 0.0011,
      "step": 90230
    },
    {
      "epoch": 16407.272727272728,
      "grad_norm": 0.006367362570017576,
      "learning_rate": 4.1042717755820883e-07,
      "loss": 0.0015,
      "step": 90240
    },
    {
      "epoch": 16409.090909090908,
      "grad_norm": 0.0013050546403974295,
      "learning_rate": 4.1031270703127124e-07,
      "loss": 0.0009,
      "step": 90250
    },
    {
      "epoch": 16410.909090909092,
      "grad_norm": 0.21999575197696686,
      "learning_rate": 4.1019824136127806e-07,
      "loss": 0.001,
      "step": 90260
    },
    {
      "epoch": 16412.727272727272,
      "grad_norm": 0.0016571137821301818,
      "learning_rate": 4.100837805544279e-07,
      "loss": 0.0013,
      "step": 90270
    },
    {
      "epoch": 16414.545454545456,
      "grad_norm": 0.01102557685226202,
      "learning_rate": 4.0996932461691945e-07,
      "loss": 0.0009,
      "step": 90280
    },
    {
      "epoch": 16416.363636363636,
      "grad_norm": 0.20513950288295746,
      "learning_rate": 4.0985487355495076e-07,
      "loss": 0.0012,
      "step": 90290
    },
    {
      "epoch": 16418.18181818182,
      "grad_norm": 0.1974005103111267,
      "learning_rate": 4.0974042737472005e-07,
      "loss": 0.001,
      "step": 90300
    },
    {
      "epoch": 16420.0,
      "grad_norm": 0.0007781056920066476,
      "learning_rate": 4.09625986082425e-07,
      "loss": 0.001,
      "step": 90310
    },
    {
      "epoch": 16421.81818181818,
      "grad_norm": 0.20507825911045074,
      "learning_rate": 4.09511549684263e-07,
      "loss": 0.0012,
      "step": 90320
    },
    {
      "epoch": 16423.636363636364,
      "grad_norm": 0.20556673407554626,
      "learning_rate": 4.0939711818643124e-07,
      "loss": 0.0009,
      "step": 90330
    },
    {
      "epoch": 16425.454545454544,
      "grad_norm": 0.0004703582962974906,
      "learning_rate": 4.09282691595127e-07,
      "loss": 0.0011,
      "step": 90340
    },
    {
      "epoch": 16427.272727272728,
      "grad_norm": 0.01304804626852274,
      "learning_rate": 4.0916826991654653e-07,
      "loss": 0.0016,
      "step": 90350
    },
    {
      "epoch": 16429.090909090908,
      "grad_norm": 0.0007761602755635977,
      "learning_rate": 4.090538531568866e-07,
      "loss": 0.0006,
      "step": 90360
    },
    {
      "epoch": 16430.909090909092,
      "grad_norm": 0.001006299047730863,
      "learning_rate": 4.089394413223431e-07,
      "loss": 0.0012,
      "step": 90370
    },
    {
      "epoch": 16432.727272727272,
      "grad_norm": 0.0010135670891031623,
      "learning_rate": 4.08825034419112e-07,
      "loss": 0.0012,
      "step": 90380
    },
    {
      "epoch": 16434.545454545456,
      "grad_norm": 0.0007666584569960833,
      "learning_rate": 4.08710632453389e-07,
      "loss": 0.0011,
      "step": 90390
    },
    {
      "epoch": 16436.363636363636,
      "grad_norm": 0.16848191618919373,
      "learning_rate": 4.0859623543136936e-07,
      "loss": 0.0008,
      "step": 90400
    },
    {
      "epoch": 16438.18181818182,
      "grad_norm": 0.194000244140625,
      "learning_rate": 4.0848184335924793e-07,
      "loss": 0.0011,
      "step": 90410
    },
    {
      "epoch": 16440.0,
      "grad_norm": 0.0006028991774655879,
      "learning_rate": 4.083674562432202e-07,
      "loss": 0.0012,
      "step": 90420
    },
    {
      "epoch": 16441.81818181818,
      "grad_norm": 0.0010491114808246493,
      "learning_rate": 4.0825307408947987e-07,
      "loss": 0.0009,
      "step": 90430
    },
    {
      "epoch": 16443.636363636364,
      "grad_norm": 0.1729840189218521,
      "learning_rate": 4.0813869690422205e-07,
      "loss": 0.0012,
      "step": 90440
    },
    {
      "epoch": 16445.454545454544,
      "grad_norm": 0.16895698010921478,
      "learning_rate": 4.080243246936399e-07,
      "loss": 0.0014,
      "step": 90450
    },
    {
      "epoch": 16447.272727272728,
      "grad_norm": 0.0006466108025051653,
      "learning_rate": 4.0790995746392773e-07,
      "loss": 0.0007,
      "step": 90460
    },
    {
      "epoch": 16449.090909090908,
      "grad_norm": 0.19812129437923431,
      "learning_rate": 4.0779559522127893e-07,
      "loss": 0.0014,
      "step": 90470
    },
    {
      "epoch": 16450.909090909092,
      "grad_norm": 0.0011177272535860538,
      "learning_rate": 4.076812379718866e-07,
      "loss": 0.001,
      "step": 90480
    },
    {
      "epoch": 16452.727272727272,
      "grad_norm": 0.0007262549479492009,
      "learning_rate": 4.0756688572194356e-07,
      "loss": 0.001,
      "step": 90490
    },
    {
      "epoch": 16454.545454545456,
      "grad_norm": 0.0017931615002453327,
      "learning_rate": 4.074525384776428e-07,
      "loss": 0.0012,
      "step": 90500
    },
    {
      "epoch": 16454.545454545456,
      "eval_loss": 4.994046688079834,
      "eval_runtime": 0.95,
      "eval_samples_per_second": 10.527,
      "eval_steps_per_second": 5.263,
      "step": 90500
    },
    {
      "epoch": 16456.363636363636,
      "grad_norm": 0.16703665256500244,
      "learning_rate": 4.073381962451763e-07,
      "loss": 0.0012,
      "step": 90510
    },
    {
      "epoch": 16458.18181818182,
      "grad_norm": 0.27940627932548523,
      "learning_rate": 4.0722385903073653e-07,
      "loss": 0.0012,
      "step": 90520
    },
    {
      "epoch": 16460.0,
      "grad_norm": 0.0006856659892946482,
      "learning_rate": 4.0710952684051505e-07,
      "loss": 0.0009,
      "step": 90530
    },
    {
      "epoch": 16461.81818181818,
      "grad_norm": 0.0007472264114767313,
      "learning_rate": 4.069951996807034e-07,
      "loss": 0.001,
      "step": 90540
    },
    {
      "epoch": 16463.636363636364,
      "grad_norm": 0.0032321629114449024,
      "learning_rate": 4.068808775574933e-07,
      "loss": 0.0012,
      "step": 90550
    },
    {
      "epoch": 16465.454545454544,
      "grad_norm": 0.00047620825353078544,
      "learning_rate": 4.0676656047707537e-07,
      "loss": 0.0009,
      "step": 90560
    },
    {
      "epoch": 16467.272727272728,
      "grad_norm": 0.0015957460273057222,
      "learning_rate": 4.066522484456405e-07,
      "loss": 0.0012,
      "step": 90570
    },
    {
      "epoch": 16469.090909090908,
      "grad_norm": 0.0011941097909584641,
      "learning_rate": 4.0653794146937927e-07,
      "loss": 0.0012,
      "step": 90580
    },
    {
      "epoch": 16470.909090909092,
      "grad_norm": 0.0013601541286334395,
      "learning_rate": 4.0642363955448166e-07,
      "loss": 0.001,
      "step": 90590
    },
    {
      "epoch": 16472.727272727272,
      "grad_norm": 0.1450229287147522,
      "learning_rate": 4.0630934270713755e-07,
      "loss": 0.0013,
      "step": 90600
    },
    {
      "epoch": 16474.545454545456,
      "grad_norm": 0.15840940177440643,
      "learning_rate": 4.061950509335371e-07,
      "loss": 0.0007,
      "step": 90610
    },
    {
      "epoch": 16476.363636363636,
      "grad_norm": 0.0009008119232021272,
      "learning_rate": 4.060807642398691e-07,
      "loss": 0.0015,
      "step": 90620
    },
    {
      "epoch": 16478.18181818182,
      "grad_norm": 0.16783709824085236,
      "learning_rate": 4.0596648263232314e-07,
      "loss": 0.0012,
      "step": 90630
    },
    {
      "epoch": 16480.0,
      "grad_norm": 0.0005840829689987004,
      "learning_rate": 4.0585220611708754e-07,
      "loss": 0.0007,
      "step": 90640
    },
    {
      "epoch": 16481.81818181818,
      "grad_norm": 0.25069671869277954,
      "learning_rate": 4.0573793470035125e-07,
      "loss": 0.0012,
      "step": 90650
    },
    {
      "epoch": 16483.636363636364,
      "grad_norm": 0.21247351169586182,
      "learning_rate": 4.056236683883025e-07,
      "loss": 0.0012,
      "step": 90660
    },
    {
      "epoch": 16485.454545454544,
      "grad_norm": 0.18677707016468048,
      "learning_rate": 4.0550940718712906e-07,
      "loss": 0.0008,
      "step": 90670
    },
    {
      "epoch": 16487.272727272728,
      "grad_norm": 0.0006508991937153041,
      "learning_rate": 4.0539515110301885e-07,
      "loss": 0.0012,
      "step": 90680
    },
    {
      "epoch": 16489.090909090908,
      "grad_norm": 0.0007020457414910197,
      "learning_rate": 4.052809001421594e-07,
      "loss": 0.001,
      "step": 90690
    },
    {
      "epoch": 16490.909090909092,
      "grad_norm": 0.0005628918879665434,
      "learning_rate": 4.0516665431073764e-07,
      "loss": 0.0012,
      "step": 90700
    },
    {
      "epoch": 16492.727272727272,
      "grad_norm": 0.18412767350673676,
      "learning_rate": 4.050524136149407e-07,
      "loss": 0.0012,
      "step": 90710
    },
    {
      "epoch": 16494.545454545456,
      "grad_norm": 0.0012107397196814418,
      "learning_rate": 4.0493817806095496e-07,
      "loss": 0.0008,
      "step": 90720
    },
    {
      "epoch": 16496.363636363636,
      "grad_norm": 0.186890110373497,
      "learning_rate": 4.0482394765496676e-07,
      "loss": 0.0012,
      "step": 90730
    },
    {
      "epoch": 16498.18181818182,
      "grad_norm": 0.26036661863327026,
      "learning_rate": 4.0470972240316246e-07,
      "loss": 0.0013,
      "step": 90740
    },
    {
      "epoch": 16500.0,
      "grad_norm": 0.17428576946258545,
      "learning_rate": 4.0459550231172755e-07,
      "loss": 0.0009,
      "step": 90750
    },
    {
      "epoch": 16501.81818181818,
      "grad_norm": 0.16851869225502014,
      "learning_rate": 4.0448128738684767e-07,
      "loss": 0.0012,
      "step": 90760
    },
    {
      "epoch": 16503.636363636364,
      "grad_norm": 0.0005395511398091912,
      "learning_rate": 4.0436707763470804e-07,
      "loss": 0.0008,
      "step": 90770
    },
    {
      "epoch": 16505.454545454544,
      "grad_norm": 0.000605706765782088,
      "learning_rate": 4.042528730614935e-07,
      "loss": 0.0013,
      "step": 90780
    },
    {
      "epoch": 16507.272727272728,
      "grad_norm": 0.15101417899131775,
      "learning_rate": 4.0413867367338883e-07,
      "loss": 0.0013,
      "step": 90790
    },
    {
      "epoch": 16509.090909090908,
      "grad_norm": 0.3011730909347534,
      "learning_rate": 4.0402447947657825e-07,
      "loss": 0.0015,
      "step": 90800
    },
    {
      "epoch": 16510.909090909092,
      "grad_norm": 0.005654544569551945,
      "learning_rate": 4.0391029047724585e-07,
      "loss": 0.0009,
      "step": 90810
    },
    {
      "epoch": 16512.727272727272,
      "grad_norm": 0.18839885294437408,
      "learning_rate": 4.037961066815758e-07,
      "loss": 0.001,
      "step": 90820
    },
    {
      "epoch": 16514.545454545456,
      "grad_norm": 0.19847926497459412,
      "learning_rate": 4.036819280957511e-07,
      "loss": 0.001,
      "step": 90830
    },
    {
      "epoch": 16516.363636363636,
      "grad_norm": 0.21953469514846802,
      "learning_rate": 4.0356775472595537e-07,
      "loss": 0.0012,
      "step": 90840
    },
    {
      "epoch": 16518.18181818182,
      "grad_norm": 0.0008707757224328816,
      "learning_rate": 4.0345358657837163e-07,
      "loss": 0.0009,
      "step": 90850
    },
    {
      "epoch": 16520.0,
      "grad_norm": 0.20774631202220917,
      "learning_rate": 4.0333942365918225e-07,
      "loss": 0.0012,
      "step": 90860
    },
    {
      "epoch": 16521.81818181818,
      "grad_norm": 0.15297774970531464,
      "learning_rate": 4.0322526597456987e-07,
      "loss": 0.0009,
      "step": 90870
    },
    {
      "epoch": 16523.636363636364,
      "grad_norm": 0.0005852405447512865,
      "learning_rate": 4.0311111353071657e-07,
      "loss": 0.0009,
      "step": 90880
    },
    {
      "epoch": 16525.454545454544,
      "grad_norm": 0.20559929311275482,
      "learning_rate": 4.029969663338041e-07,
      "loss": 0.0016,
      "step": 90890
    },
    {
      "epoch": 16527.272727272728,
      "grad_norm": 0.15101435780525208,
      "learning_rate": 4.028828243900141e-07,
      "loss": 0.0009,
      "step": 90900
    },
    {
      "epoch": 16529.090909090908,
      "grad_norm": 0.0006004056776873767,
      "learning_rate": 4.0276868770552773e-07,
      "loss": 0.001,
      "step": 90910
    },
    {
      "epoch": 16530.909090909092,
      "grad_norm": 0.15296944975852966,
      "learning_rate": 4.0265455628652597e-07,
      "loss": 0.0012,
      "step": 90920
    },
    {
      "epoch": 16532.727272727272,
      "grad_norm": 0.19177548587322235,
      "learning_rate": 4.025404301391898e-07,
      "loss": 0.0007,
      "step": 90930
    },
    {
      "epoch": 16534.545454545456,
      "grad_norm": 0.0006001737783662975,
      "learning_rate": 4.0242630926969925e-07,
      "loss": 0.0012,
      "step": 90940
    },
    {
      "epoch": 16536.363636363636,
      "grad_norm": 0.15857642889022827,
      "learning_rate": 4.023121936842346e-07,
      "loss": 0.0012,
      "step": 90950
    },
    {
      "epoch": 16538.18181818182,
      "grad_norm": 0.20828260481357574,
      "learning_rate": 4.021980833889759e-07,
      "loss": 0.0012,
      "step": 90960
    },
    {
      "epoch": 16540.0,
      "grad_norm": 0.0007338386494666338,
      "learning_rate": 4.020839783901024e-07,
      "loss": 0.001,
      "step": 90970
    },
    {
      "epoch": 16541.81818181818,
      "grad_norm": 0.0012704910477623343,
      "learning_rate": 4.019698786937935e-07,
      "loss": 0.0012,
      "step": 90980
    },
    {
      "epoch": 16543.636363636364,
      "grad_norm": 0.0004921014769934118,
      "learning_rate": 4.018557843062281e-07,
      "loss": 0.0009,
      "step": 90990
    },
    {
      "epoch": 16545.454545454544,
      "grad_norm": 0.0006663220701739192,
      "learning_rate": 4.0174169523358485e-07,
      "loss": 0.0014,
      "step": 91000
    },
    {
      "epoch": 16545.454545454544,
      "eval_loss": 5.098559379577637,
      "eval_runtime": 0.9509,
      "eval_samples_per_second": 10.516,
      "eval_steps_per_second": 5.258,
      "step": 91000
    },
    {
      "epoch": 16547.272727272728,
      "grad_norm": 0.000583567307330668,
      "learning_rate": 4.016276114820425e-07,
      "loss": 0.0011,
      "step": 91010
    },
    {
      "epoch": 16549.090909090908,
      "grad_norm": 0.17747698724269867,
      "learning_rate": 4.0151353305777866e-07,
      "loss": 0.0011,
      "step": 91020
    },
    {
      "epoch": 16550.909090909092,
      "grad_norm": 0.20725072920322418,
      "learning_rate": 4.013994599669715e-07,
      "loss": 0.0012,
      "step": 91030
    },
    {
      "epoch": 16552.727272727272,
      "grad_norm": 0.272432416677475,
      "learning_rate": 4.012853922157986e-07,
      "loss": 0.0011,
      "step": 91040
    },
    {
      "epoch": 16554.545454545456,
      "grad_norm": 0.0005587228806689382,
      "learning_rate": 4.011713298104369e-07,
      "loss": 0.0012,
      "step": 91050
    },
    {
      "epoch": 16556.363636363636,
      "grad_norm": 0.0006736572249792516,
      "learning_rate": 4.0105727275706367e-07,
      "loss": 0.0009,
      "step": 91060
    },
    {
      "epoch": 16558.18181818182,
      "grad_norm": 0.19762475788593292,
      "learning_rate": 4.0094322106185535e-07,
      "loss": 0.0012,
      "step": 91070
    },
    {
      "epoch": 16560.0,
      "grad_norm": 0.15453340113162994,
      "learning_rate": 4.008291747309884e-07,
      "loss": 0.001,
      "step": 91080
    },
    {
      "epoch": 16561.81818181818,
      "grad_norm": 0.001720243482850492,
      "learning_rate": 4.00715133770639e-07,
      "loss": 0.001,
      "step": 91090
    },
    {
      "epoch": 16563.636363636364,
      "grad_norm": 0.16038763523101807,
      "learning_rate": 4.0060109818698283e-07,
      "loss": 0.0011,
      "step": 91100
    },
    {
      "epoch": 16565.454545454544,
      "grad_norm": 0.0008493228233419359,
      "learning_rate": 4.0048706798619523e-07,
      "loss": 0.0008,
      "step": 91110
    },
    {
      "epoch": 16567.272727272728,
      "grad_norm": 0.0015192452119663358,
      "learning_rate": 4.003730431744521e-07,
      "loss": 0.0013,
      "step": 91120
    },
    {
      "epoch": 16569.090909090908,
      "grad_norm": 0.20921476185321808,
      "learning_rate": 4.002590237579274e-07,
      "loss": 0.0011,
      "step": 91130
    },
    {
      "epoch": 16570.909090909092,
      "grad_norm": 0.0011612018570303917,
      "learning_rate": 4.0014500974279654e-07,
      "loss": 0.001,
      "step": 91140
    },
    {
      "epoch": 16572.727272727272,
      "grad_norm": 0.16005530953407288,
      "learning_rate": 4.0003100113523346e-07,
      "loss": 0.0008,
      "step": 91150
    },
    {
      "epoch": 16574.545454545456,
      "grad_norm": 0.0017965688602998853,
      "learning_rate": 3.999169979414123e-07,
      "loss": 0.0011,
      "step": 91160
    },
    {
      "epoch": 16576.363636363636,
      "grad_norm": 0.000518019194714725,
      "learning_rate": 3.998030001675069e-07,
      "loss": 0.0013,
      "step": 91170
    },
    {
      "epoch": 16578.18181818182,
      "grad_norm": 0.0005987760378047824,
      "learning_rate": 3.9968900781969056e-07,
      "loss": 0.001,
      "step": 91180
    },
    {
      "epoch": 16580.0,
      "grad_norm": 0.0004756514390464872,
      "learning_rate": 3.9957502090413644e-07,
      "loss": 0.0012,
      "step": 91190
    },
    {
      "epoch": 16581.81818181818,
      "grad_norm": 0.001440537627786398,
      "learning_rate": 3.9946103942701775e-07,
      "loss": 0.0012,
      "step": 91200
    },
    {
      "epoch": 16583.636363636364,
      "grad_norm": 0.19726325571537018,
      "learning_rate": 3.993470633945065e-07,
      "loss": 0.0013,
      "step": 91210
    },
    {
      "epoch": 16585.454545454544,
      "grad_norm": 0.21977579593658447,
      "learning_rate": 3.992330928127755e-07,
      "loss": 0.0012,
      "step": 91220
    },
    {
      "epoch": 16587.272727272728,
      "grad_norm": 0.18332384526729584,
      "learning_rate": 3.9911912768799653e-07,
      "loss": 0.0009,
      "step": 91230
    },
    {
      "epoch": 16589.090909090908,
      "grad_norm": 0.18581043183803558,
      "learning_rate": 3.990051680263412e-07,
      "loss": 0.0012,
      "step": 91240
    },
    {
      "epoch": 16590.909090909092,
      "grad_norm": 0.1975337415933609,
      "learning_rate": 3.988912138339811e-07,
      "loss": 0.0011,
      "step": 91250
    },
    {
      "epoch": 16592.727272727272,
      "grad_norm": 0.1875850409269333,
      "learning_rate": 3.987772651170871e-07,
      "loss": 0.0012,
      "step": 91260
    },
    {
      "epoch": 16594.545454545456,
      "grad_norm": 0.0005222311010584235,
      "learning_rate": 3.9866332188183004e-07,
      "loss": 0.0009,
      "step": 91270
    },
    {
      "epoch": 16596.363636363636,
      "grad_norm": 0.0005914351204410195,
      "learning_rate": 3.985493841343807e-07,
      "loss": 0.0012,
      "step": 91280
    },
    {
      "epoch": 16598.18181818182,
      "grad_norm": 0.2213125377893448,
      "learning_rate": 3.9843545188090895e-07,
      "loss": 0.0012,
      "step": 91290
    },
    {
      "epoch": 16600.0,
      "grad_norm": 0.1607448011636734,
      "learning_rate": 3.9832152512758466e-07,
      "loss": 0.0011,
      "step": 91300
    },
    {
      "epoch": 16601.81818181818,
      "grad_norm": 0.0007908362313173711,
      "learning_rate": 3.9820760388057805e-07,
      "loss": 0.001,
      "step": 91310
    },
    {
      "epoch": 16603.636363636364,
      "grad_norm": 0.1566046178340912,
      "learning_rate": 3.9809368814605757e-07,
      "loss": 0.0013,
      "step": 91320
    },
    {
      "epoch": 16605.454545454544,
      "grad_norm": 0.24924340844154358,
      "learning_rate": 3.97979777930193e-07,
      "loss": 0.0012,
      "step": 91330
    },
    {
      "epoch": 16607.272727272728,
      "grad_norm": 0.15450693666934967,
      "learning_rate": 3.9786587323915257e-07,
      "loss": 0.0007,
      "step": 91340
    },
    {
      "epoch": 16609.090909090908,
      "grad_norm": 0.0008201845921576023,
      "learning_rate": 3.977519740791048e-07,
      "loss": 0.001,
      "step": 91350
    },
    {
      "epoch": 16610.909090909092,
      "grad_norm": 0.2636873126029968,
      "learning_rate": 3.9763808045621806e-07,
      "loss": 0.0012,
      "step": 91360
    },
    {
      "epoch": 16612.727272727272,
      "grad_norm": 0.001898995484225452,
      "learning_rate": 3.975241923766598e-07,
      "loss": 0.001,
      "step": 91370
    },
    {
      "epoch": 16614.545454545456,
      "grad_norm": 0.1382034569978714,
      "learning_rate": 3.9741030984659764e-07,
      "loss": 0.0014,
      "step": 91380
    },
    {
      "epoch": 16616.363636363636,
      "grad_norm": 0.0007008413667790592,
      "learning_rate": 3.9729643287219916e-07,
      "loss": 0.001,
      "step": 91390
    },
    {
      "epoch": 16618.18181818182,
      "grad_norm": 0.2520236372947693,
      "learning_rate": 3.971825614596307e-07,
      "loss": 0.0012,
      "step": 91400
    },
    {
      "epoch": 16620.0,
      "grad_norm": 0.0006388192414306104,
      "learning_rate": 3.9706869561505945e-07,
      "loss": 0.0009,
      "step": 91410
    },
    {
      "epoch": 16621.81818181818,
      "grad_norm": 0.0009389397455379367,
      "learning_rate": 3.969548353446511e-07,
      "loss": 0.0009,
      "step": 91420
    },
    {
      "epoch": 16623.636363636364,
      "grad_norm": 0.15594515204429626,
      "learning_rate": 3.9684098065457225e-07,
      "loss": 0.0015,
      "step": 91430
    },
    {
      "epoch": 16625.454545454544,
      "grad_norm": 0.0007130498997867107,
      "learning_rate": 3.9672713155098837e-07,
      "loss": 0.0006,
      "step": 91440
    },
    {
      "epoch": 16627.272727272728,
      "grad_norm": 0.0008395587792620063,
      "learning_rate": 3.9661328804006473e-07,
      "loss": 0.0012,
      "step": 91450
    },
    {
      "epoch": 16629.090909090908,
      "grad_norm": 0.0007447631796821952,
      "learning_rate": 3.9649945012796656e-07,
      "loss": 0.0012,
      "step": 91460
    },
    {
      "epoch": 16630.909090909092,
      "grad_norm": 0.0014545769663527608,
      "learning_rate": 3.963856178208588e-07,
      "loss": 0.0009,
      "step": 91470
    },
    {
      "epoch": 16632.727272727272,
      "grad_norm": 0.2540137767791748,
      "learning_rate": 3.9627179112490573e-07,
      "loss": 0.0013,
      "step": 91480
    },
    {
      "epoch": 16634.545454545456,
      "grad_norm": 0.000954013376031071,
      "learning_rate": 3.961579700462715e-07,
      "loss": 0.0012,
      "step": 91490
    },
    {
      "epoch": 16636.363636363636,
      "grad_norm": 0.0009422603761777282,
      "learning_rate": 3.960441545911204e-07,
      "loss": 0.0007,
      "step": 91500
    },
    {
      "epoch": 16636.363636363636,
      "eval_loss": 5.094102382659912,
      "eval_runtime": 0.9498,
      "eval_samples_per_second": 10.529,
      "eval_steps_per_second": 5.264,
      "step": 91500
    },
    {
      "epoch": 16638.18181818182,
      "grad_norm": 0.22529637813568115,
      "learning_rate": 3.959303447656155e-07,
      "loss": 0.0014,
      "step": 91510
    },
    {
      "epoch": 16640.0,
      "grad_norm": 0.0005218510632403195,
      "learning_rate": 3.958165405759205e-07,
      "loss": 0.001,
      "step": 91520
    },
    {
      "epoch": 16641.81818181818,
      "grad_norm": 0.0010069976560771465,
      "learning_rate": 3.9570274202819807e-07,
      "loss": 0.0012,
      "step": 91530
    },
    {
      "epoch": 16643.636363636364,
      "grad_norm": 0.2061408907175064,
      "learning_rate": 3.9558894912861106e-07,
      "loss": 0.001,
      "step": 91540
    },
    {
      "epoch": 16645.454545454544,
      "grad_norm": 0.005301016848534346,
      "learning_rate": 3.954751618833219e-07,
      "loss": 0.001,
      "step": 91550
    },
    {
      "epoch": 16647.272727272728,
      "grad_norm": 0.000575908285100013,
      "learning_rate": 3.9536138029849244e-07,
      "loss": 0.0009,
      "step": 91560
    },
    {
      "epoch": 16649.090909090908,
      "grad_norm": 0.1740780472755432,
      "learning_rate": 3.952476043802844e-07,
      "loss": 0.0013,
      "step": 91570
    },
    {
      "epoch": 16650.909090909092,
      "grad_norm": 0.21207448840141296,
      "learning_rate": 3.9513383413485967e-07,
      "loss": 0.0011,
      "step": 91580
    },
    {
      "epoch": 16652.727272727272,
      "grad_norm": 0.0009969628881663084,
      "learning_rate": 3.9502006956837873e-07,
      "loss": 0.0012,
      "step": 91590
    },
    {
      "epoch": 16654.545454545456,
      "grad_norm": 0.19552642107009888,
      "learning_rate": 3.949063106870031e-07,
      "loss": 0.0009,
      "step": 91600
    },
    {
      "epoch": 16656.363636363636,
      "grad_norm": 0.001074140309356153,
      "learning_rate": 3.947925574968926e-07,
      "loss": 0.001,
      "step": 91610
    },
    {
      "epoch": 16658.18181818182,
      "grad_norm": 0.0008401509840041399,
      "learning_rate": 3.9467881000420796e-07,
      "loss": 0.0012,
      "step": 91620
    },
    {
      "epoch": 16660.0,
      "grad_norm": 0.19328097999095917,
      "learning_rate": 3.945650682151089e-07,
      "loss": 0.0012,
      "step": 91630
    },
    {
      "epoch": 16661.81818181818,
      "grad_norm": 0.15250922739505768,
      "learning_rate": 3.9445133213575503e-07,
      "loss": 0.0012,
      "step": 91640
    },
    {
      "epoch": 16663.636363636364,
      "grad_norm": 0.0009787144372239709,
      "learning_rate": 3.943376017723057e-07,
      "loss": 0.0009,
      "step": 91650
    },
    {
      "epoch": 16665.454545454544,
      "grad_norm": 0.14359992742538452,
      "learning_rate": 3.942238771309198e-07,
      "loss": 0.0012,
      "step": 91660
    },
    {
      "epoch": 16667.272727272728,
      "grad_norm": 0.1616976410150528,
      "learning_rate": 3.94110158217756e-07,
      "loss": 0.0012,
      "step": 91670
    },
    {
      "epoch": 16669.090909090908,
      "grad_norm": 0.22325868904590607,
      "learning_rate": 3.9399644503897276e-07,
      "loss": 0.0011,
      "step": 91680
    },
    {
      "epoch": 16670.909090909092,
      "grad_norm": 0.22451522946357727,
      "learning_rate": 3.93882737600728e-07,
      "loss": 0.0012,
      "step": 91690
    },
    {
      "epoch": 16672.727272727272,
      "grad_norm": 0.25823184847831726,
      "learning_rate": 3.937690359091794e-07,
      "loss": 0.001,
      "step": 91700
    },
    {
      "epoch": 16674.545454545456,
      "grad_norm": 0.1870996206998825,
      "learning_rate": 3.9365533997048474e-07,
      "loss": 0.0011,
      "step": 91710
    },
    {
      "epoch": 16676.363636363636,
      "grad_norm": 0.2428881973028183,
      "learning_rate": 3.935416497908006e-07,
      "loss": 0.0013,
      "step": 91720
    },
    {
      "epoch": 16678.18181818182,
      "grad_norm": 0.16062097251415253,
      "learning_rate": 3.934279653762842e-07,
      "loss": 0.0009,
      "step": 91730
    },
    {
      "epoch": 16680.0,
      "grad_norm": 0.0005206604837439954,
      "learning_rate": 3.9331428673309204e-07,
      "loss": 0.001,
      "step": 91740
    },
    {
      "epoch": 16681.81818181818,
      "grad_norm": 0.2711426615715027,
      "learning_rate": 3.9320061386738e-07,
      "loss": 0.0012,
      "step": 91750
    },
    {
      "epoch": 16683.636363636364,
      "grad_norm": 0.16905662417411804,
      "learning_rate": 3.9308694678530417e-07,
      "loss": 0.001,
      "step": 91760
    },
    {
      "epoch": 16685.454545454544,
      "grad_norm": 0.0014990728814154863,
      "learning_rate": 3.9297328549302e-07,
      "loss": 0.0012,
      "step": 91770
    },
    {
      "epoch": 16687.272727272728,
      "grad_norm": 0.19836196303367615,
      "learning_rate": 3.9285962999668267e-07,
      "loss": 0.0011,
      "step": 91780
    },
    {
      "epoch": 16689.090909090908,
      "grad_norm": 0.0006753610796295106,
      "learning_rate": 3.9274598030244746e-07,
      "loss": 0.0009,
      "step": 91790
    },
    {
      "epoch": 16690.909090909092,
      "grad_norm": 0.19868135452270508,
      "learning_rate": 3.9263233641646836e-07,
      "loss": 0.0012,
      "step": 91800
    },
    {
      "epoch": 16692.727272727272,
      "grad_norm": 0.19744499027729034,
      "learning_rate": 3.9251869834490013e-07,
      "loss": 0.001,
      "step": 91810
    },
    {
      "epoch": 16694.545454545456,
      "grad_norm": 0.1723114550113678,
      "learning_rate": 3.9240506609389683e-07,
      "loss": 0.001,
      "step": 91820
    },
    {
      "epoch": 16696.363636363636,
      "grad_norm": 0.20630396902561188,
      "learning_rate": 3.9229143966961174e-07,
      "loss": 0.0012,
      "step": 91830
    },
    {
      "epoch": 16698.18181818182,
      "grad_norm": 0.0008157037082128227,
      "learning_rate": 3.921778190781985e-07,
      "loss": 0.0009,
      "step": 91840
    },
    {
      "epoch": 16700.0,
      "grad_norm": 0.00048545407480560243,
      "learning_rate": 3.9206420432581e-07,
      "loss": 0.0012,
      "step": 91850
    },
    {
      "epoch": 16701.81818181818,
      "grad_norm": 0.20914903283119202,
      "learning_rate": 3.9195059541859897e-07,
      "loss": 0.0012,
      "step": 91860
    },
    {
      "epoch": 16703.636363636364,
      "grad_norm": 0.18957844376564026,
      "learning_rate": 3.918369923627179e-07,
      "loss": 0.0011,
      "step": 91870
    },
    {
      "epoch": 16705.454545454544,
      "grad_norm": 0.19895920157432556,
      "learning_rate": 3.917233951643188e-07,
      "loss": 0.001,
      "step": 91880
    },
    {
      "epoch": 16707.272727272728,
      "grad_norm": 0.20857904851436615,
      "learning_rate": 3.9160980382955333e-07,
      "loss": 0.001,
      "step": 91890
    },
    {
      "epoch": 16709.090909090908,
      "grad_norm": 0.198616161942482,
      "learning_rate": 3.9149621836457333e-07,
      "loss": 0.0012,
      "step": 91900
    },
    {
      "epoch": 16710.909090909092,
      "grad_norm": 0.21254879236221313,
      "learning_rate": 3.913826387755293e-07,
      "loss": 0.0009,
      "step": 91910
    },
    {
      "epoch": 16712.727272727272,
      "grad_norm": 0.000670995912514627,
      "learning_rate": 3.912690650685726e-07,
      "loss": 0.0013,
      "step": 91920
    },
    {
      "epoch": 16714.545454545456,
      "grad_norm": 0.001110711833462119,
      "learning_rate": 3.9115549724985364e-07,
      "loss": 0.0009,
      "step": 91930
    },
    {
      "epoch": 16716.363636363636,
      "grad_norm": 0.0007485100068151951,
      "learning_rate": 3.910419353255223e-07,
      "loss": 0.001,
      "step": 91940
    },
    {
      "epoch": 16718.18181818182,
      "grad_norm": 0.20120768249034882,
      "learning_rate": 3.909283793017288e-07,
      "loss": 0.0012,
      "step": 91950
    },
    {
      "epoch": 16720.0,
      "grad_norm": 0.20983442664146423,
      "learning_rate": 3.908148291846224e-07,
      "loss": 0.001,
      "step": 91960
    },
    {
      "epoch": 16721.81818181818,
      "grad_norm": 0.0007066907710395753,
      "learning_rate": 3.9070128498035225e-07,
      "loss": 0.001,
      "step": 91970
    },
    {
      "epoch": 16723.636363636364,
      "grad_norm": 0.25385576486587524,
      "learning_rate": 3.905877466950678e-07,
      "loss": 0.001,
      "step": 91980
    },
    {
      "epoch": 16725.454545454544,
      "grad_norm": 0.15824562311172485,
      "learning_rate": 3.904742143349169e-07,
      "loss": 0.001,
      "step": 91990
    },
    {
      "epoch": 16727.272727272728,
      "grad_norm": 0.011086318641901016,
      "learning_rate": 3.9036068790604823e-07,
      "loss": 0.0015,
      "step": 92000
    },
    {
      "epoch": 16727.272727272728,
      "eval_loss": 5.112431526184082,
      "eval_runtime": 0.9499,
      "eval_samples_per_second": 10.527,
      "eval_steps_per_second": 5.264,
      "step": 92000
    },
    {
      "epoch": 16729.090909090908,
      "grad_norm": 0.16308064758777618,
      "learning_rate": 3.9024716741460985e-07,
      "loss": 0.0009,
      "step": 92010
    },
    {
      "epoch": 16730.909090909092,
      "grad_norm": 0.21240302920341492,
      "learning_rate": 3.90133652866749e-07,
      "loss": 0.0009,
      "step": 92020
    },
    {
      "epoch": 16732.727272727272,
      "grad_norm": 0.0014033436309546232,
      "learning_rate": 3.900201442686132e-07,
      "loss": 0.0014,
      "step": 92030
    },
    {
      "epoch": 16734.545454545456,
      "grad_norm": 0.16506771743297577,
      "learning_rate": 3.8990664162634923e-07,
      "loss": 0.001,
      "step": 92040
    },
    {
      "epoch": 16736.363636363636,
      "grad_norm": 0.15944626927375793,
      "learning_rate": 3.8979314494610395e-07,
      "loss": 0.0009,
      "step": 92050
    },
    {
      "epoch": 16738.18181818182,
      "grad_norm": 0.26467031240463257,
      "learning_rate": 3.896796542340236e-07,
      "loss": 0.0013,
      "step": 92060
    },
    {
      "epoch": 16740.0,
      "grad_norm": 0.21190693974494934,
      "learning_rate": 3.8956616949625414e-07,
      "loss": 0.0012,
      "step": 92070
    },
    {
      "epoch": 16741.81818181818,
      "grad_norm": 0.15993833541870117,
      "learning_rate": 3.8945269073894117e-07,
      "loss": 0.001,
      "step": 92080
    },
    {
      "epoch": 16743.636363636364,
      "grad_norm": 0.19902649521827698,
      "learning_rate": 3.893392179682304e-07,
      "loss": 0.0013,
      "step": 92090
    },
    {
      "epoch": 16745.454545454544,
      "grad_norm": 0.20325535535812378,
      "learning_rate": 3.8922575119026635e-07,
      "loss": 0.001,
      "step": 92100
    },
    {
      "epoch": 16747.272727272728,
      "grad_norm": 0.21272294223308563,
      "learning_rate": 3.89112290411194e-07,
      "loss": 0.0012,
      "step": 92110
    },
    {
      "epoch": 16749.090909090908,
      "grad_norm": 0.0006716361967846751,
      "learning_rate": 3.889988356371579e-07,
      "loss": 0.0009,
      "step": 92120
    },
    {
      "epoch": 16750.909090909092,
      "grad_norm": 0.16901203989982605,
      "learning_rate": 3.8888538687430177e-07,
      "loss": 0.0012,
      "step": 92130
    },
    {
      "epoch": 16752.727272727272,
      "grad_norm": 0.20924203097820282,
      "learning_rate": 3.887719441287696e-07,
      "loss": 0.0008,
      "step": 92140
    },
    {
      "epoch": 16754.545454545456,
      "grad_norm": 0.14567281305789948,
      "learning_rate": 3.886585074067046e-07,
      "loss": 0.0013,
      "step": 92150
    },
    {
      "epoch": 16756.363636363636,
      "grad_norm": 0.0005567719344981015,
      "learning_rate": 3.8854507671424973e-07,
      "loss": 0.0007,
      "step": 92160
    },
    {
      "epoch": 16758.18181818182,
      "grad_norm": 0.2617684006690979,
      "learning_rate": 3.884316520575483e-07,
      "loss": 0.0015,
      "step": 92170
    },
    {
      "epoch": 16760.0,
      "grad_norm": 0.20160481333732605,
      "learning_rate": 3.883182334427421e-07,
      "loss": 0.0011,
      "step": 92180
    },
    {
      "epoch": 16761.81818181818,
      "grad_norm": 0.0005240417085587978,
      "learning_rate": 3.882048208759734e-07,
      "loss": 0.001,
      "step": 92190
    },
    {
      "epoch": 16763.636363636364,
      "grad_norm": 0.0010978898499161005,
      "learning_rate": 3.880914143633843e-07,
      "loss": 0.0012,
      "step": 92200
    },
    {
      "epoch": 16765.454545454544,
      "grad_norm": 0.0006388426409102976,
      "learning_rate": 3.879780139111159e-07,
      "loss": 0.001,
      "step": 92210
    },
    {
      "epoch": 16767.272727272728,
      "grad_norm": 0.0006784209981560707,
      "learning_rate": 3.8786461952530946e-07,
      "loss": 0.0009,
      "step": 92220
    },
    {
      "epoch": 16769.090909090908,
      "grad_norm": 0.18613232672214508,
      "learning_rate": 3.877512312121056e-07,
      "loss": 0.0013,
      "step": 92230
    },
    {
      "epoch": 16770.909090909092,
      "grad_norm": 0.22396385669708252,
      "learning_rate": 3.876378489776449e-07,
      "loss": 0.0009,
      "step": 92240
    },
    {
      "epoch": 16772.727272727272,
      "grad_norm": 0.020121904090046883,
      "learning_rate": 3.8752447282806755e-07,
      "loss": 0.0012,
      "step": 92250
    },
    {
      "epoch": 16774.545454545456,
      "grad_norm": 0.15846092998981476,
      "learning_rate": 3.8741110276951315e-07,
      "loss": 0.001,
      "step": 92260
    },
    {
      "epoch": 16776.363636363636,
      "grad_norm": 0.0031577341724187136,
      "learning_rate": 3.8729773880812124e-07,
      "loss": 0.0009,
      "step": 92270
    },
    {
      "epoch": 16778.18181818182,
      "grad_norm": 0.0007985022384673357,
      "learning_rate": 3.8718438095003126e-07,
      "loss": 0.0014,
      "step": 92280
    },
    {
      "epoch": 16780.0,
      "grad_norm": 0.00048643132322467864,
      "learning_rate": 3.8707102920138146e-07,
      "loss": 0.0012,
      "step": 92290
    },
    {
      "epoch": 16781.81818181818,
      "grad_norm": 0.0004489912244025618,
      "learning_rate": 3.8695768356831086e-07,
      "loss": 0.0012,
      "step": 92300
    },
    {
      "epoch": 16783.636363636364,
      "grad_norm": 0.0005508196773007512,
      "learning_rate": 3.868443440569571e-07,
      "loss": 0.001,
      "step": 92310
    },
    {
      "epoch": 16785.454545454544,
      "grad_norm": 0.0006772511405870318,
      "learning_rate": 3.8673101067345825e-07,
      "loss": 0.0012,
      "step": 92320
    },
    {
      "epoch": 16787.272727272728,
      "grad_norm": 0.0008304398506879807,
      "learning_rate": 3.8661768342395197e-07,
      "loss": 0.001,
      "step": 92330
    },
    {
      "epoch": 16789.090909090908,
      "grad_norm": 0.17531347274780273,
      "learning_rate": 3.865043623145751e-07,
      "loss": 0.0011,
      "step": 92340
    },
    {
      "epoch": 16790.909090909092,
      "grad_norm": 0.007889857515692711,
      "learning_rate": 3.863910473514644e-07,
      "loss": 0.0012,
      "step": 92350
    },
    {
      "epoch": 16792.727272727272,
      "grad_norm": 0.16912952065467834,
      "learning_rate": 3.8627773854075683e-07,
      "loss": 0.0012,
      "step": 92360
    },
    {
      "epoch": 16794.545454545456,
      "grad_norm": 0.0008869656012393534,
      "learning_rate": 3.8616443588858797e-07,
      "loss": 0.0008,
      "step": 92370
    },
    {
      "epoch": 16796.363636363636,
      "grad_norm": 0.0007995173218660057,
      "learning_rate": 3.8605113940109424e-07,
      "loss": 0.0013,
      "step": 92380
    },
    {
      "epoch": 16798.18181818182,
      "grad_norm": 0.20792537927627563,
      "learning_rate": 3.859378490844104e-07,
      "loss": 0.0011,
      "step": 92390
    },
    {
      "epoch": 16800.0,
      "grad_norm": 0.20574361085891724,
      "learning_rate": 3.8582456494467206e-07,
      "loss": 0.0011,
      "step": 92400
    },
    {
      "epoch": 16801.81818181818,
      "grad_norm": 0.16189821064472198,
      "learning_rate": 3.8571128698801415e-07,
      "loss": 0.001,
      "step": 92410
    },
    {
      "epoch": 16803.636363636364,
      "grad_norm": 0.0011818032944574952,
      "learning_rate": 3.855980152205708e-07,
      "loss": 0.001,
      "step": 92420
    },
    {
      "epoch": 16805.454545454544,
      "grad_norm": 0.16275833547115326,
      "learning_rate": 3.854847496484762e-07,
      "loss": 0.0017,
      "step": 92430
    },
    {
      "epoch": 16807.272727272728,
      "grad_norm": 0.15596218407154083,
      "learning_rate": 3.853714902778644e-07,
      "loss": 0.0008,
      "step": 92440
    },
    {
      "epoch": 16809.090909090908,
      "grad_norm": 0.000844369875267148,
      "learning_rate": 3.852582371148687e-07,
      "loss": 0.001,
      "step": 92450
    },
    {
      "epoch": 16810.909090909092,
      "grad_norm": 0.2581169903278351,
      "learning_rate": 3.851449901656221e-07,
      "loss": 0.0012,
      "step": 92460
    },
    {
      "epoch": 16812.727272727272,
      "grad_norm": 0.1924227923154831,
      "learning_rate": 3.850317494362579e-07,
      "loss": 0.0012,
      "step": 92470
    },
    {
      "epoch": 16814.545454545456,
      "grad_norm": 0.19331511855125427,
      "learning_rate": 3.849185149329079e-07,
      "loss": 0.0009,
      "step": 92480
    },
    {
      "epoch": 16816.363636363636,
      "grad_norm": 0.2631776034832001,
      "learning_rate": 3.8480528666170485e-07,
      "loss": 0.0014,
      "step": 92490
    },
    {
      "epoch": 16818.18181818182,
      "grad_norm": 0.15703409910202026,
      "learning_rate": 3.846920646287799e-07,
      "loss": 0.0009,
      "step": 92500
    },
    {
      "epoch": 16818.18181818182,
      "eval_loss": 5.052618026733398,
      "eval_runtime": 0.9531,
      "eval_samples_per_second": 10.492,
      "eval_steps_per_second": 5.246,
      "step": 92500
    },
    {
      "epoch": 16820.0,
      "grad_norm": 0.2505057454109192,
      "learning_rate": 3.8457884884026497e-07,
      "loss": 0.001,
      "step": 92510
    },
    {
      "epoch": 16821.81818181818,
      "grad_norm": 0.19313113391399384,
      "learning_rate": 3.8446563930229114e-07,
      "loss": 0.0012,
      "step": 92520
    },
    {
      "epoch": 16823.636363636364,
      "grad_norm": 0.19495385885238647,
      "learning_rate": 3.843524360209889e-07,
      "loss": 0.001,
      "step": 92530
    },
    {
      "epoch": 16825.454545454544,
      "grad_norm": 0.0007234963122755289,
      "learning_rate": 3.84239239002489e-07,
      "loss": 0.0007,
      "step": 92540
    },
    {
      "epoch": 16827.272727272728,
      "grad_norm": 0.16128292679786682,
      "learning_rate": 3.841260482529214e-07,
      "loss": 0.0013,
      "step": 92550
    },
    {
      "epoch": 16829.090909090908,
      "grad_norm": 0.0007635416695848107,
      "learning_rate": 3.8401286377841567e-07,
      "loss": 0.0011,
      "step": 92560
    },
    {
      "epoch": 16830.909090909092,
      "grad_norm": 0.15308021008968353,
      "learning_rate": 3.8389968558510173e-07,
      "loss": 0.0012,
      "step": 92570
    },
    {
      "epoch": 16832.727272727272,
      "grad_norm": 0.0004862055939156562,
      "learning_rate": 3.8378651367910795e-07,
      "loss": 0.001,
      "step": 92580
    },
    {
      "epoch": 16834.545454545456,
      "grad_norm": 0.0015805334551259875,
      "learning_rate": 3.836733480665637e-07,
      "loss": 0.001,
      "step": 92590
    },
    {
      "epoch": 16836.363636363636,
      "grad_norm": 0.0009670961298979819,
      "learning_rate": 3.835601887535971e-07,
      "loss": 0.0012,
      "step": 92600
    },
    {
      "epoch": 16838.18181818182,
      "grad_norm": 0.19444164633750916,
      "learning_rate": 3.834470357463362e-07,
      "loss": 0.0012,
      "step": 92610
    },
    {
      "epoch": 16840.0,
      "grad_norm": 0.000815987994428724,
      "learning_rate": 3.833338890509087e-07,
      "loss": 0.001,
      "step": 92620
    },
    {
      "epoch": 16841.81818181818,
      "grad_norm": 0.0005508729955181479,
      "learning_rate": 3.8322074867344204e-07,
      "loss": 0.0011,
      "step": 92630
    },
    {
      "epoch": 16843.636363636364,
      "grad_norm": 0.0012212616857141256,
      "learning_rate": 3.831076146200632e-07,
      "loss": 0.0007,
      "step": 92640
    },
    {
      "epoch": 16845.454545454544,
      "grad_norm": 0.0009072573739103973,
      "learning_rate": 3.829944868968989e-07,
      "loss": 0.0014,
      "step": 92650
    },
    {
      "epoch": 16847.272727272728,
      "grad_norm": 0.19705505669116974,
      "learning_rate": 3.8288136551007543e-07,
      "loss": 0.0013,
      "step": 92660
    },
    {
      "epoch": 16849.090909090908,
      "grad_norm": 0.0005126626347191632,
      "learning_rate": 3.827682504657187e-07,
      "loss": 0.0009,
      "step": 92670
    },
    {
      "epoch": 16850.909090909092,
      "grad_norm": 0.002804049989208579,
      "learning_rate": 3.826551417699548e-07,
      "loss": 0.0011,
      "step": 92680
    },
    {
      "epoch": 16852.727272727272,
      "grad_norm": 0.0005772152217105031,
      "learning_rate": 3.8254203942890844e-07,
      "loss": 0.0012,
      "step": 92690
    },
    {
      "epoch": 16854.545454545456,
      "grad_norm": 0.0008133138180710375,
      "learning_rate": 3.8242894344870495e-07,
      "loss": 0.0009,
      "step": 92700
    },
    {
      "epoch": 16856.363636363636,
      "grad_norm": 0.1495605856180191,
      "learning_rate": 3.82315853835469e-07,
      "loss": 0.0014,
      "step": 92710
    },
    {
      "epoch": 16858.18181818182,
      "grad_norm": 0.13813164830207825,
      "learning_rate": 3.822027705953246e-07,
      "loss": 0.001,
      "step": 92720
    },
    {
      "epoch": 16860.0,
      "grad_norm": 0.21011994779109955,
      "learning_rate": 3.820896937343959e-07,
      "loss": 0.0012,
      "step": 92730
    },
    {
      "epoch": 16861.81818181818,
      "grad_norm": 0.1937243938446045,
      "learning_rate": 3.8197662325880643e-07,
      "loss": 0.001,
      "step": 92740
    },
    {
      "epoch": 16863.636363636364,
      "grad_norm": 0.27971765398979187,
      "learning_rate": 3.8186355917467926e-07,
      "loss": 0.0013,
      "step": 92750
    },
    {
      "epoch": 16865.454545454544,
      "grad_norm": 0.0006533664418384433,
      "learning_rate": 3.8175050148813773e-07,
      "loss": 0.0009,
      "step": 92760
    },
    {
      "epoch": 16867.272727272728,
      "grad_norm": 0.24003006517887115,
      "learning_rate": 3.816374502053039e-07,
      "loss": 0.0012,
      "step": 92770
    },
    {
      "epoch": 16869.090909090908,
      "grad_norm": 0.3963029086589813,
      "learning_rate": 3.815244053323001e-07,
      "loss": 0.0011,
      "step": 92780
    },
    {
      "epoch": 16870.909090909092,
      "grad_norm": 0.1557827889919281,
      "learning_rate": 3.8141136687524857e-07,
      "loss": 0.0009,
      "step": 92790
    },
    {
      "epoch": 16872.727272727272,
      "grad_norm": 0.21110038459300995,
      "learning_rate": 3.8129833484027023e-07,
      "loss": 0.001,
      "step": 92800
    },
    {
      "epoch": 16874.545454545456,
      "grad_norm": 0.2566562592983246,
      "learning_rate": 3.811853092334866e-07,
      "loss": 0.0013,
      "step": 92810
    },
    {
      "epoch": 16876.363636363636,
      "grad_norm": 0.0005401406669989228,
      "learning_rate": 3.8107229006101857e-07,
      "loss": 0.0009,
      "step": 92820
    },
    {
      "epoch": 16878.18181818182,
      "grad_norm": 0.0014421242522075772,
      "learning_rate": 3.8095927732898624e-07,
      "loss": 0.001,
      "step": 92830
    },
    {
      "epoch": 16880.0,
      "grad_norm": 0.0008772013825364411,
      "learning_rate": 3.8084627104351007e-07,
      "loss": 0.0012,
      "step": 92840
    },
    {
      "epoch": 16881.81818181818,
      "grad_norm": 0.0004634353972505778,
      "learning_rate": 3.8073327121070967e-07,
      "loss": 0.001,
      "step": 92850
    },
    {
      "epoch": 16883.636363636364,
      "grad_norm": 0.2098955363035202,
      "learning_rate": 3.8062027783670426e-07,
      "loss": 0.0014,
      "step": 92860
    },
    {
      "epoch": 16885.454545454544,
      "grad_norm": 0.15749116241931915,
      "learning_rate": 3.8050729092761344e-07,
      "loss": 0.0009,
      "step": 92870
    },
    {
      "epoch": 16887.272727272728,
      "grad_norm": 0.0010137961944565177,
      "learning_rate": 3.8039431048955534e-07,
      "loss": 0.0008,
      "step": 92880
    },
    {
      "epoch": 16889.090909090908,
      "grad_norm": 0.15734240412712097,
      "learning_rate": 3.802813365286487e-07,
      "loss": 0.0012,
      "step": 92890
    },
    {
      "epoch": 16890.909090909092,
      "grad_norm": 0.27832451462745667,
      "learning_rate": 3.801683690510115e-07,
      "loss": 0.0012,
      "step": 92900
    },
    {
      "epoch": 16892.727272727272,
      "grad_norm": 0.24816077947616577,
      "learning_rate": 3.8005540806276127e-07,
      "loss": 0.0012,
      "step": 92910
    },
    {
      "epoch": 16894.545454545456,
      "grad_norm": 0.2690129578113556,
      "learning_rate": 3.799424535700155e-07,
      "loss": 0.0009,
      "step": 92920
    },
    {
      "epoch": 16896.363636363636,
      "grad_norm": 0.2561071813106537,
      "learning_rate": 3.798295055788909e-07,
      "loss": 0.0012,
      "step": 92930
    },
    {
      "epoch": 16898.18181818182,
      "grad_norm": 0.0005806424887850881,
      "learning_rate": 3.7971656409550407e-07,
      "loss": 0.0009,
      "step": 92940
    },
    {
      "epoch": 16900.0,
      "grad_norm": 0.0008633824763819575,
      "learning_rate": 3.7960362912597177e-07,
      "loss": 0.0012,
      "step": 92950
    },
    {
      "epoch": 16901.81818181818,
      "grad_norm": 0.0006085396162234247,
      "learning_rate": 3.7949070067640924e-07,
      "loss": 0.0011,
      "step": 92960
    },
    {
      "epoch": 16903.636363636364,
      "grad_norm": 0.19927605986595154,
      "learning_rate": 3.7937777875293244e-07,
      "loss": 0.0012,
      "step": 92970
    },
    {
      "epoch": 16905.454545454544,
      "grad_norm": 0.0006356724188663065,
      "learning_rate": 3.7926486336165653e-07,
      "loss": 0.0009,
      "step": 92980
    },
    {
      "epoch": 16907.272727272728,
      "grad_norm": 0.20723581314086914,
      "learning_rate": 3.791519545086962e-07,
      "loss": 0.0012,
      "step": 92990
    },
    {
      "epoch": 16909.090909090908,
      "grad_norm": 0.0009317578515037894,
      "learning_rate": 3.790390522001662e-07,
      "loss": 0.0011,
      "step": 93000
    },
    {
      "epoch": 16909.090909090908,
      "eval_loss": 5.020805358886719,
      "eval_runtime": 0.9514,
      "eval_samples_per_second": 10.51,
      "eval_steps_per_second": 5.255,
      "step": 93000
    },
    {
      "epoch": 16910.909090909092,
      "grad_norm": 0.0010277400724589825,
      "learning_rate": 3.7892615644218037e-07,
      "loss": 0.0012,
      "step": 93010
    },
    {
      "epoch": 16912.727272727272,
      "grad_norm": 0.005807353649288416,
      "learning_rate": 3.7881326724085253e-07,
      "loss": 0.001,
      "step": 93020
    },
    {
      "epoch": 16914.545454545456,
      "grad_norm": 0.0011820083018392324,
      "learning_rate": 3.7870038460229637e-07,
      "loss": 0.0009,
      "step": 93030
    },
    {
      "epoch": 16916.363636363636,
      "grad_norm": 0.0007333597168326378,
      "learning_rate": 3.7858750853262466e-07,
      "loss": 0.001,
      "step": 93040
    },
    {
      "epoch": 16918.18181818182,
      "grad_norm": 0.15825332701206207,
      "learning_rate": 3.7847463903795006e-07,
      "loss": 0.0014,
      "step": 93050
    },
    {
      "epoch": 16920.0,
      "grad_norm": 0.0007485089008696377,
      "learning_rate": 3.783617761243855e-07,
      "loss": 0.0012,
      "step": 93060
    },
    {
      "epoch": 16921.81818181818,
      "grad_norm": 0.0036078994162380695,
      "learning_rate": 3.7824891979804224e-07,
      "loss": 0.0012,
      "step": 93070
    },
    {
      "epoch": 16923.636363636364,
      "grad_norm": 0.0006998807075433433,
      "learning_rate": 3.781360700650324e-07,
      "loss": 0.001,
      "step": 93080
    },
    {
      "epoch": 16925.454545454544,
      "grad_norm": 0.2554520070552826,
      "learning_rate": 3.780232269314672e-07,
      "loss": 0.0013,
      "step": 93090
    },
    {
      "epoch": 16927.272727272728,
      "grad_norm": 0.0008043372654356062,
      "learning_rate": 3.779103904034574e-07,
      "loss": 0.0011,
      "step": 93100
    },
    {
      "epoch": 16929.090909090908,
      "grad_norm": 0.0009765855502337217,
      "learning_rate": 3.777975604871138e-07,
      "loss": 0.0009,
      "step": 93110
    },
    {
      "epoch": 16930.909090909092,
      "grad_norm": 0.0010515990434214473,
      "learning_rate": 3.7768473718854637e-07,
      "loss": 0.0011,
      "step": 93120
    },
    {
      "epoch": 16932.727272727272,
      "grad_norm": 0.0009925068588927388,
      "learning_rate": 3.775719205138651e-07,
      "loss": 0.0012,
      "step": 93130
    },
    {
      "epoch": 16934.545454545456,
      "grad_norm": 0.25686323642730713,
      "learning_rate": 3.774591104691796e-07,
      "loss": 0.001,
      "step": 93140
    },
    {
      "epoch": 16936.363636363636,
      "grad_norm": 0.00043205308611504734,
      "learning_rate": 3.7734630706059867e-07,
      "loss": 0.0012,
      "step": 93150
    },
    {
      "epoch": 16938.18181818182,
      "grad_norm": 0.0005891002365387976,
      "learning_rate": 3.7723351029423137e-07,
      "loss": 0.001,
      "step": 93160
    },
    {
      "epoch": 16940.0,
      "grad_norm": 0.017324313521385193,
      "learning_rate": 3.7712072017618624e-07,
      "loss": 0.0012,
      "step": 93170
    },
    {
      "epoch": 16941.81818181818,
      "grad_norm": 0.0006066973437555134,
      "learning_rate": 3.7700793671257096e-07,
      "loss": 0.0009,
      "step": 93180
    },
    {
      "epoch": 16943.636363636364,
      "grad_norm": 0.15166421234607697,
      "learning_rate": 3.768951599094936e-07,
      "loss": 0.0013,
      "step": 93190
    },
    {
      "epoch": 16945.454545454544,
      "grad_norm": 0.0005585331819020212,
      "learning_rate": 3.7678238977306114e-07,
      "loss": 0.001,
      "step": 93200
    },
    {
      "epoch": 16947.272727272728,
      "grad_norm": 0.2090286910533905,
      "learning_rate": 3.766696263093808e-07,
      "loss": 0.0012,
      "step": 93210
    },
    {
      "epoch": 16949.090909090908,
      "grad_norm": 0.15127874910831451,
      "learning_rate": 3.7655686952455925e-07,
      "loss": 0.0009,
      "step": 93220
    },
    {
      "epoch": 16950.909090909092,
      "grad_norm": 0.15938149392604828,
      "learning_rate": 3.7644411942470244e-07,
      "loss": 0.0012,
      "step": 93230
    },
    {
      "epoch": 16952.727272727272,
      "grad_norm": 0.0007943384116515517,
      "learning_rate": 3.7633137601591643e-07,
      "loss": 0.0011,
      "step": 93240
    },
    {
      "epoch": 16954.545454545456,
      "grad_norm": 0.19591893255710602,
      "learning_rate": 3.7621863930430707e-07,
      "loss": 0.0011,
      "step": 93250
    },
    {
      "epoch": 16956.363636363636,
      "grad_norm": 0.0401877798140049,
      "learning_rate": 3.761059092959789e-07,
      "loss": 0.0013,
      "step": 93260
    },
    {
      "epoch": 16958.18181818182,
      "grad_norm": 0.0006036422564648092,
      "learning_rate": 3.7599318599703733e-07,
      "loss": 0.0007,
      "step": 93270
    },
    {
      "epoch": 16960.0,
      "grad_norm": 0.0006464533507823944,
      "learning_rate": 3.758804694135863e-07,
      "loss": 0.0012,
      "step": 93280
    },
    {
      "epoch": 16961.81818181818,
      "grad_norm": 0.20668500661849976,
      "learning_rate": 3.757677595517301e-07,
      "loss": 0.0009,
      "step": 93290
    },
    {
      "epoch": 16963.636363636364,
      "grad_norm": 0.26872700452804565,
      "learning_rate": 3.7565505641757266e-07,
      "loss": 0.0015,
      "step": 93300
    },
    {
      "epoch": 16965.454545454544,
      "grad_norm": 0.0007602478726767004,
      "learning_rate": 3.7554236001721703e-07,
      "loss": 0.0011,
      "step": 93310
    },
    {
      "epoch": 16967.272727272728,
      "grad_norm": 0.0017975053051486611,
      "learning_rate": 3.754296703567662e-07,
      "loss": 0.0009,
      "step": 93320
    },
    {
      "epoch": 16969.090909090908,
      "grad_norm": 0.002220190828666091,
      "learning_rate": 3.75316987442323e-07,
      "loss": 0.001,
      "step": 93330
    },
    {
      "epoch": 16970.909090909092,
      "grad_norm": 0.15855155885219574,
      "learning_rate": 3.7520431127998943e-07,
      "loss": 0.0012,
      "step": 93340
    },
    {
      "epoch": 16972.727272727272,
      "grad_norm": 0.0012032741215080023,
      "learning_rate": 3.750916418758675e-07,
      "loss": 0.0012,
      "step": 93350
    },
    {
      "epoch": 16974.545454545456,
      "grad_norm": 0.0007797665311954916,
      "learning_rate": 3.749789792360589e-07,
      "loss": 0.0011,
      "step": 93360
    },
    {
      "epoch": 16976.363636363636,
      "grad_norm": 0.0007076624897308648,
      "learning_rate": 3.7486632336666455e-07,
      "loss": 0.001,
      "step": 93370
    },
    {
      "epoch": 16978.18181818182,
      "grad_norm": 0.22032198309898376,
      "learning_rate": 3.747536742737854e-07,
      "loss": 0.0013,
      "step": 93380
    },
    {
      "epoch": 16980.0,
      "grad_norm": 0.0005857663345523179,
      "learning_rate": 3.746410319635217e-07,
      "loss": 0.001,
      "step": 93390
    },
    {
      "epoch": 16981.81818181818,
      "grad_norm": 0.007146371528506279,
      "learning_rate": 3.7452839644197354e-07,
      "loss": 0.001,
      "step": 93400
    },
    {
      "epoch": 16983.636363636364,
      "grad_norm": 0.010250886902213097,
      "learning_rate": 3.7441576771524085e-07,
      "loss": 0.0012,
      "step": 93410
    },
    {
      "epoch": 16985.454545454544,
      "grad_norm": 0.0004907551337964833,
      "learning_rate": 3.7430314578942257e-07,
      "loss": 0.0009,
      "step": 93420
    },
    {
      "epoch": 16987.272727272728,
      "grad_norm": 0.0010672895004972816,
      "learning_rate": 3.741905306706178e-07,
      "loss": 0.001,
      "step": 93430
    },
    {
      "epoch": 16989.090909090908,
      "grad_norm": 0.0010548451682552695,
      "learning_rate": 3.740779223649254e-07,
      "loss": 0.0012,
      "step": 93440
    },
    {
      "epoch": 16990.909090909092,
      "grad_norm": 0.0008551072096452117,
      "learning_rate": 3.739653208784431e-07,
      "loss": 0.0012,
      "step": 93450
    },
    {
      "epoch": 16992.727272727272,
      "grad_norm": 0.0006251568556763232,
      "learning_rate": 3.738527262172693e-07,
      "loss": 0.0009,
      "step": 93460
    },
    {
      "epoch": 16994.545454545456,
      "grad_norm": 0.19993816316127777,
      "learning_rate": 3.737401383875008e-07,
      "loss": 0.0015,
      "step": 93470
    },
    {
      "epoch": 16996.363636363636,
      "grad_norm": 0.18245457112789154,
      "learning_rate": 3.7362755739523534e-07,
      "loss": 0.0011,
      "step": 93480
    },
    {
      "epoch": 16998.18181818182,
      "grad_norm": 0.1404363065958023,
      "learning_rate": 3.7351498324656937e-07,
      "loss": 0.0012,
      "step": 93490
    },
    {
      "epoch": 17000.0,
      "grad_norm": 0.0012130015529692173,
      "learning_rate": 3.734024159475991e-07,
      "loss": 0.001,
      "step": 93500
    },
    {
      "epoch": 17000.0,
      "eval_loss": 5.152576446533203,
      "eval_runtime": 0.9525,
      "eval_samples_per_second": 10.499,
      "eval_steps_per_second": 5.249,
      "step": 93500
    },
    {
      "epoch": 17001.81818181818,
      "grad_norm": 0.0006466726190410554,
      "learning_rate": 3.732898555044208e-07,
      "loss": 0.001,
      "step": 93510
    },
    {
      "epoch": 17003.636363636364,
      "grad_norm": 0.0005462022381834686,
      "learning_rate": 3.731773019231301e-07,
      "loss": 0.001,
      "step": 93520
    },
    {
      "epoch": 17005.454545454544,
      "grad_norm": 0.0016645098803564906,
      "learning_rate": 3.7306475520982184e-07,
      "loss": 0.0012,
      "step": 93530
    },
    {
      "epoch": 17007.272727272728,
      "grad_norm": 0.0008400615188293159,
      "learning_rate": 3.729522153705916e-07,
      "loss": 0.0015,
      "step": 93540
    },
    {
      "epoch": 17009.090909090908,
      "grad_norm": 0.26061785221099854,
      "learning_rate": 3.728396824115331e-07,
      "loss": 0.0011,
      "step": 93550
    },
    {
      "epoch": 17010.909090909092,
      "grad_norm": 0.0011430773884057999,
      "learning_rate": 3.7272715633874095e-07,
      "loss": 0.0008,
      "step": 93560
    },
    {
      "epoch": 17012.727272727272,
      "grad_norm": 0.0006334061617963016,
      "learning_rate": 3.7261463715830896e-07,
      "loss": 0.0013,
      "step": 93570
    },
    {
      "epoch": 17014.545454545456,
      "grad_norm": 0.257380872964859,
      "learning_rate": 3.7250212487633024e-07,
      "loss": 0.0011,
      "step": 93580
    },
    {
      "epoch": 17016.363636363636,
      "grad_norm": 0.14615561068058014,
      "learning_rate": 3.723896194988979e-07,
      "loss": 0.0012,
      "step": 93590
    },
    {
      "epoch": 17018.18181818182,
      "grad_norm": 0.0009921099990606308,
      "learning_rate": 3.722771210321048e-07,
      "loss": 0.0008,
      "step": 93600
    },
    {
      "epoch": 17020.0,
      "grad_norm": 0.21008159220218658,
      "learning_rate": 3.721646294820429e-07,
      "loss": 0.0012,
      "step": 93610
    },
    {
      "epoch": 17021.81818181818,
      "grad_norm": 0.18057690560817719,
      "learning_rate": 3.7205214485480426e-07,
      "loss": 0.0012,
      "step": 93620
    },
    {
      "epoch": 17023.636363636364,
      "grad_norm": 0.0005463658017106354,
      "learning_rate": 3.719396671564802e-07,
      "loss": 0.0007,
      "step": 93630
    },
    {
      "epoch": 17025.454545454544,
      "grad_norm": 0.0010547003475949168,
      "learning_rate": 3.7182719639316194e-07,
      "loss": 0.0014,
      "step": 93640
    },
    {
      "epoch": 17027.272727272728,
      "grad_norm": 0.000776018132455647,
      "learning_rate": 3.717147325709407e-07,
      "loss": 0.0009,
      "step": 93650
    },
    {
      "epoch": 17029.090909090908,
      "grad_norm": 0.0005946884630247951,
      "learning_rate": 3.716022756959061e-07,
      "loss": 0.0012,
      "step": 93660
    },
    {
      "epoch": 17030.909090909092,
      "grad_norm": 0.0009888624772429466,
      "learning_rate": 3.714898257741487e-07,
      "loss": 0.0012,
      "step": 93670
    },
    {
      "epoch": 17032.727272727272,
      "grad_norm": 0.0008034121710807085,
      "learning_rate": 3.71377382811758e-07,
      "loss": 0.0009,
      "step": 93680
    },
    {
      "epoch": 17034.545454545456,
      "grad_norm": 0.24769741296768188,
      "learning_rate": 3.7126494681482314e-07,
      "loss": 0.0012,
      "step": 93690
    },
    {
      "epoch": 17036.363636363636,
      "grad_norm": 0.000744745135307312,
      "learning_rate": 3.711525177894331e-07,
      "loss": 0.0009,
      "step": 93700
    },
    {
      "epoch": 17038.18181818182,
      "grad_norm": 0.19966769218444824,
      "learning_rate": 3.710400957416765e-07,
      "loss": 0.0013,
      "step": 93710
    },
    {
      "epoch": 17040.0,
      "grad_norm": 0.21051210165023804,
      "learning_rate": 3.709276806776412e-07,
      "loss": 0.0012,
      "step": 93720
    },
    {
      "epoch": 17041.81818181818,
      "grad_norm": 0.0007583153783343732,
      "learning_rate": 3.7081527260341514e-07,
      "loss": 0.001,
      "step": 93730
    },
    {
      "epoch": 17043.636363636364,
      "grad_norm": 0.19805970788002014,
      "learning_rate": 3.7070287152508556e-07,
      "loss": 0.0012,
      "step": 93740
    },
    {
      "epoch": 17045.454545454544,
      "grad_norm": 0.22188881039619446,
      "learning_rate": 3.7059047744873955e-07,
      "loss": 0.001,
      "step": 93750
    },
    {
      "epoch": 17047.272727272728,
      "grad_norm": 0.25813162326812744,
      "learning_rate": 3.7047809038046383e-07,
      "loss": 0.0012,
      "step": 93760
    },
    {
      "epoch": 17049.090909090908,
      "grad_norm": 0.20703066885471344,
      "learning_rate": 3.7036571032634443e-07,
      "loss": 0.0009,
      "step": 93770
    },
    {
      "epoch": 17050.909090909092,
      "grad_norm": 0.15251468122005463,
      "learning_rate": 3.7025333729246725e-07,
      "loss": 0.0012,
      "step": 93780
    },
    {
      "epoch": 17052.727272727272,
      "grad_norm": 0.0006645137909799814,
      "learning_rate": 3.7014097128491793e-07,
      "loss": 0.001,
      "step": 93790
    },
    {
      "epoch": 17054.545454545456,
      "grad_norm": 0.1946868747472763,
      "learning_rate": 3.700286123097813e-07,
      "loss": 0.0012,
      "step": 93800
    },
    {
      "epoch": 17056.363636363636,
      "grad_norm": 0.0005919008981436491,
      "learning_rate": 3.6991626037314226e-07,
      "loss": 0.0007,
      "step": 93810
    },
    {
      "epoch": 17058.18181818182,
      "grad_norm": 0.18277229368686676,
      "learning_rate": 3.69803915481085e-07,
      "loss": 0.0014,
      "step": 93820
    },
    {
      "epoch": 17060.0,
      "grad_norm": 0.0004873444268014282,
      "learning_rate": 3.696915776396935e-07,
      "loss": 0.001,
      "step": 93830
    },
    {
      "epoch": 17061.81818181818,
      "grad_norm": 0.15218786895275116,
      "learning_rate": 3.6957924685505167e-07,
      "loss": 0.0012,
      "step": 93840
    },
    {
      "epoch": 17063.636363636364,
      "grad_norm": 0.0013590814778581262,
      "learning_rate": 3.6946692313324204e-07,
      "loss": 0.0008,
      "step": 93850
    },
    {
      "epoch": 17065.454545454544,
      "grad_norm": 0.1983812302350998,
      "learning_rate": 3.6935460648034787e-07,
      "loss": 0.0011,
      "step": 93860
    },
    {
      "epoch": 17067.272727272728,
      "grad_norm": 0.2548403739929199,
      "learning_rate": 3.6924229690245157e-07,
      "loss": 0.0013,
      "step": 93870
    },
    {
      "epoch": 17069.090909090908,
      "grad_norm": 0.0013971197186037898,
      "learning_rate": 3.691299944056351e-07,
      "loss": 0.0009,
      "step": 93880
    },
    {
      "epoch": 17070.909090909092,
      "grad_norm": 0.0007728689815849066,
      "learning_rate": 3.6901769899598006e-07,
      "loss": 0.0012,
      "step": 93890
    },
    {
      "epoch": 17072.727272727272,
      "grad_norm": 0.16231974959373474,
      "learning_rate": 3.689054106795677e-07,
      "loss": 0.0011,
      "step": 93900
    },
    {
      "epoch": 17074.545454545456,
      "grad_norm": 0.0012973230332136154,
      "learning_rate": 3.6879312946247896e-07,
      "loss": 0.0006,
      "step": 93910
    },
    {
      "epoch": 17076.363636363636,
      "grad_norm": 0.0012996174627915025,
      "learning_rate": 3.6868085535079443e-07,
      "loss": 0.0014,
      "step": 93920
    },
    {
      "epoch": 17078.18181818182,
      "grad_norm": 0.0007318283896893263,
      "learning_rate": 3.6856858835059396e-07,
      "loss": 0.001,
      "step": 93930
    },
    {
      "epoch": 17080.0,
      "grad_norm": 0.0005097739049233496,
      "learning_rate": 3.6845632846795757e-07,
      "loss": 0.0012,
      "step": 93940
    },
    {
      "epoch": 17081.81818181818,
      "grad_norm": 0.17294760048389435,
      "learning_rate": 3.6834407570896455e-07,
      "loss": 0.0012,
      "step": 93950
    },
    {
      "epoch": 17083.636363636364,
      "grad_norm": 0.19339725375175476,
      "learning_rate": 3.682318300796937e-07,
      "loss": 0.0011,
      "step": 93960
    },
    {
      "epoch": 17085.454545454544,
      "grad_norm": 0.0074475472792983055,
      "learning_rate": 3.6811959158622376e-07,
      "loss": 0.001,
      "step": 93970
    },
    {
      "epoch": 17087.272727272728,
      "grad_norm": 0.2497197687625885,
      "learning_rate": 3.6800736023463287e-07,
      "loss": 0.0011,
      "step": 93980
    },
    {
      "epoch": 17089.090909090908,
      "grad_norm": 0.0007613439811393619,
      "learning_rate": 3.6789513603099875e-07,
      "loss": 0.001,
      "step": 93990
    },
    {
      "epoch": 17090.909090909092,
      "grad_norm": 0.1678670048713684,
      "learning_rate": 3.6778291898139903e-07,
      "loss": 0.0012,
      "step": 94000
    },
    {
      "epoch": 17090.909090909092,
      "eval_loss": 5.147753715515137,
      "eval_runtime": 0.9531,
      "eval_samples_per_second": 10.492,
      "eval_steps_per_second": 5.246,
      "step": 94000
    },
    {
      "epoch": 17092.727272727272,
      "grad_norm": 0.16585174202919006,
      "learning_rate": 3.676707090919103e-07,
      "loss": 0.0012,
      "step": 94010
    },
    {
      "epoch": 17094.545454545456,
      "grad_norm": 0.2663760781288147,
      "learning_rate": 3.6755850636860955e-07,
      "loss": 0.0009,
      "step": 94020
    },
    {
      "epoch": 17096.363636363636,
      "grad_norm": 0.19920949637889862,
      "learning_rate": 3.6744631081757323e-07,
      "loss": 0.0011,
      "step": 94030
    },
    {
      "epoch": 17098.18181818182,
      "grad_norm": 0.0004697988915722817,
      "learning_rate": 3.673341224448766e-07,
      "loss": 0.001,
      "step": 94040
    },
    {
      "epoch": 17100.0,
      "grad_norm": 0.000530161545611918,
      "learning_rate": 3.672219412565956e-07,
      "loss": 0.0012,
      "step": 94050
    },
    {
      "epoch": 17101.81818181818,
      "grad_norm": 0.0009077558061107993,
      "learning_rate": 3.6710976725880516e-07,
      "loss": 0.0009,
      "step": 94060
    },
    {
      "epoch": 17103.636363636364,
      "grad_norm": 0.000985634746029973,
      "learning_rate": 3.6699760045758e-07,
      "loss": 0.0011,
      "step": 94070
    },
    {
      "epoch": 17105.454545454544,
      "grad_norm": 0.0009813705692067742,
      "learning_rate": 3.6688544085899443e-07,
      "loss": 0.0011,
      "step": 94080
    },
    {
      "epoch": 17107.272727272728,
      "grad_norm": 0.0006839419365860522,
      "learning_rate": 3.667732884691223e-07,
      "loss": 0.001,
      "step": 94090
    },
    {
      "epoch": 17109.090909090908,
      "grad_norm": 0.002015491481870413,
      "learning_rate": 3.666611432940372e-07,
      "loss": 0.0012,
      "step": 94100
    },
    {
      "epoch": 17110.909090909092,
      "grad_norm": 0.16734234988689423,
      "learning_rate": 3.665490053398123e-07,
      "loss": 0.001,
      "step": 94110
    },
    {
      "epoch": 17112.727272727272,
      "grad_norm": 0.005422411952167749,
      "learning_rate": 3.664368746125201e-07,
      "loss": 0.001,
      "step": 94120
    },
    {
      "epoch": 17114.545454545456,
      "grad_norm": 0.0006634401506744325,
      "learning_rate": 3.6632475111823324e-07,
      "loss": 0.0013,
      "step": 94130
    },
    {
      "epoch": 17116.363636363636,
      "grad_norm": 0.19664719700813293,
      "learning_rate": 3.662126348630237e-07,
      "loss": 0.0009,
      "step": 94140
    },
    {
      "epoch": 17118.18181818182,
      "grad_norm": 0.0005913560162298381,
      "learning_rate": 3.6610052585296273e-07,
      "loss": 0.0011,
      "step": 94150
    },
    {
      "epoch": 17120.0,
      "grad_norm": 0.18579554557800293,
      "learning_rate": 3.659884240941219e-07,
      "loss": 0.0012,
      "step": 94160
    },
    {
      "epoch": 17121.81818181818,
      "grad_norm": 0.0006735101342201233,
      "learning_rate": 3.6587632959257167e-07,
      "loss": 0.0011,
      "step": 94170
    },
    {
      "epoch": 17123.636363636364,
      "grad_norm": 0.0008801647927612066,
      "learning_rate": 3.657642423543825e-07,
      "loss": 0.0014,
      "step": 94180
    },
    {
      "epoch": 17125.454545454544,
      "grad_norm": 0.2632424533367157,
      "learning_rate": 3.6565216238562456e-07,
      "loss": 0.001,
      "step": 94190
    },
    {
      "epoch": 17127.272727272728,
      "grad_norm": 0.0006574424915015697,
      "learning_rate": 3.6554008969236715e-07,
      "loss": 0.0011,
      "step": 94200
    },
    {
      "epoch": 17129.090909090908,
      "grad_norm": 0.0006166094681248069,
      "learning_rate": 3.6542802428067966e-07,
      "loss": 0.0012,
      "step": 94210
    },
    {
      "epoch": 17130.909090909092,
      "grad_norm": 0.169459730386734,
      "learning_rate": 3.653159661566312e-07,
      "loss": 0.0012,
      "step": 94220
    },
    {
      "epoch": 17132.727272727272,
      "grad_norm": 0.19904795289039612,
      "learning_rate": 3.6520391532628945e-07,
      "loss": 0.0012,
      "step": 94230
    },
    {
      "epoch": 17134.545454545456,
      "grad_norm": 0.0009434192907065153,
      "learning_rate": 3.650918717957233e-07,
      "loss": 0.0008,
      "step": 94240
    },
    {
      "epoch": 17136.363636363636,
      "grad_norm": 0.0006287585129030049,
      "learning_rate": 3.6497983557099964e-07,
      "loss": 0.001,
      "step": 94250
    },
    {
      "epoch": 17138.18181818182,
      "grad_norm": 0.16668029129505157,
      "learning_rate": 3.6486780665818604e-07,
      "loss": 0.0012,
      "step": 94260
    },
    {
      "epoch": 17140.0,
      "grad_norm": 0.0017037491779774427,
      "learning_rate": 3.647557850633495e-07,
      "loss": 0.001,
      "step": 94270
    },
    {
      "epoch": 17141.81818181818,
      "grad_norm": 0.2529505491256714,
      "learning_rate": 3.646437707925561e-07,
      "loss": 0.001,
      "step": 94280
    },
    {
      "epoch": 17143.636363636364,
      "grad_norm": 0.21082521975040436,
      "learning_rate": 3.645317638518721e-07,
      "loss": 0.001,
      "step": 94290
    },
    {
      "epoch": 17145.454545454544,
      "grad_norm": 0.0009423606097698212,
      "learning_rate": 3.644197642473631e-07,
      "loss": 0.0012,
      "step": 94300
    },
    {
      "epoch": 17147.272727272728,
      "grad_norm": 0.1430152803659439,
      "learning_rate": 3.643077719850943e-07,
      "loss": 0.001,
      "step": 94310
    },
    {
      "epoch": 17149.090909090908,
      "grad_norm": 0.0013438013847917318,
      "learning_rate": 3.641957870711305e-07,
      "loss": 0.001,
      "step": 94320
    },
    {
      "epoch": 17150.909090909092,
      "grad_norm": 0.2099742442369461,
      "learning_rate": 3.640838095115365e-07,
      "loss": 0.001,
      "step": 94330
    },
    {
      "epoch": 17152.727272727272,
      "grad_norm": 0.0008621244342066348,
      "learning_rate": 3.63971839312376e-07,
      "loss": 0.0012,
      "step": 94340
    },
    {
      "epoch": 17154.545454545456,
      "grad_norm": 0.0008922760607674718,
      "learning_rate": 3.638598764797128e-07,
      "loss": 0.0012,
      "step": 94350
    },
    {
      "epoch": 17156.363636363636,
      "grad_norm": 0.2633261978626251,
      "learning_rate": 3.637479210196102e-07,
      "loss": 0.0011,
      "step": 94360
    },
    {
      "epoch": 17158.18181818182,
      "grad_norm": 0.001747369533404708,
      "learning_rate": 3.6363597293813086e-07,
      "loss": 0.0009,
      "step": 94370
    },
    {
      "epoch": 17160.0,
      "grad_norm": 0.0005082824500277638,
      "learning_rate": 3.6352403224133745e-07,
      "loss": 0.0011,
      "step": 94380
    },
    {
      "epoch": 17161.81818181818,
      "grad_norm": 0.011271576397120953,
      "learning_rate": 3.634120989352919e-07,
      "loss": 0.0012,
      "step": 94390
    },
    {
      "epoch": 17163.636363636364,
      "grad_norm": 0.16151417791843414,
      "learning_rate": 3.6330017302605576e-07,
      "loss": 0.0009,
      "step": 94400
    },
    {
      "epoch": 17165.454545454544,
      "grad_norm": 0.19184736907482147,
      "learning_rate": 3.631882545196908e-07,
      "loss": 0.0015,
      "step": 94410
    },
    {
      "epoch": 17167.272727272728,
      "grad_norm": 0.0007643005228601396,
      "learning_rate": 3.6307634342225716e-07,
      "loss": 0.0007,
      "step": 94420
    },
    {
      "epoch": 17169.090909090908,
      "grad_norm": 0.19440416991710663,
      "learning_rate": 3.6296443973981607e-07,
      "loss": 0.0011,
      "step": 94430
    },
    {
      "epoch": 17170.909090909092,
      "grad_norm": 0.21320605278015137,
      "learning_rate": 3.6285254347842677e-07,
      "loss": 0.001,
      "step": 94440
    },
    {
      "epoch": 17172.727272727272,
      "grad_norm": 0.2038564682006836,
      "learning_rate": 3.6274065464414936e-07,
      "loss": 0.0013,
      "step": 94450
    },
    {
      "epoch": 17174.545454545456,
      "grad_norm": 0.0007346227066591382,
      "learning_rate": 3.626287732430433e-07,
      "loss": 0.0008,
      "step": 94460
    },
    {
      "epoch": 17176.363636363636,
      "grad_norm": 0.20109239220619202,
      "learning_rate": 3.62516899281167e-07,
      "loss": 0.0014,
      "step": 94470
    },
    {
      "epoch": 17178.18181818182,
      "grad_norm": 0.00045468268217518926,
      "learning_rate": 3.624050327645791e-07,
      "loss": 0.0007,
      "step": 94480
    },
    {
      "epoch": 17180.0,
      "grad_norm": 0.26245325803756714,
      "learning_rate": 3.6229317369933784e-07,
      "loss": 0.0012,
      "step": 94490
    },
    {
      "epoch": 17181.81818181818,
      "grad_norm": 0.19907768070697784,
      "learning_rate": 3.621813220915004e-07,
      "loss": 0.001,
      "step": 94500
    },
    {
      "epoch": 17181.81818181818,
      "eval_loss": 4.952343940734863,
      "eval_runtime": 0.9564,
      "eval_samples_per_second": 10.455,
      "eval_steps_per_second": 5.228,
      "step": 94500
    },
    {
      "epoch": 17183.636363636364,
      "grad_norm": 0.16335952281951904,
      "learning_rate": 3.620694779471245e-07,
      "loss": 0.0014,
      "step": 94510
    },
    {
      "epoch": 17185.454545454544,
      "grad_norm": 0.16618622839450836,
      "learning_rate": 3.6195764127226647e-07,
      "loss": 0.0009,
      "step": 94520
    },
    {
      "epoch": 17187.272727272728,
      "grad_norm": 0.0004069966380484402,
      "learning_rate": 3.6184581207298314e-07,
      "loss": 0.0011,
      "step": 94530
    },
    {
      "epoch": 17189.090909090908,
      "grad_norm": 0.0006148525862954557,
      "learning_rate": 3.617339903553305e-07,
      "loss": 0.0011,
      "step": 94540
    },
    {
      "epoch": 17190.909090909092,
      "grad_norm": 0.1581011414527893,
      "learning_rate": 3.6162217612536393e-07,
      "loss": 0.0012,
      "step": 94550
    },
    {
      "epoch": 17192.727272727272,
      "grad_norm": 0.0006158843170851469,
      "learning_rate": 3.615103693891388e-07,
      "loss": 0.001,
      "step": 94560
    },
    {
      "epoch": 17194.545454545456,
      "grad_norm": 0.19453175365924835,
      "learning_rate": 3.6139857015271003e-07,
      "loss": 0.0012,
      "step": 94570
    },
    {
      "epoch": 17196.363636363636,
      "grad_norm": 0.21073698997497559,
      "learning_rate": 3.612867784221317e-07,
      "loss": 0.0009,
      "step": 94580
    },
    {
      "epoch": 17198.18181818182,
      "grad_norm": 0.19479915499687195,
      "learning_rate": 3.611749942034579e-07,
      "loss": 0.0012,
      "step": 94590
    },
    {
      "epoch": 17200.0,
      "grad_norm": 0.15011689066886902,
      "learning_rate": 3.610632175027427e-07,
      "loss": 0.001,
      "step": 94600
    },
    {
      "epoch": 17201.81818181818,
      "grad_norm": 0.0006171342101879418,
      "learning_rate": 3.6095144832603846e-07,
      "loss": 0.0012,
      "step": 94610
    },
    {
      "epoch": 17203.636363636364,
      "grad_norm": 0.19559070467948914,
      "learning_rate": 3.6083968667939874e-07,
      "loss": 0.0009,
      "step": 94620
    },
    {
      "epoch": 17205.454545454544,
      "grad_norm": 0.2074623703956604,
      "learning_rate": 3.6072793256887526e-07,
      "loss": 0.0013,
      "step": 94630
    },
    {
      "epoch": 17207.272727272728,
      "grad_norm": 0.1834329515695572,
      "learning_rate": 3.6061618600052023e-07,
      "loss": 0.0012,
      "step": 94640
    },
    {
      "epoch": 17209.090909090908,
      "grad_norm": 0.0009772448102012277,
      "learning_rate": 3.605044469803854e-07,
      "loss": 0.0009,
      "step": 94650
    },
    {
      "epoch": 17210.909090909092,
      "grad_norm": 0.19431360065937042,
      "learning_rate": 3.6039271551452166e-07,
      "loss": 0.0012,
      "step": 94660
    },
    {
      "epoch": 17212.727272727272,
      "grad_norm": 0.0003734659112524241,
      "learning_rate": 3.602809916089797e-07,
      "loss": 0.0012,
      "step": 94670
    },
    {
      "epoch": 17214.545454545456,
      "grad_norm": 0.15229696035385132,
      "learning_rate": 3.601692752698101e-07,
      "loss": 0.0011,
      "step": 94680
    },
    {
      "epoch": 17216.363636363636,
      "grad_norm": 0.0010534391039982438,
      "learning_rate": 3.600575665030625e-07,
      "loss": 0.0008,
      "step": 94690
    },
    {
      "epoch": 17218.18181818182,
      "grad_norm": 0.0006158173200674355,
      "learning_rate": 3.5994586531478664e-07,
      "loss": 0.0012,
      "step": 94700
    },
    {
      "epoch": 17220.0,
      "grad_norm": 0.0006362218991853297,
      "learning_rate": 3.598341717110313e-07,
      "loss": 0.0012,
      "step": 94710
    },
    {
      "epoch": 17221.81818181818,
      "grad_norm": 0.0022420999594032764,
      "learning_rate": 3.5972248569784536e-07,
      "loss": 0.001,
      "step": 94720
    },
    {
      "epoch": 17223.636363636364,
      "grad_norm": 0.25911587476730347,
      "learning_rate": 3.5961080728127724e-07,
      "loss": 0.0013,
      "step": 94730
    },
    {
      "epoch": 17225.454545454544,
      "grad_norm": 0.0011137949768453836,
      "learning_rate": 3.594991364673745e-07,
      "loss": 0.0007,
      "step": 94740
    },
    {
      "epoch": 17227.272727272728,
      "grad_norm": 0.25418177247047424,
      "learning_rate": 3.593874732621847e-07,
      "loss": 0.0013,
      "step": 94750
    },
    {
      "epoch": 17229.090909090908,
      "grad_norm": 0.16313093900680542,
      "learning_rate": 3.59275817671755e-07,
      "loss": 0.0009,
      "step": 94760
    },
    {
      "epoch": 17230.909090909092,
      "grad_norm": 0.21080845594406128,
      "learning_rate": 3.591641697021317e-07,
      "loss": 0.0012,
      "step": 94770
    },
    {
      "epoch": 17232.727272727272,
      "grad_norm": 0.25506725907325745,
      "learning_rate": 3.590525293593615e-07,
      "loss": 0.0009,
      "step": 94780
    },
    {
      "epoch": 17234.545454545456,
      "grad_norm": 0.0009465035400353372,
      "learning_rate": 3.5894089664948965e-07,
      "loss": 0.0012,
      "step": 94790
    },
    {
      "epoch": 17236.363636363636,
      "grad_norm": 0.004897181410342455,
      "learning_rate": 3.5882927157856167e-07,
      "loss": 0.0015,
      "step": 94800
    },
    {
      "epoch": 17238.18181818182,
      "grad_norm": 0.23607519268989563,
      "learning_rate": 3.58717654152623e-07,
      "loss": 0.0008,
      "step": 94810
    },
    {
      "epoch": 17240.0,
      "grad_norm": 0.15970423817634583,
      "learning_rate": 3.5860604437771756e-07,
      "loss": 0.001,
      "step": 94820
    },
    {
      "epoch": 17241.81818181818,
      "grad_norm": 0.0012092126999050379,
      "learning_rate": 3.584944422598898e-07,
      "loss": 0.001,
      "step": 94830
    },
    {
      "epoch": 17243.636363636364,
      "grad_norm": 0.2440425455570221,
      "learning_rate": 3.5838284780518355e-07,
      "loss": 0.0012,
      "step": 94840
    },
    {
      "epoch": 17245.454545454544,
      "grad_norm": 0.0005263937055133283,
      "learning_rate": 3.582712610196419e-07,
      "loss": 0.0014,
      "step": 94850
    },
    {
      "epoch": 17247.272727272728,
      "grad_norm": 0.0008157807751558721,
      "learning_rate": 3.581596819093079e-07,
      "loss": 0.0009,
      "step": 94860
    },
    {
      "epoch": 17249.090909090908,
      "grad_norm": 0.0007495877798646688,
      "learning_rate": 3.5804811048022383e-07,
      "loss": 0.0012,
      "step": 94870
    },
    {
      "epoch": 17250.909090909092,
      "grad_norm": 0.21798518300056458,
      "learning_rate": 3.579365467384319e-07,
      "loss": 0.001,
      "step": 94880
    },
    {
      "epoch": 17252.727272727272,
      "grad_norm": 0.0011783451773226261,
      "learning_rate": 3.578249906899738e-07,
      "loss": 0.0009,
      "step": 94890
    },
    {
      "epoch": 17254.545454545456,
      "grad_norm": 0.18118630349636078,
      "learning_rate": 3.5771344234089056e-07,
      "loss": 0.0015,
      "step": 94900
    },
    {
      "epoch": 17256.363636363636,
      "grad_norm": 0.0010739866411313415,
      "learning_rate": 3.5760190169722303e-07,
      "loss": 0.0011,
      "step": 94910
    },
    {
      "epoch": 17258.18181818182,
      "grad_norm": 0.20470793545246124,
      "learning_rate": 3.574903687650119e-07,
      "loss": 0.0013,
      "step": 94920
    },
    {
      "epoch": 17260.0,
      "grad_norm": 0.28784435987472534,
      "learning_rate": 3.573788435502969e-07,
      "loss": 0.0008,
      "step": 94930
    },
    {
      "epoch": 17261.81818181818,
      "grad_norm": 0.0006482134922407568,
      "learning_rate": 3.5726732605911756e-07,
      "loss": 0.0012,
      "step": 94940
    },
    {
      "epoch": 17263.636363636364,
      "grad_norm": 0.001180361257866025,
      "learning_rate": 3.5715581629751325e-07,
      "loss": 0.0011,
      "step": 94950
    },
    {
      "epoch": 17265.454545454544,
      "grad_norm": 0.0008042496046982706,
      "learning_rate": 3.570443142715224e-07,
      "loss": 0.0012,
      "step": 94960
    },
    {
      "epoch": 17267.272727272728,
      "grad_norm": 0.0007272767252288759,
      "learning_rate": 3.569328199871836e-07,
      "loss": 0.0007,
      "step": 94970
    },
    {
      "epoch": 17269.090909090908,
      "grad_norm": 0.001524564577266574,
      "learning_rate": 3.5682133345053445e-07,
      "loss": 0.0012,
      "step": 94980
    },
    {
      "epoch": 17270.909090909092,
      "grad_norm": 0.28350988030433655,
      "learning_rate": 3.567098546676124e-07,
      "loss": 0.0012,
      "step": 94990
    },
    {
      "epoch": 17272.727272727272,
      "grad_norm": 0.17041411995887756,
      "learning_rate": 3.56598383644455e-07,
      "loss": 0.0009,
      "step": 95000
    },
    {
      "epoch": 17272.727272727272,
      "eval_loss": 5.048095703125,
      "eval_runtime": 0.9507,
      "eval_samples_per_second": 10.519,
      "eval_steps_per_second": 5.259,
      "step": 95000
    },
    {
      "epoch": 17274.545454545456,
      "grad_norm": 0.2755008935928345,
      "learning_rate": 3.5648692038709817e-07,
      "loss": 0.0014,
      "step": 95010
    },
    {
      "epoch": 17276.363636363636,
      "grad_norm": 0.20181779563426971,
      "learning_rate": 3.563754649015786e-07,
      "loss": 0.0009,
      "step": 95020
    },
    {
      "epoch": 17278.18181818182,
      "grad_norm": 0.22407415509223938,
      "learning_rate": 3.5626401719393206e-07,
      "loss": 0.0012,
      "step": 95030
    },
    {
      "epoch": 17280.0,
      "grad_norm": 0.0010813622502610087,
      "learning_rate": 3.5615257727019366e-07,
      "loss": 0.001,
      "step": 95040
    },
    {
      "epoch": 17281.81818181818,
      "grad_norm": 0.15972311794757843,
      "learning_rate": 3.5604114513639854e-07,
      "loss": 0.001,
      "step": 95050
    },
    {
      "epoch": 17283.636363636364,
      "grad_norm": 0.0005970064667053521,
      "learning_rate": 3.559297207985811e-07,
      "loss": 0.0015,
      "step": 95060
    },
    {
      "epoch": 17285.454545454544,
      "grad_norm": 0.15913240611553192,
      "learning_rate": 3.5581830426277546e-07,
      "loss": 0.0012,
      "step": 95070
    },
    {
      "epoch": 17287.272727272728,
      "grad_norm": 0.0004798775480594486,
      "learning_rate": 3.557068955350154e-07,
      "loss": 0.0008,
      "step": 95080
    },
    {
      "epoch": 17289.090909090908,
      "grad_norm": 0.2531473636627197,
      "learning_rate": 3.55595494621334e-07,
      "loss": 0.0014,
      "step": 95090
    },
    {
      "epoch": 17290.909090909092,
      "grad_norm": 0.0005355357425287366,
      "learning_rate": 3.554841015277641e-07,
      "loss": 0.0008,
      "step": 95100
    },
    {
      "epoch": 17292.727272727272,
      "grad_norm": 0.15197493135929108,
      "learning_rate": 3.5537271626033837e-07,
      "loss": 0.0012,
      "step": 95110
    },
    {
      "epoch": 17294.545454545456,
      "grad_norm": 0.0004100977093912661,
      "learning_rate": 3.5526133882508855e-07,
      "loss": 0.001,
      "step": 95120
    },
    {
      "epoch": 17296.363636363636,
      "grad_norm": 0.0005292735295370221,
      "learning_rate": 3.551499692280463e-07,
      "loss": 0.0012,
      "step": 95130
    },
    {
      "epoch": 17298.18181818182,
      "grad_norm": 0.00054455257486552,
      "learning_rate": 3.5503860747524267e-07,
      "loss": 0.0011,
      "step": 95140
    },
    {
      "epoch": 17300.0,
      "grad_norm": 0.16778749227523804,
      "learning_rate": 3.5492725357270836e-07,
      "loss": 0.0012,
      "step": 95150
    },
    {
      "epoch": 17301.81818181818,
      "grad_norm": 0.1466633826494217,
      "learning_rate": 3.548159075264738e-07,
      "loss": 0.0012,
      "step": 95160
    },
    {
      "epoch": 17303.636363636364,
      "grad_norm": 0.0007731154910288751,
      "learning_rate": 3.547045693425686e-07,
      "loss": 0.0009,
      "step": 95170
    },
    {
      "epoch": 17305.454545454544,
      "grad_norm": 0.26462721824645996,
      "learning_rate": 3.5459323902702225e-07,
      "loss": 0.0014,
      "step": 95180
    },
    {
      "epoch": 17307.272727272728,
      "grad_norm": 0.001735623343847692,
      "learning_rate": 3.544819165858642e-07,
      "loss": 0.0008,
      "step": 95190
    },
    {
      "epoch": 17309.090909090908,
      "grad_norm": 0.0008111063507385552,
      "learning_rate": 3.5437060202512223e-07,
      "loss": 0.0012,
      "step": 95200
    },
    {
      "epoch": 17310.909090909092,
      "grad_norm": 0.20974969863891602,
      "learning_rate": 3.5425929535082515e-07,
      "loss": 0.0012,
      "step": 95210
    },
    {
      "epoch": 17312.727272727272,
      "grad_norm": 0.2101266235113144,
      "learning_rate": 3.5414799656900054e-07,
      "loss": 0.0014,
      "step": 95220
    },
    {
      "epoch": 17314.545454545456,
      "grad_norm": 0.15085478127002716,
      "learning_rate": 3.540367056856755e-07,
      "loss": 0.0009,
      "step": 95230
    },
    {
      "epoch": 17316.363636363636,
      "grad_norm": 0.18130628764629364,
      "learning_rate": 3.539254227068771e-07,
      "loss": 0.0015,
      "step": 95240
    },
    {
      "epoch": 17318.18181818182,
      "grad_norm": 0.0008599759894423187,
      "learning_rate": 3.5381414763863163e-07,
      "loss": 0.0008,
      "step": 95250
    },
    {
      "epoch": 17320.0,
      "grad_norm": 0.0005454026395455003,
      "learning_rate": 3.537028804869652e-07,
      "loss": 0.0012,
      "step": 95260
    },
    {
      "epoch": 17321.81818181818,
      "grad_norm": 0.0007189966272562742,
      "learning_rate": 3.535916212579034e-07,
      "loss": 0.0012,
      "step": 95270
    },
    {
      "epoch": 17323.636363636364,
      "grad_norm": 0.2597060799598694,
      "learning_rate": 3.534803699574713e-07,
      "loss": 0.001,
      "step": 95280
    },
    {
      "epoch": 17325.454545454544,
      "grad_norm": 0.0023251224774867296,
      "learning_rate": 3.533691265916936e-07,
      "loss": 0.0013,
      "step": 95290
    },
    {
      "epoch": 17327.272727272728,
      "grad_norm": 0.0010086034890264273,
      "learning_rate": 3.5325789116659486e-07,
      "loss": 0.0007,
      "step": 95300
    },
    {
      "epoch": 17329.090909090908,
      "grad_norm": 0.0003837977710645646,
      "learning_rate": 3.531466636881987e-07,
      "loss": 0.0012,
      "step": 95310
    },
    {
      "epoch": 17330.909090909092,
      "grad_norm": 0.0021742540411651134,
      "learning_rate": 3.530354441625286e-07,
      "loss": 0.0012,
      "step": 95320
    },
    {
      "epoch": 17332.727272727272,
      "grad_norm": 0.0007004496292211115,
      "learning_rate": 3.5292423259560753e-07,
      "loss": 0.0009,
      "step": 95330
    },
    {
      "epoch": 17334.545454545456,
      "grad_norm": 0.000560336746275425,
      "learning_rate": 3.5281302899345823e-07,
      "loss": 0.001,
      "step": 95340
    },
    {
      "epoch": 17336.363636363636,
      "grad_norm": 0.18354934453964233,
      "learning_rate": 3.527018333621027e-07,
      "loss": 0.0015,
      "step": 95350
    },
    {
      "epoch": 17338.18181818182,
      "grad_norm": 0.0013034626608714461,
      "learning_rate": 3.5259064570756264e-07,
      "loss": 0.0009,
      "step": 95360
    },
    {
      "epoch": 17340.0,
      "grad_norm": 0.2618931233882904,
      "learning_rate": 3.5247946603585926e-07,
      "loss": 0.0012,
      "step": 95370
    },
    {
      "epoch": 17341.81818181818,
      "grad_norm": 0.0010179219534620643,
      "learning_rate": 3.523682943530139e-07,
      "loss": 0.001,
      "step": 95380
    },
    {
      "epoch": 17343.636363636364,
      "grad_norm": 0.0007607406587339938,
      "learning_rate": 3.5225713066504614e-07,
      "loss": 0.0012,
      "step": 95390
    },
    {
      "epoch": 17345.454545454544,
      "grad_norm": 0.011174227111041546,
      "learning_rate": 3.521459749779768e-07,
      "loss": 0.001,
      "step": 95400
    },
    {
      "epoch": 17347.272727272728,
      "grad_norm": 0.0018356703221797943,
      "learning_rate": 3.520348272978247e-07,
      "loss": 0.0009,
      "step": 95410
    },
    {
      "epoch": 17349.090909090908,
      "grad_norm": 0.19357159733772278,
      "learning_rate": 3.5192368763060944e-07,
      "loss": 0.0013,
      "step": 95420
    },
    {
      "epoch": 17350.909090909092,
      "grad_norm": 0.0074736434035003185,
      "learning_rate": 3.518125559823496e-07,
      "loss": 0.0011,
      "step": 95430
    },
    {
      "epoch": 17352.727272727272,
      "grad_norm": 0.15485915541648865,
      "learning_rate": 3.5170143235906337e-07,
      "loss": 0.001,
      "step": 95440
    },
    {
      "epoch": 17354.545454545456,
      "grad_norm": 0.15574181079864502,
      "learning_rate": 3.5159031676676855e-07,
      "loss": 0.001,
      "step": 95450
    },
    {
      "epoch": 17356.363636363636,
      "grad_norm": 0.2467934936285019,
      "learning_rate": 3.514792092114826e-07,
      "loss": 0.0012,
      "step": 95460
    },
    {
      "epoch": 17358.18181818182,
      "grad_norm": 0.0005609090439975262,
      "learning_rate": 3.513681096992224e-07,
      "loss": 0.0009,
      "step": 95470
    },
    {
      "epoch": 17360.0,
      "grad_norm": 0.0006764807621948421,
      "learning_rate": 3.5125701823600447e-07,
      "loss": 0.0012,
      "step": 95480
    },
    {
      "epoch": 17361.81818181818,
      "grad_norm": 0.2222563624382019,
      "learning_rate": 3.511459348278448e-07,
      "loss": 0.0012,
      "step": 95490
    },
    {
      "epoch": 17363.636363636364,
      "grad_norm": 0.0004973176983185112,
      "learning_rate": 3.5103485948075894e-07,
      "loss": 0.0009,
      "step": 95500
    },
    {
      "epoch": 17363.636363636364,
      "eval_loss": 5.036252498626709,
      "eval_runtime": 0.9509,
      "eval_samples_per_second": 10.517,
      "eval_steps_per_second": 5.258,
      "step": 95500
    },
    {
      "epoch": 17365.454545454544,
      "grad_norm": 0.20619496703147888,
      "learning_rate": 3.509237922007625e-07,
      "loss": 0.0012,
      "step": 95510
    },
    {
      "epoch": 17367.272727272728,
      "grad_norm": 0.2516372501850128,
      "learning_rate": 3.508127329938699e-07,
      "loss": 0.0015,
      "step": 95520
    },
    {
      "epoch": 17369.090909090908,
      "grad_norm": 0.15073290467262268,
      "learning_rate": 3.507016818660955e-07,
      "loss": 0.0009,
      "step": 95530
    },
    {
      "epoch": 17370.909090909092,
      "grad_norm": 0.14902269840240479,
      "learning_rate": 3.5059063882345335e-07,
      "loss": 0.0012,
      "step": 95540
    },
    {
      "epoch": 17372.727272727272,
      "grad_norm": 0.01024548802524805,
      "learning_rate": 3.5047960387195667e-07,
      "loss": 0.001,
      "step": 95550
    },
    {
      "epoch": 17374.545454545456,
      "grad_norm": 0.1489391326904297,
      "learning_rate": 3.503685770176185e-07,
      "loss": 0.0011,
      "step": 95560
    },
    {
      "epoch": 17376.363636363636,
      "grad_norm": 0.001261545461602509,
      "learning_rate": 3.5025755826645184e-07,
      "loss": 0.0009,
      "step": 95570
    },
    {
      "epoch": 17378.18181818182,
      "grad_norm": 0.0012489997316151857,
      "learning_rate": 3.5014654762446807e-07,
      "loss": 0.0012,
      "step": 95580
    },
    {
      "epoch": 17380.0,
      "grad_norm": 0.0009365093428641558,
      "learning_rate": 3.5003554509767964e-07,
      "loss": 0.0012,
      "step": 95590
    },
    {
      "epoch": 17381.81818181818,
      "grad_norm": 0.15281058847904205,
      "learning_rate": 3.4992455069209716e-07,
      "loss": 0.001,
      "step": 95600
    },
    {
      "epoch": 17383.636363636364,
      "grad_norm": 0.19912157952785492,
      "learning_rate": 3.4981356441373175e-07,
      "loss": 0.0011,
      "step": 95610
    },
    {
      "epoch": 17385.454545454544,
      "grad_norm": 0.0018175116274505854,
      "learning_rate": 3.49702586268594e-07,
      "loss": 0.001,
      "step": 95620
    },
    {
      "epoch": 17387.272727272728,
      "grad_norm": 0.0007242009160108864,
      "learning_rate": 3.4959161626269343e-07,
      "loss": 0.0011,
      "step": 95630
    },
    {
      "epoch": 17389.090909090908,
      "grad_norm": 0.15035685896873474,
      "learning_rate": 3.4948065440203976e-07,
      "loss": 0.0012,
      "step": 95640
    },
    {
      "epoch": 17390.909090909092,
      "grad_norm": 0.0008191782981157303,
      "learning_rate": 3.493697006926421e-07,
      "loss": 0.0011,
      "step": 95650
    },
    {
      "epoch": 17392.727272727272,
      "grad_norm": 0.20759376883506775,
      "learning_rate": 3.492587551405088e-07,
      "loss": 0.0009,
      "step": 95660
    },
    {
      "epoch": 17394.545454545456,
      "grad_norm": 0.19982457160949707,
      "learning_rate": 3.4914781775164835e-07,
      "loss": 0.0014,
      "step": 95670
    },
    {
      "epoch": 17396.363636363636,
      "grad_norm": 0.18157804012298584,
      "learning_rate": 3.4903688853206813e-07,
      "loss": 0.001,
      "step": 95680
    },
    {
      "epoch": 17398.18181818182,
      "grad_norm": 0.249718576669693,
      "learning_rate": 3.489259674877756e-07,
      "loss": 0.0014,
      "step": 95690
    },
    {
      "epoch": 17400.0,
      "grad_norm": 0.2378106564283371,
      "learning_rate": 3.488150546247778e-07,
      "loss": 0.0009,
      "step": 95700
    },
    {
      "epoch": 17401.81818181818,
      "grad_norm": 0.0013707074103876948,
      "learning_rate": 3.487041499490808e-07,
      "loss": 0.001,
      "step": 95710
    },
    {
      "epoch": 17403.636363636364,
      "grad_norm": 0.0006692156894132495,
      "learning_rate": 3.4859325346669066e-07,
      "loss": 0.0013,
      "step": 95720
    },
    {
      "epoch": 17405.454545454544,
      "grad_norm": 0.0006124656065367162,
      "learning_rate": 3.484823651836131e-07,
      "loss": 0.0007,
      "step": 95730
    },
    {
      "epoch": 17407.272727272728,
      "grad_norm": 0.14837615191936493,
      "learning_rate": 3.483714851058528e-07,
      "loss": 0.0013,
      "step": 95740
    },
    {
      "epoch": 17409.090909090908,
      "grad_norm": 0.0020425778347998857,
      "learning_rate": 3.482606132394148e-07,
      "loss": 0.001,
      "step": 95750
    },
    {
      "epoch": 17410.909090909092,
      "grad_norm": 0.14927905797958374,
      "learning_rate": 3.481497495903029e-07,
      "loss": 0.001,
      "step": 95760
    },
    {
      "epoch": 17412.727272727272,
      "grad_norm": 0.0007575859199278057,
      "learning_rate": 3.480388941645209e-07,
      "loss": 0.0012,
      "step": 95770
    },
    {
      "epoch": 17414.545454545456,
      "grad_norm": 0.18783624470233917,
      "learning_rate": 3.4792804696807255e-07,
      "loss": 0.001,
      "step": 95780
    },
    {
      "epoch": 17416.363636363636,
      "grad_norm": 0.1526312232017517,
      "learning_rate": 3.4781720800696e-07,
      "loss": 0.0012,
      "step": 95790
    },
    {
      "epoch": 17418.18181818182,
      "grad_norm": 0.0008194153779186308,
      "learning_rate": 3.4770637728718607e-07,
      "loss": 0.0009,
      "step": 95800
    },
    {
      "epoch": 17420.0,
      "grad_norm": 0.1952846646308899,
      "learning_rate": 3.475955548147527e-07,
      "loss": 0.0012,
      "step": 95810
    },
    {
      "epoch": 17421.81818181818,
      "grad_norm": 0.0018506994238123298,
      "learning_rate": 3.4748474059566123e-07,
      "loss": 0.001,
      "step": 95820
    },
    {
      "epoch": 17423.636363636364,
      "grad_norm": 0.0006103177438490093,
      "learning_rate": 3.473739346359128e-07,
      "loss": 0.0012,
      "step": 95830
    },
    {
      "epoch": 17425.454545454544,
      "grad_norm": 0.17999468743801117,
      "learning_rate": 3.4726313694150814e-07,
      "loss": 0.0009,
      "step": 95840
    },
    {
      "epoch": 17427.272727272728,
      "grad_norm": 0.0016693486832082272,
      "learning_rate": 3.471523475184472e-07,
      "loss": 0.001,
      "step": 95850
    },
    {
      "epoch": 17429.090909090908,
      "grad_norm": 0.14542794227600098,
      "learning_rate": 3.4704156637272984e-07,
      "loss": 0.0013,
      "step": 95860
    },
    {
      "epoch": 17430.909090909092,
      "grad_norm": 0.20759890973567963,
      "learning_rate": 3.4693079351035515e-07,
      "loss": 0.001,
      "step": 95870
    },
    {
      "epoch": 17432.727272727272,
      "grad_norm": 0.1529623419046402,
      "learning_rate": 3.46820028937322e-07,
      "loss": 0.001,
      "step": 95880
    },
    {
      "epoch": 17434.545454545456,
      "grad_norm": 0.0006407690816558897,
      "learning_rate": 3.46709272659629e-07,
      "loss": 0.0009,
      "step": 95890
    },
    {
      "epoch": 17436.363636363636,
      "grad_norm": 0.0006973407580517232,
      "learning_rate": 3.4659852468327383e-07,
      "loss": 0.0011,
      "step": 95900
    },
    {
      "epoch": 17438.18181818182,
      "grad_norm": 0.006347527727484703,
      "learning_rate": 3.46487785014254e-07,
      "loss": 0.0015,
      "step": 95910
    },
    {
      "epoch": 17440.0,
      "grad_norm": 0.15488383173942566,
      "learning_rate": 3.4637705365856665e-07,
      "loss": 0.0009,
      "step": 95920
    },
    {
      "epoch": 17441.81818181818,
      "grad_norm": 0.001250546076335013,
      "learning_rate": 3.4626633062220813e-07,
      "loss": 0.001,
      "step": 95930
    },
    {
      "epoch": 17443.636363636364,
      "grad_norm": 0.19377540051937103,
      "learning_rate": 3.4615561591117483e-07,
      "loss": 0.0011,
      "step": 95940
    },
    {
      "epoch": 17445.454545454544,
      "grad_norm": 0.0008156203548423946,
      "learning_rate": 3.4604490953146205e-07,
      "loss": 0.001,
      "step": 95950
    },
    {
      "epoch": 17447.272727272728,
      "grad_norm": 0.004864645190536976,
      "learning_rate": 3.4593421148906523e-07,
      "loss": 0.0013,
      "step": 95960
    },
    {
      "epoch": 17449.090909090908,
      "grad_norm": 0.16014331579208374,
      "learning_rate": 3.458235217899793e-07,
      "loss": 0.0011,
      "step": 95970
    },
    {
      "epoch": 17450.909090909092,
      "grad_norm": 0.22247177362442017,
      "learning_rate": 3.4571284044019823e-07,
      "loss": 0.001,
      "step": 95980
    },
    {
      "epoch": 17452.727272727272,
      "grad_norm": 0.0009185399394482374,
      "learning_rate": 3.4560216744571604e-07,
      "loss": 0.0009,
      "step": 95990
    },
    {
      "epoch": 17454.545454545456,
      "grad_norm": 0.000861334556248039,
      "learning_rate": 3.454915028125263e-07,
      "loss": 0.0014,
      "step": 96000
    },
    {
      "epoch": 17454.545454545456,
      "eval_loss": 5.027718544006348,
      "eval_runtime": 0.9513,
      "eval_samples_per_second": 10.512,
      "eval_steps_per_second": 5.256,
      "step": 96000
    },
    {
      "epoch": 17456.363636363636,
      "grad_norm": 0.21006622910499573,
      "learning_rate": 3.453808465466217e-07,
      "loss": 0.0007,
      "step": 96010
    },
    {
      "epoch": 17458.18181818182,
      "grad_norm": 0.0014068104792386293,
      "learning_rate": 3.45270198653995e-07,
      "loss": 0.001,
      "step": 96020
    },
    {
      "epoch": 17460.0,
      "grad_norm": 0.20162595808506012,
      "learning_rate": 3.4515955914063795e-07,
      "loss": 0.0012,
      "step": 96030
    },
    {
      "epoch": 17461.81818181818,
      "grad_norm": 0.0010499248746782541,
      "learning_rate": 3.4504892801254224e-07,
      "loss": 0.001,
      "step": 96040
    },
    {
      "epoch": 17463.636363636364,
      "grad_norm": 0.1561065912246704,
      "learning_rate": 3.449383052756993e-07,
      "loss": 0.0012,
      "step": 96050
    },
    {
      "epoch": 17465.454545454544,
      "grad_norm": 0.1648160219192505,
      "learning_rate": 3.4482769093609945e-07,
      "loss": 0.001,
      "step": 96060
    },
    {
      "epoch": 17467.272727272728,
      "grad_norm": 0.000636748387478292,
      "learning_rate": 3.4471708499973286e-07,
      "loss": 0.001,
      "step": 96070
    },
    {
      "epoch": 17469.090909090908,
      "grad_norm": 0.0005422005779109895,
      "learning_rate": 3.4460648747258994e-07,
      "loss": 0.001,
      "step": 96080
    },
    {
      "epoch": 17470.909090909092,
      "grad_norm": 0.2571898400783539,
      "learning_rate": 3.4449589836065915e-07,
      "loss": 0.0012,
      "step": 96090
    },
    {
      "epoch": 17472.727272727272,
      "grad_norm": 0.0017093488713726401,
      "learning_rate": 3.4438531766993004e-07,
      "loss": 0.0012,
      "step": 96100
    },
    {
      "epoch": 17474.545454545456,
      "grad_norm": 0.0006117664743214846,
      "learning_rate": 3.442747454063907e-07,
      "loss": 0.001,
      "step": 96110
    },
    {
      "epoch": 17476.363636363636,
      "grad_norm": 0.20904402434825897,
      "learning_rate": 3.4416418157602905e-07,
      "loss": 0.0011,
      "step": 96120
    },
    {
      "epoch": 17478.18181818182,
      "grad_norm": 0.0006470631924457848,
      "learning_rate": 3.4405362618483284e-07,
      "loss": 0.0009,
      "step": 96130
    },
    {
      "epoch": 17480.0,
      "grad_norm": 0.17001204192638397,
      "learning_rate": 3.4394307923878885e-07,
      "loss": 0.0011,
      "step": 96140
    },
    {
      "epoch": 17481.81818181818,
      "grad_norm": 0.21068060398101807,
      "learning_rate": 3.438325407438837e-07,
      "loss": 0.0012,
      "step": 96150
    },
    {
      "epoch": 17483.636363636364,
      "grad_norm": 0.01716974936425686,
      "learning_rate": 3.437220107061038e-07,
      "loss": 0.001,
      "step": 96160
    },
    {
      "epoch": 17485.454545454544,
      "grad_norm": 0.2731104791164398,
      "learning_rate": 3.4361148913143436e-07,
      "loss": 0.0014,
      "step": 96170
    },
    {
      "epoch": 17487.272727272728,
      "grad_norm": 0.21121732890605927,
      "learning_rate": 3.4350097602586083e-07,
      "loss": 0.0009,
      "step": 96180
    },
    {
      "epoch": 17489.090909090908,
      "grad_norm": 0.2626027762889862,
      "learning_rate": 3.4339047139536816e-07,
      "loss": 0.0013,
      "step": 96190
    },
    {
      "epoch": 17490.909090909092,
      "grad_norm": 0.22466914355754852,
      "learning_rate": 3.432799752459402e-07,
      "loss": 0.001,
      "step": 96200
    },
    {
      "epoch": 17492.727272727272,
      "grad_norm": 0.0010011293925344944,
      "learning_rate": 3.4316948758356125e-07,
      "loss": 0.0011,
      "step": 96210
    },
    {
      "epoch": 17494.545454545456,
      "grad_norm": 0.16572502255439758,
      "learning_rate": 3.430590084142143e-07,
      "loss": 0.0012,
      "step": 96220
    },
    {
      "epoch": 17496.363636363636,
      "grad_norm": 0.0014328421093523502,
      "learning_rate": 3.4294853774388236e-07,
      "loss": 0.0008,
      "step": 96230
    },
    {
      "epoch": 17498.18181818182,
      "grad_norm": 0.18914037942886353,
      "learning_rate": 3.428380755785481e-07,
      "loss": 0.0012,
      "step": 96240
    },
    {
      "epoch": 17500.0,
      "grad_norm": 0.16056297719478607,
      "learning_rate": 3.4272762192419325e-07,
      "loss": 0.001,
      "step": 96250
    },
    {
      "epoch": 17501.81818181818,
      "grad_norm": 0.0008642529137432575,
      "learning_rate": 3.426171767867992e-07,
      "loss": 0.0008,
      "step": 96260
    },
    {
      "epoch": 17503.636363636364,
      "grad_norm": 0.21049654483795166,
      "learning_rate": 3.425067401723477e-07,
      "loss": 0.0014,
      "step": 96270
    },
    {
      "epoch": 17505.454545454544,
      "grad_norm": 0.19368813931941986,
      "learning_rate": 3.423963120868185e-07,
      "loss": 0.0013,
      "step": 96280
    },
    {
      "epoch": 17507.272727272728,
      "grad_norm": 0.0006537490990012884,
      "learning_rate": 3.4228589253619245e-07,
      "loss": 0.0007,
      "step": 96290
    },
    {
      "epoch": 17509.090909090908,
      "grad_norm": 0.26672154664993286,
      "learning_rate": 3.421754815264488e-07,
      "loss": 0.0012,
      "step": 96300
    },
    {
      "epoch": 17510.909090909092,
      "grad_norm": 0.14089611172676086,
      "learning_rate": 3.4206507906356684e-07,
      "loss": 0.0012,
      "step": 96310
    },
    {
      "epoch": 17512.727272727272,
      "grad_norm": 0.14198382198810577,
      "learning_rate": 3.4195468515352553e-07,
      "loss": 0.0012,
      "step": 96320
    },
    {
      "epoch": 17514.545454545456,
      "grad_norm": 0.0007312895031645894,
      "learning_rate": 3.41844299802303e-07,
      "loss": 0.0006,
      "step": 96330
    },
    {
      "epoch": 17516.363636363636,
      "grad_norm": 0.19357961416244507,
      "learning_rate": 3.4173392301587696e-07,
      "loss": 0.0014,
      "step": 96340
    },
    {
      "epoch": 17518.18181818182,
      "grad_norm": 0.16711564362049103,
      "learning_rate": 3.416235548002252e-07,
      "loss": 0.0012,
      "step": 96350
    },
    {
      "epoch": 17520.0,
      "grad_norm": 0.0005332715809345245,
      "learning_rate": 3.415131951613241e-07,
      "loss": 0.001,
      "step": 96360
    },
    {
      "epoch": 17521.81818181818,
      "grad_norm": 0.19821350276470184,
      "learning_rate": 3.4140284410515064e-07,
      "loss": 0.0008,
      "step": 96370
    },
    {
      "epoch": 17523.636363636364,
      "grad_norm": 0.26704785227775574,
      "learning_rate": 3.412925016376802e-07,
      "loss": 0.0013,
      "step": 96380
    },
    {
      "epoch": 17525.454545454544,
      "grad_norm": 0.1988101601600647,
      "learning_rate": 3.411821677648886e-07,
      "loss": 0.001,
      "step": 96390
    },
    {
      "epoch": 17527.272727272728,
      "grad_norm": 0.2108134776353836,
      "learning_rate": 3.4107184249275113e-07,
      "loss": 0.0012,
      "step": 96400
    },
    {
      "epoch": 17529.090909090908,
      "grad_norm": 0.001996767008677125,
      "learning_rate": 3.409615258272419e-07,
      "loss": 0.0009,
      "step": 96410
    },
    {
      "epoch": 17530.909090909092,
      "grad_norm": 0.0004889775300398469,
      "learning_rate": 3.408512177743353e-07,
      "loss": 0.001,
      "step": 96420
    },
    {
      "epoch": 17532.727272727272,
      "grad_norm": 0.0013274508528411388,
      "learning_rate": 3.4074091834000497e-07,
      "loss": 0.0013,
      "step": 96430
    },
    {
      "epoch": 17534.545454545456,
      "grad_norm": 0.1543365865945816,
      "learning_rate": 3.40630627530224e-07,
      "loss": 0.0008,
      "step": 96440
    },
    {
      "epoch": 17536.363636363636,
      "grad_norm": 0.0019251933554187417,
      "learning_rate": 3.4052034535096494e-07,
      "loss": 0.0011,
      "step": 96450
    },
    {
      "epoch": 17538.18181818182,
      "grad_norm": 0.16109992563724518,
      "learning_rate": 3.4041007180820057e-07,
      "loss": 0.0012,
      "step": 96460
    },
    {
      "epoch": 17540.0,
      "grad_norm": 0.2087802141904831,
      "learning_rate": 3.40299806907902e-07,
      "loss": 0.001,
      "step": 96470
    },
    {
      "epoch": 17541.81818181818,
      "grad_norm": 0.262195885181427,
      "learning_rate": 3.4018955065604103e-07,
      "loss": 0.0009,
      "step": 96480
    },
    {
      "epoch": 17543.636363636364,
      "grad_norm": 0.2754720151424408,
      "learning_rate": 3.4007930305858834e-07,
      "loss": 0.0013,
      "step": 96490
    },
    {
      "epoch": 17545.454545454544,
      "grad_norm": 0.15396583080291748,
      "learning_rate": 3.3996906412151417e-07,
      "loss": 0.001,
      "step": 96500
    },
    {
      "epoch": 17545.454545454544,
      "eval_loss": 4.975376129150391,
      "eval_runtime": 0.9493,
      "eval_samples_per_second": 10.534,
      "eval_steps_per_second": 5.267,
      "step": 96500
    },
    {
      "epoch": 17547.272727272728,
      "grad_norm": 0.0005997801781632006,
      "learning_rate": 3.398588338507887e-07,
      "loss": 0.001,
      "step": 96510
    },
    {
      "epoch": 17549.090909090908,
      "grad_norm": 0.0006383046275004745,
      "learning_rate": 3.397486122523811e-07,
      "loss": 0.0011,
      "step": 96520
    },
    {
      "epoch": 17550.909090909092,
      "grad_norm": 0.0007216650992631912,
      "learning_rate": 3.396383993322603e-07,
      "loss": 0.0012,
      "step": 96530
    },
    {
      "epoch": 17552.727272727272,
      "grad_norm": 0.0005888997693546116,
      "learning_rate": 3.395281950963953e-07,
      "loss": 0.0009,
      "step": 96540
    },
    {
      "epoch": 17554.545454545456,
      "grad_norm": 0.21126100420951843,
      "learning_rate": 3.394179995507535e-07,
      "loss": 0.001,
      "step": 96550
    },
    {
      "epoch": 17556.363636363636,
      "grad_norm": 0.0019404994091019034,
      "learning_rate": 3.3930781270130293e-07,
      "loss": 0.0011,
      "step": 96560
    },
    {
      "epoch": 17558.18181818182,
      "grad_norm": 0.0008005549316294491,
      "learning_rate": 3.391976345540101e-07,
      "loss": 0.0012,
      "step": 96570
    },
    {
      "epoch": 17560.0,
      "grad_norm": 0.26154443621635437,
      "learning_rate": 3.390874651148421e-07,
      "loss": 0.0012,
      "step": 96580
    },
    {
      "epoch": 17561.81818181818,
      "grad_norm": 0.0023835785686969757,
      "learning_rate": 3.389773043897651e-07,
      "loss": 0.0011,
      "step": 96590
    },
    {
      "epoch": 17563.636363636364,
      "grad_norm": 0.0008438554359599948,
      "learning_rate": 3.388671523847445e-07,
      "loss": 0.001,
      "step": 96600
    },
    {
      "epoch": 17565.454545454544,
      "grad_norm": 0.0019075252348557115,
      "learning_rate": 3.3875700910574555e-07,
      "loss": 0.0015,
      "step": 96610
    },
    {
      "epoch": 17567.272727272728,
      "grad_norm": 0.00149240018799901,
      "learning_rate": 3.3864687455873313e-07,
      "loss": 0.0007,
      "step": 96620
    },
    {
      "epoch": 17569.090909090908,
      "grad_norm": 0.005312898196280003,
      "learning_rate": 3.385367487496713e-07,
      "loss": 0.0015,
      "step": 96630
    },
    {
      "epoch": 17570.909090909092,
      "grad_norm": 0.0009026252664625645,
      "learning_rate": 3.38426631684524e-07,
      "loss": 0.0007,
      "step": 96640
    },
    {
      "epoch": 17572.727272727272,
      "grad_norm": 0.0006270153098739684,
      "learning_rate": 3.383165233692544e-07,
      "loss": 0.0009,
      "step": 96650
    },
    {
      "epoch": 17574.545454545456,
      "grad_norm": 0.0006731468602083623,
      "learning_rate": 3.3820642380982523e-07,
      "loss": 0.0012,
      "step": 96660
    },
    {
      "epoch": 17576.363636363636,
      "grad_norm": 0.177201047539711,
      "learning_rate": 3.380963330121993e-07,
      "loss": 0.0014,
      "step": 96670
    },
    {
      "epoch": 17578.18181818182,
      "grad_norm": 0.19513820111751556,
      "learning_rate": 3.3798625098233786e-07,
      "loss": 0.001,
      "step": 96680
    },
    {
      "epoch": 17580.0,
      "grad_norm": 0.0008314222213812172,
      "learning_rate": 3.3787617772620275e-07,
      "loss": 0.001,
      "step": 96690
    },
    {
      "epoch": 17581.81818181818,
      "grad_norm": 0.0006847947370260954,
      "learning_rate": 3.377661132497549e-07,
      "loss": 0.0012,
      "step": 96700
    },
    {
      "epoch": 17583.636363636364,
      "grad_norm": 0.002456302521750331,
      "learning_rate": 3.3765605755895456e-07,
      "loss": 0.0008,
      "step": 96710
    },
    {
      "epoch": 17585.454545454544,
      "grad_norm": 0.0012430483475327492,
      "learning_rate": 3.3754601065976183e-07,
      "loss": 0.0013,
      "step": 96720
    },
    {
      "epoch": 17587.272727272728,
      "grad_norm": 0.21254755556583405,
      "learning_rate": 3.3743597255813614e-07,
      "loss": 0.001,
      "step": 96730
    },
    {
      "epoch": 17589.090909090908,
      "grad_norm": 0.19722574949264526,
      "learning_rate": 3.373259432600365e-07,
      "loss": 0.0012,
      "step": 96740
    },
    {
      "epoch": 17590.909090909092,
      "grad_norm": 0.2232523411512375,
      "learning_rate": 3.3721592277142173e-07,
      "loss": 0.001,
      "step": 96750
    },
    {
      "epoch": 17592.727272727272,
      "grad_norm": 0.201860710978508,
      "learning_rate": 3.371059110982495e-07,
      "loss": 0.0012,
      "step": 96760
    },
    {
      "epoch": 17594.545454545456,
      "grad_norm": 0.0009732088656164706,
      "learning_rate": 3.3699590824647763e-07,
      "loss": 0.0007,
      "step": 96770
    },
    {
      "epoch": 17596.363636363636,
      "grad_norm": 0.2128024399280548,
      "learning_rate": 3.368859142220633e-07,
      "loss": 0.0012,
      "step": 96780
    },
    {
      "epoch": 17598.18181818182,
      "grad_norm": 0.0011063464917242527,
      "learning_rate": 3.3677592903096295e-07,
      "loss": 0.001,
      "step": 96790
    },
    {
      "epoch": 17600.0,
      "grad_norm": 0.1561642587184906,
      "learning_rate": 3.366659526791329e-07,
      "loss": 0.0012,
      "step": 96800
    },
    {
      "epoch": 17601.81818181818,
      "grad_norm": 0.01670996844768524,
      "learning_rate": 3.365559851725288e-07,
      "loss": 0.0012,
      "step": 96810
    },
    {
      "epoch": 17603.636363636364,
      "grad_norm": 0.0007576110656373203,
      "learning_rate": 3.3644602651710574e-07,
      "loss": 0.0006,
      "step": 96820
    },
    {
      "epoch": 17605.454545454544,
      "grad_norm": 0.0006593215512111783,
      "learning_rate": 3.363360767188187e-07,
      "loss": 0.0013,
      "step": 96830
    },
    {
      "epoch": 17607.272727272728,
      "grad_norm": 0.0007876881281845272,
      "learning_rate": 3.3622613578362157e-07,
      "loss": 0.0012,
      "step": 96840
    },
    {
      "epoch": 17609.090909090908,
      "grad_norm": 0.15686798095703125,
      "learning_rate": 3.361162037174683e-07,
      "loss": 0.0012,
      "step": 96850
    },
    {
      "epoch": 17610.909090909092,
      "grad_norm": 0.15768322348594666,
      "learning_rate": 3.360062805263124e-07,
      "loss": 0.0011,
      "step": 96860
    },
    {
      "epoch": 17612.727272727272,
      "grad_norm": 0.14552241563796997,
      "learning_rate": 3.358963662161062e-07,
      "loss": 0.001,
      "step": 96870
    },
    {
      "epoch": 17614.545454545456,
      "grad_norm": 0.1552625447511673,
      "learning_rate": 3.3578646079280234e-07,
      "loss": 0.001,
      "step": 96880
    },
    {
      "epoch": 17616.363636363636,
      "grad_norm": 0.0008789693820290267,
      "learning_rate": 3.356765642623527e-07,
      "loss": 0.0009,
      "step": 96890
    },
    {
      "epoch": 17618.18181818182,
      "grad_norm": 0.16310951113700867,
      "learning_rate": 3.3556667663070835e-07,
      "loss": 0.0014,
      "step": 96900
    },
    {
      "epoch": 17620.0,
      "grad_norm": 0.0005495194927789271,
      "learning_rate": 3.354567979038205e-07,
      "loss": 0.001,
      "step": 96910
    },
    {
      "epoch": 17621.81818181818,
      "grad_norm": 0.0005576200201176107,
      "learning_rate": 3.3534692808763934e-07,
      "loss": 0.001,
      "step": 96920
    },
    {
      "epoch": 17623.636363636364,
      "grad_norm": 0.0011640778975561261,
      "learning_rate": 3.352370671881147e-07,
      "loss": 0.0011,
      "step": 96930
    },
    {
      "epoch": 17625.454545454544,
      "grad_norm": 0.0005804662359878421,
      "learning_rate": 3.351272152111965e-07,
      "loss": 0.0011,
      "step": 96940
    },
    {
      "epoch": 17627.272727272728,
      "grad_norm": 0.0007474993471987545,
      "learning_rate": 3.3501737216283297e-07,
      "loss": 0.0012,
      "step": 96950
    },
    {
      "epoch": 17629.090909090908,
      "grad_norm": 0.21025928854942322,
      "learning_rate": 3.349075380489731e-07,
      "loss": 0.001,
      "step": 96960
    },
    {
      "epoch": 17630.909090909092,
      "grad_norm": 0.15959422290325165,
      "learning_rate": 3.347977128755648e-07,
      "loss": 0.0012,
      "step": 96970
    },
    {
      "epoch": 17632.727272727272,
      "grad_norm": 0.15572120249271393,
      "learning_rate": 3.3468789664855537e-07,
      "loss": 0.0012,
      "step": 96980
    },
    {
      "epoch": 17634.545454545456,
      "grad_norm": 0.0004562856338452548,
      "learning_rate": 3.3457808937389196e-07,
      "loss": 0.0007,
      "step": 96990
    },
    {
      "epoch": 17636.363636363636,
      "grad_norm": 0.16629089415073395,
      "learning_rate": 3.34468291057521e-07,
      "loss": 0.0012,
      "step": 97000
    },
    {
      "epoch": 17636.363636363636,
      "eval_loss": 5.067529678344727,
      "eval_runtime": 0.9537,
      "eval_samples_per_second": 10.485,
      "eval_steps_per_second": 5.243,
      "step": 97000
    },
    {
      "epoch": 17638.18181818182,
      "grad_norm": 0.0006373099749907851,
      "learning_rate": 3.343585017053886e-07,
      "loss": 0.0011,
      "step": 97010
    },
    {
      "epoch": 17640.0,
      "grad_norm": 0.1925407350063324,
      "learning_rate": 3.342487213234404e-07,
      "loss": 0.0012,
      "step": 97020
    },
    {
      "epoch": 17641.81818181818,
      "grad_norm": 0.2662370204925537,
      "learning_rate": 3.341389499176213e-07,
      "loss": 0.0012,
      "step": 97030
    },
    {
      "epoch": 17643.636363636364,
      "grad_norm": 0.000485808530356735,
      "learning_rate": 3.3402918749387574e-07,
      "loss": 0.0007,
      "step": 97040
    },
    {
      "epoch": 17645.454545454544,
      "grad_norm": 0.0010554089676588774,
      "learning_rate": 3.3391943405814847e-07,
      "loss": 0.0012,
      "step": 97050
    },
    {
      "epoch": 17647.272727272728,
      "grad_norm": 0.16937285661697388,
      "learning_rate": 3.3380968961638236e-07,
      "loss": 0.0015,
      "step": 97060
    },
    {
      "epoch": 17649.090909090908,
      "grad_norm": 0.2224365770816803,
      "learning_rate": 3.3369995417452084e-07,
      "loss": 0.0009,
      "step": 97070
    },
    {
      "epoch": 17650.909090909092,
      "grad_norm": 0.001102310954593122,
      "learning_rate": 3.335902277385067e-07,
      "loss": 0.001,
      "step": 97080
    },
    {
      "epoch": 17652.727272727272,
      "grad_norm": 0.21243207156658173,
      "learning_rate": 3.334805103142818e-07,
      "loss": 0.0012,
      "step": 97090
    },
    {
      "epoch": 17654.545454545456,
      "grad_norm": 0.0007050101994536817,
      "learning_rate": 3.333708019077881e-07,
      "loss": 0.0012,
      "step": 97100
    },
    {
      "epoch": 17656.363636363636,
      "grad_norm": 0.0006423183367587626,
      "learning_rate": 3.332611025249665e-07,
      "loss": 0.0009,
      "step": 97110
    },
    {
      "epoch": 17658.18181818182,
      "grad_norm": 0.004193293862044811,
      "learning_rate": 3.331514121717576e-07,
      "loss": 0.001,
      "step": 97120
    },
    {
      "epoch": 17660.0,
      "grad_norm": 0.00046488665975630283,
      "learning_rate": 3.330417308541022e-07,
      "loss": 0.0012,
      "step": 97130
    },
    {
      "epoch": 17661.81818181818,
      "grad_norm": 0.0013455779990181327,
      "learning_rate": 3.329320585779392e-07,
      "loss": 0.001,
      "step": 97140
    },
    {
      "epoch": 17663.636363636364,
      "grad_norm": 0.0010412023402750492,
      "learning_rate": 3.328223953492084e-07,
      "loss": 0.0012,
      "step": 97150
    },
    {
      "epoch": 17665.454545454544,
      "grad_norm": 0.2234930396080017,
      "learning_rate": 3.327127411738483e-07,
      "loss": 0.0012,
      "step": 97160
    },
    {
      "epoch": 17667.272727272728,
      "grad_norm": 0.17749182879924774,
      "learning_rate": 3.3260309605779714e-07,
      "loss": 0.0009,
      "step": 97170
    },
    {
      "epoch": 17669.090909090908,
      "grad_norm": 0.19726185500621796,
      "learning_rate": 3.3249346000699277e-07,
      "loss": 0.001,
      "step": 97180
    },
    {
      "epoch": 17670.909090909092,
      "grad_norm": 0.0006362413405440748,
      "learning_rate": 3.323838330273723e-07,
      "loss": 0.0009,
      "step": 97190
    },
    {
      "epoch": 17672.727272727272,
      "grad_norm": 0.15557606518268585,
      "learning_rate": 3.3227421512487255e-07,
      "loss": 0.0013,
      "step": 97200
    },
    {
      "epoch": 17674.545454545456,
      "grad_norm": 0.0009536980069242418,
      "learning_rate": 3.321646063054298e-07,
      "loss": 0.001,
      "step": 97210
    },
    {
      "epoch": 17676.363636363636,
      "grad_norm": 0.0008108047768473625,
      "learning_rate": 3.3205500657497974e-07,
      "loss": 0.0011,
      "step": 97220
    },
    {
      "epoch": 17678.18181818182,
      "grad_norm": 0.0007672832580283284,
      "learning_rate": 3.3194541593945774e-07,
      "loss": 0.001,
      "step": 97230
    },
    {
      "epoch": 17680.0,
      "grad_norm": 0.21349750459194183,
      "learning_rate": 3.3183583440479886e-07,
      "loss": 0.0013,
      "step": 97240
    },
    {
      "epoch": 17681.81818181818,
      "grad_norm": 0.16116297245025635,
      "learning_rate": 3.3172626197693673e-07,
      "loss": 0.001,
      "step": 97250
    },
    {
      "epoch": 17683.636363636364,
      "grad_norm": 0.0006583431968465447,
      "learning_rate": 3.31616698661806e-07,
      "loss": 0.0012,
      "step": 97260
    },
    {
      "epoch": 17685.454545454544,
      "grad_norm": 0.19903434813022614,
      "learning_rate": 3.3150714446533915e-07,
      "loss": 0.001,
      "step": 97270
    },
    {
      "epoch": 17687.272727272728,
      "grad_norm": 0.0018123943591490388,
      "learning_rate": 3.3139759939346956e-07,
      "loss": 0.001,
      "step": 97280
    },
    {
      "epoch": 17689.090909090908,
      "grad_norm": 0.16842691600322723,
      "learning_rate": 3.3128806345212947e-07,
      "loss": 0.0012,
      "step": 97290
    },
    {
      "epoch": 17690.909090909092,
      "grad_norm": 0.2592403292655945,
      "learning_rate": 3.311785366472506e-07,
      "loss": 0.0012,
      "step": 97300
    },
    {
      "epoch": 17692.727272727272,
      "grad_norm": 0.0015057355631142855,
      "learning_rate": 3.3106901898476426e-07,
      "loss": 0.0009,
      "step": 97310
    },
    {
      "epoch": 17694.545454545456,
      "grad_norm": 0.010702981613576412,
      "learning_rate": 3.3095951047060146e-07,
      "loss": 0.0014,
      "step": 97320
    },
    {
      "epoch": 17696.363636363636,
      "grad_norm": 0.2136290818452835,
      "learning_rate": 3.3085001111069224e-07,
      "loss": 0.0009,
      "step": 97330
    },
    {
      "epoch": 17698.18181818182,
      "grad_norm": 0.0005289891851134598,
      "learning_rate": 3.3074052091096693e-07,
      "loss": 0.0011,
      "step": 97340
    },
    {
      "epoch": 17700.0,
      "grad_norm": 0.0033707902766764164,
      "learning_rate": 3.306310398773543e-07,
      "loss": 0.0012,
      "step": 97350
    },
    {
      "epoch": 17701.81818181818,
      "grad_norm": 0.19895170629024506,
      "learning_rate": 3.3052156801578366e-07,
      "loss": 0.0012,
      "step": 97360
    },
    {
      "epoch": 17703.636363636364,
      "grad_norm": 0.16556233167648315,
      "learning_rate": 3.304121053321833e-07,
      "loss": 0.0009,
      "step": 97370
    },
    {
      "epoch": 17705.454545454544,
      "grad_norm": 0.16959859430789948,
      "learning_rate": 3.3030265183248094e-07,
      "loss": 0.001,
      "step": 97380
    },
    {
      "epoch": 17707.272727272728,
      "grad_norm": 0.0007666068850085139,
      "learning_rate": 3.3019320752260404e-07,
      "loss": 0.0011,
      "step": 97390
    },
    {
      "epoch": 17709.090909090908,
      "grad_norm": 0.0013872801791876554,
      "learning_rate": 3.300837724084795e-07,
      "loss": 0.0012,
      "step": 97400
    },
    {
      "epoch": 17710.909090909092,
      "grad_norm": 0.0005954428925178945,
      "learning_rate": 3.2997434649603364e-07,
      "loss": 0.0012,
      "step": 97410
    },
    {
      "epoch": 17712.727272727272,
      "grad_norm": 0.0005404328694567084,
      "learning_rate": 3.298649297911921e-07,
      "loss": 0.001,
      "step": 97420
    },
    {
      "epoch": 17714.545454545456,
      "grad_norm": 0.21224169433116913,
      "learning_rate": 3.297555222998809e-07,
      "loss": 0.001,
      "step": 97430
    },
    {
      "epoch": 17716.363636363636,
      "grad_norm": 0.0006245963159017265,
      "learning_rate": 3.2964612402802415e-07,
      "loss": 0.0009,
      "step": 97440
    },
    {
      "epoch": 17718.18181818182,
      "grad_norm": 0.0007192143821157515,
      "learning_rate": 3.295367349815469e-07,
      "loss": 0.0012,
      "step": 97450
    },
    {
      "epoch": 17720.0,
      "grad_norm": 0.13833807408809662,
      "learning_rate": 3.2942735516637246e-07,
      "loss": 0.0011,
      "step": 97460
    },
    {
      "epoch": 17721.81818181818,
      "grad_norm": 0.0019052712013944983,
      "learning_rate": 3.2931798458842445e-07,
      "loss": 0.0009,
      "step": 97470
    },
    {
      "epoch": 17723.636363636364,
      "grad_norm": 0.00046467490028589964,
      "learning_rate": 3.2920862325362597e-07,
      "loss": 0.0012,
      "step": 97480
    },
    {
      "epoch": 17725.454545454544,
      "grad_norm": 0.0025168140418827534,
      "learning_rate": 3.2909927116789903e-07,
      "loss": 0.0009,
      "step": 97490
    },
    {
      "epoch": 17727.272727272728,
      "grad_norm": 0.21141423285007477,
      "learning_rate": 3.2898992833716563e-07,
      "loss": 0.0013,
      "step": 97500
    },
    {
      "epoch": 17727.272727272728,
      "eval_loss": 5.0622076988220215,
      "eval_runtime": 0.9491,
      "eval_samples_per_second": 10.537,
      "eval_steps_per_second": 5.268,
      "step": 97500
    },
    {
      "epoch": 17729.090909090908,
      "grad_norm": 0.0007023100042715669,
      "learning_rate": 3.288805947673473e-07,
      "loss": 0.0011,
      "step": 97510
    },
    {
      "epoch": 17730.909090909092,
      "grad_norm": 0.19604988396167755,
      "learning_rate": 3.287712704643646e-07,
      "loss": 0.001,
      "step": 97520
    },
    {
      "epoch": 17732.727272727272,
      "grad_norm": 0.00044177344534546137,
      "learning_rate": 3.286619554341384e-07,
      "loss": 0.0012,
      "step": 97530
    },
    {
      "epoch": 17734.545454545456,
      "grad_norm": 0.1606052666902542,
      "learning_rate": 3.285526496825879e-07,
      "loss": 0.0011,
      "step": 97540
    },
    {
      "epoch": 17736.363636363636,
      "grad_norm": 0.0007071246509440243,
      "learning_rate": 3.2844335321563297e-07,
      "loss": 0.0009,
      "step": 97550
    },
    {
      "epoch": 17738.18181818182,
      "grad_norm": 0.21293562650680542,
      "learning_rate": 3.283340660391924e-07,
      "loss": 0.0013,
      "step": 97560
    },
    {
      "epoch": 17740.0,
      "grad_norm": 0.0005776594625785947,
      "learning_rate": 3.282247881591844e-07,
      "loss": 0.001,
      "step": 97570
    },
    {
      "epoch": 17741.81818181818,
      "grad_norm": 0.0004605087742675096,
      "learning_rate": 3.281155195815268e-07,
      "loss": 0.0011,
      "step": 97580
    },
    {
      "epoch": 17743.636363636364,
      "grad_norm": 0.16484779119491577,
      "learning_rate": 3.280062603121373e-07,
      "loss": 0.0015,
      "step": 97590
    },
    {
      "epoch": 17745.454545454544,
      "grad_norm": 0.16304843127727509,
      "learning_rate": 3.278970103569324e-07,
      "loss": 0.0007,
      "step": 97600
    },
    {
      "epoch": 17747.272727272728,
      "grad_norm": 0.0005483959103003144,
      "learning_rate": 3.277877697218285e-07,
      "loss": 0.0012,
      "step": 97610
    },
    {
      "epoch": 17749.090909090908,
      "grad_norm": 0.16124451160430908,
      "learning_rate": 3.276785384127415e-07,
      "loss": 0.001,
      "step": 97620
    },
    {
      "epoch": 17750.909090909092,
      "grad_norm": 0.0012250564759597182,
      "learning_rate": 3.2756931643558666e-07,
      "loss": 0.0012,
      "step": 97630
    },
    {
      "epoch": 17752.727272727272,
      "grad_norm": 0.0006625769892707467,
      "learning_rate": 3.2746010379627916e-07,
      "loss": 0.001,
      "step": 97640
    },
    {
      "epoch": 17754.545454545456,
      "grad_norm": 0.0008293824503198266,
      "learning_rate": 3.2735090050073264e-07,
      "loss": 0.0009,
      "step": 97650
    },
    {
      "epoch": 17756.363636363636,
      "grad_norm": 0.001638102112337947,
      "learning_rate": 3.272417065548615e-07,
      "loss": 0.0013,
      "step": 97660
    },
    {
      "epoch": 17758.18181818182,
      "grad_norm": 0.0005653434200212359,
      "learning_rate": 3.2713252196457897e-07,
      "loss": 0.0009,
      "step": 97670
    },
    {
      "epoch": 17760.0,
      "grad_norm": 0.13738837838172913,
      "learning_rate": 3.270233467357976e-07,
      "loss": 0.0011,
      "step": 97680
    },
    {
      "epoch": 17761.81818181818,
      "grad_norm": 0.20348188281059265,
      "learning_rate": 3.269141808744299e-07,
      "loss": 0.001,
      "step": 97690
    },
    {
      "epoch": 17763.636363636364,
      "grad_norm": 0.0008780217031016946,
      "learning_rate": 3.268050243863877e-07,
      "loss": 0.0009,
      "step": 97700
    },
    {
      "epoch": 17765.454545454544,
      "grad_norm": 0.01702244207262993,
      "learning_rate": 3.2669587727758197e-07,
      "loss": 0.0013,
      "step": 97710
    },
    {
      "epoch": 17767.272727272728,
      "grad_norm": 0.16800712049007416,
      "learning_rate": 3.26586739553924e-07,
      "loss": 0.001,
      "step": 97720
    },
    {
      "epoch": 17769.090909090908,
      "grad_norm": 0.0009213238954544067,
      "learning_rate": 3.264776112213234e-07,
      "loss": 0.0011,
      "step": 97730
    },
    {
      "epoch": 17770.909090909092,
      "grad_norm": 0.011609048582613468,
      "learning_rate": 3.2636849228569046e-07,
      "loss": 0.0012,
      "step": 97740
    },
    {
      "epoch": 17772.727272727272,
      "grad_norm": 0.1802143156528473,
      "learning_rate": 3.262593827529343e-07,
      "loss": 0.001,
      "step": 97750
    },
    {
      "epoch": 17774.545454545456,
      "grad_norm": 0.20118285715579987,
      "learning_rate": 3.261502826289636e-07,
      "loss": 0.0013,
      "step": 97760
    },
    {
      "epoch": 17776.363636363636,
      "grad_norm": 0.2749153673648834,
      "learning_rate": 3.2604119191968657e-07,
      "loss": 0.001,
      "step": 97770
    },
    {
      "epoch": 17778.18181818182,
      "grad_norm": 0.20989872515201569,
      "learning_rate": 3.25932110631011e-07,
      "loss": 0.001,
      "step": 97780
    },
    {
      "epoch": 17780.0,
      "grad_norm": 0.1704632192850113,
      "learning_rate": 3.25823038768844e-07,
      "loss": 0.0011,
      "step": 97790
    },
    {
      "epoch": 17781.81818181818,
      "grad_norm": 0.16349270939826965,
      "learning_rate": 3.257139763390925e-07,
      "loss": 0.0012,
      "step": 97800
    },
    {
      "epoch": 17783.636363636364,
      "grad_norm": 0.22581933438777924,
      "learning_rate": 3.2560492334766237e-07,
      "loss": 0.0008,
      "step": 97810
    },
    {
      "epoch": 17785.454545454544,
      "grad_norm": 0.19247548282146454,
      "learning_rate": 3.2549587980045935e-07,
      "loss": 0.0015,
      "step": 97820
    },
    {
      "epoch": 17787.272727272728,
      "grad_norm": 0.20239852368831635,
      "learning_rate": 3.2538684570338904e-07,
      "loss": 0.0009,
      "step": 97830
    },
    {
      "epoch": 17789.090909090908,
      "grad_norm": 0.1682550460100174,
      "learning_rate": 3.252778210623554e-07,
      "loss": 0.001,
      "step": 97840
    },
    {
      "epoch": 17790.909090909092,
      "grad_norm": 0.0005054420325905085,
      "learning_rate": 3.251688058832631e-07,
      "loss": 0.0012,
      "step": 97850
    },
    {
      "epoch": 17792.727272727272,
      "grad_norm": 0.001206622226163745,
      "learning_rate": 3.250598001720156e-07,
      "loss": 0.001,
      "step": 97860
    },
    {
      "epoch": 17794.545454545456,
      "grad_norm": 0.2060137838125229,
      "learning_rate": 3.24950803934516e-07,
      "loss": 0.0009,
      "step": 97870
    },
    {
      "epoch": 17796.363636363636,
      "grad_norm": 0.21566270291805267,
      "learning_rate": 3.2484181717666705e-07,
      "loss": 0.0013,
      "step": 97880
    },
    {
      "epoch": 17798.18181818182,
      "grad_norm": 0.0007683359435759485,
      "learning_rate": 3.2473283990437056e-07,
      "loss": 0.0009,
      "step": 97890
    },
    {
      "epoch": 17800.0,
      "grad_norm": 0.0008025034912861884,
      "learning_rate": 3.246238721235283e-07,
      "loss": 0.0012,
      "step": 97900
    },
    {
      "epoch": 17801.81818181818,
      "grad_norm": 0.16724793612957,
      "learning_rate": 3.2451491384004135e-07,
      "loss": 0.001,
      "step": 97910
    },
    {
      "epoch": 17803.636363636364,
      "grad_norm": 0.20364825427532196,
      "learning_rate": 3.2440596505981e-07,
      "loss": 0.001,
      "step": 97920
    },
    {
      "epoch": 17805.454545454544,
      "grad_norm": 0.21286612749099731,
      "learning_rate": 3.2429702578873465e-07,
      "loss": 0.0014,
      "step": 97930
    },
    {
      "epoch": 17807.272727272728,
      "grad_norm": 0.0013147417921572924,
      "learning_rate": 3.2418809603271476e-07,
      "loss": 0.0011,
      "step": 97940
    },
    {
      "epoch": 17809.090909090908,
      "grad_norm": 0.1782328188419342,
      "learning_rate": 3.2407917579764907e-07,
      "loss": 0.0008,
      "step": 97950
    },
    {
      "epoch": 17810.909090909092,
      "grad_norm": 0.04992053657770157,
      "learning_rate": 3.2397026508943637e-07,
      "loss": 0.0012,
      "step": 97960
    },
    {
      "epoch": 17812.727272727272,
      "grad_norm": 0.17353183031082153,
      "learning_rate": 3.2386136391397445e-07,
      "loss": 0.0012,
      "step": 97970
    },
    {
      "epoch": 17814.545454545456,
      "grad_norm": 0.17258979380130768,
      "learning_rate": 3.237524722771607e-07,
      "loss": 0.0011,
      "step": 97980
    },
    {
      "epoch": 17816.363636363636,
      "grad_norm": 0.2730357050895691,
      "learning_rate": 3.236435901848924e-07,
      "loss": 0.0012,
      "step": 97990
    },
    {
      "epoch": 17818.18181818182,
      "grad_norm": 0.21467792987823486,
      "learning_rate": 3.235347176430656e-07,
      "loss": 0.001,
      "step": 98000
    },
    {
      "epoch": 17818.18181818182,
      "eval_loss": 5.075753211975098,
      "eval_runtime": 0.9487,
      "eval_samples_per_second": 10.54,
      "eval_steps_per_second": 5.27,
      "step": 98000
    },
    {
      "epoch": 17820.0,
      "grad_norm": 0.0006855296669527888,
      "learning_rate": 3.234258546575762e-07,
      "loss": 0.001,
      "step": 98010
    },
    {
      "epoch": 17821.81818181818,
      "grad_norm": 0.0005613230168819427,
      "learning_rate": 3.2331700123432016e-07,
      "loss": 0.001,
      "step": 98020
    },
    {
      "epoch": 17823.636363636364,
      "grad_norm": 0.0016025373479351401,
      "learning_rate": 3.232081573791916e-07,
      "loss": 0.0011,
      "step": 98030
    },
    {
      "epoch": 17825.454545454544,
      "grad_norm": 0.2159195989370346,
      "learning_rate": 3.2309932309808526e-07,
      "loss": 0.0012,
      "step": 98040
    },
    {
      "epoch": 17827.272727272728,
      "grad_norm": 0.2788863480091095,
      "learning_rate": 3.229904983968951e-07,
      "loss": 0.0012,
      "step": 98050
    },
    {
      "epoch": 17829.090909090908,
      "grad_norm": 0.18311619758605957,
      "learning_rate": 3.2288168328151405e-07,
      "loss": 0.001,
      "step": 98060
    },
    {
      "epoch": 17830.909090909092,
      "grad_norm": 0.18319466710090637,
      "learning_rate": 3.227728777578352e-07,
      "loss": 0.0009,
      "step": 98070
    },
    {
      "epoch": 17832.727272727272,
      "grad_norm": 0.00047425602679140866,
      "learning_rate": 3.2266408183175076e-07,
      "loss": 0.0009,
      "step": 98080
    },
    {
      "epoch": 17834.545454545456,
      "grad_norm": 0.012131165713071823,
      "learning_rate": 3.225552955091524e-07,
      "loss": 0.0015,
      "step": 98090
    },
    {
      "epoch": 17836.363636363636,
      "grad_norm": 0.0007119246874935925,
      "learning_rate": 3.2244651879593156e-07,
      "loss": 0.0007,
      "step": 98100
    },
    {
      "epoch": 17838.18181818182,
      "grad_norm": 0.1989140510559082,
      "learning_rate": 3.223377516979786e-07,
      "loss": 0.0013,
      "step": 98110
    },
    {
      "epoch": 17840.0,
      "grad_norm": 0.017006099224090576,
      "learning_rate": 3.2222899422118407e-07,
      "loss": 0.0011,
      "step": 98120
    },
    {
      "epoch": 17841.81818181818,
      "grad_norm": 0.000858272600453347,
      "learning_rate": 3.221202463714375e-07,
      "loss": 0.0011,
      "step": 98130
    },
    {
      "epoch": 17843.636363636364,
      "grad_norm": 0.00046651382581330836,
      "learning_rate": 3.2201150815462806e-07,
      "loss": 0.0012,
      "step": 98140
    },
    {
      "epoch": 17845.454545454544,
      "grad_norm": 0.20421451330184937,
      "learning_rate": 3.219027795766445e-07,
      "loss": 0.0012,
      "step": 98150
    },
    {
      "epoch": 17847.272727272728,
      "grad_norm": 0.20568372309207916,
      "learning_rate": 3.2179406064337466e-07,
      "loss": 0.001,
      "step": 98160
    },
    {
      "epoch": 17849.090909090908,
      "grad_norm": 0.19116096198558807,
      "learning_rate": 3.2168535136070627e-07,
      "loss": 0.0012,
      "step": 98170
    },
    {
      "epoch": 17850.909090909092,
      "grad_norm": 0.0060936748050153255,
      "learning_rate": 3.215766517345265e-07,
      "loss": 0.001,
      "step": 98180
    },
    {
      "epoch": 17852.727272727272,
      "grad_norm": 0.00047883804654702544,
      "learning_rate": 3.214679617707218e-07,
      "loss": 0.0008,
      "step": 98190
    },
    {
      "epoch": 17854.545454545456,
      "grad_norm": 0.0006728918524459004,
      "learning_rate": 3.2135928147517797e-07,
      "loss": 0.0013,
      "step": 98200
    },
    {
      "epoch": 17856.363636363636,
      "grad_norm": 0.18989692628383636,
      "learning_rate": 3.2125061085378103e-07,
      "loss": 0.0011,
      "step": 98210
    },
    {
      "epoch": 17858.18181818182,
      "grad_norm": 0.17483481764793396,
      "learning_rate": 3.211419499124153e-07,
      "loss": 0.0012,
      "step": 98220
    },
    {
      "epoch": 17860.0,
      "grad_norm": 0.0009152611601166427,
      "learning_rate": 3.210332986569659e-07,
      "loss": 0.001,
      "step": 98230
    },
    {
      "epoch": 17861.81818181818,
      "grad_norm": 0.22621898353099823,
      "learning_rate": 3.209246570933162e-07,
      "loss": 0.0009,
      "step": 98240
    },
    {
      "epoch": 17863.636363636364,
      "grad_norm": 0.21557793021202087,
      "learning_rate": 3.2081602522734985e-07,
      "loss": 0.001,
      "step": 98250
    },
    {
      "epoch": 17865.454545454544,
      "grad_norm": 0.0004995171329937875,
      "learning_rate": 3.207074030649498e-07,
      "loss": 0.0012,
      "step": 98260
    },
    {
      "epoch": 17867.272727272728,
      "grad_norm": 0.19674502313137054,
      "learning_rate": 3.205987906119982e-07,
      "loss": 0.0012,
      "step": 98270
    },
    {
      "epoch": 17869.090909090908,
      "grad_norm": 0.0004600410466082394,
      "learning_rate": 3.204901878743769e-07,
      "loss": 0.001,
      "step": 98280
    },
    {
      "epoch": 17870.909090909092,
      "grad_norm": 0.22830957174301147,
      "learning_rate": 3.203815948579674e-07,
      "loss": 0.001,
      "step": 98290
    },
    {
      "epoch": 17872.727272727272,
      "grad_norm": 0.2007952630519867,
      "learning_rate": 3.2027301156865013e-07,
      "loss": 0.0009,
      "step": 98300
    },
    {
      "epoch": 17874.545454545456,
      "grad_norm": 0.1741887778043747,
      "learning_rate": 3.2016443801230557e-07,
      "loss": 0.0016,
      "step": 98310
    },
    {
      "epoch": 17876.363636363636,
      "grad_norm": 0.0004526104894466698,
      "learning_rate": 3.200558741948135e-07,
      "loss": 0.0009,
      "step": 98320
    },
    {
      "epoch": 17878.18181818182,
      "grad_norm": 0.19225718080997467,
      "learning_rate": 3.1994732012205297e-07,
      "loss": 0.0012,
      "step": 98330
    },
    {
      "epoch": 17880.0,
      "grad_norm": 0.16765742003917694,
      "learning_rate": 3.198387757999027e-07,
      "loss": 0.001,
      "step": 98340
    },
    {
      "epoch": 17881.81818181818,
      "grad_norm": 0.0006581478519365191,
      "learning_rate": 3.1973024123424076e-07,
      "loss": 0.001,
      "step": 98350
    },
    {
      "epoch": 17883.636363636364,
      "grad_norm": 0.0009774866048246622,
      "learning_rate": 3.196217164309447e-07,
      "loss": 0.0009,
      "step": 98360
    },
    {
      "epoch": 17885.454545454544,
      "grad_norm": 0.0006770608597435057,
      "learning_rate": 3.1951320139589176e-07,
      "loss": 0.0013,
      "step": 98370
    },
    {
      "epoch": 17887.272727272728,
      "grad_norm": 0.0011749344412237406,
      "learning_rate": 3.1940469613495834e-07,
      "loss": 0.0012,
      "step": 98380
    },
    {
      "epoch": 17889.090909090908,
      "grad_norm": 0.0022800632286816835,
      "learning_rate": 3.192962006540205e-07,
      "loss": 0.0011,
      "step": 98390
    },
    {
      "epoch": 17890.909090909092,
      "grad_norm": 0.1891879439353943,
      "learning_rate": 3.191877149589539e-07,
      "loss": 0.0012,
      "step": 98400
    },
    {
      "epoch": 17892.727272727272,
      "grad_norm": 0.006738677155226469,
      "learning_rate": 3.1907923905563314e-07,
      "loss": 0.0009,
      "step": 98410
    },
    {
      "epoch": 17894.545454545456,
      "grad_norm": 0.2625350058078766,
      "learning_rate": 3.1897077294993315e-07,
      "loss": 0.0012,
      "step": 98420
    },
    {
      "epoch": 17896.363636363636,
      "grad_norm": 0.000638285418972373,
      "learning_rate": 3.188623166477272e-07,
      "loss": 0.001,
      "step": 98430
    },
    {
      "epoch": 17898.18181818182,
      "grad_norm": 0.16297370195388794,
      "learning_rate": 3.18753870154889e-07,
      "loss": 0.0012,
      "step": 98440
    },
    {
      "epoch": 17900.0,
      "grad_norm": 0.0007754047983326018,
      "learning_rate": 3.1864543347729154e-07,
      "loss": 0.0011,
      "step": 98450
    },
    {
      "epoch": 17901.81818181818,
      "grad_norm": 0.000690240238327533,
      "learning_rate": 3.1853700662080684e-07,
      "loss": 0.001,
      "step": 98460
    },
    {
      "epoch": 17903.636363636364,
      "grad_norm": 0.26867541670799255,
      "learning_rate": 3.184285895913068e-07,
      "loss": 0.0015,
      "step": 98470
    },
    {
      "epoch": 17905.454545454544,
      "grad_norm": 0.20090217888355255,
      "learning_rate": 3.1832018239466275e-07,
      "loss": 0.0009,
      "step": 98480
    },
    {
      "epoch": 17907.272727272728,
      "grad_norm": 0.20360521972179413,
      "learning_rate": 3.1821178503674514e-07,
      "loss": 0.0012,
      "step": 98490
    },
    {
      "epoch": 17909.090909090908,
      "grad_norm": 0.005594572983682156,
      "learning_rate": 3.181033975234244e-07,
      "loss": 0.0013,
      "step": 98500
    },
    {
      "epoch": 17909.090909090908,
      "eval_loss": 5.154397487640381,
      "eval_runtime": 0.9505,
      "eval_samples_per_second": 10.521,
      "eval_steps_per_second": 5.26,
      "step": 98500
    },
    {
      "epoch": 17910.909090909092,
      "grad_norm": 0.0016057936009019613,
      "learning_rate": 3.179950198605698e-07,
      "loss": 0.001,
      "step": 98510
    },
    {
      "epoch": 17912.727272727272,
      "grad_norm": 0.21778428554534912,
      "learning_rate": 3.1788665205405084e-07,
      "loss": 0.0011,
      "step": 98520
    },
    {
      "epoch": 17914.545454545456,
      "grad_norm": 0.21566569805145264,
      "learning_rate": 3.177782941097361e-07,
      "loss": 0.0012,
      "step": 98530
    },
    {
      "epoch": 17916.363636363636,
      "grad_norm": 0.26612988114356995,
      "learning_rate": 3.176699460334933e-07,
      "loss": 0.0012,
      "step": 98540
    },
    {
      "epoch": 17918.18181818182,
      "grad_norm": 0.22681623697280884,
      "learning_rate": 3.1756160783119015e-07,
      "loss": 0.0011,
      "step": 98550
    },
    {
      "epoch": 17920.0,
      "grad_norm": 0.22638031840324402,
      "learning_rate": 3.174532795086937e-07,
      "loss": 0.001,
      "step": 98560
    },
    {
      "epoch": 17921.81818181818,
      "grad_norm": 0.22764445841312408,
      "learning_rate": 3.1734496107187017e-07,
      "loss": 0.0011,
      "step": 98570
    },
    {
      "epoch": 17923.636363636364,
      "grad_norm": 0.2117028385400772,
      "learning_rate": 3.172366525265856e-07,
      "loss": 0.0011,
      "step": 98580
    },
    {
      "epoch": 17925.454545454544,
      "grad_norm": 0.0007290203939191997,
      "learning_rate": 3.1712835387870525e-07,
      "loss": 0.0011,
      "step": 98590
    },
    {
      "epoch": 17927.272727272728,
      "grad_norm": 0.0011491688201203942,
      "learning_rate": 3.1702006513409393e-07,
      "loss": 0.0012,
      "step": 98600
    },
    {
      "epoch": 17929.090909090908,
      "grad_norm": 0.23393180966377258,
      "learning_rate": 3.1691178629861624e-07,
      "loss": 0.0012,
      "step": 98610
    },
    {
      "epoch": 17930.909090909092,
      "grad_norm": 0.0010110846487805247,
      "learning_rate": 3.168035173781355e-07,
      "loss": 0.0012,
      "step": 98620
    },
    {
      "epoch": 17932.727272727272,
      "grad_norm": 0.0010302555747330189,
      "learning_rate": 3.166952583785152e-07,
      "loss": 0.0012,
      "step": 98630
    },
    {
      "epoch": 17934.545454545456,
      "grad_norm": 0.0009946287609636784,
      "learning_rate": 3.1658700930561797e-07,
      "loss": 0.0009,
      "step": 98640
    },
    {
      "epoch": 17936.363636363636,
      "grad_norm": 0.0008290052064694464,
      "learning_rate": 3.1647877016530597e-07,
      "loss": 0.0012,
      "step": 98650
    },
    {
      "epoch": 17938.18181818182,
      "grad_norm": 0.00061620312044397,
      "learning_rate": 3.163705409634407e-07,
      "loss": 0.0009,
      "step": 98660
    },
    {
      "epoch": 17940.0,
      "grad_norm": 0.15916699171066284,
      "learning_rate": 3.162623217058834e-07,
      "loss": 0.0012,
      "step": 98670
    },
    {
      "epoch": 17941.81818181818,
      "grad_norm": 0.0005347952246665955,
      "learning_rate": 3.1615411239849445e-07,
      "loss": 0.0012,
      "step": 98680
    },
    {
      "epoch": 17943.636363636364,
      "grad_norm": 0.0007683420553803444,
      "learning_rate": 3.160459130471339e-07,
      "loss": 0.001,
      "step": 98690
    },
    {
      "epoch": 17945.454545454544,
      "grad_norm": 0.008699891157448292,
      "learning_rate": 3.15937723657661e-07,
      "loss": 0.0013,
      "step": 98700
    },
    {
      "epoch": 17947.272727272728,
      "grad_norm": 0.20035505294799805,
      "learning_rate": 3.15829544235935e-07,
      "loss": 0.001,
      "step": 98710
    },
    {
      "epoch": 17949.090909090908,
      "grad_norm": 0.011605039238929749,
      "learning_rate": 3.1572137478781415e-07,
      "loss": 0.0011,
      "step": 98720
    },
    {
      "epoch": 17950.909090909092,
      "grad_norm": 0.20514312386512756,
      "learning_rate": 3.156132153191562e-07,
      "loss": 0.0009,
      "step": 98730
    },
    {
      "epoch": 17952.727272727272,
      "grad_norm": 0.0005958399851806462,
      "learning_rate": 3.155050658358185e-07,
      "loss": 0.001,
      "step": 98740
    },
    {
      "epoch": 17954.545454545456,
      "grad_norm": 0.0010699959238991141,
      "learning_rate": 3.1539692634365783e-07,
      "loss": 0.0013,
      "step": 98750
    },
    {
      "epoch": 17956.363636363636,
      "grad_norm": 0.001059445203281939,
      "learning_rate": 3.152887968485303e-07,
      "loss": 0.001,
      "step": 98760
    },
    {
      "epoch": 17958.18181818182,
      "grad_norm": 0.15738548338413239,
      "learning_rate": 3.151806773562917e-07,
      "loss": 0.001,
      "step": 98770
    },
    {
      "epoch": 17960.0,
      "grad_norm": 0.16430644690990448,
      "learning_rate": 3.15072567872797e-07,
      "loss": 0.001,
      "step": 98780
    },
    {
      "epoch": 17961.81818181818,
      "grad_norm": 0.1963995099067688,
      "learning_rate": 3.149644684039008e-07,
      "loss": 0.0011,
      "step": 98790
    },
    {
      "epoch": 17963.636363636364,
      "grad_norm": 0.20315377414226532,
      "learning_rate": 3.1485637895545744e-07,
      "loss": 0.0009,
      "step": 98800
    },
    {
      "epoch": 17965.454545454544,
      "grad_norm": 0.21531544625759125,
      "learning_rate": 3.1474829953331996e-07,
      "loss": 0.001,
      "step": 98810
    },
    {
      "epoch": 17967.272727272728,
      "grad_norm": 0.19696760177612305,
      "learning_rate": 3.1464023014334164e-07,
      "loss": 0.0011,
      "step": 98820
    },
    {
      "epoch": 17969.090909090908,
      "grad_norm": 0.0008649811497889459,
      "learning_rate": 3.1453217079137487e-07,
      "loss": 0.001,
      "step": 98830
    },
    {
      "epoch": 17970.909090909092,
      "grad_norm": 0.1582213193178177,
      "learning_rate": 3.144241214832714e-07,
      "loss": 0.0012,
      "step": 98840
    },
    {
      "epoch": 17972.727272727272,
      "grad_norm": 0.0005964728188700974,
      "learning_rate": 3.143160822248827e-07,
      "loss": 0.0007,
      "step": 98850
    },
    {
      "epoch": 17974.545454545456,
      "grad_norm": 0.16421271860599518,
      "learning_rate": 3.142080530220593e-07,
      "loss": 0.0012,
      "step": 98860
    },
    {
      "epoch": 17976.363636363636,
      "grad_norm": 0.0005431673489511013,
      "learning_rate": 3.1410003388065163e-07,
      "loss": 0.0011,
      "step": 98870
    },
    {
      "epoch": 17978.18181818182,
      "grad_norm": 0.000654294912237674,
      "learning_rate": 3.139920248065094e-07,
      "loss": 0.0012,
      "step": 98880
    },
    {
      "epoch": 17980.0,
      "grad_norm": 0.00039936788380146027,
      "learning_rate": 3.138840258054815e-07,
      "loss": 0.0012,
      "step": 98890
    },
    {
      "epoch": 17981.81818181818,
      "grad_norm": 0.2004353553056717,
      "learning_rate": 3.1377603688341684e-07,
      "loss": 0.0009,
      "step": 98900
    },
    {
      "epoch": 17983.636363636364,
      "grad_norm": 0.1941492259502411,
      "learning_rate": 3.136680580461635e-07,
      "loss": 0.0011,
      "step": 98910
    },
    {
      "epoch": 17985.454545454544,
      "grad_norm": 0.16374655067920685,
      "learning_rate": 3.135600892995687e-07,
      "loss": 0.0015,
      "step": 98920
    },
    {
      "epoch": 17987.272727272728,
      "grad_norm": 0.16424059867858887,
      "learning_rate": 3.1345213064947955e-07,
      "loss": 0.001,
      "step": 98930
    },
    {
      "epoch": 17989.090909090908,
      "grad_norm": 0.20213256776332855,
      "learning_rate": 3.1334418210174263e-07,
      "loss": 0.0012,
      "step": 98940
    },
    {
      "epoch": 17990.909090909092,
      "grad_norm": 0.0009663923410698771,
      "learning_rate": 3.1323624366220347e-07,
      "loss": 0.0009,
      "step": 98950
    },
    {
      "epoch": 17992.727272727272,
      "grad_norm": 0.21102142333984375,
      "learning_rate": 3.131283153367077e-07,
      "loss": 0.0013,
      "step": 98960
    },
    {
      "epoch": 17994.545454545456,
      "grad_norm": 0.16079764068126678,
      "learning_rate": 3.1302039713109983e-07,
      "loss": 0.0009,
      "step": 98970
    },
    {
      "epoch": 17996.363636363636,
      "grad_norm": 0.1653273105621338,
      "learning_rate": 3.1291248905122415e-07,
      "loss": 0.0011,
      "step": 98980
    },
    {
      "epoch": 17998.18181818182,
      "grad_norm": 0.0006830730126239359,
      "learning_rate": 3.128045911029247e-07,
      "loss": 0.001,
      "step": 98990
    },
    {
      "epoch": 18000.0,
      "grad_norm": 0.1685485690832138,
      "learning_rate": 3.1269670329204393e-07,
      "loss": 0.0012,
      "step": 99000
    },
    {
      "epoch": 18000.0,
      "eval_loss": 5.07478666305542,
      "eval_runtime": 0.9518,
      "eval_samples_per_second": 10.507,
      "eval_steps_per_second": 5.253,
      "step": 99000
    },
    {
      "epoch": 18001.81818181818,
      "grad_norm": 0.01813630200922489,
      "learning_rate": 3.12588825624425e-07,
      "loss": 0.0012,
      "step": 99010
    },
    {
      "epoch": 18003.636363636364,
      "grad_norm": 0.0005833533359691501,
      "learning_rate": 3.124809581059098e-07,
      "loss": 0.001,
      "step": 99020
    },
    {
      "epoch": 18005.454545454544,
      "grad_norm": 0.15788154304027557,
      "learning_rate": 3.123731007423396e-07,
      "loss": 0.0011,
      "step": 99030
    },
    {
      "epoch": 18007.272727272728,
      "grad_norm": 0.20010867714881897,
      "learning_rate": 3.122652535395556e-07,
      "loss": 0.001,
      "step": 99040
    },
    {
      "epoch": 18009.090909090908,
      "grad_norm": 0.037230316549539566,
      "learning_rate": 3.12157416503398e-07,
      "loss": 0.0012,
      "step": 99050
    },
    {
      "epoch": 18010.909090909092,
      "grad_norm": 0.0007360653253272176,
      "learning_rate": 3.1204958963970663e-07,
      "loss": 0.001,
      "step": 99060
    },
    {
      "epoch": 18012.727272727272,
      "grad_norm": 0.0012301768874749541,
      "learning_rate": 3.1194177295432094e-07,
      "loss": 0.0007,
      "step": 99070
    },
    {
      "epoch": 18014.545454545456,
      "grad_norm": 0.27839037775993347,
      "learning_rate": 3.1183396645307945e-07,
      "loss": 0.0015,
      "step": 99080
    },
    {
      "epoch": 18016.363636363636,
      "grad_norm": 0.21606329083442688,
      "learning_rate": 3.1172617014182034e-07,
      "loss": 0.001,
      "step": 99090
    },
    {
      "epoch": 18018.18181818182,
      "grad_norm": 0.000969588931184262,
      "learning_rate": 3.116183840263815e-07,
      "loss": 0.0011,
      "step": 99100
    },
    {
      "epoch": 18020.0,
      "grad_norm": 0.2042912393808365,
      "learning_rate": 3.1151060811259974e-07,
      "loss": 0.0012,
      "step": 99110
    },
    {
      "epoch": 18021.81818181818,
      "grad_norm": 0.22506144642829895,
      "learning_rate": 3.114028424063118e-07,
      "loss": 0.0012,
      "step": 99120
    },
    {
      "epoch": 18023.636363636364,
      "grad_norm": 0.22555279731750488,
      "learning_rate": 3.1129508691335325e-07,
      "loss": 0.0009,
      "step": 99130
    },
    {
      "epoch": 18025.454545454544,
      "grad_norm": 0.15765520930290222,
      "learning_rate": 3.1118734163955986e-07,
      "loss": 0.0013,
      "step": 99140
    },
    {
      "epoch": 18027.272727272728,
      "grad_norm": 0.19712819159030914,
      "learning_rate": 3.1107960659076647e-07,
      "loss": 0.0009,
      "step": 99150
    },
    {
      "epoch": 18029.090909090908,
      "grad_norm": 0.25546425580978394,
      "learning_rate": 3.109718817728073e-07,
      "loss": 0.0011,
      "step": 99160
    },
    {
      "epoch": 18030.909090909092,
      "grad_norm": 0.1585666835308075,
      "learning_rate": 3.108641671915159e-07,
      "loss": 0.0012,
      "step": 99170
    },
    {
      "epoch": 18032.727272727272,
      "grad_norm": 0.0005439369124360383,
      "learning_rate": 3.1075646285272606e-07,
      "loss": 0.001,
      "step": 99180
    },
    {
      "epoch": 18034.545454545456,
      "grad_norm": 0.0007595128263346851,
      "learning_rate": 3.1064876876226964e-07,
      "loss": 0.0012,
      "step": 99190
    },
    {
      "epoch": 18036.363636363636,
      "grad_norm": 0.15174612402915955,
      "learning_rate": 3.1054108492597953e-07,
      "loss": 0.001,
      "step": 99200
    },
    {
      "epoch": 18038.18181818182,
      "grad_norm": 0.26769500970840454,
      "learning_rate": 3.104334113496865e-07,
      "loss": 0.0013,
      "step": 99210
    },
    {
      "epoch": 18040.0,
      "grad_norm": 0.2253476083278656,
      "learning_rate": 3.1032574803922195e-07,
      "loss": 0.0009,
      "step": 99220
    },
    {
      "epoch": 18041.81818181818,
      "grad_norm": 0.17542466521263123,
      "learning_rate": 3.102180950004164e-07,
      "loss": 0.0012,
      "step": 99230
    },
    {
      "epoch": 18043.636363636364,
      "grad_norm": 0.0006771240150555968,
      "learning_rate": 3.1011045223909947e-07,
      "loss": 0.0013,
      "step": 99240
    },
    {
      "epoch": 18045.454545454544,
      "grad_norm": 0.21328206360340118,
      "learning_rate": 3.1000281976110056e-07,
      "loss": 0.001,
      "step": 99250
    },
    {
      "epoch": 18047.272727272728,
      "grad_norm": 0.001294161076657474,
      "learning_rate": 3.0989519757224845e-07,
      "loss": 0.0014,
      "step": 99260
    },
    {
      "epoch": 18049.090909090908,
      "grad_norm": 0.20261910557746887,
      "learning_rate": 3.0978758567837127e-07,
      "loss": 0.001,
      "step": 99270
    },
    {
      "epoch": 18050.909090909092,
      "grad_norm": 0.16555964946746826,
      "learning_rate": 3.096799840852965e-07,
      "loss": 0.001,
      "step": 99280
    },
    {
      "epoch": 18052.727272727272,
      "grad_norm": 0.0005230632959865034,
      "learning_rate": 3.095723927988517e-07,
      "loss": 0.0012,
      "step": 99290
    },
    {
      "epoch": 18054.545454545456,
      "grad_norm": 0.0010425851214677095,
      "learning_rate": 3.0946481182486297e-07,
      "loss": 0.0009,
      "step": 99300
    },
    {
      "epoch": 18056.363636363636,
      "grad_norm": 0.22557280957698822,
      "learning_rate": 3.093572411691564e-07,
      "loss": 0.0013,
      "step": 99310
    },
    {
      "epoch": 18058.18181818182,
      "grad_norm": 0.263948917388916,
      "learning_rate": 3.0924968083755745e-07,
      "loss": 0.0011,
      "step": 99320
    },
    {
      "epoch": 18060.0,
      "grad_norm": 0.0006679004291072488,
      "learning_rate": 3.091421308358908e-07,
      "loss": 0.001,
      "step": 99330
    },
    {
      "epoch": 18061.81818181818,
      "grad_norm": 0.0013611278263852,
      "learning_rate": 3.090345911699809e-07,
      "loss": 0.0012,
      "step": 99340
    },
    {
      "epoch": 18063.636363636364,
      "grad_norm": 0.28532862663269043,
      "learning_rate": 3.089270618456514e-07,
      "loss": 0.001,
      "step": 99350
    },
    {
      "epoch": 18065.454545454544,
      "grad_norm": 0.16861574351787567,
      "learning_rate": 3.0881954286872537e-07,
      "loss": 0.0014,
      "step": 99360
    },
    {
      "epoch": 18067.272727272728,
      "grad_norm": 0.0009636202012188733,
      "learning_rate": 3.0871203424502586e-07,
      "loss": 0.0006,
      "step": 99370
    },
    {
      "epoch": 18069.090909090908,
      "grad_norm": 0.0059499298222362995,
      "learning_rate": 3.086045359803742e-07,
      "loss": 0.0015,
      "step": 99380
    },
    {
      "epoch": 18070.909090909092,
      "grad_norm": 0.2123420387506485,
      "learning_rate": 3.084970480805926e-07,
      "loss": 0.0009,
      "step": 99390
    },
    {
      "epoch": 18072.727272727272,
      "grad_norm": 0.27026429772377014,
      "learning_rate": 3.0838957055150135e-07,
      "loss": 0.001,
      "step": 99400
    },
    {
      "epoch": 18074.545454545456,
      "grad_norm": 0.0006929411902092397,
      "learning_rate": 3.0828210339892114e-07,
      "loss": 0.0009,
      "step": 99410
    },
    {
      "epoch": 18076.363636363636,
      "grad_norm": 0.000757852743845433,
      "learning_rate": 3.081746466286719e-07,
      "loss": 0.0012,
      "step": 99420
    },
    {
      "epoch": 18078.18181818182,
      "grad_norm": 0.1581404060125351,
      "learning_rate": 3.0806720024657255e-07,
      "loss": 0.0012,
      "step": 99430
    },
    {
      "epoch": 18080.0,
      "grad_norm": 0.0005808828282169998,
      "learning_rate": 3.0795976425844193e-07,
      "loss": 0.001,
      "step": 99440
    },
    {
      "epoch": 18081.81818181818,
      "grad_norm": 0.21027205884456635,
      "learning_rate": 3.078523386700982e-07,
      "loss": 0.0012,
      "step": 99450
    },
    {
      "epoch": 18083.636363636364,
      "grad_norm": 0.00050726457266137,
      "learning_rate": 3.0774492348735877e-07,
      "loss": 0.0009,
      "step": 99460
    },
    {
      "epoch": 18085.454545454544,
      "grad_norm": 0.000754296372178942,
      "learning_rate": 3.076375187160407e-07,
      "loss": 0.001,
      "step": 99470
    },
    {
      "epoch": 18087.272727272728,
      "grad_norm": 0.2618371546268463,
      "learning_rate": 3.075301243619603e-07,
      "loss": 0.0013,
      "step": 99480
    },
    {
      "epoch": 18089.090909090908,
      "grad_norm": 0.19803090393543243,
      "learning_rate": 3.074227404309336e-07,
      "loss": 0.0009,
      "step": 99490
    },
    {
      "epoch": 18090.909090909092,
      "grad_norm": 0.0009280765079893172,
      "learning_rate": 3.073153669287759e-07,
      "loss": 0.001,
      "step": 99500
    },
    {
      "epoch": 18090.909090909092,
      "eval_loss": 5.050881862640381,
      "eval_runtime": 0.9501,
      "eval_samples_per_second": 10.525,
      "eval_steps_per_second": 5.263,
      "step": 99500
    },
    {
      "epoch": 18092.727272727272,
      "grad_norm": 0.21440395712852478,
      "learning_rate": 3.0720800386130176e-07,
      "loss": 0.001,
      "step": 99510
    },
    {
      "epoch": 18094.545454545456,
      "grad_norm": 0.00044052558951079845,
      "learning_rate": 3.071006512343254e-07,
      "loss": 0.0012,
      "step": 99520
    },
    {
      "epoch": 18096.363636363636,
      "grad_norm": 0.211660698056221,
      "learning_rate": 3.0699330905366055e-07,
      "loss": 0.0012,
      "step": 99530
    },
    {
      "epoch": 18098.18181818182,
      "grad_norm": 0.01183033175766468,
      "learning_rate": 3.0688597732512e-07,
      "loss": 0.0012,
      "step": 99540
    },
    {
      "epoch": 18100.0,
      "grad_norm": 0.0007566557032987475,
      "learning_rate": 3.067786560545163e-07,
      "loss": 0.0009,
      "step": 99550
    },
    {
      "epoch": 18101.81818181818,
      "grad_norm": 0.26085248589515686,
      "learning_rate": 3.066713452476617e-07,
      "loss": 0.0012,
      "step": 99560
    },
    {
      "epoch": 18103.636363636364,
      "grad_norm": 0.0009452977101318538,
      "learning_rate": 3.065640449103669e-07,
      "loss": 0.001,
      "step": 99570
    },
    {
      "epoch": 18105.454545454544,
      "grad_norm": 0.2007576823234558,
      "learning_rate": 3.064567550484433e-07,
      "loss": 0.0009,
      "step": 99580
    },
    {
      "epoch": 18107.272727272728,
      "grad_norm": 0.21455949544906616,
      "learning_rate": 3.063494756677005e-07,
      "loss": 0.0012,
      "step": 99590
    },
    {
      "epoch": 18109.090909090908,
      "grad_norm": 0.0009027245687320828,
      "learning_rate": 3.0624220677394854e-07,
      "loss": 0.001,
      "step": 99600
    },
    {
      "epoch": 18110.909090909092,
      "grad_norm": 0.19606845080852509,
      "learning_rate": 3.0613494837299636e-07,
      "loss": 0.0012,
      "step": 99610
    },
    {
      "epoch": 18112.727272727272,
      "grad_norm": 0.2710583806037903,
      "learning_rate": 3.060277004706524e-07,
      "loss": 0.0009,
      "step": 99620
    },
    {
      "epoch": 18114.545454545456,
      "grad_norm": 0.20186161994934082,
      "learning_rate": 3.059204630727247e-07,
      "loss": 0.0012,
      "step": 99630
    },
    {
      "epoch": 18116.363636363636,
      "grad_norm": 0.16817444562911987,
      "learning_rate": 3.0581323618502063e-07,
      "loss": 0.0013,
      "step": 99640
    },
    {
      "epoch": 18118.18181818182,
      "grad_norm": 0.16352564096450806,
      "learning_rate": 3.057060198133468e-07,
      "loss": 0.0009,
      "step": 99650
    },
    {
      "epoch": 18120.0,
      "grad_norm": 0.17973783612251282,
      "learning_rate": 3.0559881396350963e-07,
      "loss": 0.0011,
      "step": 99660
    },
    {
      "epoch": 18121.81818181818,
      "grad_norm": 0.1630762219429016,
      "learning_rate": 3.0549161864131444e-07,
      "loss": 0.0012,
      "step": 99670
    },
    {
      "epoch": 18123.636363636364,
      "grad_norm": 0.000757182075176388,
      "learning_rate": 3.053844338525666e-07,
      "loss": 0.0011,
      "step": 99680
    },
    {
      "epoch": 18125.454545454544,
      "grad_norm": 0.0005529990303330123,
      "learning_rate": 3.052772596030708e-07,
      "loss": 0.0011,
      "step": 99690
    },
    {
      "epoch": 18127.272727272728,
      "grad_norm": 0.0006718334625475109,
      "learning_rate": 3.0517009589863054e-07,
      "loss": 0.0007,
      "step": 99700
    },
    {
      "epoch": 18129.090909090908,
      "grad_norm": 0.172851100564003,
      "learning_rate": 3.0506294274504937e-07,
      "loss": 0.0014,
      "step": 99710
    },
    {
      "epoch": 18130.909090909092,
      "grad_norm": 0.0006558955064974725,
      "learning_rate": 3.0495580014813014e-07,
      "loss": 0.001,
      "step": 99720
    },
    {
      "epoch": 18132.727272727272,
      "grad_norm": 0.21461760997772217,
      "learning_rate": 3.0484866811367507e-07,
      "loss": 0.001,
      "step": 99730
    },
    {
      "epoch": 18134.545454545456,
      "grad_norm": 0.16984038054943085,
      "learning_rate": 3.0474154664748576e-07,
      "loss": 0.0011,
      "step": 99740
    },
    {
      "epoch": 18136.363636363636,
      "grad_norm": 0.0007707642507739365,
      "learning_rate": 3.0463443575536317e-07,
      "loss": 0.0009,
      "step": 99750
    },
    {
      "epoch": 18138.18181818182,
      "grad_norm": 0.20286712050437927,
      "learning_rate": 3.045273354431079e-07,
      "loss": 0.0013,
      "step": 99760
    },
    {
      "epoch": 18140.0,
      "grad_norm": 0.0007916201138868928,
      "learning_rate": 3.0442024571652016e-07,
      "loss": 0.001,
      "step": 99770
    },
    {
      "epoch": 18141.81818181818,
      "grad_norm": 0.26331114768981934,
      "learning_rate": 3.0431316658139876e-07,
      "loss": 0.0011,
      "step": 99780
    },
    {
      "epoch": 18143.636363636364,
      "grad_norm": 0.25444522500038147,
      "learning_rate": 3.042060980435429e-07,
      "loss": 0.001,
      "step": 99790
    },
    {
      "epoch": 18145.454545454544,
      "grad_norm": 0.0019394677365198731,
      "learning_rate": 3.0409904010875076e-07,
      "loss": 0.001,
      "step": 99800
    },
    {
      "epoch": 18147.272727272728,
      "grad_norm": 0.0008385141263715923,
      "learning_rate": 3.0399199278281984e-07,
      "loss": 0.0009,
      "step": 99810
    },
    {
      "epoch": 18149.090909090908,
      "grad_norm": 0.1710144728422165,
      "learning_rate": 3.038849560715473e-07,
      "loss": 0.0013,
      "step": 99820
    },
    {
      "epoch": 18150.909090909092,
      "grad_norm": 0.0007021137862466276,
      "learning_rate": 3.037779299807295e-07,
      "loss": 0.0011,
      "step": 99830
    },
    {
      "epoch": 18152.727272727272,
      "grad_norm": 0.16492658853530884,
      "learning_rate": 3.036709145161625e-07,
      "loss": 0.0012,
      "step": 99840
    },
    {
      "epoch": 18154.545454545456,
      "grad_norm": 0.2444554716348648,
      "learning_rate": 3.0356390968364166e-07,
      "loss": 0.0012,
      "step": 99850
    },
    {
      "epoch": 18156.363636363636,
      "grad_norm": 0.001798161189071834,
      "learning_rate": 3.0345691548896144e-07,
      "loss": 0.0009,
      "step": 99860
    },
    {
      "epoch": 18158.18181818182,
      "grad_norm": 0.005436164792627096,
      "learning_rate": 3.0334993193791624e-07,
      "loss": 0.0011,
      "step": 99870
    },
    {
      "epoch": 18160.0,
      "grad_norm": 0.0011024187551811337,
      "learning_rate": 3.032429590362999e-07,
      "loss": 0.0009,
      "step": 99880
    },
    {
      "epoch": 18161.81818181818,
      "grad_norm": 0.0005311580607667565,
      "learning_rate": 3.031359967899051e-07,
      "loss": 0.0011,
      "step": 99890
    },
    {
      "epoch": 18163.636363636364,
      "grad_norm": 0.24887672066688538,
      "learning_rate": 3.0302904520452443e-07,
      "loss": 0.0013,
      "step": 99900
    },
    {
      "epoch": 18165.454545454544,
      "grad_norm": 0.18350526690483093,
      "learning_rate": 3.0292210428594977e-07,
      "loss": 0.0013,
      "step": 99910
    },
    {
      "epoch": 18167.272727272728,
      "grad_norm": 0.00260131130926311,
      "learning_rate": 3.028151740399724e-07,
      "loss": 0.0006,
      "step": 99920
    },
    {
      "epoch": 18169.090909090908,
      "grad_norm": 0.0013839207822456956,
      "learning_rate": 3.0270825447238314e-07,
      "loss": 0.0012,
      "step": 99930
    },
    {
      "epoch": 18170.909090909092,
      "grad_norm": 0.0013710971688851714,
      "learning_rate": 3.02601345588972e-07,
      "loss": 0.0012,
      "step": 99940
    },
    {
      "epoch": 18172.727272727272,
      "grad_norm": 0.0011789817363023758,
      "learning_rate": 3.0249444739552843e-07,
      "loss": 0.0008,
      "step": 99950
    },
    {
      "epoch": 18174.545454545456,
      "grad_norm": 0.00097894505597651,
      "learning_rate": 3.023875598978419e-07,
      "loss": 0.0013,
      "step": 99960
    },
    {
      "epoch": 18176.363636363636,
      "grad_norm": 0.001102475100196898,
      "learning_rate": 3.022806831017002e-07,
      "loss": 0.0009,
      "step": 99970
    },
    {
      "epoch": 18178.18181818182,
      "grad_norm": 0.21574650704860687,
      "learning_rate": 3.0217381701289146e-07,
      "loss": 0.0014,
      "step": 99980
    },
    {
      "epoch": 18180.0,
      "grad_norm": 0.21605408191680908,
      "learning_rate": 3.0206696163720313e-07,
      "loss": 0.001,
      "step": 99990
    },
    {
      "epoch": 18181.81818181818,
      "grad_norm": 0.042603421956300735,
      "learning_rate": 3.0196011698042156e-07,
      "loss": 0.001,
      "step": 100000
    },
    {
      "epoch": 18181.81818181818,
      "eval_loss": 5.105471134185791,
      "eval_runtime": 0.952,
      "eval_samples_per_second": 10.505,
      "eval_steps_per_second": 5.252,
      "step": 100000
    },
    {
      "epoch": 18183.636363636364,
      "grad_norm": 0.19842585921287537,
      "learning_rate": 3.018532830483331e-07,
      "loss": 0.001,
      "step": 100010
    },
    {
      "epoch": 18185.454545454544,
      "grad_norm": 0.0005620036972686648,
      "learning_rate": 3.017464598467229e-07,
      "loss": 0.0011,
      "step": 100020
    },
    {
      "epoch": 18187.272727272728,
      "grad_norm": 0.0005836751079186797,
      "learning_rate": 3.0163964738137614e-07,
      "loss": 0.0011,
      "step": 100030
    },
    {
      "epoch": 18189.090909090908,
      "grad_norm": 0.0008144788444042206,
      "learning_rate": 3.0153284565807723e-07,
      "loss": 0.0012,
      "step": 100040
    },
    {
      "epoch": 18190.909090909092,
      "grad_norm": 0.0005597237031906843,
      "learning_rate": 3.014260546826097e-07,
      "loss": 0.0012,
      "step": 100050
    },
    {
      "epoch": 18192.727272727272,
      "grad_norm": 0.21709373593330383,
      "learning_rate": 3.013192744607568e-07,
      "loss": 0.0011,
      "step": 100060
    },
    {
      "epoch": 18194.545454545456,
      "grad_norm": 0.18713423609733582,
      "learning_rate": 3.012125049983014e-07,
      "loss": 0.001,
      "step": 100070
    },
    {
      "epoch": 18196.363636363636,
      "grad_norm": 0.20906633138656616,
      "learning_rate": 3.0110574630102513e-07,
      "loss": 0.0011,
      "step": 100080
    },
    {
      "epoch": 18198.18181818182,
      "grad_norm": 0.2714690566062927,
      "learning_rate": 3.009989983747097e-07,
      "loss": 0.0014,
      "step": 100090
    },
    {
      "epoch": 18200.0,
      "grad_norm": 0.0005759096820838749,
      "learning_rate": 3.008922612251358e-07,
      "loss": 0.0009,
      "step": 100100
    },
    {
      "epoch": 18201.81818181818,
      "grad_norm": 0.19198645651340485,
      "learning_rate": 3.007855348580837e-07,
      "loss": 0.001,
      "step": 100110
    },
    {
      "epoch": 18203.636363636364,
      "grad_norm": 0.0012868334306403995,
      "learning_rate": 3.0067881927933317e-07,
      "loss": 0.0012,
      "step": 100120
    },
    {
      "epoch": 18205.454545454544,
      "grad_norm": 0.1569105088710785,
      "learning_rate": 3.0057211449466326e-07,
      "loss": 0.0009,
      "step": 100130
    },
    {
      "epoch": 18207.272727272728,
      "grad_norm": 0.0005519356345757842,
      "learning_rate": 3.0046542050985237e-07,
      "loss": 0.0012,
      "step": 100140
    },
    {
      "epoch": 18209.090909090908,
      "grad_norm": 0.0005257376469671726,
      "learning_rate": 3.0035873733067875e-07,
      "loss": 0.0012,
      "step": 100150
    },
    {
      "epoch": 18210.909090909092,
      "grad_norm": 0.21454869210720062,
      "learning_rate": 3.002520649629193e-07,
      "loss": 0.0012,
      "step": 100160
    },
    {
      "epoch": 18212.727272727272,
      "grad_norm": 0.1709350347518921,
      "learning_rate": 3.0014540341235115e-07,
      "loss": 0.0009,
      "step": 100170
    },
    {
      "epoch": 18214.545454545456,
      "grad_norm": 0.20174042880535126,
      "learning_rate": 3.0003875268475044e-07,
      "loss": 0.0013,
      "step": 100180
    },
    {
      "epoch": 18216.363636363636,
      "grad_norm": 0.22721803188323975,
      "learning_rate": 2.9993211278589247e-07,
      "loss": 0.0012,
      "step": 100190
    },
    {
      "epoch": 18218.18181818182,
      "grad_norm": 0.157005175948143,
      "learning_rate": 2.9982548372155256e-07,
      "loss": 0.0009,
      "step": 100200
    },
    {
      "epoch": 18220.0,
      "grad_norm": 0.17363514006137848,
      "learning_rate": 2.997188654975049e-07,
      "loss": 0.0011,
      "step": 100210
    },
    {
      "epoch": 18221.81818181818,
      "grad_norm": 0.0009120454196818173,
      "learning_rate": 2.996122581195234e-07,
      "loss": 0.001,
      "step": 100220
    },
    {
      "epoch": 18223.636363636364,
      "grad_norm": 0.18465019762516022,
      "learning_rate": 2.995056615933814e-07,
      "loss": 0.0013,
      "step": 100230
    },
    {
      "epoch": 18225.454545454544,
      "grad_norm": 0.0007027426618151367,
      "learning_rate": 2.9939907592485137e-07,
      "loss": 0.0011,
      "step": 100240
    },
    {
      "epoch": 18227.272727272728,
      "grad_norm": 0.20493268966674805,
      "learning_rate": 2.992925011197053e-07,
      "loss": 0.001,
      "step": 100250
    },
    {
      "epoch": 18229.090909090908,
      "grad_norm": 0.2036919742822647,
      "learning_rate": 2.9918593718371504e-07,
      "loss": 0.0011,
      "step": 100260
    },
    {
      "epoch": 18230.909090909092,
      "grad_norm": 0.0011012534378096461,
      "learning_rate": 2.9907938412265116e-07,
      "loss": 0.0012,
      "step": 100270
    },
    {
      "epoch": 18232.727272727272,
      "grad_norm": 0.0005719278124161065,
      "learning_rate": 2.989728419422841e-07,
      "loss": 0.0009,
      "step": 100280
    },
    {
      "epoch": 18234.545454545456,
      "grad_norm": 0.0006918992730788887,
      "learning_rate": 2.9886631064838354e-07,
      "loss": 0.0012,
      "step": 100290
    },
    {
      "epoch": 18236.363636363636,
      "grad_norm": 0.0007043086225166917,
      "learning_rate": 2.9875979024671845e-07,
      "loss": 0.0011,
      "step": 100300
    },
    {
      "epoch": 18238.18181818182,
      "grad_norm": 0.0008213719702325761,
      "learning_rate": 2.986532807430576e-07,
      "loss": 0.001,
      "step": 100310
    },
    {
      "epoch": 18240.0,
      "grad_norm": 0.0007489545969292521,
      "learning_rate": 2.985467821431687e-07,
      "loss": 0.0012,
      "step": 100320
    },
    {
      "epoch": 18241.81818181818,
      "grad_norm": 0.0009166219388134778,
      "learning_rate": 2.9844029445281913e-07,
      "loss": 0.0008,
      "step": 100330
    },
    {
      "epoch": 18243.636363636364,
      "grad_norm": 0.16456201672554016,
      "learning_rate": 2.98333817677776e-07,
      "loss": 0.0013,
      "step": 100340
    },
    {
      "epoch": 18245.454545454544,
      "grad_norm": 0.2556706964969635,
      "learning_rate": 2.9822735182380494e-07,
      "loss": 0.0012,
      "step": 100350
    },
    {
      "epoch": 18247.272727272728,
      "grad_norm": 0.000708528037648648,
      "learning_rate": 2.981208968966721e-07,
      "loss": 0.0008,
      "step": 100360
    },
    {
      "epoch": 18249.090909090908,
      "grad_norm": 0.0012139019090682268,
      "learning_rate": 2.9801445290214177e-07,
      "loss": 0.0012,
      "step": 100370
    },
    {
      "epoch": 18250.909090909092,
      "grad_norm": 0.0006081141764298081,
      "learning_rate": 2.979080198459788e-07,
      "loss": 0.0012,
      "step": 100380
    },
    {
      "epoch": 18252.727272727272,
      "grad_norm": 0.21075452864170074,
      "learning_rate": 2.9780159773394706e-07,
      "loss": 0.0012,
      "step": 100390
    },
    {
      "epoch": 18254.545454545456,
      "grad_norm": 0.17191003262996674,
      "learning_rate": 2.976951865718095e-07,
      "loss": 0.001,
      "step": 100400
    },
    {
      "epoch": 18256.363636363636,
      "grad_norm": 0.0009562947670929134,
      "learning_rate": 2.975887863653288e-07,
      "loss": 0.0009,
      "step": 100410
    },
    {
      "epoch": 18258.18181818182,
      "grad_norm": 0.18784886598587036,
      "learning_rate": 2.9748239712026714e-07,
      "loss": 0.0012,
      "step": 100420
    },
    {
      "epoch": 18260.0,
      "grad_norm": 0.0004096823395229876,
      "learning_rate": 2.973760188423857e-07,
      "loss": 0.001,
      "step": 100430
    },
    {
      "epoch": 18261.81818181818,
      "grad_norm": 0.27330124378204346,
      "learning_rate": 2.972696515374455e-07,
      "loss": 0.0012,
      "step": 100440
    },
    {
      "epoch": 18263.636363636364,
      "grad_norm": 0.21283258497714996,
      "learning_rate": 2.971632952112066e-07,
      "loss": 0.0009,
      "step": 100450
    },
    {
      "epoch": 18265.454545454544,
      "grad_norm": 0.21332669258117676,
      "learning_rate": 2.9705694986942864e-07,
      "loss": 0.0011,
      "step": 100460
    },
    {
      "epoch": 18267.272727272728,
      "grad_norm": 0.000980266835540533,
      "learning_rate": 2.9695061551787105e-07,
      "loss": 0.0009,
      "step": 100470
    },
    {
      "epoch": 18269.090909090908,
      "grad_norm": 0.0011260374449193478,
      "learning_rate": 2.968442921622918e-07,
      "loss": 0.0012,
      "step": 100480
    },
    {
      "epoch": 18270.909090909092,
      "grad_norm": 0.0007318263524211943,
      "learning_rate": 2.96737979808449e-07,
      "loss": 0.0009,
      "step": 100490
    },
    {
      "epoch": 18272.727272727272,
      "grad_norm": 0.0006101624458096921,
      "learning_rate": 2.9663167846209996e-07,
      "loss": 0.001,
      "step": 100500
    },
    {
      "epoch": 18272.727272727272,
      "eval_loss": 5.115614891052246,
      "eval_runtime": 0.9525,
      "eval_samples_per_second": 10.499,
      "eval_steps_per_second": 5.25,
      "step": 100500
    },
    {
      "epoch": 18274.545454545456,
      "grad_norm": 0.20552390813827515,
      "learning_rate": 2.9652538812900113e-07,
      "loss": 0.0017,
      "step": 100510
    },
    {
      "epoch": 18276.363636363636,
      "grad_norm": 0.0004944148240610957,
      "learning_rate": 2.9641910881490864e-07,
      "loss": 0.0008,
      "step": 100520
    },
    {
      "epoch": 18278.18181818182,
      "grad_norm": 0.16491900384426117,
      "learning_rate": 2.9631284052557824e-07,
      "loss": 0.0016,
      "step": 100530
    },
    {
      "epoch": 18280.0,
      "grad_norm": 0.0003744920832104981,
      "learning_rate": 2.962065832667644e-07,
      "loss": 0.0008,
      "step": 100540
    },
    {
      "epoch": 18281.81818181818,
      "grad_norm": 0.0007206795271486044,
      "learning_rate": 2.9610033704422175e-07,
      "loss": 0.001,
      "step": 100550
    },
    {
      "epoch": 18283.636363636364,
      "grad_norm": 0.0034225357230752707,
      "learning_rate": 2.959941018637036e-07,
      "loss": 0.0014,
      "step": 100560
    },
    {
      "epoch": 18285.454545454544,
      "grad_norm": 0.0006499250303022563,
      "learning_rate": 2.958878777309633e-07,
      "loss": 0.0009,
      "step": 100570
    },
    {
      "epoch": 18287.272727272728,
      "grad_norm": 0.17355884611606598,
      "learning_rate": 2.957816646517534e-07,
      "loss": 0.0011,
      "step": 100580
    },
    {
      "epoch": 18289.090909090908,
      "grad_norm": 0.0012010440696030855,
      "learning_rate": 2.956754626318255e-07,
      "loss": 0.0011,
      "step": 100590
    },
    {
      "epoch": 18290.909090909092,
      "grad_norm": 0.0009472942911088467,
      "learning_rate": 2.9556927167693104e-07,
      "loss": 0.0012,
      "step": 100600
    },
    {
      "epoch": 18292.727272727272,
      "grad_norm": 0.20007511973381042,
      "learning_rate": 2.954630917928208e-07,
      "loss": 0.0009,
      "step": 100610
    },
    {
      "epoch": 18294.545454545456,
      "grad_norm": 0.165912464261055,
      "learning_rate": 2.953569229852447e-07,
      "loss": 0.0012,
      "step": 100620
    },
    {
      "epoch": 18296.363636363636,
      "grad_norm": 0.26164937019348145,
      "learning_rate": 2.952507652599524e-07,
      "loss": 0.0011,
      "step": 100630
    },
    {
      "epoch": 18298.18181818182,
      "grad_norm": 0.1993253529071808,
      "learning_rate": 2.9514461862269257e-07,
      "loss": 0.0012,
      "step": 100640
    },
    {
      "epoch": 18300.0,
      "grad_norm": 0.22618705034255981,
      "learning_rate": 2.950384830792136e-07,
      "loss": 0.0011,
      "step": 100650
    },
    {
      "epoch": 18301.81818181818,
      "grad_norm": 0.1802866905927658,
      "learning_rate": 2.949323586352633e-07,
      "loss": 0.0009,
      "step": 100660
    },
    {
      "epoch": 18303.636363636364,
      "grad_norm": 0.0007793423719704151,
      "learning_rate": 2.9482624529658857e-07,
      "loss": 0.0011,
      "step": 100670
    },
    {
      "epoch": 18305.454545454544,
      "grad_norm": 0.0010359424632042646,
      "learning_rate": 2.94720143068936e-07,
      "loss": 0.0009,
      "step": 100680
    },
    {
      "epoch": 18307.272727272728,
      "grad_norm": 0.17302019894123077,
      "learning_rate": 2.9461405195805143e-07,
      "loss": 0.0015,
      "step": 100690
    },
    {
      "epoch": 18309.090909090908,
      "grad_norm": 0.0008151071378961205,
      "learning_rate": 2.945079719696802e-07,
      "loss": 0.001,
      "step": 100700
    },
    {
      "epoch": 18310.909090909092,
      "grad_norm": 0.0005906615988351405,
      "learning_rate": 2.9440190310956695e-07,
      "loss": 0.001,
      "step": 100710
    },
    {
      "epoch": 18312.727272727272,
      "grad_norm": 0.0011124812299385667,
      "learning_rate": 2.942958453834556e-07,
      "loss": 0.0015,
      "step": 100720
    },
    {
      "epoch": 18314.545454545456,
      "grad_norm": 0.17518024146556854,
      "learning_rate": 2.9418979879708984e-07,
      "loss": 0.0011,
      "step": 100730
    },
    {
      "epoch": 18316.363636363636,
      "grad_norm": 0.2672456204891205,
      "learning_rate": 2.9408376335621265e-07,
      "loss": 0.0011,
      "step": 100740
    },
    {
      "epoch": 18318.18181818182,
      "grad_norm": 0.16370782256126404,
      "learning_rate": 2.939777390665658e-07,
      "loss": 0.001,
      "step": 100750
    },
    {
      "epoch": 18320.0,
      "grad_norm": 0.0006691018352285028,
      "learning_rate": 2.9387172593389145e-07,
      "loss": 0.001,
      "step": 100760
    },
    {
      "epoch": 18321.81818181818,
      "grad_norm": 0.21112215518951416,
      "learning_rate": 2.9376572396393044e-07,
      "loss": 0.0009,
      "step": 100770
    },
    {
      "epoch": 18323.636363636364,
      "grad_norm": 0.2106008529663086,
      "learning_rate": 2.936597331624232e-07,
      "loss": 0.0013,
      "step": 100780
    },
    {
      "epoch": 18325.454545454544,
      "grad_norm": 0.21142202615737915,
      "learning_rate": 2.935537535351097e-07,
      "loss": 0.0009,
      "step": 100790
    },
    {
      "epoch": 18327.272727272728,
      "grad_norm": 0.0066214934922754765,
      "learning_rate": 2.9344778508772914e-07,
      "loss": 0.0013,
      "step": 100800
    },
    {
      "epoch": 18329.090909090908,
      "grad_norm": 0.19751961529254913,
      "learning_rate": 2.933418278260201e-07,
      "loss": 0.0012,
      "step": 100810
    },
    {
      "epoch": 18330.909090909092,
      "grad_norm": 0.005659944377839565,
      "learning_rate": 2.9323588175572066e-07,
      "loss": 0.001,
      "step": 100820
    },
    {
      "epoch": 18332.727272727272,
      "grad_norm": 0.1587837040424347,
      "learning_rate": 2.931299468825682e-07,
      "loss": 0.0011,
      "step": 100830
    },
    {
      "epoch": 18334.545454545456,
      "grad_norm": 0.16175517439842224,
      "learning_rate": 2.930240232122995e-07,
      "loss": 0.0012,
      "step": 100840
    },
    {
      "epoch": 18336.363636363636,
      "grad_norm": 0.16012707352638245,
      "learning_rate": 2.9291811075065085e-07,
      "loss": 0.0009,
      "step": 100850
    },
    {
      "epoch": 18338.18181818182,
      "grad_norm": 0.15306277573108673,
      "learning_rate": 2.9281220950335796e-07,
      "loss": 0.0012,
      "step": 100860
    },
    {
      "epoch": 18340.0,
      "grad_norm": 0.2008403092622757,
      "learning_rate": 2.927063194761556e-07,
      "loss": 0.001,
      "step": 100870
    },
    {
      "epoch": 18341.81818181818,
      "grad_norm": 0.0006577451131306589,
      "learning_rate": 2.926004406747784e-07,
      "loss": 0.001,
      "step": 100880
    },
    {
      "epoch": 18343.636363636364,
      "grad_norm": 0.19956818222999573,
      "learning_rate": 2.924945731049599e-07,
      "loss": 0.0012,
      "step": 100890
    },
    {
      "epoch": 18345.454545454544,
      "grad_norm": 0.2517220973968506,
      "learning_rate": 2.9238871677243347e-07,
      "loss": 0.001,
      "step": 100900
    },
    {
      "epoch": 18347.272727272728,
      "grad_norm": 0.0008787044207565486,
      "learning_rate": 2.9228287168293154e-07,
      "loss": 0.0011,
      "step": 100910
    },
    {
      "epoch": 18349.090909090908,
      "grad_norm": 0.38007280230522156,
      "learning_rate": 2.921770378421861e-07,
      "loss": 0.0011,
      "step": 100920
    },
    {
      "epoch": 18350.909090909092,
      "grad_norm": 0.0005372314481064677,
      "learning_rate": 2.920712152559287e-07,
      "loss": 0.0011,
      "step": 100930
    },
    {
      "epoch": 18352.727272727272,
      "grad_norm": 0.16265946626663208,
      "learning_rate": 2.919654039298896e-07,
      "loss": 0.001,
      "step": 100940
    },
    {
      "epoch": 18354.545454545456,
      "grad_norm": 0.0006305854767560959,
      "learning_rate": 2.9185960386979946e-07,
      "loss": 0.0008,
      "step": 100950
    },
    {
      "epoch": 18356.363636363636,
      "grad_norm": 0.21158309280872345,
      "learning_rate": 2.9175381508138755e-07,
      "loss": 0.0015,
      "step": 100960
    },
    {
      "epoch": 18358.18181818182,
      "grad_norm": 0.24221576750278473,
      "learning_rate": 2.9164803757038264e-07,
      "loss": 0.0011,
      "step": 100970
    },
    {
      "epoch": 18360.0,
      "grad_norm": 0.2696380615234375,
      "learning_rate": 2.9154227134251334e-07,
      "loss": 0.0009,
      "step": 100980
    },
    {
      "epoch": 18361.81818181818,
      "grad_norm": 0.0012976559810340405,
      "learning_rate": 2.91436516403507e-07,
      "loss": 0.0012,
      "step": 100990
    },
    {
      "epoch": 18363.636363636364,
      "grad_norm": 0.0006615038146264851,
      "learning_rate": 2.9133077275909107e-07,
      "loss": 0.0008,
      "step": 101000
    },
    {
      "epoch": 18363.636363636364,
      "eval_loss": 5.111311912536621,
      "eval_runtime": 0.9514,
      "eval_samples_per_second": 10.511,
      "eval_steps_per_second": 5.256,
      "step": 101000
    },
    {
      "epoch": 18365.454545454544,
      "grad_norm": 0.2155335545539856,
      "learning_rate": 2.912250404149918e-07,
      "loss": 0.0015,
      "step": 101010
    },
    {
      "epoch": 18367.272727272728,
      "grad_norm": 0.22808900475502014,
      "learning_rate": 2.911193193769349e-07,
      "loss": 0.001,
      "step": 101020
    },
    {
      "epoch": 18369.090909090908,
      "grad_norm": 0.0006214038003236055,
      "learning_rate": 2.910136096506459e-07,
      "loss": 0.0009,
      "step": 101030
    },
    {
      "epoch": 18370.909090909092,
      "grad_norm": 0.2623639404773712,
      "learning_rate": 2.9090791124184934e-07,
      "loss": 0.001,
      "step": 101040
    },
    {
      "epoch": 18372.727272727272,
      "grad_norm": 0.004179312847554684,
      "learning_rate": 2.90802224156269e-07,
      "loss": 0.0012,
      "step": 101050
    },
    {
      "epoch": 18374.545454545456,
      "grad_norm": 0.28486672043800354,
      "learning_rate": 2.9069654839962866e-07,
      "loss": 0.0014,
      "step": 101060
    },
    {
      "epoch": 18376.363636363636,
      "grad_norm": 0.19586685299873352,
      "learning_rate": 2.9059088397765086e-07,
      "loss": 0.0007,
      "step": 101070
    },
    {
      "epoch": 18378.18181818182,
      "grad_norm": 0.2102128565311432,
      "learning_rate": 2.904852308960577e-07,
      "loss": 0.0012,
      "step": 101080
    },
    {
      "epoch": 18380.0,
      "grad_norm": 0.21225927770137787,
      "learning_rate": 2.90379589160571e-07,
      "loss": 0.001,
      "step": 101090
    },
    {
      "epoch": 18381.81818181818,
      "grad_norm": 0.26294243335723877,
      "learning_rate": 2.902739587769114e-07,
      "loss": 0.001,
      "step": 101100
    },
    {
      "epoch": 18383.636363636364,
      "grad_norm": 0.0010718911653384566,
      "learning_rate": 2.901683397507996e-07,
      "loss": 0.001,
      "step": 101110
    },
    {
      "epoch": 18385.454545454544,
      "grad_norm": 0.21300575137138367,
      "learning_rate": 2.900627320879551e-07,
      "loss": 0.0011,
      "step": 101120
    },
    {
      "epoch": 18387.272727272728,
      "grad_norm": 0.2017589509487152,
      "learning_rate": 2.8995713579409685e-07,
      "loss": 0.0012,
      "step": 101130
    },
    {
      "epoch": 18389.090909090908,
      "grad_norm": 0.22455145418643951,
      "learning_rate": 2.8985155087494366e-07,
      "loss": 0.0012,
      "step": 101140
    },
    {
      "epoch": 18390.909090909092,
      "grad_norm": 0.20045308768749237,
      "learning_rate": 2.8974597733621324e-07,
      "loss": 0.0009,
      "step": 101150
    },
    {
      "epoch": 18392.727272727272,
      "grad_norm": 0.154220849275589,
      "learning_rate": 2.896404151836227e-07,
      "loss": 0.0013,
      "step": 101160
    },
    {
      "epoch": 18394.545454545456,
      "grad_norm": 0.0005218182923272252,
      "learning_rate": 2.895348644228889e-07,
      "loss": 0.0009,
      "step": 101170
    },
    {
      "epoch": 18396.363636363636,
      "grad_norm": 0.0010753724491223693,
      "learning_rate": 2.894293250597277e-07,
      "loss": 0.0009,
      "step": 101180
    },
    {
      "epoch": 18398.18181818182,
      "grad_norm": 0.15439780056476593,
      "learning_rate": 2.8932379709985466e-07,
      "loss": 0.0013,
      "step": 101190
    },
    {
      "epoch": 18400.0,
      "grad_norm": 0.15610209107398987,
      "learning_rate": 2.8921828054898455e-07,
      "loss": 0.001,
      "step": 101200
    },
    {
      "epoch": 18401.81818181818,
      "grad_norm": 0.0173485167324543,
      "learning_rate": 2.891127754128312e-07,
      "loss": 0.0012,
      "step": 101210
    },
    {
      "epoch": 18403.636363636364,
      "grad_norm": 0.0005110322963446379,
      "learning_rate": 2.8900728169710865e-07,
      "loss": 0.0007,
      "step": 101220
    },
    {
      "epoch": 18405.454545454544,
      "grad_norm": 0.150959774851799,
      "learning_rate": 2.889017994075296e-07,
      "loss": 0.0014,
      "step": 101230
    },
    {
      "epoch": 18407.272727272728,
      "grad_norm": 0.15676471590995789,
      "learning_rate": 2.887963285498061e-07,
      "loss": 0.001,
      "step": 101240
    },
    {
      "epoch": 18409.090909090908,
      "grad_norm": 0.17746959626674652,
      "learning_rate": 2.8869086912965036e-07,
      "loss": 0.0011,
      "step": 101250
    },
    {
      "epoch": 18410.909090909092,
      "grad_norm": 0.15553595125675201,
      "learning_rate": 2.885854211527731e-07,
      "loss": 0.0012,
      "step": 101260
    },
    {
      "epoch": 18412.727272727272,
      "grad_norm": 0.0006279973313212395,
      "learning_rate": 2.884799846248848e-07,
      "loss": 0.001,
      "step": 101270
    },
    {
      "epoch": 18414.545454545456,
      "grad_norm": 0.21303854882717133,
      "learning_rate": 2.8837455955169543e-07,
      "loss": 0.0011,
      "step": 101280
    },
    {
      "epoch": 18416.363636363636,
      "grad_norm": 0.0006810303893871605,
      "learning_rate": 2.8826914593891394e-07,
      "loss": 0.0012,
      "step": 101290
    },
    {
      "epoch": 18418.18181818182,
      "grad_norm": 0.15784761309623718,
      "learning_rate": 2.8816374379224927e-07,
      "loss": 0.0011,
      "step": 101300
    },
    {
      "epoch": 18420.0,
      "grad_norm": 0.0008299662149511278,
      "learning_rate": 2.8805835311740926e-07,
      "loss": 0.001,
      "step": 101310
    },
    {
      "epoch": 18421.81818181818,
      "grad_norm": 0.0010104492539539933,
      "learning_rate": 2.8795297392010105e-07,
      "loss": 0.001,
      "step": 101320
    },
    {
      "epoch": 18423.636363636364,
      "grad_norm": 0.26186370849609375,
      "learning_rate": 2.8784760620603174e-07,
      "loss": 0.0012,
      "step": 101330
    },
    {
      "epoch": 18425.454545454544,
      "grad_norm": 0.21334005892276764,
      "learning_rate": 2.877422499809072e-07,
      "loss": 0.0009,
      "step": 101340
    },
    {
      "epoch": 18427.272727272728,
      "grad_norm": 0.0012642457149922848,
      "learning_rate": 2.876369052504327e-07,
      "loss": 0.0013,
      "step": 101350
    },
    {
      "epoch": 18429.090909090908,
      "grad_norm": 0.0006186586688272655,
      "learning_rate": 2.875315720203136e-07,
      "loss": 0.0009,
      "step": 101360
    },
    {
      "epoch": 18430.909090909092,
      "grad_norm": 0.0009088132064789534,
      "learning_rate": 2.8742625029625367e-07,
      "loss": 0.0012,
      "step": 101370
    },
    {
      "epoch": 18432.727272727272,
      "grad_norm": 0.21187381446361542,
      "learning_rate": 2.8732094008395693e-07,
      "loss": 0.0009,
      "step": 101380
    },
    {
      "epoch": 18434.545454545456,
      "grad_norm": 0.0007191244512796402,
      "learning_rate": 2.8721564138912626e-07,
      "loss": 0.0012,
      "step": 101390
    },
    {
      "epoch": 18436.363636363636,
      "grad_norm": 0.0015995758585631847,
      "learning_rate": 2.8711035421746363e-07,
      "loss": 0.0014,
      "step": 101400
    },
    {
      "epoch": 18438.18181818182,
      "grad_norm": 0.21460476517677307,
      "learning_rate": 2.870050785746713e-07,
      "loss": 0.0008,
      "step": 101410
    },
    {
      "epoch": 18440.0,
      "grad_norm": 0.21391630172729492,
      "learning_rate": 2.8689981446645027e-07,
      "loss": 0.0012,
      "step": 101420
    },
    {
      "epoch": 18441.81818181818,
      "grad_norm": 0.0006525139324367046,
      "learning_rate": 2.867945618985007e-07,
      "loss": 0.001,
      "step": 101430
    },
    {
      "epoch": 18443.636363636364,
      "grad_norm": 0.20121575891971588,
      "learning_rate": 2.8668932087652293e-07,
      "loss": 0.0012,
      "step": 101440
    },
    {
      "epoch": 18445.454545454544,
      "grad_norm": 0.21228814125061035,
      "learning_rate": 2.8658409140621596e-07,
      "loss": 0.001,
      "step": 101450
    },
    {
      "epoch": 18447.272727272728,
      "grad_norm": 0.1565345972776413,
      "learning_rate": 2.8647887349327826e-07,
      "loss": 0.0013,
      "step": 101460
    },
    {
      "epoch": 18449.090909090908,
      "grad_norm": 0.2703526020050049,
      "learning_rate": 2.8637366714340813e-07,
      "loss": 0.0009,
      "step": 101470
    },
    {
      "epoch": 18450.909090909092,
      "grad_norm": 0.0011234849225729704,
      "learning_rate": 2.862684723623027e-07,
      "loss": 0.0012,
      "step": 101480
    },
    {
      "epoch": 18452.727272727272,
      "grad_norm": 0.0007471388089470565,
      "learning_rate": 2.86163289155659e-07,
      "loss": 0.0012,
      "step": 101490
    },
    {
      "epoch": 18454.545454545456,
      "grad_norm": 0.0008061744156293571,
      "learning_rate": 2.86058117529173e-07,
      "loss": 0.0012,
      "step": 101500
    },
    {
      "epoch": 18454.545454545456,
      "eval_loss": 5.146655082702637,
      "eval_runtime": 0.9514,
      "eval_samples_per_second": 10.511,
      "eval_steps_per_second": 5.255,
      "step": 101500
    },
    {
      "epoch": 18456.363636363636,
      "grad_norm": 0.20385228097438812,
      "learning_rate": 2.8595295748853983e-07,
      "loss": 0.0012,
      "step": 101510
    },
    {
      "epoch": 18458.18181818182,
      "grad_norm": 0.0006300011882558465,
      "learning_rate": 2.858478090394549e-07,
      "loss": 0.0008,
      "step": 101520
    },
    {
      "epoch": 18460.0,
      "grad_norm": 0.0006800739211030304,
      "learning_rate": 2.8574267218761215e-07,
      "loss": 0.0012,
      "step": 101530
    },
    {
      "epoch": 18461.81818181818,
      "grad_norm": 0.03981267660856247,
      "learning_rate": 2.856375469387051e-07,
      "loss": 0.0012,
      "step": 101540
    },
    {
      "epoch": 18463.636363636364,
      "grad_norm": 0.15174148976802826,
      "learning_rate": 2.8553243329842715e-07,
      "loss": 0.001,
      "step": 101550
    },
    {
      "epoch": 18465.454545454544,
      "grad_norm": 0.0004580215900205076,
      "learning_rate": 2.854273312724702e-07,
      "loss": 0.0009,
      "step": 101560
    },
    {
      "epoch": 18467.272727272728,
      "grad_norm": 0.0005444992566481233,
      "learning_rate": 2.85322240866526e-07,
      "loss": 0.0013,
      "step": 101570
    },
    {
      "epoch": 18469.090909090908,
      "grad_norm": 0.2027067393064499,
      "learning_rate": 2.8521716208628596e-07,
      "loss": 0.001,
      "step": 101580
    },
    {
      "epoch": 18470.909090909092,
      "grad_norm": 0.1646023988723755,
      "learning_rate": 2.8511209493744007e-07,
      "loss": 0.0012,
      "step": 101590
    },
    {
      "epoch": 18472.727272727272,
      "grad_norm": 0.0005251239635981619,
      "learning_rate": 2.850070394256787e-07,
      "loss": 0.001,
      "step": 101600
    },
    {
      "epoch": 18474.545454545456,
      "grad_norm": 0.017511552199721336,
      "learning_rate": 2.8490199555669075e-07,
      "loss": 0.001,
      "step": 101610
    },
    {
      "epoch": 18476.363636363636,
      "grad_norm": 0.21531298756599426,
      "learning_rate": 2.8479696333616467e-07,
      "loss": 0.001,
      "step": 101620
    },
    {
      "epoch": 18478.18181818182,
      "grad_norm": 0.1956818550825119,
      "learning_rate": 2.846919427697886e-07,
      "loss": 0.0012,
      "step": 101630
    },
    {
      "epoch": 18480.0,
      "grad_norm": 0.2036563605070114,
      "learning_rate": 2.8458693386324995e-07,
      "loss": 0.001,
      "step": 101640
    },
    {
      "epoch": 18481.81818181818,
      "grad_norm": 0.20931437611579895,
      "learning_rate": 2.844819366222349e-07,
      "loss": 0.0011,
      "step": 101650
    },
    {
      "epoch": 18483.636363636364,
      "grad_norm": 0.0009868661873042583,
      "learning_rate": 2.843769510524301e-07,
      "loss": 0.001,
      "step": 101660
    },
    {
      "epoch": 18485.454545454544,
      "grad_norm": 0.001389734330587089,
      "learning_rate": 2.8427197715952046e-07,
      "loss": 0.001,
      "step": 101670
    },
    {
      "epoch": 18487.272727272728,
      "grad_norm": 0.20083847641944885,
      "learning_rate": 2.8416701494919137e-07,
      "loss": 0.0013,
      "step": 101680
    },
    {
      "epoch": 18489.090909090908,
      "grad_norm": 0.0006937321159057319,
      "learning_rate": 2.8406206442712614e-07,
      "loss": 0.001,
      "step": 101690
    },
    {
      "epoch": 18490.909090909092,
      "grad_norm": 0.0008366321562789381,
      "learning_rate": 2.8395712559900874e-07,
      "loss": 0.0012,
      "step": 101700
    },
    {
      "epoch": 18492.727272727272,
      "grad_norm": 0.21287018060684204,
      "learning_rate": 2.838521984705223e-07,
      "loss": 0.0012,
      "step": 101710
    },
    {
      "epoch": 18494.545454545456,
      "grad_norm": 0.0005364092066884041,
      "learning_rate": 2.837472830473488e-07,
      "loss": 0.0009,
      "step": 101720
    },
    {
      "epoch": 18496.363636363636,
      "grad_norm": 0.1546957790851593,
      "learning_rate": 2.836423793351696e-07,
      "loss": 0.0012,
      "step": 101730
    },
    {
      "epoch": 18498.18181818182,
      "grad_norm": 0.0008071189513429999,
      "learning_rate": 2.835374873396662e-07,
      "loss": 0.0009,
      "step": 101740
    },
    {
      "epoch": 18500.0,
      "grad_norm": 0.1316985934972763,
      "learning_rate": 2.8343260706651863e-07,
      "loss": 0.0011,
      "step": 101750
    },
    {
      "epoch": 18501.81818181818,
      "grad_norm": 0.0008328596595674753,
      "learning_rate": 2.833277385214064e-07,
      "loss": 0.0012,
      "step": 101760
    },
    {
      "epoch": 18503.636363636364,
      "grad_norm": 0.010603184811770916,
      "learning_rate": 2.832228817100091e-07,
      "loss": 0.0009,
      "step": 101770
    },
    {
      "epoch": 18505.454545454544,
      "grad_norm": 0.1526155173778534,
      "learning_rate": 2.831180366380046e-07,
      "loss": 0.0012,
      "step": 101780
    },
    {
      "epoch": 18507.272727272728,
      "grad_norm": 0.15012818574905396,
      "learning_rate": 2.830132033110713e-07,
      "loss": 0.0011,
      "step": 101790
    },
    {
      "epoch": 18509.090909090908,
      "grad_norm": 0.213907852768898,
      "learning_rate": 2.82908381734886e-07,
      "loss": 0.0012,
      "step": 101800
    },
    {
      "epoch": 18510.909090909092,
      "grad_norm": 0.2104799449443817,
      "learning_rate": 2.8280357191512503e-07,
      "loss": 0.0012,
      "step": 101810
    },
    {
      "epoch": 18512.727272727272,
      "grad_norm": 0.0006584942457266152,
      "learning_rate": 2.8269877385746485e-07,
      "loss": 0.0012,
      "step": 101820
    },
    {
      "epoch": 18514.545454545456,
      "grad_norm": 0.0005832597962580621,
      "learning_rate": 2.825939875675804e-07,
      "loss": 0.0009,
      "step": 101830
    },
    {
      "epoch": 18516.363636363636,
      "grad_norm": 0.2748679518699646,
      "learning_rate": 2.8248921305114606e-07,
      "loss": 0.0013,
      "step": 101840
    },
    {
      "epoch": 18518.18181818182,
      "grad_norm": 0.15086708962917328,
      "learning_rate": 2.823844503138363e-07,
      "loss": 0.0009,
      "step": 101850
    },
    {
      "epoch": 18520.0,
      "grad_norm": 0.15614096820354462,
      "learning_rate": 2.82279699361324e-07,
      "loss": 0.0011,
      "step": 101860
    },
    {
      "epoch": 18521.81818181818,
      "grad_norm": 0.1976739466190338,
      "learning_rate": 2.8217496019928243e-07,
      "loss": 0.0012,
      "step": 101870
    },
    {
      "epoch": 18523.636363636364,
      "grad_norm": 0.010203166864812374,
      "learning_rate": 2.82070232833383e-07,
      "loss": 0.001,
      "step": 101880
    },
    {
      "epoch": 18525.454545454544,
      "grad_norm": 0.0008690690156072378,
      "learning_rate": 2.819655172692974e-07,
      "loss": 0.0012,
      "step": 101890
    },
    {
      "epoch": 18527.272727272728,
      "grad_norm": 0.0006097822333686054,
      "learning_rate": 2.8186081351269665e-07,
      "loss": 0.001,
      "step": 101900
    },
    {
      "epoch": 18529.090909090908,
      "grad_norm": 0.2650744915008545,
      "learning_rate": 2.817561215692508e-07,
      "loss": 0.0013,
      "step": 101910
    },
    {
      "epoch": 18530.909090909092,
      "grad_norm": 0.0005660526803694665,
      "learning_rate": 2.81651441444629e-07,
      "loss": 0.001,
      "step": 101920
    },
    {
      "epoch": 18532.727272727272,
      "grad_norm": 0.15828394889831543,
      "learning_rate": 2.815467731445006e-07,
      "loss": 0.0013,
      "step": 101930
    },
    {
      "epoch": 18534.545454545456,
      "grad_norm": 0.0006290960009209812,
      "learning_rate": 2.8144211667453364e-07,
      "loss": 0.0009,
      "step": 101940
    },
    {
      "epoch": 18536.363636363636,
      "grad_norm": 0.0004605854628607631,
      "learning_rate": 2.813374720403957e-07,
      "loss": 0.0008,
      "step": 101950
    },
    {
      "epoch": 18538.18181818182,
      "grad_norm": 0.0006636544712819159,
      "learning_rate": 2.8123283924775355e-07,
      "loss": 0.0012,
      "step": 101960
    },
    {
      "epoch": 18540.0,
      "grad_norm": 0.1964724361896515,
      "learning_rate": 2.811282183022736e-07,
      "loss": 0.0012,
      "step": 101970
    },
    {
      "epoch": 18541.81818181818,
      "grad_norm": 0.202489972114563,
      "learning_rate": 2.810236092096218e-07,
      "loss": 0.001,
      "step": 101980
    },
    {
      "epoch": 18543.636363636364,
      "grad_norm": 0.27061033248901367,
      "learning_rate": 2.8091901197546296e-07,
      "loss": 0.0014,
      "step": 101990
    },
    {
      "epoch": 18545.454545454544,
      "grad_norm": 0.1524433195590973,
      "learning_rate": 2.808144266054612e-07,
      "loss": 0.0008,
      "step": 102000
    },
    {
      "epoch": 18545.454545454544,
      "eval_loss": 5.113805770874023,
      "eval_runtime": 0.9559,
      "eval_samples_per_second": 10.461,
      "eval_steps_per_second": 5.231,
      "step": 102000
    },
    {
      "epoch": 18547.272727272728,
      "grad_norm": 0.2454393357038498,
      "learning_rate": 2.8070985310528077e-07,
      "loss": 0.0015,
      "step": 102010
    },
    {
      "epoch": 18549.090909090908,
      "grad_norm": 0.19931794703006744,
      "learning_rate": 2.8060529148058456e-07,
      "loss": 0.0008,
      "step": 102020
    },
    {
      "epoch": 18550.909090909092,
      "grad_norm": 0.22536246478557587,
      "learning_rate": 2.8050074173703465e-07,
      "loss": 0.0012,
      "step": 102030
    },
    {
      "epoch": 18552.727272727272,
      "grad_norm": 0.0017941652331501245,
      "learning_rate": 2.803962038802934e-07,
      "loss": 0.0009,
      "step": 102040
    },
    {
      "epoch": 18554.545454545456,
      "grad_norm": 0.21337909996509552,
      "learning_rate": 2.802916779160216e-07,
      "loss": 0.0013,
      "step": 102050
    },
    {
      "epoch": 18556.363636363636,
      "grad_norm": 0.1531677097082138,
      "learning_rate": 2.801871638498803e-07,
      "loss": 0.0009,
      "step": 102060
    },
    {
      "epoch": 18558.18181818182,
      "grad_norm": 0.0006994535797275603,
      "learning_rate": 2.8008266168752846e-07,
      "loss": 0.0012,
      "step": 102070
    },
    {
      "epoch": 18560.0,
      "grad_norm": 0.0015283981338143349,
      "learning_rate": 2.79978171434626e-07,
      "loss": 0.0012,
      "step": 102080
    },
    {
      "epoch": 18561.81818181818,
      "grad_norm": 0.1521877497434616,
      "learning_rate": 2.7987369309683146e-07,
      "loss": 0.0012,
      "step": 102090
    },
    {
      "epoch": 18563.636363636364,
      "grad_norm": 0.0005326987593434751,
      "learning_rate": 2.7976922667980267e-07,
      "loss": 0.001,
      "step": 102100
    },
    {
      "epoch": 18565.454545454544,
      "grad_norm": 0.0008691883995197713,
      "learning_rate": 2.796647721891968e-07,
      "loss": 0.001,
      "step": 102110
    },
    {
      "epoch": 18567.272727272728,
      "grad_norm": 0.001723574590869248,
      "learning_rate": 2.795603296306708e-07,
      "loss": 0.0011,
      "step": 102120
    },
    {
      "epoch": 18569.090909090908,
      "grad_norm": 0.20185382664203644,
      "learning_rate": 2.794558990098804e-07,
      "loss": 0.0013,
      "step": 102130
    },
    {
      "epoch": 18570.909090909092,
      "grad_norm": 0.0013257416430860758,
      "learning_rate": 2.793514803324811e-07,
      "loss": 0.0009,
      "step": 102140
    },
    {
      "epoch": 18572.727272727272,
      "grad_norm": 0.000694223796017468,
      "learning_rate": 2.792470736041274e-07,
      "loss": 0.0009,
      "step": 102150
    },
    {
      "epoch": 18574.545454545456,
      "grad_norm": 0.1558143049478531,
      "learning_rate": 2.7914267883047353e-07,
      "loss": 0.0014,
      "step": 102160
    },
    {
      "epoch": 18576.363636363636,
      "grad_norm": 0.2667384147644043,
      "learning_rate": 2.790382960171731e-07,
      "loss": 0.0012,
      "step": 102170
    },
    {
      "epoch": 18578.18181818182,
      "grad_norm": 0.0005775598692707717,
      "learning_rate": 2.7893392516987867e-07,
      "loss": 0.0008,
      "step": 102180
    },
    {
      "epoch": 18580.0,
      "grad_norm": 0.2051927149295807,
      "learning_rate": 2.788295662942423e-07,
      "loss": 0.0012,
      "step": 102190
    },
    {
      "epoch": 18581.81818181818,
      "grad_norm": 0.006066953297704458,
      "learning_rate": 2.787252193959155e-07,
      "loss": 0.0012,
      "step": 102200
    },
    {
      "epoch": 18583.636363636364,
      "grad_norm": 0.2667407989501953,
      "learning_rate": 2.7862088448054934e-07,
      "loss": 0.001,
      "step": 102210
    },
    {
      "epoch": 18585.454545454544,
      "grad_norm": 0.0005279023898765445,
      "learning_rate": 2.7851656155379364e-07,
      "loss": 0.001,
      "step": 102220
    },
    {
      "epoch": 18587.272727272728,
      "grad_norm": 0.2562295198440552,
      "learning_rate": 2.7841225062129795e-07,
      "loss": 0.0013,
      "step": 102230
    },
    {
      "epoch": 18589.090909090908,
      "grad_norm": 0.0005354995955713093,
      "learning_rate": 2.7830795168871125e-07,
      "loss": 0.0008,
      "step": 102240
    },
    {
      "epoch": 18590.909090909092,
      "grad_norm": 0.15416620671749115,
      "learning_rate": 2.7820366476168225e-07,
      "loss": 0.001,
      "step": 102250
    },
    {
      "epoch": 18592.727272727272,
      "grad_norm": 0.16540741920471191,
      "learning_rate": 2.7809938984585757e-07,
      "loss": 0.0013,
      "step": 102260
    },
    {
      "epoch": 18594.545454545456,
      "grad_norm": 0.00043657279456965625,
      "learning_rate": 2.7799512694688466e-07,
      "loss": 0.0012,
      "step": 102270
    },
    {
      "epoch": 18596.363636363636,
      "grad_norm": 0.21314141154289246,
      "learning_rate": 2.7789087607040994e-07,
      "loss": 0.0009,
      "step": 102280
    },
    {
      "epoch": 18598.18181818182,
      "grad_norm": 0.0006272129248827696,
      "learning_rate": 2.7778663722207883e-07,
      "loss": 0.0011,
      "step": 102290
    },
    {
      "epoch": 18600.0,
      "grad_norm": 0.0008954288787208498,
      "learning_rate": 2.776824104075364e-07,
      "loss": 0.0012,
      "step": 102300
    },
    {
      "epoch": 18601.81818181818,
      "grad_norm": 0.21469701826572418,
      "learning_rate": 2.7757819563242666e-07,
      "loss": 0.001,
      "step": 102310
    },
    {
      "epoch": 18603.636363636364,
      "grad_norm": 0.22482949495315552,
      "learning_rate": 2.774739929023937e-07,
      "loss": 0.0013,
      "step": 102320
    },
    {
      "epoch": 18605.454545454544,
      "grad_norm": 0.0006480104639194906,
      "learning_rate": 2.773698022230804e-07,
      "loss": 0.0006,
      "step": 102330
    },
    {
      "epoch": 18607.272727272728,
      "grad_norm": 0.0005929746548645198,
      "learning_rate": 2.7726562360012877e-07,
      "loss": 0.0012,
      "step": 102340
    },
    {
      "epoch": 18609.090909090908,
      "grad_norm": 0.0005419764784164727,
      "learning_rate": 2.771614570391809e-07,
      "loss": 0.0012,
      "step": 102350
    },
    {
      "epoch": 18610.909090909092,
      "grad_norm": 0.26770782470703125,
      "learning_rate": 2.77057302545878e-07,
      "loss": 0.0012,
      "step": 102360
    },
    {
      "epoch": 18612.727272727272,
      "grad_norm": 0.2521325647830963,
      "learning_rate": 2.7695316012586023e-07,
      "loss": 0.0012,
      "step": 102370
    },
    {
      "epoch": 18614.545454545456,
      "grad_norm": 0.13446567952632904,
      "learning_rate": 2.768490297847671e-07,
      "loss": 0.001,
      "step": 102380
    },
    {
      "epoch": 18616.363636363636,
      "grad_norm": 0.21278032660484314,
      "learning_rate": 2.767449115282382e-07,
      "loss": 0.0009,
      "step": 102390
    },
    {
      "epoch": 18618.18181818182,
      "grad_norm": 0.20408260822296143,
      "learning_rate": 2.7664080536191174e-07,
      "loss": 0.0012,
      "step": 102400
    },
    {
      "epoch": 18620.0,
      "grad_norm": 0.1566748023033142,
      "learning_rate": 2.7653671129142554e-07,
      "loss": 0.001,
      "step": 102410
    },
    {
      "epoch": 18621.81818181818,
      "grad_norm": 0.001215813565067947,
      "learning_rate": 2.764326293224164e-07,
      "loss": 0.001,
      "step": 102420
    },
    {
      "epoch": 18623.636363636364,
      "grad_norm": 0.0004981912788935006,
      "learning_rate": 2.763285594605211e-07,
      "loss": 0.001,
      "step": 102430
    },
    {
      "epoch": 18625.454545454544,
      "grad_norm": 0.267282098531723,
      "learning_rate": 2.7622450171137595e-07,
      "loss": 0.0013,
      "step": 102440
    },
    {
      "epoch": 18627.272727272728,
      "grad_norm": 0.0013211817713454366,
      "learning_rate": 2.7612045608061517e-07,
      "loss": 0.0009,
      "step": 102450
    },
    {
      "epoch": 18629.090909090908,
      "grad_norm": 0.000892340496648103,
      "learning_rate": 2.760164225738737e-07,
      "loss": 0.0011,
      "step": 102460
    },
    {
      "epoch": 18630.909090909092,
      "grad_norm": 0.001332291983999312,
      "learning_rate": 2.759124011967856e-07,
      "loss": 0.0012,
      "step": 102470
    },
    {
      "epoch": 18632.727272727272,
      "grad_norm": 0.21853385865688324,
      "learning_rate": 2.75808391954984e-07,
      "loss": 0.001,
      "step": 102480
    },
    {
      "epoch": 18634.545454545456,
      "grad_norm": 0.0007956684567034245,
      "learning_rate": 2.7570439485410114e-07,
      "loss": 0.0009,
      "step": 102490
    },
    {
      "epoch": 18636.363636363636,
      "grad_norm": 0.21375642716884613,
      "learning_rate": 2.756004098997689e-07,
      "loss": 0.0016,
      "step": 102500
    },
    {
      "epoch": 18636.363636363636,
      "eval_loss": 5.061081886291504,
      "eval_runtime": 0.9492,
      "eval_samples_per_second": 10.535,
      "eval_steps_per_second": 5.268,
      "step": 102500
    },
    {
      "epoch": 18638.18181818182,
      "grad_norm": 0.005317154340445995,
      "learning_rate": 2.7549643709761895e-07,
      "loss": 0.0009,
      "step": 102510
    },
    {
      "epoch": 18640.0,
      "grad_norm": 0.0014570208732038736,
      "learning_rate": 2.7539247645328144e-07,
      "loss": 0.0009,
      "step": 102520
    },
    {
      "epoch": 18641.81818181818,
      "grad_norm": 0.2667918801307678,
      "learning_rate": 2.7528852797238633e-07,
      "loss": 0.0012,
      "step": 102530
    },
    {
      "epoch": 18643.636363636364,
      "grad_norm": 0.15608058869838715,
      "learning_rate": 2.7518459166056297e-07,
      "loss": 0.0011,
      "step": 102540
    },
    {
      "epoch": 18645.454545454544,
      "grad_norm": 0.0008446291903965175,
      "learning_rate": 2.7508066752344003e-07,
      "loss": 0.0008,
      "step": 102550
    },
    {
      "epoch": 18647.272727272728,
      "grad_norm": 0.15836495161056519,
      "learning_rate": 2.749767555666455e-07,
      "loss": 0.0013,
      "step": 102560
    },
    {
      "epoch": 18649.090909090908,
      "grad_norm": 0.3208991289138794,
      "learning_rate": 2.748728557958063e-07,
      "loss": 0.0012,
      "step": 102570
    },
    {
      "epoch": 18650.909090909092,
      "grad_norm": 0.22810576856136322,
      "learning_rate": 2.7476896821654914e-07,
      "loss": 0.0009,
      "step": 102580
    },
    {
      "epoch": 18652.727272727272,
      "grad_norm": 0.0010214585345238447,
      "learning_rate": 2.7466509283450024e-07,
      "loss": 0.0011,
      "step": 102590
    },
    {
      "epoch": 18654.545454545456,
      "grad_norm": 0.0014913725899532437,
      "learning_rate": 2.745612296552847e-07,
      "loss": 0.001,
      "step": 102600
    },
    {
      "epoch": 18656.363636363636,
      "grad_norm": 0.23279675841331482,
      "learning_rate": 2.7445737868452703e-07,
      "loss": 0.0015,
      "step": 102610
    },
    {
      "epoch": 18658.18181818182,
      "grad_norm": 0.000793609069660306,
      "learning_rate": 2.7435353992785126e-07,
      "loss": 0.001,
      "step": 102620
    },
    {
      "epoch": 18660.0,
      "grad_norm": 0.2172701209783554,
      "learning_rate": 2.7424971339088117e-07,
      "loss": 0.0012,
      "step": 102630
    },
    {
      "epoch": 18661.81818181818,
      "grad_norm": 0.21813100576400757,
      "learning_rate": 2.7414589907923856e-07,
      "loss": 0.0011,
      "step": 102640
    },
    {
      "epoch": 18663.636363636364,
      "grad_norm": 0.20730414986610413,
      "learning_rate": 2.740420969985459e-07,
      "loss": 0.0013,
      "step": 102650
    },
    {
      "epoch": 18665.454545454544,
      "grad_norm": 0.21646364033222198,
      "learning_rate": 2.7393830715442456e-07,
      "loss": 0.001,
      "step": 102660
    },
    {
      "epoch": 18667.272727272728,
      "grad_norm": 0.15862219035625458,
      "learning_rate": 2.738345295524952e-07,
      "loss": 0.0012,
      "step": 102670
    },
    {
      "epoch": 18669.090909090908,
      "grad_norm": 0.0007517137564718723,
      "learning_rate": 2.737307641983776e-07,
      "loss": 0.0007,
      "step": 102680
    },
    {
      "epoch": 18670.909090909092,
      "grad_norm": 0.0007223130669444799,
      "learning_rate": 2.736270110976912e-07,
      "loss": 0.0012,
      "step": 102690
    },
    {
      "epoch": 18672.727272727272,
      "grad_norm": 0.0011426100973039865,
      "learning_rate": 2.735232702560546e-07,
      "loss": 0.0008,
      "step": 102700
    },
    {
      "epoch": 18674.545454545456,
      "grad_norm": 0.20408207178115845,
      "learning_rate": 2.73419541679086e-07,
      "loss": 0.0014,
      "step": 102710
    },
    {
      "epoch": 18676.363636363636,
      "grad_norm": 0.1377510130405426,
      "learning_rate": 2.733158253724024e-07,
      "loss": 0.001,
      "step": 102720
    },
    {
      "epoch": 18678.18181818182,
      "grad_norm": 0.0026536816731095314,
      "learning_rate": 2.7321212134162063e-07,
      "loss": 0.0009,
      "step": 102730
    },
    {
      "epoch": 18680.0,
      "grad_norm": 0.0006891317316330969,
      "learning_rate": 2.7310842959235725e-07,
      "loss": 0.0012,
      "step": 102740
    },
    {
      "epoch": 18681.81818181818,
      "grad_norm": 0.005961360409855843,
      "learning_rate": 2.730047501302266e-07,
      "loss": 0.001,
      "step": 102750
    },
    {
      "epoch": 18683.636363636364,
      "grad_norm": 0.002288615796715021,
      "learning_rate": 2.7290108296084413e-07,
      "loss": 0.001,
      "step": 102760
    },
    {
      "epoch": 18685.454545454544,
      "grad_norm": 0.20267663896083832,
      "learning_rate": 2.727974280898234e-07,
      "loss": 0.0012,
      "step": 102770
    },
    {
      "epoch": 18687.272727272728,
      "grad_norm": 0.0007647379534319043,
      "learning_rate": 2.7269378552277805e-07,
      "loss": 0.0009,
      "step": 102780
    },
    {
      "epoch": 18689.090909090908,
      "grad_norm": 0.00047103865654207766,
      "learning_rate": 2.725901552653207e-07,
      "loss": 0.0012,
      "step": 102790
    },
    {
      "epoch": 18690.909090909092,
      "grad_norm": 0.20545467734336853,
      "learning_rate": 2.7248653732306314e-07,
      "loss": 0.001,
      "step": 102800
    },
    {
      "epoch": 18692.727272727272,
      "grad_norm": 0.0005130119388923049,
      "learning_rate": 2.7238293170161687e-07,
      "loss": 0.0012,
      "step": 102810
    },
    {
      "epoch": 18694.545454545456,
      "grad_norm": 0.0005178715800866485,
      "learning_rate": 2.72279338406593e-07,
      "loss": 0.0011,
      "step": 102820
    },
    {
      "epoch": 18696.363636363636,
      "grad_norm": 0.0006325229187496006,
      "learning_rate": 2.721757574436008e-07,
      "loss": 0.0011,
      "step": 102830
    },
    {
      "epoch": 18698.18181818182,
      "grad_norm": 0.15785720944404602,
      "learning_rate": 2.720721888182501e-07,
      "loss": 0.0012,
      "step": 102840
    },
    {
      "epoch": 18700.0,
      "grad_norm": 0.16471517086029053,
      "learning_rate": 2.7196863253614923e-07,
      "loss": 0.001,
      "step": 102850
    },
    {
      "epoch": 18701.81818181818,
      "grad_norm": 0.20076192915439606,
      "learning_rate": 2.718650886029066e-07,
      "loss": 0.0012,
      "step": 102860
    },
    {
      "epoch": 18703.636363636364,
      "grad_norm": 0.21764540672302246,
      "learning_rate": 2.7176155702412935e-07,
      "loss": 0.0008,
      "step": 102870
    },
    {
      "epoch": 18705.454545454544,
      "grad_norm": 0.2031124383211136,
      "learning_rate": 2.7165803780542395e-07,
      "loss": 0.0011,
      "step": 102880
    },
    {
      "epoch": 18707.272727272728,
      "grad_norm": 0.0005596733535639942,
      "learning_rate": 2.715545309523968e-07,
      "loss": 0.0011,
      "step": 102890
    },
    {
      "epoch": 18709.090909090908,
      "grad_norm": 0.26594775915145874,
      "learning_rate": 2.71451036470653e-07,
      "loss": 0.0012,
      "step": 102900
    },
    {
      "epoch": 18710.909090909092,
      "grad_norm": 0.16131870448589325,
      "learning_rate": 2.7134755436579713e-07,
      "loss": 0.001,
      "step": 102910
    },
    {
      "epoch": 18712.727272727272,
      "grad_norm": 0.1732063889503479,
      "learning_rate": 2.712440846434334e-07,
      "loss": 0.0012,
      "step": 102920
    },
    {
      "epoch": 18714.545454545456,
      "grad_norm": 0.15487805008888245,
      "learning_rate": 2.711406273091651e-07,
      "loss": 0.0009,
      "step": 102930
    },
    {
      "epoch": 18716.363636363636,
      "grad_norm": 0.00117501150816679,
      "learning_rate": 2.710371823685946e-07,
      "loss": 0.001,
      "step": 102940
    },
    {
      "epoch": 18718.18181818182,
      "grad_norm": 0.0016763070598244667,
      "learning_rate": 2.7093374982732424e-07,
      "loss": 0.001,
      "step": 102950
    },
    {
      "epoch": 18720.0,
      "grad_norm": 0.2209390252828598,
      "learning_rate": 2.7083032969095503e-07,
      "loss": 0.0012,
      "step": 102960
    },
    {
      "epoch": 18721.81818181818,
      "grad_norm": 0.0022610679734498262,
      "learning_rate": 2.7072692196508794e-07,
      "loss": 0.0012,
      "step": 102970
    },
    {
      "epoch": 18723.636363636364,
      "grad_norm": 0.20192286372184753,
      "learning_rate": 2.706235266553227e-07,
      "loss": 0.001,
      "step": 102980
    },
    {
      "epoch": 18725.454545454544,
      "grad_norm": 0.0005184942274354398,
      "learning_rate": 2.7052014376725844e-07,
      "loss": 0.001,
      "step": 102990
    },
    {
      "epoch": 18727.272727272728,
      "grad_norm": 0.011657090857625008,
      "learning_rate": 2.7041677330649406e-07,
      "loss": 0.0012,
      "step": 103000
    },
    {
      "epoch": 18727.272727272728,
      "eval_loss": 5.076968193054199,
      "eval_runtime": 0.951,
      "eval_samples_per_second": 10.515,
      "eval_steps_per_second": 5.258,
      "step": 103000
    },
    {
      "epoch": 18729.090909090908,
      "grad_norm": 0.27938732504844666,
      "learning_rate": 2.703134152786277e-07,
      "loss": 0.0011,
      "step": 103010
    },
    {
      "epoch": 18730.909090909092,
      "grad_norm": 0.21478348970413208,
      "learning_rate": 2.702100696892561e-07,
      "loss": 0.001,
      "step": 103020
    },
    {
      "epoch": 18732.727272727272,
      "grad_norm": 0.002097098855301738,
      "learning_rate": 2.701067365439762e-07,
      "loss": 0.0012,
      "step": 103030
    },
    {
      "epoch": 18734.545454545456,
      "grad_norm": 0.0006587722455151379,
      "learning_rate": 2.700034158483837e-07,
      "loss": 0.0012,
      "step": 103040
    },
    {
      "epoch": 18736.363636363636,
      "grad_norm": 0.0011259173043072224,
      "learning_rate": 2.6990010760807417e-07,
      "loss": 0.0011,
      "step": 103050
    },
    {
      "epoch": 18738.18181818182,
      "grad_norm": 0.0010943630477413535,
      "learning_rate": 2.697968118286421e-07,
      "loss": 0.0007,
      "step": 103060
    },
    {
      "epoch": 18740.0,
      "grad_norm": 0.0007538528298027813,
      "learning_rate": 2.696935285156809e-07,
      "loss": 0.0012,
      "step": 103070
    },
    {
      "epoch": 18741.81818181818,
      "grad_norm": 0.0005076708039268851,
      "learning_rate": 2.695902576747846e-07,
      "loss": 0.001,
      "step": 103080
    },
    {
      "epoch": 18743.636363636364,
      "grad_norm": 0.14106927812099457,
      "learning_rate": 2.6948699931154526e-07,
      "loss": 0.0008,
      "step": 103090
    },
    {
      "epoch": 18745.454545454544,
      "grad_norm": 0.1732732653617859,
      "learning_rate": 2.693837534315546e-07,
      "loss": 0.0013,
      "step": 103100
    },
    {
      "epoch": 18747.272727272728,
      "grad_norm": 0.0005313633009791374,
      "learning_rate": 2.6928052004040434e-07,
      "loss": 0.0009,
      "step": 103110
    },
    {
      "epoch": 18749.090909090908,
      "grad_norm": 0.2149655669927597,
      "learning_rate": 2.691772991436847e-07,
      "loss": 0.0013,
      "step": 103120
    },
    {
      "epoch": 18750.909090909092,
      "grad_norm": 0.19446329772472382,
      "learning_rate": 2.690740907469853e-07,
      "loss": 0.001,
      "step": 103130
    },
    {
      "epoch": 18752.727272727272,
      "grad_norm": 0.20467232167720795,
      "learning_rate": 2.689708948558958e-07,
      "loss": 0.0012,
      "step": 103140
    },
    {
      "epoch": 18754.545454545456,
      "grad_norm": 0.16411496698856354,
      "learning_rate": 2.688677114760043e-07,
      "loss": 0.0012,
      "step": 103150
    },
    {
      "epoch": 18756.363636363636,
      "grad_norm": 0.2145676463842392,
      "learning_rate": 2.687645406128989e-07,
      "loss": 0.0011,
      "step": 103160
    },
    {
      "epoch": 18758.18181818182,
      "grad_norm": 0.0005416026688180864,
      "learning_rate": 2.6866138227216657e-07,
      "loss": 0.0009,
      "step": 103170
    },
    {
      "epoch": 18760.0,
      "grad_norm": 0.21076764166355133,
      "learning_rate": 2.6855823645939364e-07,
      "loss": 0.0011,
      "step": 103180
    },
    {
      "epoch": 18761.81818181818,
      "grad_norm": 0.15468378365039825,
      "learning_rate": 2.6845510318016616e-07,
      "loss": 0.0009,
      "step": 103190
    },
    {
      "epoch": 18763.636363636364,
      "grad_norm": 0.16122283041477203,
      "learning_rate": 2.683519824400692e-07,
      "loss": 0.0015,
      "step": 103200
    },
    {
      "epoch": 18765.454545454544,
      "grad_norm": 0.21835985779762268,
      "learning_rate": 2.6824887424468694e-07,
      "loss": 0.0011,
      "step": 103210
    },
    {
      "epoch": 18767.272727272728,
      "grad_norm": 0.0005142230656929314,
      "learning_rate": 2.681457785996034e-07,
      "loss": 0.001,
      "step": 103220
    },
    {
      "epoch": 18769.090909090908,
      "grad_norm": 0.0013287963811308146,
      "learning_rate": 2.6804269551040135e-07,
      "loss": 0.001,
      "step": 103230
    },
    {
      "epoch": 18770.909090909092,
      "grad_norm": 0.2026827186346054,
      "learning_rate": 2.6793962498266364e-07,
      "loss": 0.0011,
      "step": 103240
    },
    {
      "epoch": 18772.727272727272,
      "grad_norm": 0.1628454327583313,
      "learning_rate": 2.6783656702197154e-07,
      "loss": 0.0013,
      "step": 103250
    },
    {
      "epoch": 18774.545454545456,
      "grad_norm": 0.22584962844848633,
      "learning_rate": 2.6773352163390616e-07,
      "loss": 0.001,
      "step": 103260
    },
    {
      "epoch": 18776.363636363636,
      "grad_norm": 0.16120734810829163,
      "learning_rate": 2.6763048882404796e-07,
      "loss": 0.0009,
      "step": 103270
    },
    {
      "epoch": 18778.18181818182,
      "grad_norm": 0.0009748698794282973,
      "learning_rate": 2.675274685979766e-07,
      "loss": 0.001,
      "step": 103280
    },
    {
      "epoch": 18780.0,
      "grad_norm": 0.0019537012558430433,
      "learning_rate": 2.674244609612708e-07,
      "loss": 0.0012,
      "step": 103290
    },
    {
      "epoch": 18781.81818181818,
      "grad_norm": 0.27250316739082336,
      "learning_rate": 2.673214659195092e-07,
      "loss": 0.0012,
      "step": 103300
    },
    {
      "epoch": 18783.636363636364,
      "grad_norm": 0.2181290239095688,
      "learning_rate": 2.6721848347826935e-07,
      "loss": 0.0008,
      "step": 103310
    },
    {
      "epoch": 18785.454545454544,
      "grad_norm": 0.16174088418483734,
      "learning_rate": 2.671155136431278e-07,
      "loss": 0.0015,
      "step": 103320
    },
    {
      "epoch": 18787.272727272728,
      "grad_norm": 0.0008540471317246556,
      "learning_rate": 2.670125564196614e-07,
      "loss": 0.0008,
      "step": 103330
    },
    {
      "epoch": 18789.090909090908,
      "grad_norm": 0.0010967615526169538,
      "learning_rate": 2.669096118134452e-07,
      "loss": 0.0011,
      "step": 103340
    },
    {
      "epoch": 18790.909090909092,
      "grad_norm": 0.15948684513568878,
      "learning_rate": 2.6680667983005445e-07,
      "loss": 0.0012,
      "step": 103350
    },
    {
      "epoch": 18792.727272727272,
      "grad_norm": 0.2029832899570465,
      "learning_rate": 2.6670376047506315e-07,
      "loss": 0.001,
      "step": 103360
    },
    {
      "epoch": 18794.545454545456,
      "grad_norm": 0.16571442782878876,
      "learning_rate": 2.666008537540447e-07,
      "loss": 0.0011,
      "step": 103370
    },
    {
      "epoch": 18796.363636363636,
      "grad_norm": 0.21404385566711426,
      "learning_rate": 2.664979596725724e-07,
      "loss": 0.001,
      "step": 103380
    },
    {
      "epoch": 18798.18181818182,
      "grad_norm": 0.20095251500606537,
      "learning_rate": 2.66395078236218e-07,
      "loss": 0.001,
      "step": 103390
    },
    {
      "epoch": 18800.0,
      "grad_norm": 0.000808750803116709,
      "learning_rate": 2.6629220945055287e-07,
      "loss": 0.001,
      "step": 103400
    },
    {
      "epoch": 18801.81818181818,
      "grad_norm": 0.2005734145641327,
      "learning_rate": 2.6618935332114816e-07,
      "loss": 0.0012,
      "step": 103410
    },
    {
      "epoch": 18803.636363636364,
      "grad_norm": 0.16536180675029755,
      "learning_rate": 2.6608650985357364e-07,
      "loss": 0.001,
      "step": 103420
    },
    {
      "epoch": 18805.454545454544,
      "grad_norm": 0.2695413827896118,
      "learning_rate": 2.6598367905339903e-07,
      "loss": 0.0012,
      "step": 103430
    },
    {
      "epoch": 18807.272727272728,
      "grad_norm": 0.0011226542992517352,
      "learning_rate": 2.658808609261928e-07,
      "loss": 0.0008,
      "step": 103440
    },
    {
      "epoch": 18809.090909090908,
      "grad_norm": 0.187072291970253,
      "learning_rate": 2.6577805547752295e-07,
      "loss": 0.0013,
      "step": 103450
    },
    {
      "epoch": 18810.909090909092,
      "grad_norm": 0.16011692583560944,
      "learning_rate": 2.6567526271295705e-07,
      "loss": 0.0011,
      "step": 103460
    },
    {
      "epoch": 18812.727272727272,
      "grad_norm": 0.0005962662980891764,
      "learning_rate": 2.655724826380617e-07,
      "loss": 0.0011,
      "step": 103470
    },
    {
      "epoch": 18814.545454545456,
      "grad_norm": 0.0012136704754084349,
      "learning_rate": 2.654697152584027e-07,
      "loss": 0.0011,
      "step": 103480
    },
    {
      "epoch": 18816.363636363636,
      "grad_norm": 0.0010255520464852452,
      "learning_rate": 2.653669605795455e-07,
      "loss": 0.0008,
      "step": 103490
    },
    {
      "epoch": 18818.18181818182,
      "grad_norm": 0.2763729393482208,
      "learning_rate": 2.6526421860705473e-07,
      "loss": 0.0013,
      "step": 103500
    },
    {
      "epoch": 18818.18181818182,
      "eval_loss": 5.026055335998535,
      "eval_runtime": 0.9543,
      "eval_samples_per_second": 10.479,
      "eval_steps_per_second": 5.239,
      "step": 103500
    },
    {
      "epoch": 18820.0,
      "grad_norm": 0.0005569813656620681,
      "learning_rate": 2.65161489346494e-07,
      "loss": 0.001,
      "step": 103510
    },
    {
      "epoch": 18821.81818181818,
      "grad_norm": 0.21664376556873322,
      "learning_rate": 2.6505877280342696e-07,
      "loss": 0.0012,
      "step": 103520
    },
    {
      "epoch": 18823.636363636364,
      "grad_norm": 0.22823260724544525,
      "learning_rate": 2.649560689834158e-07,
      "loss": 0.0008,
      "step": 103530
    },
    {
      "epoch": 18825.454545454544,
      "grad_norm": 0.17181088030338287,
      "learning_rate": 2.648533778920227e-07,
      "loss": 0.0015,
      "step": 103540
    },
    {
      "epoch": 18827.272727272728,
      "grad_norm": 0.0007564124534837902,
      "learning_rate": 2.6475069953480854e-07,
      "loss": 0.0007,
      "step": 103550
    },
    {
      "epoch": 18829.090909090908,
      "grad_norm": 0.1692289412021637,
      "learning_rate": 2.646480339173337e-07,
      "loss": 0.0012,
      "step": 103560
    },
    {
      "epoch": 18830.909090909092,
      "grad_norm": 0.16620849072933197,
      "learning_rate": 2.645453810451583e-07,
      "loss": 0.0012,
      "step": 103570
    },
    {
      "epoch": 18832.727272727272,
      "grad_norm": 0.0005912457127124071,
      "learning_rate": 2.644427409238412e-07,
      "loss": 0.0012,
      "step": 103580
    },
    {
      "epoch": 18834.545454545456,
      "grad_norm": 0.2774493098258972,
      "learning_rate": 2.643401135589407e-07,
      "loss": 0.0011,
      "step": 103590
    },
    {
      "epoch": 18836.363636363636,
      "grad_norm": 0.26350176334381104,
      "learning_rate": 2.6423749895601487e-07,
      "loss": 0.0011,
      "step": 103600
    },
    {
      "epoch": 18838.18181818182,
      "grad_norm": 0.1585545390844345,
      "learning_rate": 2.6413489712062024e-07,
      "loss": 0.0011,
      "step": 103610
    },
    {
      "epoch": 18840.0,
      "grad_norm": 0.2099272906780243,
      "learning_rate": 2.640323080583137e-07,
      "loss": 0.001,
      "step": 103620
    },
    {
      "epoch": 18841.81818181818,
      "grad_norm": 0.0005329825216904283,
      "learning_rate": 2.6392973177465037e-07,
      "loss": 0.001,
      "step": 103630
    },
    {
      "epoch": 18843.636363636364,
      "grad_norm": 0.0006102603510953486,
      "learning_rate": 2.6382716827518533e-07,
      "loss": 0.001,
      "step": 103640
    },
    {
      "epoch": 18845.454545454544,
      "grad_norm": 0.000975963135715574,
      "learning_rate": 2.6372461756547306e-07,
      "loss": 0.0013,
      "step": 103650
    },
    {
      "epoch": 18847.272727272728,
      "grad_norm": 0.2576656937599182,
      "learning_rate": 2.6362207965106684e-07,
      "loss": 0.0011,
      "step": 103660
    },
    {
      "epoch": 18849.090909090908,
      "grad_norm": 0.15804550051689148,
      "learning_rate": 2.6351955453751953e-07,
      "loss": 0.0012,
      "step": 103670
    },
    {
      "epoch": 18850.909090909092,
      "grad_norm": 0.17619778215885162,
      "learning_rate": 2.6341704223038346e-07,
      "loss": 0.0009,
      "step": 103680
    },
    {
      "epoch": 18852.727272727272,
      "grad_norm": 0.0016140799270942807,
      "learning_rate": 2.6331454273521015e-07,
      "loss": 0.0011,
      "step": 103690
    },
    {
      "epoch": 18854.545454545456,
      "grad_norm": 0.23363655805587769,
      "learning_rate": 2.6321205605755e-07,
      "loss": 0.0014,
      "step": 103700
    },
    {
      "epoch": 18856.363636363636,
      "grad_norm": 0.26250922679901123,
      "learning_rate": 2.6310958220295354e-07,
      "loss": 0.0009,
      "step": 103710
    },
    {
      "epoch": 18858.18181818182,
      "grad_norm": 0.15709207952022552,
      "learning_rate": 2.6300712117696976e-07,
      "loss": 0.0011,
      "step": 103720
    },
    {
      "epoch": 18860.0,
      "grad_norm": 0.2698376774787903,
      "learning_rate": 2.629046729851478e-07,
      "loss": 0.0012,
      "step": 103730
    },
    {
      "epoch": 18861.81818181818,
      "grad_norm": 0.15843962132930756,
      "learning_rate": 2.6280223763303543e-07,
      "loss": 0.0012,
      "step": 103740
    },
    {
      "epoch": 18863.636363636364,
      "grad_norm": 0.0004779918526764959,
      "learning_rate": 2.626998151261798e-07,
      "loss": 0.0012,
      "step": 103750
    },
    {
      "epoch": 18865.454545454544,
      "grad_norm": 0.0006593028665520251,
      "learning_rate": 2.625974054701278e-07,
      "loss": 0.0009,
      "step": 103760
    },
    {
      "epoch": 18867.272727272728,
      "grad_norm": 0.2046995460987091,
      "learning_rate": 2.6249500867042517e-07,
      "loss": 0.001,
      "step": 103770
    },
    {
      "epoch": 18869.090909090908,
      "grad_norm": 0.0006955860881134868,
      "learning_rate": 2.62392624732617e-07,
      "loss": 0.001,
      "step": 103780
    },
    {
      "epoch": 18870.909090909092,
      "grad_norm": 0.00045856620999984443,
      "learning_rate": 2.6229025366224834e-07,
      "loss": 0.0012,
      "step": 103790
    },
    {
      "epoch": 18872.727272727272,
      "grad_norm": 0.16486290097236633,
      "learning_rate": 2.621878954648623e-07,
      "loss": 0.001,
      "step": 103800
    },
    {
      "epoch": 18874.545454545456,
      "grad_norm": 0.0007132950122468174,
      "learning_rate": 2.620855501460028e-07,
      "loss": 0.0007,
      "step": 103810
    },
    {
      "epoch": 18876.363636363636,
      "grad_norm": 0.16074971854686737,
      "learning_rate": 2.6198321771121147e-07,
      "loss": 0.0015,
      "step": 103820
    },
    {
      "epoch": 18878.18181818182,
      "grad_norm": 0.2158474177122116,
      "learning_rate": 2.6188089816603036e-07,
      "loss": 0.001,
      "step": 103830
    },
    {
      "epoch": 18880.0,
      "grad_norm": 0.17045807838439941,
      "learning_rate": 2.617785915160008e-07,
      "loss": 0.001,
      "step": 103840
    },
    {
      "epoch": 18881.81818181818,
      "grad_norm": 0.193348690867424,
      "learning_rate": 2.616762977666628e-07,
      "loss": 0.001,
      "step": 103850
    },
    {
      "epoch": 18883.636363636364,
      "grad_norm": 0.20731985569000244,
      "learning_rate": 2.6157401692355596e-07,
      "loss": 0.0012,
      "step": 103860
    },
    {
      "epoch": 18885.454545454544,
      "grad_norm": 0.18838918209075928,
      "learning_rate": 2.6147174899221954e-07,
      "loss": 0.0012,
      "step": 103870
    },
    {
      "epoch": 18887.272727272728,
      "grad_norm": 0.0006454415270127356,
      "learning_rate": 2.6136949397819153e-07,
      "loss": 0.0007,
      "step": 103880
    },
    {
      "epoch": 18889.090909090908,
      "grad_norm": 0.0006547113880515099,
      "learning_rate": 2.6126725188700924e-07,
      "loss": 0.0012,
      "step": 103890
    },
    {
      "epoch": 18890.909090909092,
      "grad_norm": 0.0020238622091710567,
      "learning_rate": 2.6116502272421015e-07,
      "loss": 0.0012,
      "step": 103900
    },
    {
      "epoch": 18892.727272727272,
      "grad_norm": 0.0005028379382565618,
      "learning_rate": 2.610628064953297e-07,
      "loss": 0.0007,
      "step": 103910
    },
    {
      "epoch": 18894.545454545456,
      "grad_norm": 0.16917866468429565,
      "learning_rate": 2.609606032059039e-07,
      "loss": 0.0015,
      "step": 103920
    },
    {
      "epoch": 18896.363636363636,
      "grad_norm": 0.0015180185437202454,
      "learning_rate": 2.6085841286146727e-07,
      "loss": 0.001,
      "step": 103930
    },
    {
      "epoch": 18898.18181818182,
      "grad_norm": 0.001023684861138463,
      "learning_rate": 2.607562354675535e-07,
      "loss": 0.0009,
      "step": 103940
    },
    {
      "epoch": 18900.0,
      "grad_norm": 0.21880455315113068,
      "learning_rate": 2.606540710296966e-07,
      "loss": 0.0012,
      "step": 103950
    },
    {
      "epoch": 18901.81818181818,
      "grad_norm": 0.21992769837379456,
      "learning_rate": 2.605519195534288e-07,
      "loss": 0.0012,
      "step": 103960
    },
    {
      "epoch": 18903.636363636364,
      "grad_norm": 0.0005464121932163835,
      "learning_rate": 2.604497810442819e-07,
      "loss": 0.001,
      "step": 103970
    },
    {
      "epoch": 18905.454545454544,
      "grad_norm": 0.0009974119020625949,
      "learning_rate": 2.6034765550778745e-07,
      "loss": 0.0012,
      "step": 103980
    },
    {
      "epoch": 18907.272727272728,
      "grad_norm": 0.26177099347114563,
      "learning_rate": 2.6024554294947575e-07,
      "loss": 0.001,
      "step": 103990
    },
    {
      "epoch": 18909.090909090908,
      "grad_norm": 0.0007591704488731921,
      "learning_rate": 2.6014344337487703e-07,
      "loss": 0.0009,
      "step": 104000
    },
    {
      "epoch": 18909.090909090908,
      "eval_loss": 5.1430888175964355,
      "eval_runtime": 0.9499,
      "eval_samples_per_second": 10.528,
      "eval_steps_per_second": 5.264,
      "step": 104000
    },
    {
      "epoch": 18910.909090909092,
      "grad_norm": 0.15622009336948395,
      "learning_rate": 2.6004135678951977e-07,
      "loss": 0.0011,
      "step": 104010
    },
    {
      "epoch": 18912.727272727272,
      "grad_norm": 0.0007385098142549396,
      "learning_rate": 2.5993928319893265e-07,
      "loss": 0.0012,
      "step": 104020
    },
    {
      "epoch": 18914.545454545456,
      "grad_norm": 0.20248550176620483,
      "learning_rate": 2.598372226086437e-07,
      "loss": 0.001,
      "step": 104030
    },
    {
      "epoch": 18916.363636363636,
      "grad_norm": 0.0005946697201579809,
      "learning_rate": 2.5973517502417966e-07,
      "loss": 0.0012,
      "step": 104040
    },
    {
      "epoch": 18918.18181818182,
      "grad_norm": 0.20077645778656006,
      "learning_rate": 2.5963314045106665e-07,
      "loss": 0.0011,
      "step": 104050
    },
    {
      "epoch": 18920.0,
      "grad_norm": 0.1655431091785431,
      "learning_rate": 2.595311188948307e-07,
      "loss": 0.001,
      "step": 104060
    },
    {
      "epoch": 18921.81818181818,
      "grad_norm": 0.0005524118314497173,
      "learning_rate": 2.5942911036099654e-07,
      "loss": 0.0012,
      "step": 104070
    },
    {
      "epoch": 18923.636363636364,
      "grad_norm": 0.2071029394865036,
      "learning_rate": 2.5932711485508817e-07,
      "loss": 0.0007,
      "step": 104080
    },
    {
      "epoch": 18925.454545454544,
      "grad_norm": 0.0005800159415230155,
      "learning_rate": 2.5922513238262913e-07,
      "loss": 0.0011,
      "step": 104090
    },
    {
      "epoch": 18927.272727272728,
      "grad_norm": 0.0006938286242075264,
      "learning_rate": 2.591231629491423e-07,
      "loss": 0.0012,
      "step": 104100
    },
    {
      "epoch": 18929.090909090908,
      "grad_norm": 0.0005338774644769728,
      "learning_rate": 2.5902120656015e-07,
      "loss": 0.0012,
      "step": 104110
    },
    {
      "epoch": 18930.909090909092,
      "grad_norm": 0.006348034366965294,
      "learning_rate": 2.589192632211731e-07,
      "loss": 0.0012,
      "step": 104120
    },
    {
      "epoch": 18932.727272727272,
      "grad_norm": 0.27855220437049866,
      "learning_rate": 2.588173329377324e-07,
      "loss": 0.0014,
      "step": 104130
    },
    {
      "epoch": 18934.545454545456,
      "grad_norm": 0.0021162242628633976,
      "learning_rate": 2.5871541571534814e-07,
      "loss": 0.0011,
      "step": 104140
    },
    {
      "epoch": 18936.363636363636,
      "grad_norm": 0.000478374888189137,
      "learning_rate": 2.586135115595394e-07,
      "loss": 0.0007,
      "step": 104150
    },
    {
      "epoch": 18938.18181818182,
      "grad_norm": 0.00031162367668002844,
      "learning_rate": 2.5851162047582473e-07,
      "loss": 0.0012,
      "step": 104160
    },
    {
      "epoch": 18940.0,
      "grad_norm": 0.2507694661617279,
      "learning_rate": 2.584097424697217e-07,
      "loss": 0.0012,
      "step": 104170
    },
    {
      "epoch": 18941.81818181818,
      "grad_norm": 0.0017192679224535823,
      "learning_rate": 2.5830787754674766e-07,
      "loss": 0.0012,
      "step": 104180
    },
    {
      "epoch": 18943.636363636364,
      "grad_norm": 0.20534436404705048,
      "learning_rate": 2.5820602571241947e-07,
      "loss": 0.0011,
      "step": 104190
    },
    {
      "epoch": 18945.454545454544,
      "grad_norm": 0.20582227408885956,
      "learning_rate": 2.581041869722519e-07,
      "loss": 0.0009,
      "step": 104200
    },
    {
      "epoch": 18947.272727272728,
      "grad_norm": 0.0006020591827109456,
      "learning_rate": 2.5800236133176046e-07,
      "loss": 0.0012,
      "step": 104210
    },
    {
      "epoch": 18949.090909090908,
      "grad_norm": 0.16021935641765594,
      "learning_rate": 2.579005487964596e-07,
      "loss": 0.0012,
      "step": 104220
    },
    {
      "epoch": 18950.909090909092,
      "grad_norm": 0.04600570350885391,
      "learning_rate": 2.5779874937186277e-07,
      "loss": 0.0012,
      "step": 104230
    },
    {
      "epoch": 18952.727272727272,
      "grad_norm": 0.21743419766426086,
      "learning_rate": 2.5769696306348256e-07,
      "loss": 0.0012,
      "step": 104240
    },
    {
      "epoch": 18954.545454545456,
      "grad_norm": 0.0005351131549105048,
      "learning_rate": 2.575951898768315e-07,
      "loss": 0.0012,
      "step": 104250
    },
    {
      "epoch": 18956.363636363636,
      "grad_norm": 0.0005593547830358148,
      "learning_rate": 2.574934298174209e-07,
      "loss": 0.0009,
      "step": 104260
    },
    {
      "epoch": 18958.18181818182,
      "grad_norm": 0.2824091613292694,
      "learning_rate": 2.5739168289076146e-07,
      "loss": 0.0012,
      "step": 104270
    },
    {
      "epoch": 18960.0,
      "grad_norm": 0.0004892270662821829,
      "learning_rate": 2.57289949102363e-07,
      "loss": 0.0009,
      "step": 104280
    },
    {
      "epoch": 18961.81818181818,
      "grad_norm": 0.26341256499290466,
      "learning_rate": 2.5718822845773514e-07,
      "loss": 0.0012,
      "step": 104290
    },
    {
      "epoch": 18963.636363636364,
      "grad_norm": 0.16531984508037567,
      "learning_rate": 2.570865209623867e-07,
      "loss": 0.0009,
      "step": 104300
    },
    {
      "epoch": 18965.454545454544,
      "grad_norm": 0.16675852239131927,
      "learning_rate": 2.5698482662182494e-07,
      "loss": 0.0013,
      "step": 104310
    },
    {
      "epoch": 18967.272727272728,
      "grad_norm": 0.0009410686325281858,
      "learning_rate": 2.568831454415573e-07,
      "loss": 0.0009,
      "step": 104320
    },
    {
      "epoch": 18969.090909090908,
      "grad_norm": 0.19870422780513763,
      "learning_rate": 2.5678147742709054e-07,
      "loss": 0.0013,
      "step": 104330
    },
    {
      "epoch": 18970.909090909092,
      "grad_norm": 0.0004868767864536494,
      "learning_rate": 2.566798225839301e-07,
      "loss": 0.0012,
      "step": 104340
    },
    {
      "epoch": 18972.727272727272,
      "grad_norm": 0.001027138321660459,
      "learning_rate": 2.5657818091758116e-07,
      "loss": 0.0012,
      "step": 104350
    },
    {
      "epoch": 18974.545454545456,
      "grad_norm": 0.15843690931797028,
      "learning_rate": 2.5647655243354774e-07,
      "loss": 0.0009,
      "step": 104360
    },
    {
      "epoch": 18976.363636363636,
      "grad_norm": 0.0009406651952303946,
      "learning_rate": 2.563749371373337e-07,
      "loss": 0.0012,
      "step": 104370
    },
    {
      "epoch": 18978.18181818182,
      "grad_norm": 0.1647474616765976,
      "learning_rate": 2.5627333503444235e-07,
      "loss": 0.001,
      "step": 104380
    },
    {
      "epoch": 18980.0,
      "grad_norm": 0.01758444495499134,
      "learning_rate": 2.56171746130375e-07,
      "loss": 0.001,
      "step": 104390
    },
    {
      "epoch": 18981.81818181818,
      "grad_norm": 0.0015654907329007983,
      "learning_rate": 2.5607017043063353e-07,
      "loss": 0.001,
      "step": 104400
    },
    {
      "epoch": 18983.636363636364,
      "grad_norm": 0.20512329041957855,
      "learning_rate": 2.5596860794071893e-07,
      "loss": 0.0011,
      "step": 104410
    },
    {
      "epoch": 18985.454545454544,
      "grad_norm": 0.0010301374131813645,
      "learning_rate": 2.558670586661311e-07,
      "loss": 0.0009,
      "step": 104420
    },
    {
      "epoch": 18987.272727272728,
      "grad_norm": 0.0006544953212141991,
      "learning_rate": 2.557655226123693e-07,
      "loss": 0.0013,
      "step": 104430
    },
    {
      "epoch": 18989.090909090908,
      "grad_norm": 0.21593791246414185,
      "learning_rate": 2.5566399978493183e-07,
      "loss": 0.001,
      "step": 104440
    },
    {
      "epoch": 18990.909090909092,
      "grad_norm": 0.0009348192252218723,
      "learning_rate": 2.555624901893171e-07,
      "loss": 0.0011,
      "step": 104450
    },
    {
      "epoch": 18992.727272727272,
      "grad_norm": 0.011569928377866745,
      "learning_rate": 2.5546099383102205e-07,
      "loss": 0.001,
      "step": 104460
    },
    {
      "epoch": 18994.545454545456,
      "grad_norm": 0.01781482994556427,
      "learning_rate": 2.553595107155429e-07,
      "loss": 0.0012,
      "step": 104470
    },
    {
      "epoch": 18996.363636363636,
      "grad_norm": 0.007103440351784229,
      "learning_rate": 2.5525804084837566e-07,
      "loss": 0.0012,
      "step": 104480
    },
    {
      "epoch": 18998.18181818182,
      "grad_norm": 0.1672622263431549,
      "learning_rate": 2.5515658423501566e-07,
      "loss": 0.0009,
      "step": 104490
    },
    {
      "epoch": 19000.0,
      "grad_norm": 0.16383935511112213,
      "learning_rate": 2.550551408809565e-07,
      "loss": 0.001,
      "step": 104500
    },
    {
      "epoch": 19000.0,
      "eval_loss": 5.093598365783691,
      "eval_runtime": 0.9541,
      "eval_samples_per_second": 10.481,
      "eval_steps_per_second": 5.241,
      "step": 104500
    },
    {
      "epoch": 19001.81818181818,
      "grad_norm": 0.00046031843521632254,
      "learning_rate": 2.5495371079169216e-07,
      "loss": 0.001,
      "step": 104510
    },
    {
      "epoch": 19003.636363636364,
      "grad_norm": 0.2704082429409027,
      "learning_rate": 2.5485229397271563e-07,
      "loss": 0.0012,
      "step": 104520
    },
    {
      "epoch": 19005.454545454544,
      "grad_norm": 0.21629785001277924,
      "learning_rate": 2.547508904295189e-07,
      "loss": 0.001,
      "step": 104530
    },
    {
      "epoch": 19007.272727272728,
      "grad_norm": 0.25749507546424866,
      "learning_rate": 2.5464950016759336e-07,
      "loss": 0.0012,
      "step": 104540
    },
    {
      "epoch": 19009.090909090908,
      "grad_norm": 0.0005121041322126985,
      "learning_rate": 2.5454812319242956e-07,
      "loss": 0.0009,
      "step": 104550
    },
    {
      "epoch": 19010.909090909092,
      "grad_norm": 0.0006999223842285573,
      "learning_rate": 2.5444675950951774e-07,
      "loss": 0.0012,
      "step": 104560
    },
    {
      "epoch": 19012.727272727272,
      "grad_norm": 0.1732800006866455,
      "learning_rate": 2.543454091243474e-07,
      "loss": 0.0013,
      "step": 104570
    },
    {
      "epoch": 19014.545454545456,
      "grad_norm": 0.0008795142057351768,
      "learning_rate": 2.5424407204240647e-07,
      "loss": 0.0006,
      "step": 104580
    },
    {
      "epoch": 19016.363636363636,
      "grad_norm": 0.17306411266326904,
      "learning_rate": 2.5414274826918314e-07,
      "loss": 0.0013,
      "step": 104590
    },
    {
      "epoch": 19018.18181818182,
      "grad_norm": 0.00044127184082753956,
      "learning_rate": 2.5404143781016466e-07,
      "loss": 0.001,
      "step": 104600
    },
    {
      "epoch": 19020.0,
      "grad_norm": 0.16234923899173737,
      "learning_rate": 2.539401406708372e-07,
      "loss": 0.0012,
      "step": 104610
    },
    {
      "epoch": 19021.81818181818,
      "grad_norm": 0.1724122315645218,
      "learning_rate": 2.538388568566866e-07,
      "loss": 0.0012,
      "step": 104620
    },
    {
      "epoch": 19023.636363636364,
      "grad_norm": 0.011429035104811192,
      "learning_rate": 2.5373758637319734e-07,
      "loss": 0.0011,
      "step": 104630
    },
    {
      "epoch": 19025.454545454544,
      "grad_norm": 0.0005160551518201828,
      "learning_rate": 2.5363632922585425e-07,
      "loss": 0.0008,
      "step": 104640
    },
    {
      "epoch": 19027.272727272728,
      "grad_norm": 0.16676528751850128,
      "learning_rate": 2.5353508542014056e-07,
      "loss": 0.0013,
      "step": 104650
    },
    {
      "epoch": 19029.090909090908,
      "grad_norm": 0.0005449588061310351,
      "learning_rate": 2.5343385496153885e-07,
      "loss": 0.0011,
      "step": 104660
    },
    {
      "epoch": 19030.909090909092,
      "grad_norm": 0.26582831144332886,
      "learning_rate": 2.533326378555314e-07,
      "loss": 0.0009,
      "step": 104670
    },
    {
      "epoch": 19032.727272727272,
      "grad_norm": 0.000809068325906992,
      "learning_rate": 2.532314341075998e-07,
      "loss": 0.0012,
      "step": 104680
    },
    {
      "epoch": 19034.545454545456,
      "grad_norm": 0.01165818702429533,
      "learning_rate": 2.531302437232241e-07,
      "loss": 0.0015,
      "step": 104690
    },
    {
      "epoch": 19036.363636363636,
      "grad_norm": 0.17381753027439117,
      "learning_rate": 2.530290667078846e-07,
      "loss": 0.0009,
      "step": 104700
    },
    {
      "epoch": 19038.18181818182,
      "grad_norm": 0.21391580998897552,
      "learning_rate": 2.529279030670601e-07,
      "loss": 0.001,
      "step": 104710
    },
    {
      "epoch": 19040.0,
      "grad_norm": 0.0010642727138474584,
      "learning_rate": 2.5282675280622956e-07,
      "loss": 0.001,
      "step": 104720
    },
    {
      "epoch": 19041.81818181818,
      "grad_norm": 0.2648541331291199,
      "learning_rate": 2.527256159308703e-07,
      "loss": 0.001,
      "step": 104730
    },
    {
      "epoch": 19043.636363636364,
      "grad_norm": 0.17460371553897858,
      "learning_rate": 2.5262449244645925e-07,
      "loss": 0.0009,
      "step": 104740
    },
    {
      "epoch": 19045.454545454544,
      "grad_norm": 0.0004419568576849997,
      "learning_rate": 2.52523382358473e-07,
      "loss": 0.0015,
      "step": 104750
    },
    {
      "epoch": 19047.272727272728,
      "grad_norm": 0.0010877989698201418,
      "learning_rate": 2.524222856723869e-07,
      "loss": 0.0008,
      "step": 104760
    },
    {
      "epoch": 19049.090909090908,
      "grad_norm": 0.0007854760624468327,
      "learning_rate": 2.5232120239367555e-07,
      "loss": 0.0012,
      "step": 104770
    },
    {
      "epoch": 19050.909090909092,
      "grad_norm": 0.0008138041011989117,
      "learning_rate": 2.522201325278135e-07,
      "loss": 0.0012,
      "step": 104780
    },
    {
      "epoch": 19052.727272727272,
      "grad_norm": 0.2157985270023346,
      "learning_rate": 2.521190760802736e-07,
      "loss": 0.001,
      "step": 104790
    },
    {
      "epoch": 19054.545454545456,
      "grad_norm": 0.0006076389690861106,
      "learning_rate": 2.52018033056529e-07,
      "loss": 0.0009,
      "step": 104800
    },
    {
      "epoch": 19056.363636363636,
      "grad_norm": 0.0007260199054144323,
      "learning_rate": 2.519170034620513e-07,
      "loss": 0.001,
      "step": 104810
    },
    {
      "epoch": 19058.18181818182,
      "grad_norm": 0.17626486718654633,
      "learning_rate": 2.5181598730231155e-07,
      "loss": 0.0013,
      "step": 104820
    },
    {
      "epoch": 19060.0,
      "grad_norm": 0.17056037485599518,
      "learning_rate": 2.5171498458278063e-07,
      "loss": 0.001,
      "step": 104830
    },
    {
      "epoch": 19061.81818181818,
      "grad_norm": 0.2180492877960205,
      "learning_rate": 2.516139953089279e-07,
      "loss": 0.0012,
      "step": 104840
    },
    {
      "epoch": 19063.636363636364,
      "grad_norm": 0.0008335894672200084,
      "learning_rate": 2.515130194862223e-07,
      "loss": 0.0009,
      "step": 104850
    },
    {
      "epoch": 19065.454545454544,
      "grad_norm": 0.16572853922843933,
      "learning_rate": 2.514120571201323e-07,
      "loss": 0.0014,
      "step": 104860
    },
    {
      "epoch": 19067.272727272728,
      "grad_norm": 0.0069167655892670155,
      "learning_rate": 2.513111082161258e-07,
      "loss": 0.0012,
      "step": 104870
    },
    {
      "epoch": 19069.090909090908,
      "grad_norm": 0.0003881768207065761,
      "learning_rate": 2.512101727796687e-07,
      "loss": 0.0009,
      "step": 104880
    },
    {
      "epoch": 19070.909090909092,
      "grad_norm": 0.21819160878658295,
      "learning_rate": 2.511092508162279e-07,
      "loss": 0.0011,
      "step": 104890
    },
    {
      "epoch": 19072.727272727272,
      "grad_norm": 0.0007230817573145032,
      "learning_rate": 2.510083423312682e-07,
      "loss": 0.001,
      "step": 104900
    },
    {
      "epoch": 19074.545454545456,
      "grad_norm": 0.0007858846802264452,
      "learning_rate": 2.509074473302546e-07,
      "loss": 0.001,
      "step": 104910
    },
    {
      "epoch": 19076.363636363636,
      "grad_norm": 0.2738316059112549,
      "learning_rate": 2.5080656581865087e-07,
      "loss": 0.0015,
      "step": 104920
    },
    {
      "epoch": 19078.18181818182,
      "grad_norm": 0.27373945713043213,
      "learning_rate": 2.5070569780192e-07,
      "loss": 0.0009,
      "step": 104930
    },
    {
      "epoch": 19080.0,
      "grad_norm": 0.0017550407210364938,
      "learning_rate": 2.5060484328552467e-07,
      "loss": 0.0009,
      "step": 104940
    },
    {
      "epoch": 19081.81818181818,
      "grad_norm": 0.21675410866737366,
      "learning_rate": 2.5050400227492645e-07,
      "loss": 0.0012,
      "step": 104950
    },
    {
      "epoch": 19083.636363636364,
      "grad_norm": 0.2162708342075348,
      "learning_rate": 2.5040317477558613e-07,
      "loss": 0.0011,
      "step": 104960
    },
    {
      "epoch": 19085.454545454544,
      "grad_norm": 0.20528051257133484,
      "learning_rate": 2.503023607929644e-07,
      "loss": 0.0009,
      "step": 104970
    },
    {
      "epoch": 19087.272727272728,
      "grad_norm": 0.0006075534038245678,
      "learning_rate": 2.502015603325202e-07,
      "loss": 0.0012,
      "step": 104980
    },
    {
      "epoch": 19089.090909090908,
      "grad_norm": 0.0037042899057269096,
      "learning_rate": 2.5010077339971283e-07,
      "loss": 0.001,
      "step": 104990
    },
    {
      "epoch": 19090.909090909092,
      "grad_norm": 0.17925117909908295,
      "learning_rate": 2.500000000000001e-07,
      "loss": 0.0011,
      "step": 105000
    },
    {
      "epoch": 19090.909090909092,
      "eval_loss": 5.097172737121582,
      "eval_runtime": 0.9514,
      "eval_samples_per_second": 10.51,
      "eval_steps_per_second": 5.255,
      "step": 105000
    },
    {
      "epoch": 19092.727272727272,
      "grad_norm": 0.0010199376847594976,
      "learning_rate": 2.4989924013883905e-07,
      "loss": 0.0009,
      "step": 105010
    },
    {
      "epoch": 19094.545454545456,
      "grad_norm": 0.0005174133693799376,
      "learning_rate": 2.4979849382168677e-07,
      "loss": 0.0009,
      "step": 105020
    },
    {
      "epoch": 19096.363636363636,
      "grad_norm": 0.20518545806407928,
      "learning_rate": 2.4969776105399877e-07,
      "loss": 0.0015,
      "step": 105030
    },
    {
      "epoch": 19098.18181818182,
      "grad_norm": 0.0015346335712820292,
      "learning_rate": 2.4959704184123005e-07,
      "loss": 0.0009,
      "step": 105040
    },
    {
      "epoch": 19100.0,
      "grad_norm": 0.0004942212835885584,
      "learning_rate": 2.494963361888353e-07,
      "loss": 0.0012,
      "step": 105050
    },
    {
      "epoch": 19101.81818181818,
      "grad_norm": 0.0006336338701657951,
      "learning_rate": 2.49395644102268e-07,
      "loss": 0.0011,
      "step": 105060
    },
    {
      "epoch": 19103.636363636364,
      "grad_norm": 0.17815497517585754,
      "learning_rate": 2.4929496558698085e-07,
      "loss": 0.0012,
      "step": 105070
    },
    {
      "epoch": 19105.454545454544,
      "grad_norm": 0.0007215455989353359,
      "learning_rate": 2.4919430064842644e-07,
      "loss": 0.0012,
      "step": 105080
    },
    {
      "epoch": 19107.272727272728,
      "grad_norm": 0.1700439453125,
      "learning_rate": 2.490936492920557e-07,
      "loss": 0.0009,
      "step": 105090
    },
    {
      "epoch": 19109.090909090908,
      "grad_norm": 0.21327731013298035,
      "learning_rate": 2.4899301152331985e-07,
      "loss": 0.0012,
      "step": 105100
    },
    {
      "epoch": 19110.909090909092,
      "grad_norm": 0.0005772538715973496,
      "learning_rate": 2.488923873476686e-07,
      "loss": 0.001,
      "step": 105110
    },
    {
      "epoch": 19112.727272727272,
      "grad_norm": 0.16782408952713013,
      "learning_rate": 2.48791776770551e-07,
      "loss": 0.0012,
      "step": 105120
    },
    {
      "epoch": 19114.545454545456,
      "grad_norm": 0.2050427943468094,
      "learning_rate": 2.4869117979741585e-07,
      "loss": 0.0011,
      "step": 105130
    },
    {
      "epoch": 19116.363636363636,
      "grad_norm": 0.20235762000083923,
      "learning_rate": 2.485905964337107e-07,
      "loss": 0.0009,
      "step": 105140
    },
    {
      "epoch": 19118.18181818182,
      "grad_norm": 0.0006023002788424492,
      "learning_rate": 2.484900266848824e-07,
      "loss": 0.001,
      "step": 105150
    },
    {
      "epoch": 19120.0,
      "grad_norm": 0.0006950528477318585,
      "learning_rate": 2.4838947055637777e-07,
      "loss": 0.0012,
      "step": 105160
    },
    {
      "epoch": 19121.81818181818,
      "grad_norm": 0.0014291093684732914,
      "learning_rate": 2.482889280536418e-07,
      "loss": 0.0013,
      "step": 105170
    },
    {
      "epoch": 19123.636363636364,
      "grad_norm": 0.000961463782005012,
      "learning_rate": 2.4818839918211956e-07,
      "loss": 0.0011,
      "step": 105180
    },
    {
      "epoch": 19125.454545454544,
      "grad_norm": 0.16746988892555237,
      "learning_rate": 2.4808788394725515e-07,
      "loss": 0.0009,
      "step": 105190
    },
    {
      "epoch": 19127.272727272728,
      "grad_norm": 0.2627973258495331,
      "learning_rate": 2.479873823544916e-07,
      "loss": 0.0017,
      "step": 105200
    },
    {
      "epoch": 19129.090909090908,
      "grad_norm": 0.0011906163999810815,
      "learning_rate": 2.478868944092719e-07,
      "loss": 0.0008,
      "step": 105210
    },
    {
      "epoch": 19130.909090909092,
      "grad_norm": 0.0007151069585233927,
      "learning_rate": 2.477864201170377e-07,
      "loss": 0.0012,
      "step": 105220
    },
    {
      "epoch": 19132.727272727272,
      "grad_norm": 0.2281852513551712,
      "learning_rate": 2.4768595948322983e-07,
      "loss": 0.0012,
      "step": 105230
    },
    {
      "epoch": 19134.545454545456,
      "grad_norm": 0.0006620338535867631,
      "learning_rate": 2.475855125132892e-07,
      "loss": 0.0012,
      "step": 105240
    },
    {
      "epoch": 19136.363636363636,
      "grad_norm": 0.2654600143432617,
      "learning_rate": 2.474850792126551e-07,
      "loss": 0.0012,
      "step": 105250
    },
    {
      "epoch": 19138.18181818182,
      "grad_norm": 0.19750115275382996,
      "learning_rate": 2.473846595867663e-07,
      "loss": 0.0009,
      "step": 105260
    },
    {
      "epoch": 19140.0,
      "grad_norm": 0.25603151321411133,
      "learning_rate": 2.472842536410613e-07,
      "loss": 0.001,
      "step": 105270
    },
    {
      "epoch": 19141.81818181818,
      "grad_norm": 0.19954244792461395,
      "learning_rate": 2.4718386138097715e-07,
      "loss": 0.001,
      "step": 105280
    },
    {
      "epoch": 19143.636363636364,
      "grad_norm": 0.20340141654014587,
      "learning_rate": 2.4708348281195085e-07,
      "loss": 0.001,
      "step": 105290
    },
    {
      "epoch": 19145.454545454544,
      "grad_norm": 0.0006975468713790178,
      "learning_rate": 2.469831179394182e-07,
      "loss": 0.0012,
      "step": 105300
    },
    {
      "epoch": 19147.272727272728,
      "grad_norm": 0.0007754648686386645,
      "learning_rate": 2.468827667688141e-07,
      "loss": 0.0011,
      "step": 105310
    },
    {
      "epoch": 19149.090909090908,
      "grad_norm": 0.21316556632518768,
      "learning_rate": 2.467824293055734e-07,
      "loss": 0.0012,
      "step": 105320
    },
    {
      "epoch": 19150.909090909092,
      "grad_norm": 0.1708541065454483,
      "learning_rate": 2.466821055551297e-07,
      "loss": 0.0012,
      "step": 105330
    },
    {
      "epoch": 19152.727272727272,
      "grad_norm": 0.0020301660988479853,
      "learning_rate": 2.465817955229156e-07,
      "loss": 0.0008,
      "step": 105340
    },
    {
      "epoch": 19154.545454545456,
      "grad_norm": 0.017730409279465675,
      "learning_rate": 2.464814992143638e-07,
      "loss": 0.0013,
      "step": 105350
    },
    {
      "epoch": 19156.363636363636,
      "grad_norm": 0.19381216168403625,
      "learning_rate": 2.4638121663490543e-07,
      "loss": 0.0011,
      "step": 105360
    },
    {
      "epoch": 19158.18181818182,
      "grad_norm": 0.0006796797388233244,
      "learning_rate": 2.4628094778997146e-07,
      "loss": 0.0011,
      "step": 105370
    },
    {
      "epoch": 19160.0,
      "grad_norm": 0.18633824586868286,
      "learning_rate": 2.461806926849917e-07,
      "loss": 0.0012,
      "step": 105380
    },
    {
      "epoch": 19161.81818181818,
      "grad_norm": 0.0004891867283731699,
      "learning_rate": 2.4608045132539537e-07,
      "loss": 0.0011,
      "step": 105390
    },
    {
      "epoch": 19163.636363636364,
      "grad_norm": 0.0005335428286343813,
      "learning_rate": 2.459802237166111e-07,
      "loss": 0.0012,
      "step": 105400
    },
    {
      "epoch": 19165.454545454544,
      "grad_norm": 0.0010808700462803245,
      "learning_rate": 2.458800098640666e-07,
      "loss": 0.0012,
      "step": 105410
    },
    {
      "epoch": 19167.272727272728,
      "grad_norm": 0.16595427691936493,
      "learning_rate": 2.4577980977318863e-07,
      "loss": 0.0008,
      "step": 105420
    },
    {
      "epoch": 19169.090909090908,
      "grad_norm": 0.00047199492109939456,
      "learning_rate": 2.4567962344940383e-07,
      "loss": 0.001,
      "step": 105430
    },
    {
      "epoch": 19170.909090909092,
      "grad_norm": 0.16877992451190948,
      "learning_rate": 2.4557945089813754e-07,
      "loss": 0.0012,
      "step": 105440
    },
    {
      "epoch": 19172.727272727272,
      "grad_norm": 0.0007784928311593831,
      "learning_rate": 2.4547929212481435e-07,
      "loss": 0.001,
      "step": 105450
    },
    {
      "epoch": 19174.545454545456,
      "grad_norm": 0.001759242033585906,
      "learning_rate": 2.4537914713485857e-07,
      "loss": 0.0011,
      "step": 105460
    },
    {
      "epoch": 19176.363636363636,
      "grad_norm": 0.2284824252128601,
      "learning_rate": 2.452790159336932e-07,
      "loss": 0.0013,
      "step": 105470
    },
    {
      "epoch": 19178.18181818182,
      "grad_norm": 0.01801498793065548,
      "learning_rate": 2.451788985267411e-07,
      "loss": 0.0012,
      "step": 105480
    },
    {
      "epoch": 19180.0,
      "grad_norm": 0.21586909890174866,
      "learning_rate": 2.4507879491942386e-07,
      "loss": 0.0009,
      "step": 105490
    },
    {
      "epoch": 19181.81818181818,
      "grad_norm": 0.0009443640010431409,
      "learning_rate": 2.4497870511716237e-07,
      "loss": 0.001,
      "step": 105500
    },
    {
      "epoch": 19181.81818181818,
      "eval_loss": 5.163138389587402,
      "eval_runtime": 0.9517,
      "eval_samples_per_second": 10.507,
      "eval_steps_per_second": 5.254,
      "step": 105500
    },
    {
      "epoch": 19183.636363636364,
      "grad_norm": 0.20657747983932495,
      "learning_rate": 2.4487862912537714e-07,
      "loss": 0.0012,
      "step": 105510
    },
    {
      "epoch": 19185.454545454544,
      "grad_norm": 0.16514913737773895,
      "learning_rate": 2.4477856694948766e-07,
      "loss": 0.0012,
      "step": 105520
    },
    {
      "epoch": 19187.272727272728,
      "grad_norm": 0.0007901712669990957,
      "learning_rate": 2.4467851859491255e-07,
      "loss": 0.0009,
      "step": 105530
    },
    {
      "epoch": 19189.090909090908,
      "grad_norm": 0.15992745757102966,
      "learning_rate": 2.445784840670701e-07,
      "loss": 0.0012,
      "step": 105540
    },
    {
      "epoch": 19190.909090909092,
      "grad_norm": 0.0030036410316824913,
      "learning_rate": 2.444784633713773e-07,
      "loss": 0.0012,
      "step": 105550
    },
    {
      "epoch": 19192.727272727272,
      "grad_norm": 0.1625959426164627,
      "learning_rate": 2.443784565132511e-07,
      "loss": 0.0013,
      "step": 105560
    },
    {
      "epoch": 19194.545454545456,
      "grad_norm": 0.0011575977550819516,
      "learning_rate": 2.4427846349810704e-07,
      "loss": 0.0007,
      "step": 105570
    },
    {
      "epoch": 19196.363636363636,
      "grad_norm": 0.00034667839645408094,
      "learning_rate": 2.4417848433136e-07,
      "loss": 0.0012,
      "step": 105580
    },
    {
      "epoch": 19198.18181818182,
      "grad_norm": 0.000943151128012687,
      "learning_rate": 2.4407851901842463e-07,
      "loss": 0.001,
      "step": 105590
    },
    {
      "epoch": 19200.0,
      "grad_norm": 0.25645366311073303,
      "learning_rate": 2.439785675647143e-07,
      "loss": 0.0012,
      "step": 105600
    },
    {
      "epoch": 19201.81818181818,
      "grad_norm": 0.18476064503192902,
      "learning_rate": 2.438786299756416e-07,
      "loss": 0.0011,
      "step": 105610
    },
    {
      "epoch": 19203.636363636364,
      "grad_norm": 0.0005325080128386617,
      "learning_rate": 2.43778706256619e-07,
      "loss": 0.001,
      "step": 105620
    },
    {
      "epoch": 19205.454545454544,
      "grad_norm": 0.16163402795791626,
      "learning_rate": 2.4367879641305757e-07,
      "loss": 0.0011,
      "step": 105630
    },
    {
      "epoch": 19207.272727272728,
      "grad_norm": 0.001076787244528532,
      "learning_rate": 2.435789004503676e-07,
      "loss": 0.0009,
      "step": 105640
    },
    {
      "epoch": 19209.090909090908,
      "grad_norm": 0.21441099047660828,
      "learning_rate": 2.434790183739593e-07,
      "loss": 0.0013,
      "step": 105650
    },
    {
      "epoch": 19210.909090909092,
      "grad_norm": 0.17128212749958038,
      "learning_rate": 2.4337915018924144e-07,
      "loss": 0.001,
      "step": 105660
    },
    {
      "epoch": 19212.727272727272,
      "grad_norm": 0.15716615319252014,
      "learning_rate": 2.432792959016227e-07,
      "loss": 0.0009,
      "step": 105670
    },
    {
      "epoch": 19214.545454545456,
      "grad_norm": 0.2560672461986542,
      "learning_rate": 2.431794555165099e-07,
      "loss": 0.0013,
      "step": 105680
    },
    {
      "epoch": 19216.363636363636,
      "grad_norm": 0.254838764667511,
      "learning_rate": 2.430796290393102e-07,
      "loss": 0.0012,
      "step": 105690
    },
    {
      "epoch": 19218.18181818182,
      "grad_norm": 0.017622120678424835,
      "learning_rate": 2.4297981647542984e-07,
      "loss": 0.0011,
      "step": 105700
    },
    {
      "epoch": 19220.0,
      "grad_norm": 0.16604989767074585,
      "learning_rate": 2.4288001783027395e-07,
      "loss": 0.001,
      "step": 105710
    },
    {
      "epoch": 19221.81818181818,
      "grad_norm": 0.018381446599960327,
      "learning_rate": 2.427802331092467e-07,
      "loss": 0.001,
      "step": 105720
    },
    {
      "epoch": 19223.636363636364,
      "grad_norm": 0.2141435444355011,
      "learning_rate": 2.4268046231775243e-07,
      "loss": 0.0012,
      "step": 105730
    },
    {
      "epoch": 19225.454545454544,
      "grad_norm": 0.20532464981079102,
      "learning_rate": 2.4258070546119357e-07,
      "loss": 0.0009,
      "step": 105740
    },
    {
      "epoch": 19227.272727272728,
      "grad_norm": 0.0013188933953642845,
      "learning_rate": 2.424809625449729e-07,
      "loss": 0.001,
      "step": 105750
    },
    {
      "epoch": 19229.090909090908,
      "grad_norm": 0.21562102437019348,
      "learning_rate": 2.4238123357449163e-07,
      "loss": 0.0012,
      "step": 105760
    },
    {
      "epoch": 19230.909090909092,
      "grad_norm": 0.003568009939044714,
      "learning_rate": 2.422815185551504e-07,
      "loss": 0.0012,
      "step": 105770
    },
    {
      "epoch": 19232.727272727272,
      "grad_norm": 0.21775905787944794,
      "learning_rate": 2.421818174923495e-07,
      "loss": 0.0011,
      "step": 105780
    },
    {
      "epoch": 19234.545454545456,
      "grad_norm": 0.0015249806456267834,
      "learning_rate": 2.42082130391488e-07,
      "loss": 0.0007,
      "step": 105790
    },
    {
      "epoch": 19236.363636363636,
      "grad_norm": 0.16432780027389526,
      "learning_rate": 2.419824572579642e-07,
      "loss": 0.0013,
      "step": 105800
    },
    {
      "epoch": 19238.18181818182,
      "grad_norm": 0.21095901727676392,
      "learning_rate": 2.4188279809717626e-07,
      "loss": 0.0012,
      "step": 105810
    },
    {
      "epoch": 19240.0,
      "grad_norm": 0.20436176657676697,
      "learning_rate": 2.4178315291452086e-07,
      "loss": 0.0012,
      "step": 105820
    },
    {
      "epoch": 19241.81818181818,
      "grad_norm": 0.0005768444389104843,
      "learning_rate": 2.41683521715394e-07,
      "loss": 0.001,
      "step": 105830
    },
    {
      "epoch": 19243.636363636364,
      "grad_norm": 0.15251372754573822,
      "learning_rate": 2.4158390450519157e-07,
      "loss": 0.001,
      "step": 105840
    },
    {
      "epoch": 19245.454545454544,
      "grad_norm": 0.03626665472984314,
      "learning_rate": 2.414843012893079e-07,
      "loss": 0.0014,
      "step": 105850
    },
    {
      "epoch": 19247.272727272728,
      "grad_norm": 0.2126259058713913,
      "learning_rate": 2.4138471207313743e-07,
      "loss": 0.001,
      "step": 105860
    },
    {
      "epoch": 19249.090909090908,
      "grad_norm": 0.00645537069067359,
      "learning_rate": 2.412851368620726e-07,
      "loss": 0.0012,
      "step": 105870
    },
    {
      "epoch": 19250.909090909092,
      "grad_norm": 0.15642426908016205,
      "learning_rate": 2.411855756615062e-07,
      "loss": 0.001,
      "step": 105880
    },
    {
      "epoch": 19252.727272727272,
      "grad_norm": 0.00037508521927520633,
      "learning_rate": 2.410860284768301e-07,
      "loss": 0.001,
      "step": 105890
    },
    {
      "epoch": 19254.545454545456,
      "grad_norm": 0.0024056024849414825,
      "learning_rate": 2.4098649531343494e-07,
      "loss": 0.001,
      "step": 105900
    },
    {
      "epoch": 19256.363636363636,
      "grad_norm": 0.2661476731300354,
      "learning_rate": 2.408869761767108e-07,
      "loss": 0.0011,
      "step": 105910
    },
    {
      "epoch": 19258.18181818182,
      "grad_norm": 0.16242055594921112,
      "learning_rate": 2.407874710720473e-07,
      "loss": 0.0013,
      "step": 105920
    },
    {
      "epoch": 19260.0,
      "grad_norm": 0.2850740849971771,
      "learning_rate": 2.40687980004833e-07,
      "loss": 0.0008,
      "step": 105930
    },
    {
      "epoch": 19261.81818181818,
      "grad_norm": 0.17311985790729523,
      "learning_rate": 2.405885029804557e-07,
      "loss": 0.001,
      "step": 105940
    },
    {
      "epoch": 19263.636363636364,
      "grad_norm": 0.011619952507317066,
      "learning_rate": 2.4048904000430226e-07,
      "loss": 0.0012,
      "step": 105950
    },
    {
      "epoch": 19265.454545454544,
      "grad_norm": 0.26063090562820435,
      "learning_rate": 2.4038959108175927e-07,
      "loss": 0.0012,
      "step": 105960
    },
    {
      "epoch": 19267.272727272728,
      "grad_norm": 0.0007146110292524099,
      "learning_rate": 2.402901562182125e-07,
      "loss": 0.001,
      "step": 105970
    },
    {
      "epoch": 19269.090909090908,
      "grad_norm": 0.0008916747174225748,
      "learning_rate": 2.401907354190465e-07,
      "loss": 0.0011,
      "step": 105980
    },
    {
      "epoch": 19270.909090909092,
      "grad_norm": 0.0003674620820675045,
      "learning_rate": 2.400913286896452e-07,
      "loss": 0.001,
      "step": 105990
    },
    {
      "epoch": 19272.727272727272,
      "grad_norm": 0.15611957013607025,
      "learning_rate": 2.399919360353923e-07,
      "loss": 0.0011,
      "step": 106000
    },
    {
      "epoch": 19272.727272727272,
      "eval_loss": 5.139086723327637,
      "eval_runtime": 0.95,
      "eval_samples_per_second": 10.526,
      "eval_steps_per_second": 5.263,
      "step": 106000
    },
    {
      "epoch": 19274.545454545456,
      "grad_norm": 0.0005783522501587868,
      "learning_rate": 2.398925574616701e-07,
      "loss": 0.0012,
      "step": 106010
    },
    {
      "epoch": 19276.363636363636,
      "grad_norm": 0.2742944359779358,
      "learning_rate": 2.397931929738603e-07,
      "loss": 0.0012,
      "step": 106020
    },
    {
      "epoch": 19278.18181818182,
      "grad_norm": 0.00042659780592657626,
      "learning_rate": 2.3969384257734386e-07,
      "loss": 0.0009,
      "step": 106030
    },
    {
      "epoch": 19280.0,
      "grad_norm": 0.005457197781652212,
      "learning_rate": 2.3959450627750115e-07,
      "loss": 0.0012,
      "step": 106040
    },
    {
      "epoch": 19281.81818181818,
      "grad_norm": 0.13804113864898682,
      "learning_rate": 2.39495184079712e-07,
      "loss": 0.001,
      "step": 106050
    },
    {
      "epoch": 19283.636363636364,
      "grad_norm": 0.17470651865005493,
      "learning_rate": 2.3939587598935434e-07,
      "loss": 0.0012,
      "step": 106060
    },
    {
      "epoch": 19285.454545454544,
      "grad_norm": 0.0006410013302229345,
      "learning_rate": 2.3929658201180665e-07,
      "loss": 0.001,
      "step": 106070
    },
    {
      "epoch": 19287.272727272728,
      "grad_norm": 0.14591263234615326,
      "learning_rate": 2.391973021524461e-07,
      "loss": 0.0013,
      "step": 106080
    },
    {
      "epoch": 19289.090909090908,
      "grad_norm": 0.0005637933500111103,
      "learning_rate": 2.3909803641664903e-07,
      "loss": 0.0009,
      "step": 106090
    },
    {
      "epoch": 19290.909090909092,
      "grad_norm": 0.0005727172829210758,
      "learning_rate": 2.3899878480979097e-07,
      "loss": 0.0012,
      "step": 106100
    },
    {
      "epoch": 19292.727272727272,
      "grad_norm": 0.0014172176597639918,
      "learning_rate": 2.3889954733724707e-07,
      "loss": 0.0009,
      "step": 106110
    },
    {
      "epoch": 19294.545454545456,
      "grad_norm": 0.1489022970199585,
      "learning_rate": 2.3880032400439134e-07,
      "loss": 0.001,
      "step": 106120
    },
    {
      "epoch": 19296.363636363636,
      "grad_norm": 0.2690122425556183,
      "learning_rate": 2.3870111481659705e-07,
      "loss": 0.0014,
      "step": 106130
    },
    {
      "epoch": 19298.18181818182,
      "grad_norm": 0.1671425700187683,
      "learning_rate": 2.386019197792367e-07,
      "loss": 0.001,
      "step": 106140
    },
    {
      "epoch": 19300.0,
      "grad_norm": 0.0011153077939525247,
      "learning_rate": 2.3850273889768235e-07,
      "loss": 0.001,
      "step": 106150
    },
    {
      "epoch": 19301.81818181818,
      "grad_norm": 0.2104085236787796,
      "learning_rate": 2.3840357217730518e-07,
      "loss": 0.0011,
      "step": 106160
    },
    {
      "epoch": 19303.636363636364,
      "grad_norm": 0.1612076312303543,
      "learning_rate": 2.3830441962347525e-07,
      "loss": 0.001,
      "step": 106170
    },
    {
      "epoch": 19305.454545454544,
      "grad_norm": 0.0007218524115160108,
      "learning_rate": 2.3820528124156199e-07,
      "loss": 0.0012,
      "step": 106180
    },
    {
      "epoch": 19307.272727272728,
      "grad_norm": 0.00045753715676255524,
      "learning_rate": 2.3810615703693444e-07,
      "loss": 0.0009,
      "step": 106190
    },
    {
      "epoch": 19309.090909090908,
      "grad_norm": 0.0017198880668729544,
      "learning_rate": 2.380070470149605e-07,
      "loss": 0.0012,
      "step": 106200
    },
    {
      "epoch": 19310.909090909092,
      "grad_norm": 0.16136884689331055,
      "learning_rate": 2.3790795118100736e-07,
      "loss": 0.001,
      "step": 106210
    },
    {
      "epoch": 19312.727272727272,
      "grad_norm": 0.0005857732612639666,
      "learning_rate": 2.3780886954044128e-07,
      "loss": 0.0012,
      "step": 106220
    },
    {
      "epoch": 19314.545454545456,
      "grad_norm": 0.000470090308226645,
      "learning_rate": 2.3770980209862812e-07,
      "loss": 0.0009,
      "step": 106230
    },
    {
      "epoch": 19316.363636363636,
      "grad_norm": 0.15548759698867798,
      "learning_rate": 2.3761074886093324e-07,
      "loss": 0.0013,
      "step": 106240
    },
    {
      "epoch": 19318.18181818182,
      "grad_norm": 0.000569331634324044,
      "learning_rate": 2.3751170983271996e-07,
      "loss": 0.0009,
      "step": 106250
    },
    {
      "epoch": 19320.0,
      "grad_norm": 0.0037283150013536215,
      "learning_rate": 2.374126850193521e-07,
      "loss": 0.0012,
      "step": 106260
    },
    {
      "epoch": 19321.81818181818,
      "grad_norm": 0.00044678663834929466,
      "learning_rate": 2.3731367442619237e-07,
      "loss": 0.001,
      "step": 106270
    },
    {
      "epoch": 19323.636363636364,
      "grad_norm": 0.2615220248699188,
      "learning_rate": 2.3721467805860252e-07,
      "loss": 0.0014,
      "step": 106280
    },
    {
      "epoch": 19325.454545454544,
      "grad_norm": 0.0008059353567659855,
      "learning_rate": 2.3711569592194358e-07,
      "loss": 0.0008,
      "step": 106290
    },
    {
      "epoch": 19327.272727272728,
      "grad_norm": 0.0006830389611423016,
      "learning_rate": 2.3701672802157563e-07,
      "loss": 0.0009,
      "step": 106300
    },
    {
      "epoch": 19329.090909090908,
      "grad_norm": 0.40866154432296753,
      "learning_rate": 2.3691777436285865e-07,
      "loss": 0.0013,
      "step": 106310
    },
    {
      "epoch": 19330.909090909092,
      "grad_norm": 0.0010138229699805379,
      "learning_rate": 2.3681883495115112e-07,
      "loss": 0.0012,
      "step": 106320
    },
    {
      "epoch": 19332.727272727272,
      "grad_norm": 0.20276010036468506,
      "learning_rate": 2.3671990979181096e-07,
      "loss": 0.001,
      "step": 106330
    },
    {
      "epoch": 19334.545454545456,
      "grad_norm": 0.16792748868465424,
      "learning_rate": 2.3662099889019537e-07,
      "loss": 0.001,
      "step": 106340
    },
    {
      "epoch": 19336.363636363636,
      "grad_norm": 0.19535097479820251,
      "learning_rate": 2.365221022516612e-07,
      "loss": 0.0011,
      "step": 106350
    },
    {
      "epoch": 19338.18181818182,
      "grad_norm": 0.17102177441120148,
      "learning_rate": 2.3642321988156378e-07,
      "loss": 0.0011,
      "step": 106360
    },
    {
      "epoch": 19340.0,
      "grad_norm": 0.2623211741447449,
      "learning_rate": 2.363243517852579e-07,
      "loss": 0.001,
      "step": 106370
    },
    {
      "epoch": 19341.81818181818,
      "grad_norm": 0.2715851068496704,
      "learning_rate": 2.3622549796809803e-07,
      "loss": 0.0012,
      "step": 106380
    },
    {
      "epoch": 19343.636363636364,
      "grad_norm": 0.0006293028127402067,
      "learning_rate": 2.3612665843543733e-07,
      "loss": 0.0008,
      "step": 106390
    },
    {
      "epoch": 19345.454545454544,
      "grad_norm": 0.17110930383205414,
      "learning_rate": 2.3602783319262843e-07,
      "loss": 0.0013,
      "step": 106400
    },
    {
      "epoch": 19347.272727272728,
      "grad_norm": 0.0004772166721522808,
      "learning_rate": 2.359290222450228e-07,
      "loss": 0.0011,
      "step": 106410
    },
    {
      "epoch": 19349.090909090908,
      "grad_norm": 0.21314722299575806,
      "learning_rate": 2.3583022559797184e-07,
      "loss": 0.0012,
      "step": 106420
    },
    {
      "epoch": 19350.909090909092,
      "grad_norm": 0.0006925410125404596,
      "learning_rate": 2.3573144325682603e-07,
      "loss": 0.0009,
      "step": 106430
    },
    {
      "epoch": 19352.727272727272,
      "grad_norm": 0.0007091622101143003,
      "learning_rate": 2.3563267522693414e-07,
      "loss": 0.0012,
      "step": 106440
    },
    {
      "epoch": 19354.545454545456,
      "grad_norm": 0.0011673499830067158,
      "learning_rate": 2.355339215136453e-07,
      "loss": 0.001,
      "step": 106450
    },
    {
      "epoch": 19356.363636363636,
      "grad_norm": 0.2644840478897095,
      "learning_rate": 2.3543518212230763e-07,
      "loss": 0.0012,
      "step": 106460
    },
    {
      "epoch": 19358.18181818182,
      "grad_norm": 0.21274110674858093,
      "learning_rate": 2.3533645705826803e-07,
      "loss": 0.0013,
      "step": 106470
    },
    {
      "epoch": 19360.0,
      "grad_norm": 0.000758544949349016,
      "learning_rate": 2.3523774632687294e-07,
      "loss": 0.0007,
      "step": 106480
    },
    {
      "epoch": 19361.81818181818,
      "grad_norm": 0.2134038805961609,
      "learning_rate": 2.3513904993346773e-07,
      "loss": 0.0012,
      "step": 106490
    },
    {
      "epoch": 19363.636363636364,
      "grad_norm": 0.17302724719047546,
      "learning_rate": 2.350403678833976e-07,
      "loss": 0.0011,
      "step": 106500
    },
    {
      "epoch": 19363.636363636364,
      "eval_loss": 5.130447864532471,
      "eval_runtime": 0.9523,
      "eval_samples_per_second": 10.501,
      "eval_steps_per_second": 5.251,
      "step": 106500
    },
    {
      "epoch": 19365.454545454544,
      "grad_norm": 0.21289891004562378,
      "learning_rate": 2.3494170018200643e-07,
      "loss": 0.0013,
      "step": 106510
    },
    {
      "epoch": 19367.272727272728,
      "grad_norm": 0.000517156207934022,
      "learning_rate": 2.348430468346373e-07,
      "loss": 0.0006,
      "step": 106520
    },
    {
      "epoch": 19369.090909090908,
      "grad_norm": 0.0005623375182040036,
      "learning_rate": 2.3474440784663285e-07,
      "loss": 0.0012,
      "step": 106530
    },
    {
      "epoch": 19370.909090909092,
      "grad_norm": 0.006327492650598288,
      "learning_rate": 2.3464578322333506e-07,
      "loss": 0.001,
      "step": 106540
    },
    {
      "epoch": 19372.727272727272,
      "grad_norm": 0.0013569260481745005,
      "learning_rate": 2.3454717297008463e-07,
      "loss": 0.0013,
      "step": 106550
    },
    {
      "epoch": 19374.545454545456,
      "grad_norm": 0.017632795497775078,
      "learning_rate": 2.3444857709222176e-07,
      "loss": 0.001,
      "step": 106560
    },
    {
      "epoch": 19376.363636363636,
      "grad_norm": 0.0014369967393577099,
      "learning_rate": 2.343499955950856e-07,
      "loss": 0.0009,
      "step": 106570
    },
    {
      "epoch": 19378.18181818182,
      "grad_norm": 0.000565278111025691,
      "learning_rate": 2.3425142848401518e-07,
      "loss": 0.0011,
      "step": 106580
    },
    {
      "epoch": 19380.0,
      "grad_norm": 0.21255677938461304,
      "learning_rate": 2.3415287576434807e-07,
      "loss": 0.0013,
      "step": 106590
    },
    {
      "epoch": 19381.81818181818,
      "grad_norm": 0.0007886087405495346,
      "learning_rate": 2.3405433744142118e-07,
      "loss": 0.0012,
      "step": 106600
    },
    {
      "epoch": 19383.636363636364,
      "grad_norm": 0.1725105494260788,
      "learning_rate": 2.3395581352057093e-07,
      "loss": 0.001,
      "step": 106610
    },
    {
      "epoch": 19385.454545454544,
      "grad_norm": 0.2164096087217331,
      "learning_rate": 2.3385730400713315e-07,
      "loss": 0.0009,
      "step": 106620
    },
    {
      "epoch": 19387.272727272728,
      "grad_norm": 0.1480109840631485,
      "learning_rate": 2.337588089064419e-07,
      "loss": 0.0015,
      "step": 106630
    },
    {
      "epoch": 19389.090909090908,
      "grad_norm": 0.19974273443222046,
      "learning_rate": 2.336603282238316e-07,
      "loss": 0.0009,
      "step": 106640
    },
    {
      "epoch": 19390.909090909092,
      "grad_norm": 0.0008235966088250279,
      "learning_rate": 2.3356186196463496e-07,
      "loss": 0.001,
      "step": 106650
    },
    {
      "epoch": 19392.727272727272,
      "grad_norm": 0.17671483755111694,
      "learning_rate": 2.3346341013418485e-07,
      "loss": 0.0011,
      "step": 106660
    },
    {
      "epoch": 19394.545454545456,
      "grad_norm": 0.000675167131703347,
      "learning_rate": 2.3336497273781253e-07,
      "loss": 0.0008,
      "step": 106670
    },
    {
      "epoch": 19396.363636363636,
      "grad_norm": 0.21062947809696198,
      "learning_rate": 2.332665497808487e-07,
      "loss": 0.0012,
      "step": 106680
    },
    {
      "epoch": 19398.18181818182,
      "grad_norm": 0.1580827832221985,
      "learning_rate": 2.331681412686237e-07,
      "loss": 0.0012,
      "step": 106690
    },
    {
      "epoch": 19400.0,
      "grad_norm": 0.0007359079900197685,
      "learning_rate": 2.3306974720646665e-07,
      "loss": 0.001,
      "step": 106700
    },
    {
      "epoch": 19401.81818181818,
      "grad_norm": 0.0007483658264391124,
      "learning_rate": 2.3297136759970577e-07,
      "loss": 0.0009,
      "step": 106710
    },
    {
      "epoch": 19403.636363636364,
      "grad_norm": 0.2677946388721466,
      "learning_rate": 2.3287300245366892e-07,
      "loss": 0.0015,
      "step": 106720
    },
    {
      "epoch": 19405.454545454544,
      "grad_norm": 0.16384091973304749,
      "learning_rate": 2.327746517736831e-07,
      "loss": 0.0009,
      "step": 106730
    },
    {
      "epoch": 19407.272727272728,
      "grad_norm": 0.2637407183647156,
      "learning_rate": 2.3267631556507438e-07,
      "loss": 0.0012,
      "step": 106740
    },
    {
      "epoch": 19409.090909090908,
      "grad_norm": 0.004592412617057562,
      "learning_rate": 2.3257799383316795e-07,
      "loss": 0.0009,
      "step": 106750
    },
    {
      "epoch": 19410.909090909092,
      "grad_norm": 0.1641658991575241,
      "learning_rate": 2.324796865832882e-07,
      "loss": 0.0012,
      "step": 106760
    },
    {
      "epoch": 19412.727272727272,
      "grad_norm": 0.0005463495035655797,
      "learning_rate": 2.3238139382075927e-07,
      "loss": 0.0008,
      "step": 106770
    },
    {
      "epoch": 19414.545454545456,
      "grad_norm": 0.0008563004084862769,
      "learning_rate": 2.322831155509039e-07,
      "loss": 0.0015,
      "step": 106780
    },
    {
      "epoch": 19416.363636363636,
      "grad_norm": 0.26728418469429016,
      "learning_rate": 2.3218485177904418e-07,
      "loss": 0.0011,
      "step": 106790
    },
    {
      "epoch": 19418.18181818182,
      "grad_norm": 0.0005374590982683003,
      "learning_rate": 2.3208660251050156e-07,
      "loss": 0.0009,
      "step": 106800
    },
    {
      "epoch": 19420.0,
      "grad_norm": 0.0005470499745570123,
      "learning_rate": 2.3198836775059711e-07,
      "loss": 0.0012,
      "step": 106810
    },
    {
      "epoch": 19421.81818181818,
      "grad_norm": 0.0007561442907899618,
      "learning_rate": 2.3189014750464992e-07,
      "loss": 0.001,
      "step": 106820
    },
    {
      "epoch": 19423.636363636364,
      "grad_norm": 0.0009540702449157834,
      "learning_rate": 2.317919417779795e-07,
      "loss": 0.0012,
      "step": 106830
    },
    {
      "epoch": 19425.454545454544,
      "grad_norm": 0.20018911361694336,
      "learning_rate": 2.3169375057590384e-07,
      "loss": 0.0012,
      "step": 106840
    },
    {
      "epoch": 19427.272727272728,
      "grad_norm": 0.017504410818219185,
      "learning_rate": 2.3159557390374074e-07,
      "loss": 0.001,
      "step": 106850
    },
    {
      "epoch": 19429.090909090908,
      "grad_norm": 0.16818086802959442,
      "learning_rate": 2.3149741176680665e-07,
      "loss": 0.0012,
      "step": 106860
    },
    {
      "epoch": 19430.909090909092,
      "grad_norm": 0.20211569964885712,
      "learning_rate": 2.3139926417041734e-07,
      "loss": 0.0009,
      "step": 106870
    },
    {
      "epoch": 19432.727272727272,
      "grad_norm": 0.2157493382692337,
      "learning_rate": 2.3130113111988818e-07,
      "loss": 0.0009,
      "step": 106880
    },
    {
      "epoch": 19434.545454545456,
      "grad_norm": 0.20473337173461914,
      "learning_rate": 2.3120301262053348e-07,
      "loss": 0.0014,
      "step": 106890
    },
    {
      "epoch": 19436.363636363636,
      "grad_norm": 0.15620553493499756,
      "learning_rate": 2.3110490867766642e-07,
      "loss": 0.001,
      "step": 106900
    },
    {
      "epoch": 19438.18181818182,
      "grad_norm": 0.16001661121845245,
      "learning_rate": 2.310068192966002e-07,
      "loss": 0.0009,
      "step": 106910
    },
    {
      "epoch": 19440.0,
      "grad_norm": 0.0008194242254830897,
      "learning_rate": 2.3090874448264635e-07,
      "loss": 0.0011,
      "step": 106920
    },
    {
      "epoch": 19441.81818181818,
      "grad_norm": 0.273428738117218,
      "learning_rate": 2.3081068424111648e-07,
      "loss": 0.0012,
      "step": 106930
    },
    {
      "epoch": 19443.636363636364,
      "grad_norm": 0.26040327548980713,
      "learning_rate": 2.307126385773207e-07,
      "loss": 0.0011,
      "step": 106940
    },
    {
      "epoch": 19445.454545454544,
      "grad_norm": 0.0004944055108353496,
      "learning_rate": 2.3061460749656841e-07,
      "loss": 0.0007,
      "step": 106950
    },
    {
      "epoch": 19447.272727272728,
      "grad_norm": 0.0014328547986224294,
      "learning_rate": 2.3051659100416888e-07,
      "loss": 0.0012,
      "step": 106960
    },
    {
      "epoch": 19449.090909090908,
      "grad_norm": 0.0005428775912150741,
      "learning_rate": 2.3041858910542983e-07,
      "loss": 0.0012,
      "step": 106970
    },
    {
      "epoch": 19450.909090909092,
      "grad_norm": 0.0020366052631288767,
      "learning_rate": 2.3032060180565827e-07,
      "loss": 0.0012,
      "step": 106980
    },
    {
      "epoch": 19452.727272727272,
      "grad_norm": 0.000589038769248873,
      "learning_rate": 2.3022262911016088e-07,
      "loss": 0.0011,
      "step": 106990
    },
    {
      "epoch": 19454.545454545456,
      "grad_norm": 0.0006467645289376378,
      "learning_rate": 2.301246710242437e-07,
      "loss": 0.0011,
      "step": 107000
    },
    {
      "epoch": 19454.545454545456,
      "eval_loss": 5.133821487426758,
      "eval_runtime": 0.9484,
      "eval_samples_per_second": 10.544,
      "eval_steps_per_second": 5.272,
      "step": 107000
    },
    {
      "epoch": 19456.363636363636,
      "grad_norm": 0.0005250542890280485,
      "learning_rate": 2.3002672755321073e-07,
      "loss": 0.0009,
      "step": 107010
    },
    {
      "epoch": 19458.18181818182,
      "grad_norm": 0.0005170881631784141,
      "learning_rate": 2.2992879870236658e-07,
      "loss": 0.0012,
      "step": 107020
    },
    {
      "epoch": 19460.0,
      "grad_norm": 0.0006629585986956954,
      "learning_rate": 2.2983088447701422e-07,
      "loss": 0.0012,
      "step": 107030
    },
    {
      "epoch": 19461.81818181818,
      "grad_norm": 0.16158416867256165,
      "learning_rate": 2.2973298488245645e-07,
      "loss": 0.0012,
      "step": 107040
    },
    {
      "epoch": 19463.636363636364,
      "grad_norm": 0.0007114456966519356,
      "learning_rate": 2.2963509992399478e-07,
      "loss": 0.001,
      "step": 107050
    },
    {
      "epoch": 19465.454545454544,
      "grad_norm": 0.1623772233724594,
      "learning_rate": 2.2953722960692985e-07,
      "loss": 0.0015,
      "step": 107060
    },
    {
      "epoch": 19467.272727272728,
      "grad_norm": 0.04607042297720909,
      "learning_rate": 2.294393739365621e-07,
      "loss": 0.0009,
      "step": 107070
    },
    {
      "epoch": 19469.090909090908,
      "grad_norm": 0.2146618366241455,
      "learning_rate": 2.2934153291819074e-07,
      "loss": 0.0009,
      "step": 107080
    },
    {
      "epoch": 19470.909090909092,
      "grad_norm": 0.00133115379139781,
      "learning_rate": 2.2924370655711406e-07,
      "loss": 0.0012,
      "step": 107090
    },
    {
      "epoch": 19472.727272727272,
      "grad_norm": 0.0004311412340030074,
      "learning_rate": 2.2914589485863012e-07,
      "loss": 0.001,
      "step": 107100
    },
    {
      "epoch": 19474.545454545456,
      "grad_norm": 0.0010281603317707777,
      "learning_rate": 2.2904809782803565e-07,
      "loss": 0.0011,
      "step": 107110
    },
    {
      "epoch": 19476.363636363636,
      "grad_norm": 0.001032804255373776,
      "learning_rate": 2.289503154706266e-07,
      "loss": 0.0008,
      "step": 107120
    },
    {
      "epoch": 19478.18181818182,
      "grad_norm": 0.005800880491733551,
      "learning_rate": 2.288525477916986e-07,
      "loss": 0.0015,
      "step": 107130
    },
    {
      "epoch": 19480.0,
      "grad_norm": 0.16066281497478485,
      "learning_rate": 2.2875479479654593e-07,
      "loss": 0.0009,
      "step": 107140
    },
    {
      "epoch": 19481.81818181818,
      "grad_norm": 0.26758038997650146,
      "learning_rate": 2.2865705649046257e-07,
      "loss": 0.0012,
      "step": 107150
    },
    {
      "epoch": 19483.636363636364,
      "grad_norm": 0.0008429109584540129,
      "learning_rate": 2.2855933287874135e-07,
      "loss": 0.0009,
      "step": 107160
    },
    {
      "epoch": 19485.454545454544,
      "grad_norm": 0.012492809444665909,
      "learning_rate": 2.2846162396667424e-07,
      "loss": 0.0012,
      "step": 107170
    },
    {
      "epoch": 19487.272727272728,
      "grad_norm": 0.20932120084762573,
      "learning_rate": 2.2836392975955287e-07,
      "loss": 0.001,
      "step": 107180
    },
    {
      "epoch": 19489.090909090908,
      "grad_norm": 0.2740136981010437,
      "learning_rate": 2.2826625026266777e-07,
      "loss": 0.0011,
      "step": 107190
    },
    {
      "epoch": 19490.909090909092,
      "grad_norm": 0.15834346413612366,
      "learning_rate": 2.2816858548130835e-07,
      "loss": 0.0012,
      "step": 107200
    },
    {
      "epoch": 19492.727272727272,
      "grad_norm": 0.0005812180461362004,
      "learning_rate": 2.28070935420764e-07,
      "loss": 0.001,
      "step": 107210
    },
    {
      "epoch": 19494.545454545456,
      "grad_norm": 0.1654246747493744,
      "learning_rate": 2.2797330008632253e-07,
      "loss": 0.0011,
      "step": 107220
    },
    {
      "epoch": 19496.363636363636,
      "grad_norm": 0.0005204119370318949,
      "learning_rate": 2.2787567948327163e-07,
      "loss": 0.0014,
      "step": 107230
    },
    {
      "epoch": 19498.18181818182,
      "grad_norm": 0.0007617890951223671,
      "learning_rate": 2.2777807361689776e-07,
      "loss": 0.0007,
      "step": 107240
    },
    {
      "epoch": 19500.0,
      "grad_norm": 0.0010328912176191807,
      "learning_rate": 2.2768048249248644e-07,
      "loss": 0.0012,
      "step": 107250
    },
    {
      "epoch": 19501.81818181818,
      "grad_norm": 0.25850725173950195,
      "learning_rate": 2.27582906115323e-07,
      "loss": 0.0012,
      "step": 107260
    },
    {
      "epoch": 19503.636363636364,
      "grad_norm": 0.0006921582389622927,
      "learning_rate": 2.2748534449069146e-07,
      "loss": 0.0009,
      "step": 107270
    },
    {
      "epoch": 19505.454545454544,
      "grad_norm": 0.0004861463967245072,
      "learning_rate": 2.2738779762387495e-07,
      "loss": 0.0012,
      "step": 107280
    },
    {
      "epoch": 19507.272727272728,
      "grad_norm": 0.19888384640216827,
      "learning_rate": 2.272902655201565e-07,
      "loss": 0.0013,
      "step": 107290
    },
    {
      "epoch": 19509.090909090908,
      "grad_norm": 0.0023788136895745993,
      "learning_rate": 2.2719274818481764e-07,
      "loss": 0.0007,
      "step": 107300
    },
    {
      "epoch": 19510.909090909092,
      "grad_norm": 0.01725812628865242,
      "learning_rate": 2.270952456231392e-07,
      "loss": 0.001,
      "step": 107310
    },
    {
      "epoch": 19512.727272727272,
      "grad_norm": 0.00045807051355950534,
      "learning_rate": 2.269977578404017e-07,
      "loss": 0.0013,
      "step": 107320
    },
    {
      "epoch": 19514.545454545456,
      "grad_norm": 0.2031380832195282,
      "learning_rate": 2.2690028484188418e-07,
      "loss": 0.0012,
      "step": 107330
    },
    {
      "epoch": 19516.363636363636,
      "grad_norm": 0.0010646727168932557,
      "learning_rate": 2.2680282663286548e-07,
      "loss": 0.0007,
      "step": 107340
    },
    {
      "epoch": 19518.18181818182,
      "grad_norm": 0.0005694889114238322,
      "learning_rate": 2.2670538321862326e-07,
      "loss": 0.0012,
      "step": 107350
    },
    {
      "epoch": 19520.0,
      "grad_norm": 0.0008998427074402571,
      "learning_rate": 2.2660795460443433e-07,
      "loss": 0.0012,
      "step": 107360
    },
    {
      "epoch": 19521.81818181818,
      "grad_norm": 0.21355926990509033,
      "learning_rate": 2.265105407955752e-07,
      "loss": 0.001,
      "step": 107370
    },
    {
      "epoch": 19523.636363636364,
      "grad_norm": 0.0008189312648028135,
      "learning_rate": 2.2641314179732102e-07,
      "loss": 0.0011,
      "step": 107380
    },
    {
      "epoch": 19525.454545454544,
      "grad_norm": 0.2029617428779602,
      "learning_rate": 2.2631575761494626e-07,
      "loss": 0.0013,
      "step": 107390
    },
    {
      "epoch": 19527.272727272728,
      "grad_norm": 0.1704164296388626,
      "learning_rate": 2.262183882537249e-07,
      "loss": 0.0009,
      "step": 107400
    },
    {
      "epoch": 19529.090909090908,
      "grad_norm": 0.0014513791538774967,
      "learning_rate": 2.261210337189297e-07,
      "loss": 0.001,
      "step": 107410
    },
    {
      "epoch": 19530.909090909092,
      "grad_norm": 0.001016614492982626,
      "learning_rate": 2.260236940158331e-07,
      "loss": 0.0012,
      "step": 107420
    },
    {
      "epoch": 19532.727272727272,
      "grad_norm": 0.16394883394241333,
      "learning_rate": 2.2592636914970632e-07,
      "loss": 0.0008,
      "step": 107430
    },
    {
      "epoch": 19534.545454545456,
      "grad_norm": 0.21328216791152954,
      "learning_rate": 2.2582905912581963e-07,
      "loss": 0.0013,
      "step": 107440
    },
    {
      "epoch": 19536.363636363636,
      "grad_norm": 0.0021059345453977585,
      "learning_rate": 2.2573176394944327e-07,
      "loss": 0.001,
      "step": 107450
    },
    {
      "epoch": 19538.18181818182,
      "grad_norm": 0.0006143714999780059,
      "learning_rate": 2.2563448362584586e-07,
      "loss": 0.001,
      "step": 107460
    },
    {
      "epoch": 19540.0,
      "grad_norm": 0.16018883883953094,
      "learning_rate": 2.2553721816029547e-07,
      "loss": 0.0012,
      "step": 107470
    },
    {
      "epoch": 19541.81818181818,
      "grad_norm": 0.0003726933791767806,
      "learning_rate": 2.2543996755805977e-07,
      "loss": 0.0009,
      "step": 107480
    },
    {
      "epoch": 19543.636363636364,
      "grad_norm": 0.21645231544971466,
      "learning_rate": 2.253427318244051e-07,
      "loss": 0.0012,
      "step": 107490
    },
    {
      "epoch": 19545.454545454544,
      "grad_norm": 0.16103622317314148,
      "learning_rate": 2.25245510964597e-07,
      "loss": 0.0015,
      "step": 107500
    },
    {
      "epoch": 19545.454545454544,
      "eval_loss": 5.149731159210205,
      "eval_runtime": 0.9529,
      "eval_samples_per_second": 10.495,
      "eval_steps_per_second": 5.247,
      "step": 107500
    },
    {
      "epoch": 19547.272727272728,
      "grad_norm": 0.21301497519016266,
      "learning_rate": 2.251483049839007e-07,
      "loss": 0.0009,
      "step": 107510
    },
    {
      "epoch": 19549.090909090908,
      "grad_norm": 0.017428837716579437,
      "learning_rate": 2.2505111388758008e-07,
      "loss": 0.0011,
      "step": 107520
    },
    {
      "epoch": 19550.909090909092,
      "grad_norm": 0.1654728502035141,
      "learning_rate": 2.2495393768089866e-07,
      "loss": 0.001,
      "step": 107530
    },
    {
      "epoch": 19552.727272727272,
      "grad_norm": 0.0010112306335940957,
      "learning_rate": 2.2485677636911887e-07,
      "loss": 0.0011,
      "step": 107540
    },
    {
      "epoch": 19554.545454545456,
      "grad_norm": 0.2113812267780304,
      "learning_rate": 2.247596299575022e-07,
      "loss": 0.0012,
      "step": 107550
    },
    {
      "epoch": 19556.363636363636,
      "grad_norm": 0.0009205974056385458,
      "learning_rate": 2.2466249845130986e-07,
      "loss": 0.0009,
      "step": 107560
    },
    {
      "epoch": 19558.18181818182,
      "grad_norm": 0.0009120390168391168,
      "learning_rate": 2.2456538185580176e-07,
      "loss": 0.001,
      "step": 107570
    },
    {
      "epoch": 19560.0,
      "grad_norm": 0.19592896103858948,
      "learning_rate": 2.24468280176237e-07,
      "loss": 0.0012,
      "step": 107580
    },
    {
      "epoch": 19561.81818181818,
      "grad_norm": 0.0006824588635936379,
      "learning_rate": 2.2437119341787442e-07,
      "loss": 0.0009,
      "step": 107590
    },
    {
      "epoch": 19563.636363636364,
      "grad_norm": 0.000603558321017772,
      "learning_rate": 2.242741215859713e-07,
      "loss": 0.0012,
      "step": 107600
    },
    {
      "epoch": 19565.454545454544,
      "grad_norm": 0.16674935817718506,
      "learning_rate": 2.241770646857849e-07,
      "loss": 0.001,
      "step": 107610
    },
    {
      "epoch": 19567.272727272728,
      "grad_norm": 0.1598522812128067,
      "learning_rate": 2.24080022722571e-07,
      "loss": 0.0013,
      "step": 107620
    },
    {
      "epoch": 19569.090909090908,
      "grad_norm": 0.21182909607887268,
      "learning_rate": 2.2398299570158464e-07,
      "loss": 0.0009,
      "step": 107630
    },
    {
      "epoch": 19570.909090909092,
      "grad_norm": 0.00046520447358489037,
      "learning_rate": 2.238859836280807e-07,
      "loss": 0.0012,
      "step": 107640
    },
    {
      "epoch": 19572.727272727272,
      "grad_norm": 0.0007057085749693215,
      "learning_rate": 2.2378898650731254e-07,
      "loss": 0.0011,
      "step": 107650
    },
    {
      "epoch": 19574.545454545456,
      "grad_norm": 0.00047465026727877557,
      "learning_rate": 2.2369200434453277e-07,
      "loss": 0.0012,
      "step": 107660
    },
    {
      "epoch": 19576.363636363636,
      "grad_norm": 0.0007120794616639614,
      "learning_rate": 2.235950371449938e-07,
      "loss": 0.0007,
      "step": 107670
    },
    {
      "epoch": 19578.18181818182,
      "grad_norm": 0.16361218690872192,
      "learning_rate": 2.234980849139466e-07,
      "loss": 0.0013,
      "step": 107680
    },
    {
      "epoch": 19580.0,
      "grad_norm": 0.0009285731357522309,
      "learning_rate": 2.2340114765664136e-07,
      "loss": 0.001,
      "step": 107690
    },
    {
      "epoch": 19581.81818181818,
      "grad_norm": 0.0005973461084067822,
      "learning_rate": 2.23304225378328e-07,
      "loss": 0.001,
      "step": 107700
    },
    {
      "epoch": 19583.636363636364,
      "grad_norm": 0.15636380016803741,
      "learning_rate": 2.232073180842549e-07,
      "loss": 0.0013,
      "step": 107710
    },
    {
      "epoch": 19585.454545454544,
      "grad_norm": 0.0005170699441805482,
      "learning_rate": 2.2311042577967043e-07,
      "loss": 0.0007,
      "step": 107720
    },
    {
      "epoch": 19587.272727272728,
      "grad_norm": 0.0010988671565428376,
      "learning_rate": 2.2301354846982145e-07,
      "loss": 0.0012,
      "step": 107730
    },
    {
      "epoch": 19589.090909090908,
      "grad_norm": 0.2607431411743164,
      "learning_rate": 2.2291668615995408e-07,
      "loss": 0.0012,
      "step": 107740
    },
    {
      "epoch": 19590.909090909092,
      "grad_norm": 0.0006816561217419803,
      "learning_rate": 2.2281983885531426e-07,
      "loss": 0.0011,
      "step": 107750
    },
    {
      "epoch": 19592.727272727272,
      "grad_norm": 0.26540377736091614,
      "learning_rate": 2.2272300656114646e-07,
      "loss": 0.0012,
      "step": 107760
    },
    {
      "epoch": 19594.545454545456,
      "grad_norm": 0.0007169932359829545,
      "learning_rate": 2.2262618928269438e-07,
      "loss": 0.0007,
      "step": 107770
    },
    {
      "epoch": 19596.363636363636,
      "grad_norm": 0.20288732647895813,
      "learning_rate": 2.2252938702520142e-07,
      "loss": 0.0011,
      "step": 107780
    },
    {
      "epoch": 19598.18181818182,
      "grad_norm": 0.04550137743353844,
      "learning_rate": 2.224325997939095e-07,
      "loss": 0.0015,
      "step": 107790
    },
    {
      "epoch": 19600.0,
      "grad_norm": 0.27595576643943787,
      "learning_rate": 2.2233582759406062e-07,
      "loss": 0.0009,
      "step": 107800
    },
    {
      "epoch": 19601.81818181818,
      "grad_norm": 0.16713178157806396,
      "learning_rate": 2.2223907043089458e-07,
      "loss": 0.001,
      "step": 107810
    },
    {
      "epoch": 19603.636363636364,
      "grad_norm": 0.000853991718031466,
      "learning_rate": 2.221423283096517e-07,
      "loss": 0.0012,
      "step": 107820
    },
    {
      "epoch": 19605.454545454544,
      "grad_norm": 0.21313239634037018,
      "learning_rate": 2.22045601235571e-07,
      "loss": 0.001,
      "step": 107830
    },
    {
      "epoch": 19607.272727272728,
      "grad_norm": 0.0003690228913910687,
      "learning_rate": 2.2194888921389056e-07,
      "loss": 0.001,
      "step": 107840
    },
    {
      "epoch": 19609.090909090908,
      "grad_norm": 0.001354680978693068,
      "learning_rate": 2.218521922498476e-07,
      "loss": 0.0012,
      "step": 107850
    },
    {
      "epoch": 19610.909090909092,
      "grad_norm": 0.000938788871280849,
      "learning_rate": 2.217555103486789e-07,
      "loss": 0.0012,
      "step": 107860
    },
    {
      "epoch": 19612.727272727272,
      "grad_norm": 0.2133326381444931,
      "learning_rate": 2.2165884351562014e-07,
      "loss": 0.0011,
      "step": 107870
    },
    {
      "epoch": 19614.545454545456,
      "grad_norm": 0.20430710911750793,
      "learning_rate": 2.215621917559062e-07,
      "loss": 0.0012,
      "step": 107880
    },
    {
      "epoch": 19616.363636363636,
      "grad_norm": 0.16003189980983734,
      "learning_rate": 2.214655550747709e-07,
      "loss": 0.0013,
      "step": 107890
    },
    {
      "epoch": 19618.18181818182,
      "grad_norm": 0.0009722799877636135,
      "learning_rate": 2.2136893347744783e-07,
      "loss": 0.0006,
      "step": 107900
    },
    {
      "epoch": 19620.0,
      "grad_norm": 0.2694241404533386,
      "learning_rate": 2.2127232696916963e-07,
      "loss": 0.0014,
      "step": 107910
    },
    {
      "epoch": 19621.81818181818,
      "grad_norm": 0.0012316977372393012,
      "learning_rate": 2.2117573555516772e-07,
      "loss": 0.0012,
      "step": 107920
    },
    {
      "epoch": 19623.636363636364,
      "grad_norm": 0.0006587225361727178,
      "learning_rate": 2.2107915924067272e-07,
      "loss": 0.0009,
      "step": 107930
    },
    {
      "epoch": 19625.454545454544,
      "grad_norm": 0.1590416431427002,
      "learning_rate": 2.2098259803091507e-07,
      "loss": 0.0012,
      "step": 107940
    },
    {
      "epoch": 19627.272727272728,
      "grad_norm": 0.008928362280130386,
      "learning_rate": 2.208860519311238e-07,
      "loss": 0.001,
      "step": 107950
    },
    {
      "epoch": 19629.090909090908,
      "grad_norm": 0.0010974116157740355,
      "learning_rate": 2.2078952094652704e-07,
      "loss": 0.0012,
      "step": 107960
    },
    {
      "epoch": 19630.909090909092,
      "grad_norm": 0.17210759222507477,
      "learning_rate": 2.2069300508235272e-07,
      "loss": 0.0012,
      "step": 107970
    },
    {
      "epoch": 19632.727272727272,
      "grad_norm": 0.19598469138145447,
      "learning_rate": 2.2059650434382727e-07,
      "loss": 0.0011,
      "step": 107980
    },
    {
      "epoch": 19634.545454545456,
      "grad_norm": 0.19736000895500183,
      "learning_rate": 2.2050001873617712e-07,
      "loss": 0.0012,
      "step": 107990
    },
    {
      "epoch": 19636.363636363636,
      "grad_norm": 0.017380716279149055,
      "learning_rate": 2.2040354826462664e-07,
      "loss": 0.0012,
      "step": 108000
    },
    {
      "epoch": 19636.363636363636,
      "eval_loss": 5.0569539070129395,
      "eval_runtime": 0.9525,
      "eval_samples_per_second": 10.499,
      "eval_steps_per_second": 5.249,
      "step": 108000
    },
    {
      "epoch": 19638.18181818182,
      "grad_norm": 0.0010620967950671911,
      "learning_rate": 2.2030709293440048e-07,
      "loss": 0.0009,
      "step": 108010
    },
    {
      "epoch": 19640.0,
      "grad_norm": 0.17376582324504852,
      "learning_rate": 2.202106527507223e-07,
      "loss": 0.0012,
      "step": 108020
    },
    {
      "epoch": 19641.81818181818,
      "grad_norm": 0.001687899581156671,
      "learning_rate": 2.2011422771881454e-07,
      "loss": 0.0009,
      "step": 108030
    },
    {
      "epoch": 19643.636363636364,
      "grad_norm": 0.1601746827363968,
      "learning_rate": 2.2001781784389882e-07,
      "loss": 0.0013,
      "step": 108040
    },
    {
      "epoch": 19645.454545454544,
      "grad_norm": 0.21340401470661163,
      "learning_rate": 2.1992142313119662e-07,
      "loss": 0.001,
      "step": 108050
    },
    {
      "epoch": 19647.272727272728,
      "grad_norm": 0.21964019536972046,
      "learning_rate": 2.1982504358592773e-07,
      "loss": 0.0012,
      "step": 108060
    },
    {
      "epoch": 19649.090909090908,
      "grad_norm": 0.21242037415504456,
      "learning_rate": 2.1972867921331172e-07,
      "loss": 0.001,
      "step": 108070
    },
    {
      "epoch": 19650.909090909092,
      "grad_norm": 0.0005432106554508209,
      "learning_rate": 2.196323300185668e-07,
      "loss": 0.001,
      "step": 108080
    },
    {
      "epoch": 19652.727272727272,
      "grad_norm": 0.17259864509105682,
      "learning_rate": 2.1953599600691097e-07,
      "loss": 0.001,
      "step": 108090
    },
    {
      "epoch": 19654.545454545456,
      "grad_norm": 0.0007068744744174182,
      "learning_rate": 2.194396771835612e-07,
      "loss": 0.001,
      "step": 108100
    },
    {
      "epoch": 19656.363636363636,
      "grad_norm": 0.0009213582961820066,
      "learning_rate": 2.1934337355373345e-07,
      "loss": 0.0013,
      "step": 108110
    },
    {
      "epoch": 19658.18181818182,
      "grad_norm": 0.0011287200031802058,
      "learning_rate": 2.192470851226428e-07,
      "loss": 0.0009,
      "step": 108120
    },
    {
      "epoch": 19660.0,
      "grad_norm": 0.0009878091514110565,
      "learning_rate": 2.1915081189550394e-07,
      "loss": 0.0012,
      "step": 108130
    },
    {
      "epoch": 19661.81818181818,
      "grad_norm": 0.0011437159264460206,
      "learning_rate": 2.1905455387753036e-07,
      "loss": 0.001,
      "step": 108140
    },
    {
      "epoch": 19663.636363636364,
      "grad_norm": 0.0013224078575149179,
      "learning_rate": 2.1895831107393482e-07,
      "loss": 0.0012,
      "step": 108150
    },
    {
      "epoch": 19665.454545454544,
      "grad_norm": 0.1723354011774063,
      "learning_rate": 2.1886208348992913e-07,
      "loss": 0.0009,
      "step": 108160
    },
    {
      "epoch": 19667.272727272728,
      "grad_norm": 0.1489536464214325,
      "learning_rate": 2.1876587113072452e-07,
      "loss": 0.0012,
      "step": 108170
    },
    {
      "epoch": 19669.090909090908,
      "grad_norm": 0.21488352119922638,
      "learning_rate": 2.186696740015318e-07,
      "loss": 0.0012,
      "step": 108180
    },
    {
      "epoch": 19670.909090909092,
      "grad_norm": 0.1640343815088272,
      "learning_rate": 2.1857349210755954e-07,
      "loss": 0.0007,
      "step": 108190
    },
    {
      "epoch": 19672.727272727272,
      "grad_norm": 0.0006510738166980445,
      "learning_rate": 2.1847732545401688e-07,
      "loss": 0.0015,
      "step": 108200
    },
    {
      "epoch": 19674.545454545456,
      "grad_norm": 0.1697636991739273,
      "learning_rate": 2.1838117404611178e-07,
      "loss": 0.0012,
      "step": 108210
    },
    {
      "epoch": 19676.363636363636,
      "grad_norm": 0.16140200197696686,
      "learning_rate": 2.182850378890511e-07,
      "loss": 0.0011,
      "step": 108220
    },
    {
      "epoch": 19678.18181818182,
      "grad_norm": 0.0007737810374237597,
      "learning_rate": 2.181889169880408e-07,
      "loss": 0.001,
      "step": 108230
    },
    {
      "epoch": 19680.0,
      "grad_norm": 0.0003769779286812991,
      "learning_rate": 2.180928113482866e-07,
      "loss": 0.0012,
      "step": 108240
    },
    {
      "epoch": 19681.81818181818,
      "grad_norm": 0.00191599375102669,
      "learning_rate": 2.179967209749929e-07,
      "loss": 0.001,
      "step": 108250
    },
    {
      "epoch": 19683.636363636364,
      "grad_norm": 0.16506220400333405,
      "learning_rate": 2.1790064587336325e-07,
      "loss": 0.0014,
      "step": 108260
    },
    {
      "epoch": 19685.454545454544,
      "grad_norm": 0.16576172411441803,
      "learning_rate": 2.1780458604860052e-07,
      "loss": 0.0009,
      "step": 108270
    },
    {
      "epoch": 19687.272727272728,
      "grad_norm": 0.01720215566456318,
      "learning_rate": 2.1770854150590685e-07,
      "loss": 0.0012,
      "step": 108280
    },
    {
      "epoch": 19689.090909090908,
      "grad_norm": 0.0009184556547552347,
      "learning_rate": 2.1761251225048382e-07,
      "loss": 0.0009,
      "step": 108290
    },
    {
      "epoch": 19690.909090909092,
      "grad_norm": 0.0006284845294430852,
      "learning_rate": 2.1751649828753106e-07,
      "loss": 0.0009,
      "step": 108300
    },
    {
      "epoch": 19692.727272727272,
      "grad_norm": 0.0007729295757599175,
      "learning_rate": 2.1742049962224857e-07,
      "loss": 0.0015,
      "step": 108310
    },
    {
      "epoch": 19694.545454545456,
      "grad_norm": 0.0005898664239794016,
      "learning_rate": 2.1732451625983518e-07,
      "loss": 0.0012,
      "step": 108320
    },
    {
      "epoch": 19696.363636363636,
      "grad_norm": 0.2020014375448227,
      "learning_rate": 2.172285482054887e-07,
      "loss": 0.001,
      "step": 108330
    },
    {
      "epoch": 19698.18181818182,
      "grad_norm": 0.21774640679359436,
      "learning_rate": 2.171325954644061e-07,
      "loss": 0.001,
      "step": 108340
    },
    {
      "epoch": 19700.0,
      "grad_norm": 0.26479679346084595,
      "learning_rate": 2.1703665804178357e-07,
      "loss": 0.0012,
      "step": 108350
    },
    {
      "epoch": 19701.81818181818,
      "grad_norm": 0.0007364966440945864,
      "learning_rate": 2.169407359428166e-07,
      "loss": 0.0012,
      "step": 108360
    },
    {
      "epoch": 19703.636363636364,
      "grad_norm": 0.21757638454437256,
      "learning_rate": 2.1684482917270014e-07,
      "loss": 0.0012,
      "step": 108370
    },
    {
      "epoch": 19705.454545454544,
      "grad_norm": 0.017074918374419212,
      "learning_rate": 2.1674893773662733e-07,
      "loss": 0.0009,
      "step": 108380
    },
    {
      "epoch": 19707.272727272728,
      "grad_norm": 0.0005538949044421315,
      "learning_rate": 2.166530616397913e-07,
      "loss": 0.0009,
      "step": 108390
    },
    {
      "epoch": 19709.090909090908,
      "grad_norm": 0.0009011570364236832,
      "learning_rate": 2.165572008873845e-07,
      "loss": 0.0012,
      "step": 108400
    },
    {
      "epoch": 19710.909090909092,
      "grad_norm": 0.0009413363877683878,
      "learning_rate": 2.164613554845978e-07,
      "loss": 0.0012,
      "step": 108410
    },
    {
      "epoch": 19712.727272727272,
      "grad_norm": 0.20225924253463745,
      "learning_rate": 2.1636552543662184e-07,
      "loss": 0.001,
      "step": 108420
    },
    {
      "epoch": 19714.545454545456,
      "grad_norm": 0.20972926914691925,
      "learning_rate": 2.1626971074864596e-07,
      "loss": 0.0012,
      "step": 108430
    },
    {
      "epoch": 19716.363636363636,
      "grad_norm": 0.000542712863534689,
      "learning_rate": 2.1617391142585922e-07,
      "loss": 0.0009,
      "step": 108440
    },
    {
      "epoch": 19718.18181818182,
      "grad_norm": 0.00107563566416502,
      "learning_rate": 2.160781274734495e-07,
      "loss": 0.0012,
      "step": 108450
    },
    {
      "epoch": 19720.0,
      "grad_norm": 0.21461515128612518,
      "learning_rate": 2.159823588966036e-07,
      "loss": 0.0012,
      "step": 108460
    },
    {
      "epoch": 19721.81818181818,
      "grad_norm": 0.24231870472431183,
      "learning_rate": 2.158866057005081e-07,
      "loss": 0.0011,
      "step": 108470
    },
    {
      "epoch": 19723.636363636364,
      "grad_norm": 0.25670126080513,
      "learning_rate": 2.1579086789034867e-07,
      "loss": 0.0011,
      "step": 108480
    },
    {
      "epoch": 19725.454545454544,
      "grad_norm": 0.15641269087791443,
      "learning_rate": 2.156951454713093e-07,
      "loss": 0.0013,
      "step": 108490
    },
    {
      "epoch": 19727.272727272728,
      "grad_norm": 0.2659749388694763,
      "learning_rate": 2.155994384485742e-07,
      "loss": 0.001,
      "step": 108500
    },
    {
      "epoch": 19727.272727272728,
      "eval_loss": 5.148972034454346,
      "eval_runtime": 0.9515,
      "eval_samples_per_second": 10.509,
      "eval_steps_per_second": 5.255,
      "step": 108500
    },
    {
      "epoch": 19729.090909090908,
      "grad_norm": 0.00065712007926777,
      "learning_rate": 2.1550374682732604e-07,
      "loss": 0.0009,
      "step": 108510
    },
    {
      "epoch": 19730.909090909092,
      "grad_norm": 0.25518912076950073,
      "learning_rate": 2.1540807061274724e-07,
      "loss": 0.001,
      "step": 108520
    },
    {
      "epoch": 19732.727272727272,
      "grad_norm": 0.0006741145043633878,
      "learning_rate": 2.1531240981001892e-07,
      "loss": 0.001,
      "step": 108530
    },
    {
      "epoch": 19734.545454545456,
      "grad_norm": 0.21493925154209137,
      "learning_rate": 2.152167644243213e-07,
      "loss": 0.0013,
      "step": 108540
    },
    {
      "epoch": 19736.363636363636,
      "grad_norm": 0.0011560616549104452,
      "learning_rate": 2.151211344608342e-07,
      "loss": 0.0009,
      "step": 108550
    },
    {
      "epoch": 19738.18181818182,
      "grad_norm": 0.0004161359975114465,
      "learning_rate": 2.1502551992473666e-07,
      "loss": 0.0012,
      "step": 108560
    },
    {
      "epoch": 19740.0,
      "grad_norm": 0.000602460466325283,
      "learning_rate": 2.1492992082120597e-07,
      "loss": 0.0012,
      "step": 108570
    },
    {
      "epoch": 19741.81818181818,
      "grad_norm": 0.0006038655992597342,
      "learning_rate": 2.148343371554196e-07,
      "loss": 0.0012,
      "step": 108580
    },
    {
      "epoch": 19743.636363636364,
      "grad_norm": 0.2165212780237198,
      "learning_rate": 2.1473876893255387e-07,
      "loss": 0.0009,
      "step": 108590
    },
    {
      "epoch": 19745.454545454544,
      "grad_norm": 0.0006321967812255025,
      "learning_rate": 2.146432161577842e-07,
      "loss": 0.0009,
      "step": 108600
    },
    {
      "epoch": 19747.272727272728,
      "grad_norm": 0.0010581478709354997,
      "learning_rate": 2.14547678836285e-07,
      "loss": 0.0013,
      "step": 108610
    },
    {
      "epoch": 19749.090909090908,
      "grad_norm": 0.002019610023126006,
      "learning_rate": 2.1445215697322993e-07,
      "loss": 0.0011,
      "step": 108620
    },
    {
      "epoch": 19750.909090909092,
      "grad_norm": 0.0004606371803674847,
      "learning_rate": 2.1435665057379231e-07,
      "loss": 0.0011,
      "step": 108630
    },
    {
      "epoch": 19752.727272727272,
      "grad_norm": 0.005811604671180248,
      "learning_rate": 2.142611596431439e-07,
      "loss": 0.0013,
      "step": 108640
    },
    {
      "epoch": 19754.545454545456,
      "grad_norm": 0.17330174148082733,
      "learning_rate": 2.141656841864558e-07,
      "loss": 0.001,
      "step": 108650
    },
    {
      "epoch": 19756.363636363636,
      "grad_norm": 0.000578031234908849,
      "learning_rate": 2.140702242088987e-07,
      "loss": 0.001,
      "step": 108660
    },
    {
      "epoch": 19758.18181818182,
      "grad_norm": 0.0005152198718860745,
      "learning_rate": 2.1397477971564236e-07,
      "loss": 0.001,
      "step": 108670
    },
    {
      "epoch": 19760.0,
      "grad_norm": 0.15422245860099792,
      "learning_rate": 2.1387935071185486e-07,
      "loss": 0.0012,
      "step": 108680
    },
    {
      "epoch": 19761.81818181818,
      "grad_norm": 0.0011931973276659846,
      "learning_rate": 2.1378393720270466e-07,
      "loss": 0.0012,
      "step": 108690
    },
    {
      "epoch": 19763.636363636364,
      "grad_norm": 0.0007367741782218218,
      "learning_rate": 2.1368853919335832e-07,
      "loss": 0.0007,
      "step": 108700
    },
    {
      "epoch": 19765.454545454544,
      "grad_norm": 0.0004932548617944121,
      "learning_rate": 2.1359315668898254e-07,
      "loss": 0.0013,
      "step": 108710
    },
    {
      "epoch": 19767.272727272728,
      "grad_norm": 0.0004543311952147633,
      "learning_rate": 2.1349778969474246e-07,
      "loss": 0.0009,
      "step": 108720
    },
    {
      "epoch": 19769.090909090908,
      "grad_norm": 0.2105100005865097,
      "learning_rate": 2.1340243821580234e-07,
      "loss": 0.0012,
      "step": 108730
    },
    {
      "epoch": 19770.909090909092,
      "grad_norm": 0.000785570708103478,
      "learning_rate": 2.1330710225732617e-07,
      "loss": 0.0009,
      "step": 108740
    },
    {
      "epoch": 19772.727272727272,
      "grad_norm": 0.0008439485682174563,
      "learning_rate": 2.1321178182447709e-07,
      "loss": 0.0016,
      "step": 108750
    },
    {
      "epoch": 19774.545454545456,
      "grad_norm": 0.16210126876831055,
      "learning_rate": 2.1311647692241636e-07,
      "loss": 0.0007,
      "step": 108760
    },
    {
      "epoch": 19776.363636363636,
      "grad_norm": 0.0008999091223813593,
      "learning_rate": 2.1302118755630573e-07,
      "loss": 0.0011,
      "step": 108770
    },
    {
      "epoch": 19778.18181818182,
      "grad_norm": 0.0013999827206134796,
      "learning_rate": 2.1292591373130515e-07,
      "loss": 0.0011,
      "step": 108780
    },
    {
      "epoch": 19780.0,
      "grad_norm": 0.16074422001838684,
      "learning_rate": 2.128306554525744e-07,
      "loss": 0.0012,
      "step": 108790
    },
    {
      "epoch": 19781.81818181818,
      "grad_norm": 0.001152955461293459,
      "learning_rate": 2.12735412725272e-07,
      "loss": 0.0008,
      "step": 108800
    },
    {
      "epoch": 19783.636363636364,
      "grad_norm": 0.00045254273572936654,
      "learning_rate": 2.126401855545556e-07,
      "loss": 0.0012,
      "step": 108810
    },
    {
      "epoch": 19785.454545454544,
      "grad_norm": 0.2161492258310318,
      "learning_rate": 2.125449739455824e-07,
      "loss": 0.0015,
      "step": 108820
    },
    {
      "epoch": 19787.272727272728,
      "grad_norm": 0.010180680081248283,
      "learning_rate": 2.1244977790350843e-07,
      "loss": 0.001,
      "step": 108830
    },
    {
      "epoch": 19789.090909090908,
      "grad_norm": 0.0005519873229786754,
      "learning_rate": 2.1235459743348872e-07,
      "loss": 0.0009,
      "step": 108840
    },
    {
      "epoch": 19790.909090909092,
      "grad_norm": 0.0005094772204756737,
      "learning_rate": 2.122594325406779e-07,
      "loss": 0.0012,
      "step": 108850
    },
    {
      "epoch": 19792.727272727272,
      "grad_norm": 0.20368005335330963,
      "learning_rate": 2.121642832302299e-07,
      "loss": 0.001,
      "step": 108860
    },
    {
      "epoch": 19794.545454545456,
      "grad_norm": 0.0006283984403125942,
      "learning_rate": 2.1206914950729672e-07,
      "loss": 0.001,
      "step": 108870
    },
    {
      "epoch": 19796.363636363636,
      "grad_norm": 0.0005070932093076408,
      "learning_rate": 2.119740313770308e-07,
      "loss": 0.0012,
      "step": 108880
    },
    {
      "epoch": 19798.18181818182,
      "grad_norm": 0.21523891389369965,
      "learning_rate": 2.1187892884458287e-07,
      "loss": 0.001,
      "step": 108890
    },
    {
      "epoch": 19800.0,
      "grad_norm": 0.21553759276866913,
      "learning_rate": 2.117838419151034e-07,
      "loss": 0.001,
      "step": 108900
    },
    {
      "epoch": 19801.81818181818,
      "grad_norm": 0.13850826025009155,
      "learning_rate": 2.116887705937417e-07,
      "loss": 0.001,
      "step": 108910
    },
    {
      "epoch": 19803.636363636364,
      "grad_norm": 0.21398213505744934,
      "learning_rate": 2.1159371488564593e-07,
      "loss": 0.0012,
      "step": 108920
    },
    {
      "epoch": 19805.454545454544,
      "grad_norm": 0.000759656133595854,
      "learning_rate": 2.114986747959643e-07,
      "loss": 0.0009,
      "step": 108930
    },
    {
      "epoch": 19807.272727272728,
      "grad_norm": 0.0012033437378704548,
      "learning_rate": 2.1140365032984332e-07,
      "loss": 0.0012,
      "step": 108940
    },
    {
      "epoch": 19809.090909090908,
      "grad_norm": 0.003076031571254134,
      "learning_rate": 2.1130864149242877e-07,
      "loss": 0.0011,
      "step": 108950
    },
    {
      "epoch": 19810.909090909092,
      "grad_norm": 0.0006248875870369375,
      "learning_rate": 2.112136482888663e-07,
      "loss": 0.001,
      "step": 108960
    },
    {
      "epoch": 19812.727272727272,
      "grad_norm": 0.0010799771407619119,
      "learning_rate": 2.1111867072429967e-07,
      "loss": 0.001,
      "step": 108970
    },
    {
      "epoch": 19814.545454545456,
      "grad_norm": 0.16274207830429077,
      "learning_rate": 2.1102370880387272e-07,
      "loss": 0.0013,
      "step": 108980
    },
    {
      "epoch": 19816.363636363636,
      "grad_norm": 0.0007618755334988236,
      "learning_rate": 2.109287625327279e-07,
      "loss": 0.0008,
      "step": 108990
    },
    {
      "epoch": 19818.18181818182,
      "grad_norm": 0.2599441111087799,
      "learning_rate": 2.1083383191600672e-07,
      "loss": 0.0015,
      "step": 109000
    },
    {
      "epoch": 19818.18181818182,
      "eval_loss": 5.142953872680664,
      "eval_runtime": 0.9523,
      "eval_samples_per_second": 10.501,
      "eval_steps_per_second": 5.251,
      "step": 109000
    },
    {
      "epoch": 19820.0,
      "grad_norm": 0.1698109358549118,
      "learning_rate": 2.1073891695885043e-07,
      "loss": 0.0009,
      "step": 109010
    },
    {
      "epoch": 19821.81818181818,
      "grad_norm": 0.22101162374019623,
      "learning_rate": 2.1064401766639893e-07,
      "loss": 0.0012,
      "step": 109020
    },
    {
      "epoch": 19823.636363636364,
      "grad_norm": 0.16043046116828918,
      "learning_rate": 2.1054913404379126e-07,
      "loss": 0.0012,
      "step": 109030
    },
    {
      "epoch": 19825.454545454544,
      "grad_norm": 0.0011305519146844745,
      "learning_rate": 2.1045426609616606e-07,
      "loss": 0.0008,
      "step": 109040
    },
    {
      "epoch": 19827.272727272728,
      "grad_norm": 0.0011185528710484505,
      "learning_rate": 2.1035941382866067e-07,
      "loss": 0.0013,
      "step": 109050
    },
    {
      "epoch": 19829.090909090908,
      "grad_norm": 0.0009311306057497859,
      "learning_rate": 2.102645772464116e-07,
      "loss": 0.0009,
      "step": 109060
    },
    {
      "epoch": 19830.909090909092,
      "grad_norm": 0.0015647753607481718,
      "learning_rate": 2.1016975635455497e-07,
      "loss": 0.0012,
      "step": 109070
    },
    {
      "epoch": 19832.727272727272,
      "grad_norm": 0.1490078866481781,
      "learning_rate": 2.1007495115822537e-07,
      "loss": 0.0012,
      "step": 109080
    },
    {
      "epoch": 19834.545454545456,
      "grad_norm": 0.000549168384168297,
      "learning_rate": 2.0998016166255728e-07,
      "loss": 0.0007,
      "step": 109090
    },
    {
      "epoch": 19836.363636363636,
      "grad_norm": 0.0010787550127133727,
      "learning_rate": 2.0988538787268373e-07,
      "loss": 0.0012,
      "step": 109100
    },
    {
      "epoch": 19838.18181818182,
      "grad_norm": 0.0006885969778522849,
      "learning_rate": 2.0979062979373695e-07,
      "loss": 0.001,
      "step": 109110
    },
    {
      "epoch": 19840.0,
      "grad_norm": 0.20206919312477112,
      "learning_rate": 2.0969588743084893e-07,
      "loss": 0.0012,
      "step": 109120
    },
    {
      "epoch": 19841.81818181818,
      "grad_norm": 0.2182602882385254,
      "learning_rate": 2.0960116078915007e-07,
      "loss": 0.0012,
      "step": 109130
    },
    {
      "epoch": 19843.636363636364,
      "grad_norm": 0.001014436362311244,
      "learning_rate": 2.0950644987377008e-07,
      "loss": 0.001,
      "step": 109140
    },
    {
      "epoch": 19845.454545454544,
      "grad_norm": 0.16625156998634338,
      "learning_rate": 2.0941175468983835e-07,
      "loss": 0.0009,
      "step": 109150
    },
    {
      "epoch": 19847.272727272728,
      "grad_norm": 0.000682785059325397,
      "learning_rate": 2.0931707524248266e-07,
      "loss": 0.001,
      "step": 109160
    },
    {
      "epoch": 19849.090909090908,
      "grad_norm": 0.28263068199157715,
      "learning_rate": 2.092224115368306e-07,
      "loss": 0.0012,
      "step": 109170
    },
    {
      "epoch": 19850.909090909092,
      "grad_norm": 0.000533221464138478,
      "learning_rate": 2.0912776357800849e-07,
      "loss": 0.001,
      "step": 109180
    },
    {
      "epoch": 19852.727272727272,
      "grad_norm": 0.0004968569846823812,
      "learning_rate": 2.0903313137114172e-07,
      "loss": 0.001,
      "step": 109190
    },
    {
      "epoch": 19854.545454545456,
      "grad_norm": 0.27529609203338623,
      "learning_rate": 2.0893851492135532e-07,
      "loss": 0.0015,
      "step": 109200
    },
    {
      "epoch": 19856.363636363636,
      "grad_norm": 0.0006647476111538708,
      "learning_rate": 2.0884391423377307e-07,
      "loss": 0.0006,
      "step": 109210
    },
    {
      "epoch": 19858.18181818182,
      "grad_norm": 0.16071896255016327,
      "learning_rate": 2.087493293135178e-07,
      "loss": 0.0013,
      "step": 109220
    },
    {
      "epoch": 19860.0,
      "grad_norm": 0.2190162092447281,
      "learning_rate": 2.0865476016571203e-07,
      "loss": 0.0011,
      "step": 109230
    },
    {
      "epoch": 19861.81818181818,
      "grad_norm": 0.2731877565383911,
      "learning_rate": 2.0856020679547687e-07,
      "loss": 0.0012,
      "step": 109240
    },
    {
      "epoch": 19863.636363636364,
      "grad_norm": 0.23015354573726654,
      "learning_rate": 2.0846566920793264e-07,
      "loss": 0.0012,
      "step": 109250
    },
    {
      "epoch": 19865.454545454544,
      "grad_norm": 0.0012738666264340281,
      "learning_rate": 2.0837114740819927e-07,
      "loss": 0.0009,
      "step": 109260
    },
    {
      "epoch": 19867.272727272728,
      "grad_norm": 0.18045105040073395,
      "learning_rate": 2.0827664140139523e-07,
      "loss": 0.0012,
      "step": 109270
    },
    {
      "epoch": 19869.090909090908,
      "grad_norm": 0.16281500458717346,
      "learning_rate": 2.0818215119263871e-07,
      "loss": 0.0011,
      "step": 109280
    },
    {
      "epoch": 19870.909090909092,
      "grad_norm": 0.22518882155418396,
      "learning_rate": 2.080876767870466e-07,
      "loss": 0.0012,
      "step": 109290
    },
    {
      "epoch": 19872.727272727272,
      "grad_norm": 0.0005757146282121539,
      "learning_rate": 2.0799321818973488e-07,
      "loss": 0.0012,
      "step": 109300
    },
    {
      "epoch": 19874.545454545456,
      "grad_norm": 0.000910367292817682,
      "learning_rate": 2.0789877540581923e-07,
      "loss": 0.001,
      "step": 109310
    },
    {
      "epoch": 19876.363636363636,
      "grad_norm": 0.20972634851932526,
      "learning_rate": 2.07804348440414e-07,
      "loss": 0.0013,
      "step": 109320
    },
    {
      "epoch": 19878.18181818182,
      "grad_norm": 0.0009874351089820266,
      "learning_rate": 2.0770993729863262e-07,
      "loss": 0.0011,
      "step": 109330
    },
    {
      "epoch": 19880.0,
      "grad_norm": 0.0018766368739306927,
      "learning_rate": 2.0761554198558816e-07,
      "loss": 0.0012,
      "step": 109340
    },
    {
      "epoch": 19881.81818181818,
      "grad_norm": 0.0005096826935186982,
      "learning_rate": 2.0752116250639224e-07,
      "loss": 0.0009,
      "step": 109350
    },
    {
      "epoch": 19883.636363636364,
      "grad_norm": 0.0007897153263911605,
      "learning_rate": 2.0742679886615615e-07,
      "loss": 0.0014,
      "step": 109360
    },
    {
      "epoch": 19885.454545454544,
      "grad_norm": 0.2546091377735138,
      "learning_rate": 2.0733245106999003e-07,
      "loss": 0.001,
      "step": 109370
    },
    {
      "epoch": 19887.272727272728,
      "grad_norm": 0.0010437839664518833,
      "learning_rate": 2.0723811912300292e-07,
      "loss": 0.0009,
      "step": 109380
    },
    {
      "epoch": 19889.090909090908,
      "grad_norm": 0.0010133605683222413,
      "learning_rate": 2.071438030303037e-07,
      "loss": 0.0011,
      "step": 109390
    },
    {
      "epoch": 19890.909090909092,
      "grad_norm": 0.0006956671713851392,
      "learning_rate": 2.0704950279699983e-07,
      "loss": 0.0012,
      "step": 109400
    },
    {
      "epoch": 19892.727272727272,
      "grad_norm": 0.15688864886760712,
      "learning_rate": 2.0695521842819785e-07,
      "loss": 0.0012,
      "step": 109410
    },
    {
      "epoch": 19894.545454545456,
      "grad_norm": 0.011163158342242241,
      "learning_rate": 2.06860949929004e-07,
      "loss": 0.0013,
      "step": 109420
    },
    {
      "epoch": 19896.363636363636,
      "grad_norm": 0.0005104748415760696,
      "learning_rate": 2.0676669730452317e-07,
      "loss": 0.0006,
      "step": 109430
    },
    {
      "epoch": 19898.18181818182,
      "grad_norm": 0.21485686302185059,
      "learning_rate": 2.0667246055985938e-07,
      "loss": 0.0016,
      "step": 109440
    },
    {
      "epoch": 19900.0,
      "grad_norm": 0.0004235208034515381,
      "learning_rate": 2.0657823970011617e-07,
      "loss": 0.0008,
      "step": 109450
    },
    {
      "epoch": 19901.81818181818,
      "grad_norm": 0.21490463614463806,
      "learning_rate": 2.0648403473039582e-07,
      "loss": 0.0011,
      "step": 109460
    },
    {
      "epoch": 19903.636363636364,
      "grad_norm": 0.20089948177337646,
      "learning_rate": 2.063898456558002e-07,
      "loss": 0.0013,
      "step": 109470
    },
    {
      "epoch": 19905.454545454544,
      "grad_norm": 0.0006493414985015988,
      "learning_rate": 2.0629567248142987e-07,
      "loss": 0.0012,
      "step": 109480
    },
    {
      "epoch": 19907.272727272728,
      "grad_norm": 0.21822680532932281,
      "learning_rate": 2.0620151521238448e-07,
      "loss": 0.001,
      "step": 109490
    },
    {
      "epoch": 19909.090909090908,
      "grad_norm": 0.0005849294830113649,
      "learning_rate": 2.0610737385376348e-07,
      "loss": 0.0009,
      "step": 109500
    },
    {
      "epoch": 19909.090909090908,
      "eval_loss": 5.117886543273926,
      "eval_runtime": 0.9501,
      "eval_samples_per_second": 10.525,
      "eval_steps_per_second": 5.263,
      "step": 109500
    },
    {
      "epoch": 19910.909090909092,
      "grad_norm": 0.2536925971508026,
      "learning_rate": 2.0601324841066474e-07,
      "loss": 0.0012,
      "step": 109510
    },
    {
      "epoch": 19912.727272727272,
      "grad_norm": 0.0006263137911446393,
      "learning_rate": 2.0591913888818552e-07,
      "loss": 0.0012,
      "step": 109520
    },
    {
      "epoch": 19914.545454545456,
      "grad_norm": 0.0004942138912156224,
      "learning_rate": 2.0582504529142246e-07,
      "loss": 0.001,
      "step": 109530
    },
    {
      "epoch": 19916.363636363636,
      "grad_norm": 0.0007241068524308503,
      "learning_rate": 2.0573096762547082e-07,
      "loss": 0.0012,
      "step": 109540
    },
    {
      "epoch": 19918.18181818182,
      "grad_norm": 0.16446398198604584,
      "learning_rate": 2.0563690589542572e-07,
      "loss": 0.0013,
      "step": 109550
    },
    {
      "epoch": 19920.0,
      "grad_norm": 0.21778883039951324,
      "learning_rate": 2.0554286010638072e-07,
      "loss": 0.001,
      "step": 109560
    },
    {
      "epoch": 19921.81818181818,
      "grad_norm": 0.0008490029140375555,
      "learning_rate": 2.0544883026342862e-07,
      "loss": 0.0009,
      "step": 109570
    },
    {
      "epoch": 19923.636363636364,
      "grad_norm": 0.15445435047149658,
      "learning_rate": 2.05354816371662e-07,
      "loss": 0.0015,
      "step": 109580
    },
    {
      "epoch": 19925.454545454544,
      "grad_norm": 0.0005139941349625587,
      "learning_rate": 2.052608184361718e-07,
      "loss": 0.0009,
      "step": 109590
    },
    {
      "epoch": 19927.272727272728,
      "grad_norm": 0.0006184361991472542,
      "learning_rate": 2.0516683646204836e-07,
      "loss": 0.0009,
      "step": 109600
    },
    {
      "epoch": 19929.090909090908,
      "grad_norm": 0.000534671125933528,
      "learning_rate": 2.0507287045438143e-07,
      "loss": 0.0012,
      "step": 109610
    },
    {
      "epoch": 19930.909090909092,
      "grad_norm": 0.0006972622941248119,
      "learning_rate": 2.0497892041825955e-07,
      "loss": 0.001,
      "step": 109620
    },
    {
      "epoch": 19932.727272727272,
      "grad_norm": 0.26958364248275757,
      "learning_rate": 2.0488498635877028e-07,
      "loss": 0.0013,
      "step": 109630
    },
    {
      "epoch": 19934.545454545456,
      "grad_norm": 0.0013288554036989808,
      "learning_rate": 2.0479106828100096e-07,
      "loss": 0.0008,
      "step": 109640
    },
    {
      "epoch": 19936.363636363636,
      "grad_norm": 0.0008158991113305092,
      "learning_rate": 2.0469716619003723e-07,
      "loss": 0.0011,
      "step": 109650
    },
    {
      "epoch": 19938.18181818182,
      "grad_norm": 0.0006665721884928644,
      "learning_rate": 2.0460328009096494e-07,
      "loss": 0.0011,
      "step": 109660
    },
    {
      "epoch": 19940.0,
      "grad_norm": 0.00041153765050694346,
      "learning_rate": 2.0450940998886755e-07,
      "loss": 0.0012,
      "step": 109670
    },
    {
      "epoch": 19941.81818181818,
      "grad_norm": 0.0006516380817629397,
      "learning_rate": 2.0441555588882896e-07,
      "loss": 0.0009,
      "step": 109680
    },
    {
      "epoch": 19943.636363636364,
      "grad_norm": 0.23183339834213257,
      "learning_rate": 2.0432171779593198e-07,
      "loss": 0.0018,
      "step": 109690
    },
    {
      "epoch": 19945.454545454544,
      "grad_norm": 0.2001837193965912,
      "learning_rate": 2.042278957152581e-07,
      "loss": 0.001,
      "step": 109700
    },
    {
      "epoch": 19947.272727272728,
      "grad_norm": 0.2377723753452301,
      "learning_rate": 2.04134089651888e-07,
      "loss": 0.0011,
      "step": 109710
    },
    {
      "epoch": 19949.090909090908,
      "grad_norm": 0.16406235098838806,
      "learning_rate": 2.0404029961090202e-07,
      "loss": 0.0009,
      "step": 109720
    },
    {
      "epoch": 19950.909090909092,
      "grad_norm": 0.0008753467700444162,
      "learning_rate": 2.03946525597379e-07,
      "loss": 0.0012,
      "step": 109730
    },
    {
      "epoch": 19952.727272727272,
      "grad_norm": 0.23429688811302185,
      "learning_rate": 2.0385276761639765e-07,
      "loss": 0.0012,
      "step": 109740
    },
    {
      "epoch": 19954.545454545456,
      "grad_norm": 0.17253977060317993,
      "learning_rate": 2.037590256730347e-07,
      "loss": 0.0007,
      "step": 109750
    },
    {
      "epoch": 19956.363636363636,
      "grad_norm": 0.1891057938337326,
      "learning_rate": 2.03665299772367e-07,
      "loss": 0.0012,
      "step": 109760
    },
    {
      "epoch": 19958.18181818182,
      "grad_norm": 0.0005797569756396115,
      "learning_rate": 2.0357158991947037e-07,
      "loss": 0.0011,
      "step": 109770
    },
    {
      "epoch": 19960.0,
      "grad_norm": 0.0005289008840918541,
      "learning_rate": 2.0347789611941945e-07,
      "loss": 0.0012,
      "step": 109780
    },
    {
      "epoch": 19961.81818181818,
      "grad_norm": 0.1668487787246704,
      "learning_rate": 2.0338421837728797e-07,
      "loss": 0.0012,
      "step": 109790
    },
    {
      "epoch": 19963.636363636364,
      "grad_norm": 0.23228059709072113,
      "learning_rate": 2.0329055669814933e-07,
      "loss": 0.0009,
      "step": 109800
    },
    {
      "epoch": 19965.454545454544,
      "grad_norm": 0.0007260825368575752,
      "learning_rate": 2.0319691108707547e-07,
      "loss": 0.0009,
      "step": 109810
    },
    {
      "epoch": 19967.272727272728,
      "grad_norm": 0.27369385957717896,
      "learning_rate": 2.0310328154913758e-07,
      "loss": 0.0016,
      "step": 109820
    },
    {
      "epoch": 19969.090909090908,
      "grad_norm": 0.000525598181411624,
      "learning_rate": 2.0300966808940645e-07,
      "loss": 0.0007,
      "step": 109830
    },
    {
      "epoch": 19970.909090909092,
      "grad_norm": 0.15638259053230286,
      "learning_rate": 2.0291607071295124e-07,
      "loss": 0.001,
      "step": 109840
    },
    {
      "epoch": 19972.727272727272,
      "grad_norm": 0.18745282292366028,
      "learning_rate": 2.028224894248412e-07,
      "loss": 0.0012,
      "step": 109850
    },
    {
      "epoch": 19974.545454545456,
      "grad_norm": 0.0006685337284579873,
      "learning_rate": 2.027289242301435e-07,
      "loss": 0.001,
      "step": 109860
    },
    {
      "epoch": 19976.363636363636,
      "grad_norm": 0.21649067103862762,
      "learning_rate": 2.026353751339253e-07,
      "loss": 0.0014,
      "step": 109870
    },
    {
      "epoch": 19978.18181818182,
      "grad_norm": 0.00045842654071748257,
      "learning_rate": 2.0254184214125299e-07,
      "loss": 0.0009,
      "step": 109880
    },
    {
      "epoch": 19980.0,
      "grad_norm": 0.16208548843860626,
      "learning_rate": 2.0244832525719152e-07,
      "loss": 0.0012,
      "step": 109890
    },
    {
      "epoch": 19981.81818181818,
      "grad_norm": 0.0007702008588239551,
      "learning_rate": 2.0235482448680507e-07,
      "loss": 0.0009,
      "step": 109900
    },
    {
      "epoch": 19983.636363636364,
      "grad_norm": 0.20415866374969482,
      "learning_rate": 2.0226133983515747e-07,
      "loss": 0.0015,
      "step": 109910
    },
    {
      "epoch": 19985.454545454544,
      "grad_norm": 0.1576438993215561,
      "learning_rate": 2.02167871307311e-07,
      "loss": 0.0009,
      "step": 109920
    },
    {
      "epoch": 19987.272727272728,
      "grad_norm": 0.0014620096189901233,
      "learning_rate": 2.0207441890832777e-07,
      "loss": 0.0009,
      "step": 109930
    },
    {
      "epoch": 19989.090909090908,
      "grad_norm": 0.000548648415133357,
      "learning_rate": 2.0198098264326803e-07,
      "loss": 0.0012,
      "step": 109940
    },
    {
      "epoch": 19990.909090909092,
      "grad_norm": 0.0006634060409851372,
      "learning_rate": 2.0188756251719203e-07,
      "loss": 0.001,
      "step": 109950
    },
    {
      "epoch": 19992.727272727272,
      "grad_norm": 0.1590518057346344,
      "learning_rate": 2.0179415853515907e-07,
      "loss": 0.0011,
      "step": 109960
    },
    {
      "epoch": 19994.545454545456,
      "grad_norm": 0.20877376198768616,
      "learning_rate": 2.0170077070222724e-07,
      "loss": 0.0012,
      "step": 109970
    },
    {
      "epoch": 19996.363636363636,
      "grad_norm": 0.000682978832628578,
      "learning_rate": 2.0160739902345358e-07,
      "loss": 0.0009,
      "step": 109980
    },
    {
      "epoch": 19998.18181818182,
      "grad_norm": 0.2312130481004715,
      "learning_rate": 2.0151404350389507e-07,
      "loss": 0.0012,
      "step": 109990
    },
    {
      "epoch": 20000.0,
      "grad_norm": 0.20577073097229004,
      "learning_rate": 2.01420704148607e-07,
      "loss": 0.001,
      "step": 110000
    },
    {
      "epoch": 20000.0,
      "eval_loss": 5.133653163909912,
      "eval_runtime": 0.9509,
      "eval_samples_per_second": 10.516,
      "eval_steps_per_second": 5.258,
      "step": 110000
    },
    {
      "epoch": 20001.81818181818,
      "grad_norm": 0.000425983511377126,
      "learning_rate": 2.0132738096264412e-07,
      "loss": 0.0012,
      "step": 110010
    },
    {
      "epoch": 20003.636363636364,
      "grad_norm": 0.2695108652114868,
      "learning_rate": 2.0123407395106017e-07,
      "loss": 0.0008,
      "step": 110020
    },
    {
      "epoch": 20005.454545454544,
      "grad_norm": 0.0005708517273887992,
      "learning_rate": 2.011407831189082e-07,
      "loss": 0.0012,
      "step": 110030
    },
    {
      "epoch": 20007.272727272728,
      "grad_norm": 0.2072165608406067,
      "learning_rate": 2.0104750847124075e-07,
      "loss": 0.0012,
      "step": 110040
    },
    {
      "epoch": 20009.090909090908,
      "grad_norm": 0.0005536354728974402,
      "learning_rate": 2.0095425001310817e-07,
      "loss": 0.001,
      "step": 110050
    },
    {
      "epoch": 20010.909090909092,
      "grad_norm": 0.21104587614536285,
      "learning_rate": 2.0086100774956133e-07,
      "loss": 0.0012,
      "step": 110060
    },
    {
      "epoch": 20012.727272727272,
      "grad_norm": 0.2026827484369278,
      "learning_rate": 2.0076778168564979e-07,
      "loss": 0.001,
      "step": 110070
    },
    {
      "epoch": 20014.545454545456,
      "grad_norm": 0.0004084342217538506,
      "learning_rate": 2.0067457182642196e-07,
      "loss": 0.0012,
      "step": 110080
    },
    {
      "epoch": 20016.363636363636,
      "grad_norm": 0.0006768449675291777,
      "learning_rate": 2.0058137817692529e-07,
      "loss": 0.0007,
      "step": 110090
    },
    {
      "epoch": 20018.18181818182,
      "grad_norm": 0.26141199469566345,
      "learning_rate": 2.0048820074220711e-07,
      "loss": 0.0015,
      "step": 110100
    },
    {
      "epoch": 20020.0,
      "grad_norm": 0.0005466281436383724,
      "learning_rate": 2.003950395273129e-07,
      "loss": 0.001,
      "step": 110110
    },
    {
      "epoch": 20021.81818181818,
      "grad_norm": 0.0005154826212674379,
      "learning_rate": 2.0030189453728835e-07,
      "loss": 0.0011,
      "step": 110120
    },
    {
      "epoch": 20023.636363636364,
      "grad_norm": 0.16588276624679565,
      "learning_rate": 2.0020876577717688e-07,
      "loss": 0.0011,
      "step": 110130
    },
    {
      "epoch": 20025.454545454544,
      "grad_norm": 0.0008193400572054088,
      "learning_rate": 2.001156532520222e-07,
      "loss": 0.0011,
      "step": 110140
    },
    {
      "epoch": 20027.272727272728,
      "grad_norm": 0.0005373528110794723,
      "learning_rate": 2.0002255696686688e-07,
      "loss": 0.0009,
      "step": 110150
    },
    {
      "epoch": 20029.090909090908,
      "grad_norm": 0.0006441693985834718,
      "learning_rate": 1.9992947692675227e-07,
      "loss": 0.0012,
      "step": 110160
    },
    {
      "epoch": 20030.909090909092,
      "grad_norm": 0.15765947103500366,
      "learning_rate": 1.99836413136719e-07,
      "loss": 0.0012,
      "step": 110170
    },
    {
      "epoch": 20032.727272727272,
      "grad_norm": 0.2692717909812927,
      "learning_rate": 1.997433656018071e-07,
      "loss": 0.001,
      "step": 110180
    },
    {
      "epoch": 20034.545454545456,
      "grad_norm": 0.16049697995185852,
      "learning_rate": 1.9965033432705537e-07,
      "loss": 0.0012,
      "step": 110190
    },
    {
      "epoch": 20036.363636363636,
      "grad_norm": 0.1656292974948883,
      "learning_rate": 1.9955731931750181e-07,
      "loss": 0.001,
      "step": 110200
    },
    {
      "epoch": 20038.18181818182,
      "grad_norm": 0.15838490426540375,
      "learning_rate": 1.994643205781834e-07,
      "loss": 0.0011,
      "step": 110210
    },
    {
      "epoch": 20040.0,
      "grad_norm": 0.16353321075439453,
      "learning_rate": 1.9937133811413663e-07,
      "loss": 0.001,
      "step": 110220
    },
    {
      "epoch": 20041.81818181818,
      "grad_norm": 0.15855956077575684,
      "learning_rate": 1.992783719303972e-07,
      "loss": 0.0012,
      "step": 110230
    },
    {
      "epoch": 20043.636363636364,
      "grad_norm": 0.2561418414115906,
      "learning_rate": 1.991854220319989e-07,
      "loss": 0.0012,
      "step": 110240
    },
    {
      "epoch": 20045.454545454544,
      "grad_norm": 0.203555628657341,
      "learning_rate": 1.990924884239758e-07,
      "loss": 0.0009,
      "step": 110250
    },
    {
      "epoch": 20047.272727272728,
      "grad_norm": 0.0006144645158201456,
      "learning_rate": 1.9899957111136072e-07,
      "loss": 0.0011,
      "step": 110260
    },
    {
      "epoch": 20049.090909090908,
      "grad_norm": 0.2094111144542694,
      "learning_rate": 1.9890667009918538e-07,
      "loss": 0.001,
      "step": 110270
    },
    {
      "epoch": 20050.909090909092,
      "grad_norm": 0.001748821116052568,
      "learning_rate": 1.9881378539248077e-07,
      "loss": 0.0012,
      "step": 110280
    },
    {
      "epoch": 20052.727272727272,
      "grad_norm": 0.001805350766517222,
      "learning_rate": 1.9872091699627685e-07,
      "loss": 0.0011,
      "step": 110290
    },
    {
      "epoch": 20054.545454545456,
      "grad_norm": 0.0013629155000671744,
      "learning_rate": 1.9862806491560314e-07,
      "loss": 0.001,
      "step": 110300
    },
    {
      "epoch": 20056.363636363636,
      "grad_norm": 0.21620605885982513,
      "learning_rate": 1.9853522915548775e-07,
      "loss": 0.0012,
      "step": 110310
    },
    {
      "epoch": 20058.18181818182,
      "grad_norm": 0.17457666993141174,
      "learning_rate": 1.984424097209581e-07,
      "loss": 0.0012,
      "step": 110320
    },
    {
      "epoch": 20060.0,
      "grad_norm": 0.0013270315248519182,
      "learning_rate": 1.9834960661704076e-07,
      "loss": 0.001,
      "step": 110330
    },
    {
      "epoch": 20061.81818181818,
      "grad_norm": 0.21794764697551727,
      "learning_rate": 1.982568198487617e-07,
      "loss": 0.0012,
      "step": 110340
    },
    {
      "epoch": 20063.636363636364,
      "grad_norm": 0.0004622602427843958,
      "learning_rate": 1.9816404942114546e-07,
      "loss": 0.0009,
      "step": 110350
    },
    {
      "epoch": 20065.454545454544,
      "grad_norm": 0.0013947512488812208,
      "learning_rate": 1.9807129533921595e-07,
      "loss": 0.0012,
      "step": 110360
    },
    {
      "epoch": 20067.272727272728,
      "grad_norm": 0.0010249984916299582,
      "learning_rate": 1.9797855760799608e-07,
      "loss": 0.001,
      "step": 110370
    },
    {
      "epoch": 20069.090909090908,
      "grad_norm": 0.0006313013145700097,
      "learning_rate": 1.9788583623250836e-07,
      "loss": 0.001,
      "step": 110380
    },
    {
      "epoch": 20070.909090909092,
      "grad_norm": 0.16436360776424408,
      "learning_rate": 1.9779313121777381e-07,
      "loss": 0.0011,
      "step": 110390
    },
    {
      "epoch": 20072.727272727272,
      "grad_norm": 0.205504909157753,
      "learning_rate": 1.9770044256881258e-07,
      "loss": 0.0008,
      "step": 110400
    },
    {
      "epoch": 20074.545454545456,
      "grad_norm": 0.15582561492919922,
      "learning_rate": 1.976077702906444e-07,
      "loss": 0.0015,
      "step": 110410
    },
    {
      "epoch": 20076.363636363636,
      "grad_norm": 0.0007635086076334119,
      "learning_rate": 1.975151143882882e-07,
      "loss": 0.0006,
      "step": 110420
    },
    {
      "epoch": 20078.18181818182,
      "grad_norm": 0.15638601779937744,
      "learning_rate": 1.9742247486676096e-07,
      "loss": 0.0013,
      "step": 110430
    },
    {
      "epoch": 20080.0,
      "grad_norm": 0.0005419969093054533,
      "learning_rate": 1.973298517310799e-07,
      "loss": 0.001,
      "step": 110440
    },
    {
      "epoch": 20081.81818181818,
      "grad_norm": 0.23403790593147278,
      "learning_rate": 1.9723724498626104e-07,
      "loss": 0.0012,
      "step": 110450
    },
    {
      "epoch": 20083.636363636364,
      "grad_norm": 0.21893253922462463,
      "learning_rate": 1.9714465463731934e-07,
      "loss": 0.001,
      "step": 110460
    },
    {
      "epoch": 20085.454545454544,
      "grad_norm": 0.0030868409667164087,
      "learning_rate": 1.9705208068926894e-07,
      "loss": 0.0009,
      "step": 110470
    },
    {
      "epoch": 20087.272727272728,
      "grad_norm": 0.16277900338172913,
      "learning_rate": 1.9695952314712287e-07,
      "loss": 0.0013,
      "step": 110480
    },
    {
      "epoch": 20089.090909090908,
      "grad_norm": 0.27059218287467957,
      "learning_rate": 1.968669820158939e-07,
      "loss": 0.001,
      "step": 110490
    },
    {
      "epoch": 20090.909090909092,
      "grad_norm": 0.22321680188179016,
      "learning_rate": 1.9677445730059344e-07,
      "loss": 0.0012,
      "step": 110500
    },
    {
      "epoch": 20090.909090909092,
      "eval_loss": 5.1404571533203125,
      "eval_runtime": 0.9524,
      "eval_samples_per_second": 10.5,
      "eval_steps_per_second": 5.25,
      "step": 110500
    },
    {
      "epoch": 20092.727272727272,
      "grad_norm": 0.0007915106252767146,
      "learning_rate": 1.9668194900623174e-07,
      "loss": 0.0007,
      "step": 110510
    },
    {
      "epoch": 20094.545454545456,
      "grad_norm": 0.15587304532527924,
      "learning_rate": 1.9658945713781883e-07,
      "loss": 0.0014,
      "step": 110520
    },
    {
      "epoch": 20096.363636363636,
      "grad_norm": 0.0006309552700258791,
      "learning_rate": 1.964969817003636e-07,
      "loss": 0.0011,
      "step": 110530
    },
    {
      "epoch": 20098.18181818182,
      "grad_norm": 0.2593790590763092,
      "learning_rate": 1.9640452269887392e-07,
      "loss": 0.0013,
      "step": 110540
    },
    {
      "epoch": 20100.0,
      "grad_norm": 0.19546227157115936,
      "learning_rate": 1.9631208013835676e-07,
      "loss": 0.0009,
      "step": 110550
    },
    {
      "epoch": 20101.81818181818,
      "grad_norm": 0.2106504738330841,
      "learning_rate": 1.9621965402381812e-07,
      "loss": 0.001,
      "step": 110560
    },
    {
      "epoch": 20103.636363636364,
      "grad_norm": 0.0007031692075543106,
      "learning_rate": 1.9612724436026363e-07,
      "loss": 0.001,
      "step": 110570
    },
    {
      "epoch": 20105.454545454544,
      "grad_norm": 0.26115483045578003,
      "learning_rate": 1.9603485115269742e-07,
      "loss": 0.0013,
      "step": 110580
    },
    {
      "epoch": 20107.272727272728,
      "grad_norm": 0.0008391848532482982,
      "learning_rate": 1.9594247440612288e-07,
      "loss": 0.0008,
      "step": 110590
    },
    {
      "epoch": 20109.090909090908,
      "grad_norm": 0.0007634062203578651,
      "learning_rate": 1.958501141255427e-07,
      "loss": 0.0012,
      "step": 110600
    },
    {
      "epoch": 20110.909090909092,
      "grad_norm": 0.0005219907616265118,
      "learning_rate": 1.9575777031595902e-07,
      "loss": 0.0012,
      "step": 110610
    },
    {
      "epoch": 20112.727272727272,
      "grad_norm": 0.22417813539505005,
      "learning_rate": 1.9566544298237186e-07,
      "loss": 0.001,
      "step": 110620
    },
    {
      "epoch": 20114.545454545456,
      "grad_norm": 0.0013649408938363194,
      "learning_rate": 1.955731321297817e-07,
      "loss": 0.0009,
      "step": 110630
    },
    {
      "epoch": 20116.363636363636,
      "grad_norm": 0.21240951120853424,
      "learning_rate": 1.9548083776318724e-07,
      "loss": 0.0011,
      "step": 110640
    },
    {
      "epoch": 20118.18181818182,
      "grad_norm": 0.0007893123547546566,
      "learning_rate": 1.9538855988758684e-07,
      "loss": 0.0011,
      "step": 110650
    },
    {
      "epoch": 20120.0,
      "grad_norm": 0.0005167825147509575,
      "learning_rate": 1.9529629850797773e-07,
      "loss": 0.0012,
      "step": 110660
    },
    {
      "epoch": 20121.81818181818,
      "grad_norm": 0.00045925640733912587,
      "learning_rate": 1.9520405362935595e-07,
      "loss": 0.001,
      "step": 110670
    },
    {
      "epoch": 20123.636363636364,
      "grad_norm": 0.0014106865273788571,
      "learning_rate": 1.9511182525671732e-07,
      "loss": 0.001,
      "step": 110680
    },
    {
      "epoch": 20125.454545454544,
      "grad_norm": 0.0011517711682245135,
      "learning_rate": 1.9501961339505623e-07,
      "loss": 0.0012,
      "step": 110690
    },
    {
      "epoch": 20127.272727272728,
      "grad_norm": 0.000600455270614475,
      "learning_rate": 1.9492741804936618e-07,
      "loss": 0.0009,
      "step": 110700
    },
    {
      "epoch": 20129.090909090908,
      "grad_norm": 0.2073899656534195,
      "learning_rate": 1.9483523922464013e-07,
      "loss": 0.0012,
      "step": 110710
    },
    {
      "epoch": 20130.909090909092,
      "grad_norm": 0.0011150860227644444,
      "learning_rate": 1.9474307692587006e-07,
      "loss": 0.001,
      "step": 110720
    },
    {
      "epoch": 20132.727272727272,
      "grad_norm": 0.28262224793434143,
      "learning_rate": 1.9465093115804687e-07,
      "loss": 0.001,
      "step": 110730
    },
    {
      "epoch": 20134.545454545456,
      "grad_norm": 0.2074221521615982,
      "learning_rate": 1.9455880192616052e-07,
      "loss": 0.0011,
      "step": 110740
    },
    {
      "epoch": 20136.363636363636,
      "grad_norm": 0.27799320220947266,
      "learning_rate": 1.9446668923520009e-07,
      "loss": 0.0012,
      "step": 110750
    },
    {
      "epoch": 20138.18181818182,
      "grad_norm": 0.0006220104987733066,
      "learning_rate": 1.9437459309015424e-07,
      "loss": 0.0009,
      "step": 110760
    },
    {
      "epoch": 20140.0,
      "grad_norm": 0.0033092517405748367,
      "learning_rate": 1.942825134960101e-07,
      "loss": 0.0012,
      "step": 110770
    },
    {
      "epoch": 20141.81818181818,
      "grad_norm": 0.20236186683177948,
      "learning_rate": 1.941904504577541e-07,
      "loss": 0.0009,
      "step": 110780
    },
    {
      "epoch": 20143.636363636364,
      "grad_norm": 0.0006398017285391688,
      "learning_rate": 1.9409840398037198e-07,
      "loss": 0.0011,
      "step": 110790
    },
    {
      "epoch": 20145.454545454544,
      "grad_norm": 0.001410391996614635,
      "learning_rate": 1.9400637406884872e-07,
      "loss": 0.0011,
      "step": 110800
    },
    {
      "epoch": 20147.272727272728,
      "grad_norm": 0.2741812467575073,
      "learning_rate": 1.9391436072816758e-07,
      "loss": 0.0013,
      "step": 110810
    },
    {
      "epoch": 20149.090909090908,
      "grad_norm": 0.0005821697995997965,
      "learning_rate": 1.9382236396331188e-07,
      "loss": 0.0009,
      "step": 110820
    },
    {
      "epoch": 20150.909090909092,
      "grad_norm": 0.2188463807106018,
      "learning_rate": 1.9373038377926326e-07,
      "loss": 0.0012,
      "step": 110830
    },
    {
      "epoch": 20152.727272727272,
      "grad_norm": 0.001051018014550209,
      "learning_rate": 1.9363842018100323e-07,
      "loss": 0.0008,
      "step": 110840
    },
    {
      "epoch": 20154.545454545456,
      "grad_norm": 0.2037270963191986,
      "learning_rate": 1.9354647317351187e-07,
      "loss": 0.0013,
      "step": 110850
    },
    {
      "epoch": 20156.363636363636,
      "grad_norm": 0.20155978202819824,
      "learning_rate": 1.9345454276176827e-07,
      "loss": 0.001,
      "step": 110860
    },
    {
      "epoch": 20158.18181818182,
      "grad_norm": 0.0014449245063588023,
      "learning_rate": 1.9336262895075116e-07,
      "loss": 0.001,
      "step": 110870
    },
    {
      "epoch": 20160.0,
      "grad_norm": 0.16599328815937042,
      "learning_rate": 1.93270731745438e-07,
      "loss": 0.0012,
      "step": 110880
    },
    {
      "epoch": 20161.81818181818,
      "grad_norm": 0.20454369485378265,
      "learning_rate": 1.931788511508051e-07,
      "loss": 0.001,
      "step": 110890
    },
    {
      "epoch": 20163.636363636364,
      "grad_norm": 0.16260926425457,
      "learning_rate": 1.930869871718287e-07,
      "loss": 0.0011,
      "step": 110900
    },
    {
      "epoch": 20165.454545454544,
      "grad_norm": 0.16628775000572205,
      "learning_rate": 1.9299513981348315e-07,
      "loss": 0.0012,
      "step": 110910
    },
    {
      "epoch": 20167.272727272728,
      "grad_norm": 0.2669380307197571,
      "learning_rate": 1.929033090807427e-07,
      "loss": 0.0014,
      "step": 110920
    },
    {
      "epoch": 20169.090909090908,
      "grad_norm": 0.0006098077283240855,
      "learning_rate": 1.9281149497858024e-07,
      "loss": 0.0007,
      "step": 110930
    },
    {
      "epoch": 20170.909090909092,
      "grad_norm": 0.012119943276047707,
      "learning_rate": 1.9271969751196776e-07,
      "loss": 0.0012,
      "step": 110940
    },
    {
      "epoch": 20172.727272727272,
      "grad_norm": 0.23230233788490295,
      "learning_rate": 1.9262791668587673e-07,
      "loss": 0.0013,
      "step": 110950
    },
    {
      "epoch": 20174.545454545456,
      "grad_norm": 0.0006723096594214439,
      "learning_rate": 1.9253615250527738e-07,
      "loss": 0.0008,
      "step": 110960
    },
    {
      "epoch": 20176.363636363636,
      "grad_norm": 0.1626228541135788,
      "learning_rate": 1.924444049751389e-07,
      "loss": 0.0012,
      "step": 110970
    },
    {
      "epoch": 20178.18181818182,
      "grad_norm": 0.00135961570776999,
      "learning_rate": 1.9235267410043022e-07,
      "loss": 0.001,
      "step": 110980
    },
    {
      "epoch": 20180.0,
      "grad_norm": 0.27922335267066956,
      "learning_rate": 1.922609598861187e-07,
      "loss": 0.0012,
      "step": 110990
    },
    {
      "epoch": 20181.81818181818,
      "grad_norm": 0.17169715464115143,
      "learning_rate": 1.9216926233717084e-07,
      "loss": 0.001,
      "step": 111000
    },
    {
      "epoch": 20181.81818181818,
      "eval_loss": 5.1524763107299805,
      "eval_runtime": 0.9549,
      "eval_samples_per_second": 10.473,
      "eval_steps_per_second": 5.236,
      "step": 111000
    },
    {
      "epoch": 20183.636363636364,
      "grad_norm": 0.0012568329693749547,
      "learning_rate": 1.9207758145855296e-07,
      "loss": 0.0012,
      "step": 111010
    },
    {
      "epoch": 20185.454545454544,
      "grad_norm": 0.2784913182258606,
      "learning_rate": 1.919859172552295e-07,
      "loss": 0.0013,
      "step": 111020
    },
    {
      "epoch": 20187.272727272728,
      "grad_norm": 0.21879884600639343,
      "learning_rate": 1.9189426973216478e-07,
      "loss": 0.0009,
      "step": 111030
    },
    {
      "epoch": 20189.090909090908,
      "grad_norm": 0.22190728783607483,
      "learning_rate": 1.918026388943218e-07,
      "loss": 0.0012,
      "step": 111040
    },
    {
      "epoch": 20190.909090909092,
      "grad_norm": 0.22137576341629028,
      "learning_rate": 1.9171102474666256e-07,
      "loss": 0.0012,
      "step": 111050
    },
    {
      "epoch": 20192.727272727272,
      "grad_norm": 0.0008991848444566131,
      "learning_rate": 1.9161942729414876e-07,
      "loss": 0.001,
      "step": 111060
    },
    {
      "epoch": 20194.545454545456,
      "grad_norm": 0.011671893298625946,
      "learning_rate": 1.9152784654174048e-07,
      "loss": 0.0014,
      "step": 111070
    },
    {
      "epoch": 20196.363636363636,
      "grad_norm": 0.0005180835723876953,
      "learning_rate": 1.9143628249439708e-07,
      "loss": 0.0006,
      "step": 111080
    },
    {
      "epoch": 20198.18181818182,
      "grad_norm": 0.0006814018124714494,
      "learning_rate": 1.9134473515707756e-07,
      "loss": 0.0012,
      "step": 111090
    },
    {
      "epoch": 20200.0,
      "grad_norm": 0.20504163205623627,
      "learning_rate": 1.912532045347392e-07,
      "loss": 0.0012,
      "step": 111100
    },
    {
      "epoch": 20201.81818181818,
      "grad_norm": 0.27927282452583313,
      "learning_rate": 1.9116169063233907e-07,
      "loss": 0.0011,
      "step": 111110
    },
    {
      "epoch": 20203.636363636364,
      "grad_norm": 0.0005266069201752543,
      "learning_rate": 1.9107019345483288e-07,
      "loss": 0.001,
      "step": 111120
    },
    {
      "epoch": 20205.454545454544,
      "grad_norm": 0.011702626943588257,
      "learning_rate": 1.909787130071755e-07,
      "loss": 0.0012,
      "step": 111130
    },
    {
      "epoch": 20207.272727272728,
      "grad_norm": 0.1645783931016922,
      "learning_rate": 1.908872492943212e-07,
      "loss": 0.001,
      "step": 111140
    },
    {
      "epoch": 20209.090909090908,
      "grad_norm": 0.22121331095695496,
      "learning_rate": 1.90795802321223e-07,
      "loss": 0.0012,
      "step": 111150
    },
    {
      "epoch": 20210.909090909092,
      "grad_norm": 0.0006797123933210969,
      "learning_rate": 1.9070437209283302e-07,
      "loss": 0.001,
      "step": 111160
    },
    {
      "epoch": 20212.727272727272,
      "grad_norm": 0.0005575258401222527,
      "learning_rate": 1.9061295861410288e-07,
      "loss": 0.001,
      "step": 111170
    },
    {
      "epoch": 20214.545454545456,
      "grad_norm": 0.20692208409309387,
      "learning_rate": 1.9052156188998285e-07,
      "loss": 0.0012,
      "step": 111180
    },
    {
      "epoch": 20216.363636363636,
      "grad_norm": 0.16466809809207916,
      "learning_rate": 1.9043018192542226e-07,
      "loss": 0.001,
      "step": 111190
    },
    {
      "epoch": 20218.18181818182,
      "grad_norm": 0.0007396973087452352,
      "learning_rate": 1.9033881872537005e-07,
      "loss": 0.0009,
      "step": 111200
    },
    {
      "epoch": 20220.0,
      "grad_norm": 0.23147204518318176,
      "learning_rate": 1.9024747229477362e-07,
      "loss": 0.0012,
      "step": 111210
    },
    {
      "epoch": 20221.81818181818,
      "grad_norm": 0.21993985772132874,
      "learning_rate": 1.901561426385801e-07,
      "loss": 0.0012,
      "step": 111220
    },
    {
      "epoch": 20223.636363636364,
      "grad_norm": 0.1657715141773224,
      "learning_rate": 1.9006482976173516e-07,
      "loss": 0.0009,
      "step": 111230
    },
    {
      "epoch": 20225.454545454544,
      "grad_norm": 0.0006104537169449031,
      "learning_rate": 1.8997353366918367e-07,
      "loss": 0.0013,
      "step": 111240
    },
    {
      "epoch": 20227.272727272728,
      "grad_norm": 0.15924954414367676,
      "learning_rate": 1.8988225436587003e-07,
      "loss": 0.001,
      "step": 111250
    },
    {
      "epoch": 20229.090909090908,
      "grad_norm": 0.2093672752380371,
      "learning_rate": 1.8979099185673714e-07,
      "loss": 0.0012,
      "step": 111260
    },
    {
      "epoch": 20230.909090909092,
      "grad_norm": 0.00039022808778099716,
      "learning_rate": 1.896997461467272e-07,
      "loss": 0.0009,
      "step": 111270
    },
    {
      "epoch": 20232.727272727272,
      "grad_norm": 0.0008685833308845758,
      "learning_rate": 1.8960851724078175e-07,
      "loss": 0.0013,
      "step": 111280
    },
    {
      "epoch": 20234.545454545456,
      "grad_norm": 0.0006461904849857092,
      "learning_rate": 1.89517305143841e-07,
      "loss": 0.0009,
      "step": 111290
    },
    {
      "epoch": 20236.363636363636,
      "grad_norm": 0.0012487032217904925,
      "learning_rate": 1.8942610986084484e-07,
      "loss": 0.0009,
      "step": 111300
    },
    {
      "epoch": 20238.18181818182,
      "grad_norm": 0.0004663976142182946,
      "learning_rate": 1.8933493139673158e-07,
      "loss": 0.0012,
      "step": 111310
    },
    {
      "epoch": 20240.0,
      "grad_norm": 0.22020769119262695,
      "learning_rate": 1.892437697564388e-07,
      "loss": 0.0012,
      "step": 111320
    },
    {
      "epoch": 20241.81818181818,
      "grad_norm": 0.21908904612064362,
      "learning_rate": 1.8915262494490365e-07,
      "loss": 0.0012,
      "step": 111330
    },
    {
      "epoch": 20243.636363636364,
      "grad_norm": 0.00039784141699783504,
      "learning_rate": 1.890614969670618e-07,
      "loss": 0.0009,
      "step": 111340
    },
    {
      "epoch": 20245.454545454544,
      "grad_norm": 0.21954533457756042,
      "learning_rate": 1.8897038582784807e-07,
      "loss": 0.0014,
      "step": 111350
    },
    {
      "epoch": 20247.272727272728,
      "grad_norm": 0.1631266325712204,
      "learning_rate": 1.8887929153219684e-07,
      "loss": 0.001,
      "step": 111360
    },
    {
      "epoch": 20249.090909090908,
      "grad_norm": 0.0005263709463179111,
      "learning_rate": 1.887882140850411e-07,
      "loss": 0.001,
      "step": 111370
    },
    {
      "epoch": 20250.909090909092,
      "grad_norm": 0.2755633294582367,
      "learning_rate": 1.8869715349131282e-07,
      "loss": 0.0012,
      "step": 111380
    },
    {
      "epoch": 20252.727272727272,
      "grad_norm": 0.2704751491546631,
      "learning_rate": 1.8860610975594383e-07,
      "loss": 0.0009,
      "step": 111390
    },
    {
      "epoch": 20254.545454545456,
      "grad_norm": 0.1654142439365387,
      "learning_rate": 1.8851508288386398e-07,
      "loss": 0.0012,
      "step": 111400
    },
    {
      "epoch": 20256.363636363636,
      "grad_norm": 0.20812442898750305,
      "learning_rate": 1.8842407288000327e-07,
      "loss": 0.0013,
      "step": 111410
    },
    {
      "epoch": 20258.18181818182,
      "grad_norm": 0.21173030138015747,
      "learning_rate": 1.8833307974929002e-07,
      "loss": 0.0009,
      "step": 111420
    },
    {
      "epoch": 20260.0,
      "grad_norm": 0.0006396409589797258,
      "learning_rate": 1.882421034966517e-07,
      "loss": 0.001,
      "step": 111430
    },
    {
      "epoch": 20261.81818181818,
      "grad_norm": 0.19672957062721252,
      "learning_rate": 1.881511441270155e-07,
      "loss": 0.001,
      "step": 111440
    },
    {
      "epoch": 20263.636363636364,
      "grad_norm": 0.17697501182556152,
      "learning_rate": 1.8806020164530701e-07,
      "loss": 0.0013,
      "step": 111450
    },
    {
      "epoch": 20265.454545454544,
      "grad_norm": 0.0007356349960900843,
      "learning_rate": 1.8796927605645096e-07,
      "loss": 0.0008,
      "step": 111460
    },
    {
      "epoch": 20267.272727272728,
      "grad_norm": 0.0007397370645776391,
      "learning_rate": 1.878783673653717e-07,
      "loss": 0.0012,
      "step": 111470
    },
    {
      "epoch": 20269.090909090908,
      "grad_norm": 0.0008467741427011788,
      "learning_rate": 1.8778747557699221e-07,
      "loss": 0.0011,
      "step": 111480
    },
    {
      "epoch": 20270.909090909092,
      "grad_norm": 0.001685899798758328,
      "learning_rate": 1.8769660069623445e-07,
      "loss": 0.0012,
      "step": 111490
    },
    {
      "epoch": 20272.727272727272,
      "grad_norm": 0.20833593606948853,
      "learning_rate": 1.8760574272801998e-07,
      "loss": 0.0012,
      "step": 111500
    },
    {
      "epoch": 20272.727272727272,
      "eval_loss": 5.158012866973877,
      "eval_runtime": 0.9498,
      "eval_samples_per_second": 10.529,
      "eval_steps_per_second": 5.264,
      "step": 111500
    },
    {
      "epoch": 20274.545454545456,
      "grad_norm": 0.2679218053817749,
      "learning_rate": 1.8751490167726886e-07,
      "loss": 0.001,
      "step": 111510
    },
    {
      "epoch": 20276.363636363636,
      "grad_norm": 0.001316882437095046,
      "learning_rate": 1.8742407754890081e-07,
      "loss": 0.001,
      "step": 111520
    },
    {
      "epoch": 20278.18181818182,
      "grad_norm": 0.20765866339206696,
      "learning_rate": 1.8733327034783419e-07,
      "loss": 0.0011,
      "step": 111530
    },
    {
      "epoch": 20280.0,
      "grad_norm": 0.005337718408554792,
      "learning_rate": 1.8724248007898646e-07,
      "loss": 0.0011,
      "step": 111540
    },
    {
      "epoch": 20281.81818181818,
      "grad_norm": 0.2154206782579422,
      "learning_rate": 1.871517067472746e-07,
      "loss": 0.001,
      "step": 111550
    },
    {
      "epoch": 20283.636363636364,
      "grad_norm": 0.0006681635859422386,
      "learning_rate": 1.8706095035761415e-07,
      "loss": 0.0013,
      "step": 111560
    },
    {
      "epoch": 20285.454545454544,
      "grad_norm": 0.011231082491576672,
      "learning_rate": 1.869702109149199e-07,
      "loss": 0.0009,
      "step": 111570
    },
    {
      "epoch": 20287.272727272728,
      "grad_norm": 0.0007167549920268357,
      "learning_rate": 1.8687948842410594e-07,
      "loss": 0.0009,
      "step": 111580
    },
    {
      "epoch": 20289.090909090908,
      "grad_norm": 0.0013335272669792175,
      "learning_rate": 1.867887828900851e-07,
      "loss": 0.0012,
      "step": 111590
    },
    {
      "epoch": 20290.909090909092,
      "grad_norm": 0.2086605727672577,
      "learning_rate": 1.8669809431776988e-07,
      "loss": 0.0012,
      "step": 111600
    },
    {
      "epoch": 20292.727272727272,
      "grad_norm": 0.0014968523755669594,
      "learning_rate": 1.8660742271207087e-07,
      "loss": 0.0009,
      "step": 111610
    },
    {
      "epoch": 20294.545454545456,
      "grad_norm": 0.16764087975025177,
      "learning_rate": 1.865167680778985e-07,
      "loss": 0.0013,
      "step": 111620
    },
    {
      "epoch": 20296.363636363636,
      "grad_norm": 0.0004414576687850058,
      "learning_rate": 1.864261304201624e-07,
      "loss": 0.0009,
      "step": 111630
    },
    {
      "epoch": 20298.18181818182,
      "grad_norm": 0.006905434187501669,
      "learning_rate": 1.8633550974377076e-07,
      "loss": 0.0013,
      "step": 111640
    },
    {
      "epoch": 20300.0,
      "grad_norm": 0.16835974156856537,
      "learning_rate": 1.862449060536309e-07,
      "loss": 0.0009,
      "step": 111650
    },
    {
      "epoch": 20301.81818181818,
      "grad_norm": 0.0005183746688999236,
      "learning_rate": 1.8615431935464982e-07,
      "loss": 0.0009,
      "step": 111660
    },
    {
      "epoch": 20303.636363636364,
      "grad_norm": 0.1719624400138855,
      "learning_rate": 1.8606374965173283e-07,
      "loss": 0.0013,
      "step": 111670
    },
    {
      "epoch": 20305.454545454544,
      "grad_norm": 0.0009211530559696257,
      "learning_rate": 1.8597319694978457e-07,
      "loss": 0.0011,
      "step": 111680
    },
    {
      "epoch": 20307.272727272728,
      "grad_norm": 0.164471834897995,
      "learning_rate": 1.8588266125370928e-07,
      "loss": 0.001,
      "step": 111690
    },
    {
      "epoch": 20309.090909090908,
      "grad_norm": 0.000767085759434849,
      "learning_rate": 1.8579214256840936e-07,
      "loss": 0.001,
      "step": 111700
    },
    {
      "epoch": 20310.909090909092,
      "grad_norm": 0.0006736044306308031,
      "learning_rate": 1.857016408987872e-07,
      "loss": 0.0012,
      "step": 111710
    },
    {
      "epoch": 20312.727272727272,
      "grad_norm": 0.001068539684638381,
      "learning_rate": 1.856111562497437e-07,
      "loss": 0.0011,
      "step": 111720
    },
    {
      "epoch": 20314.545454545456,
      "grad_norm": 0.0017805846873670816,
      "learning_rate": 1.8552068862617876e-07,
      "loss": 0.0012,
      "step": 111730
    },
    {
      "epoch": 20316.363636363636,
      "grad_norm": 0.0008241430041380227,
      "learning_rate": 1.8543023803299196e-07,
      "loss": 0.001,
      "step": 111740
    },
    {
      "epoch": 20318.18181818182,
      "grad_norm": 0.16152715682983398,
      "learning_rate": 1.8533980447508135e-07,
      "loss": 0.001,
      "step": 111750
    },
    {
      "epoch": 20320.0,
      "grad_norm": 0.20693491399288177,
      "learning_rate": 1.8524938795734419e-07,
      "loss": 0.0011,
      "step": 111760
    },
    {
      "epoch": 20321.81818181818,
      "grad_norm": 0.1858695149421692,
      "learning_rate": 1.851589884846772e-07,
      "loss": 0.0014,
      "step": 111770
    },
    {
      "epoch": 20323.636363636364,
      "grad_norm": 0.0006265323027037084,
      "learning_rate": 1.850686060619756e-07,
      "loss": 0.0009,
      "step": 111780
    },
    {
      "epoch": 20325.454545454544,
      "grad_norm": 0.2175552397966385,
      "learning_rate": 1.8497824069413442e-07,
      "loss": 0.0012,
      "step": 111790
    },
    {
      "epoch": 20327.272727272728,
      "grad_norm": 0.21905186772346497,
      "learning_rate": 1.8488789238604674e-07,
      "loss": 0.001,
      "step": 111800
    },
    {
      "epoch": 20329.090909090908,
      "grad_norm": 0.001303221215493977,
      "learning_rate": 1.8479756114260557e-07,
      "loss": 0.001,
      "step": 111810
    },
    {
      "epoch": 20330.909090909092,
      "grad_norm": 0.0008282781927846372,
      "learning_rate": 1.8470724696870294e-07,
      "loss": 0.0009,
      "step": 111820
    },
    {
      "epoch": 20332.727272727272,
      "grad_norm": 0.1995943933725357,
      "learning_rate": 1.8461694986922954e-07,
      "loss": 0.0013,
      "step": 111830
    },
    {
      "epoch": 20334.545454545456,
      "grad_norm": 0.16981807351112366,
      "learning_rate": 1.8452666984907516e-07,
      "loss": 0.001,
      "step": 111840
    },
    {
      "epoch": 20336.363636363636,
      "grad_norm": 0.15599045157432556,
      "learning_rate": 1.8443640691312917e-07,
      "loss": 0.001,
      "step": 111850
    },
    {
      "epoch": 20338.18181818182,
      "grad_norm": 0.2658662796020508,
      "learning_rate": 1.843461610662796e-07,
      "loss": 0.0013,
      "step": 111860
    },
    {
      "epoch": 20340.0,
      "grad_norm": 0.0007486372487619519,
      "learning_rate": 1.8425593231341357e-07,
      "loss": 0.0009,
      "step": 111870
    },
    {
      "epoch": 20341.81818181818,
      "grad_norm": 0.2675544023513794,
      "learning_rate": 1.8416572065941721e-07,
      "loss": 0.0011,
      "step": 111880
    },
    {
      "epoch": 20343.636363636364,
      "grad_norm": 0.2826271057128906,
      "learning_rate": 1.8407552610917597e-07,
      "loss": 0.0012,
      "step": 111890
    },
    {
      "epoch": 20345.454545454544,
      "grad_norm": 0.0004264468152541667,
      "learning_rate": 1.8398534866757455e-07,
      "loss": 0.0011,
      "step": 111900
    },
    {
      "epoch": 20347.272727272728,
      "grad_norm": 0.0006266806158237159,
      "learning_rate": 1.838951883394962e-07,
      "loss": 0.0007,
      "step": 111910
    },
    {
      "epoch": 20349.090909090908,
      "grad_norm": 0.2193593680858612,
      "learning_rate": 1.8380504512982326e-07,
      "loss": 0.0013,
      "step": 111920
    },
    {
      "epoch": 20350.909090909092,
      "grad_norm": 0.0038589583709836006,
      "learning_rate": 1.8371491904343778e-07,
      "loss": 0.0012,
      "step": 111930
    },
    {
      "epoch": 20352.727272727272,
      "grad_norm": 0.2607860267162323,
      "learning_rate": 1.836248100852203e-07,
      "loss": 0.0012,
      "step": 111940
    },
    {
      "epoch": 20354.545454545456,
      "grad_norm": 0.20510418713092804,
      "learning_rate": 1.8353471826005036e-07,
      "loss": 0.0007,
      "step": 111950
    },
    {
      "epoch": 20356.363636363636,
      "grad_norm": 0.17790015041828156,
      "learning_rate": 1.834446435728072e-07,
      "loss": 0.0015,
      "step": 111960
    },
    {
      "epoch": 20358.18181818182,
      "grad_norm": 0.28787684440612793,
      "learning_rate": 1.833545860283684e-07,
      "loss": 0.001,
      "step": 111970
    },
    {
      "epoch": 20360.0,
      "grad_norm": 0.20171672105789185,
      "learning_rate": 1.832645456316115e-07,
      "loss": 0.0009,
      "step": 111980
    },
    {
      "epoch": 20361.81818181818,
      "grad_norm": 0.0007933605811558664,
      "learning_rate": 1.8317452238741176e-07,
      "loss": 0.001,
      "step": 111990
    },
    {
      "epoch": 20363.636363636364,
      "grad_norm": 0.0004461928328964859,
      "learning_rate": 1.830845163006448e-07,
      "loss": 0.0011,
      "step": 112000
    },
    {
      "epoch": 20363.636363636364,
      "eval_loss": 5.016517639160156,
      "eval_runtime": 0.9519,
      "eval_samples_per_second": 10.505,
      "eval_steps_per_second": 5.253,
      "step": 112000
    },
    {
      "epoch": 20365.454545454544,
      "grad_norm": 0.0004151753382757306,
      "learning_rate": 1.8299452737618492e-07,
      "loss": 0.001,
      "step": 112010
    },
    {
      "epoch": 20367.272727272728,
      "grad_norm": 0.000983910751529038,
      "learning_rate": 1.8290455561890528e-07,
      "loss": 0.0013,
      "step": 112020
    },
    {
      "epoch": 20369.090909090908,
      "grad_norm": 0.17193369567394257,
      "learning_rate": 1.8281460103367802e-07,
      "loss": 0.0011,
      "step": 112030
    },
    {
      "epoch": 20370.909090909092,
      "grad_norm": 0.21700263023376465,
      "learning_rate": 1.8272466362537492e-07,
      "loss": 0.0012,
      "step": 112040
    },
    {
      "epoch": 20372.727272727272,
      "grad_norm": 0.0008859179215505719,
      "learning_rate": 1.8263474339886624e-07,
      "loss": 0.0007,
      "step": 112050
    },
    {
      "epoch": 20374.545454545456,
      "grad_norm": 0.0004853777354583144,
      "learning_rate": 1.825448403590217e-07,
      "loss": 0.0012,
      "step": 112060
    },
    {
      "epoch": 20376.363636363636,
      "grad_norm": 0.18128976225852966,
      "learning_rate": 1.8245495451070954e-07,
      "loss": 0.0013,
      "step": 112070
    },
    {
      "epoch": 20378.18181818182,
      "grad_norm": 0.21046407520771027,
      "learning_rate": 1.8236508585879778e-07,
      "loss": 0.001,
      "step": 112080
    },
    {
      "epoch": 20380.0,
      "grad_norm": 0.17124076187610626,
      "learning_rate": 1.822752344081533e-07,
      "loss": 0.0011,
      "step": 112090
    },
    {
      "epoch": 20381.81818181818,
      "grad_norm": 0.2192641645669937,
      "learning_rate": 1.8218540016364174e-07,
      "loss": 0.001,
      "step": 112100
    },
    {
      "epoch": 20383.636363636364,
      "grad_norm": 0.2802976369857788,
      "learning_rate": 1.820955831301279e-07,
      "loss": 0.0014,
      "step": 112110
    },
    {
      "epoch": 20385.454545454544,
      "grad_norm": 0.001606693840585649,
      "learning_rate": 1.8200578331247607e-07,
      "loss": 0.0009,
      "step": 112120
    },
    {
      "epoch": 20387.272727272728,
      "grad_norm": 0.27554088830947876,
      "learning_rate": 1.8191600071554908e-07,
      "loss": 0.0012,
      "step": 112130
    },
    {
      "epoch": 20389.090909090908,
      "grad_norm": 0.001152198645286262,
      "learning_rate": 1.8182623534420905e-07,
      "loss": 0.001,
      "step": 112140
    },
    {
      "epoch": 20390.909090909092,
      "grad_norm": 0.0009225037647411227,
      "learning_rate": 1.8173648720331702e-07,
      "loss": 0.001,
      "step": 112150
    },
    {
      "epoch": 20392.727272727272,
      "grad_norm": 0.2195177972316742,
      "learning_rate": 1.8164675629773335e-07,
      "loss": 0.0012,
      "step": 112160
    },
    {
      "epoch": 20394.545454545456,
      "grad_norm": 0.21939213573932648,
      "learning_rate": 1.8155704263231775e-07,
      "loss": 0.0012,
      "step": 112170
    },
    {
      "epoch": 20396.363636363636,
      "grad_norm": 0.0007704009185545146,
      "learning_rate": 1.8146734621192783e-07,
      "loss": 0.0009,
      "step": 112180
    },
    {
      "epoch": 20398.18181818182,
      "grad_norm": 0.0005218234146013856,
      "learning_rate": 1.8137766704142137e-07,
      "loss": 0.0012,
      "step": 112190
    },
    {
      "epoch": 20400.0,
      "grad_norm": 0.001244832412339747,
      "learning_rate": 1.812880051256551e-07,
      "loss": 0.0012,
      "step": 112200
    },
    {
      "epoch": 20401.81818181818,
      "grad_norm": 0.20736674964427948,
      "learning_rate": 1.8119836046948444e-07,
      "loss": 0.0012,
      "step": 112210
    },
    {
      "epoch": 20403.636363636364,
      "grad_norm": 0.18596789240837097,
      "learning_rate": 1.8110873307776397e-07,
      "loss": 0.0008,
      "step": 112220
    },
    {
      "epoch": 20405.454545454544,
      "grad_norm": 0.0010895468294620514,
      "learning_rate": 1.8101912295534727e-07,
      "loss": 0.0013,
      "step": 112230
    },
    {
      "epoch": 20407.272727272728,
      "grad_norm": 0.18603341281414032,
      "learning_rate": 1.809295301070874e-07,
      "loss": 0.001,
      "step": 112240
    },
    {
      "epoch": 20409.090909090908,
      "grad_norm": 0.19320127367973328,
      "learning_rate": 1.8083995453783603e-07,
      "loss": 0.0012,
      "step": 112250
    },
    {
      "epoch": 20410.909090909092,
      "grad_norm": 0.22934257984161377,
      "learning_rate": 1.8075039625244388e-07,
      "loss": 0.0009,
      "step": 112260
    },
    {
      "epoch": 20412.727272727272,
      "grad_norm": 0.000682763522490859,
      "learning_rate": 1.8066085525576103e-07,
      "loss": 0.0012,
      "step": 112270
    },
    {
      "epoch": 20414.545454545456,
      "grad_norm": 0.0006799175753258169,
      "learning_rate": 1.8057133155263682e-07,
      "loss": 0.0009,
      "step": 112280
    },
    {
      "epoch": 20416.363636363636,
      "grad_norm": 0.0005651086685247719,
      "learning_rate": 1.80481825147919e-07,
      "loss": 0.0012,
      "step": 112290
    },
    {
      "epoch": 20418.18181818182,
      "grad_norm": 0.26363804936408997,
      "learning_rate": 1.8039233604645464e-07,
      "loss": 0.0014,
      "step": 112300
    },
    {
      "epoch": 20420.0,
      "grad_norm": 0.0009283372201025486,
      "learning_rate": 1.8030286425309033e-07,
      "loss": 0.0009,
      "step": 112310
    },
    {
      "epoch": 20421.81818181818,
      "grad_norm": 0.008181150071322918,
      "learning_rate": 1.8021340977267103e-07,
      "loss": 0.0012,
      "step": 112320
    },
    {
      "epoch": 20423.636363636364,
      "grad_norm": 0.18602405488491058,
      "learning_rate": 1.8012397261004119e-07,
      "loss": 0.0012,
      "step": 112330
    },
    {
      "epoch": 20425.454545454544,
      "grad_norm": 0.0005572104128077626,
      "learning_rate": 1.8003455277004403e-07,
      "loss": 0.0007,
      "step": 112340
    },
    {
      "epoch": 20427.272727272728,
      "grad_norm": 0.0006537321605719626,
      "learning_rate": 1.7994515025752216e-07,
      "loss": 0.0014,
      "step": 112350
    },
    {
      "epoch": 20429.090909090908,
      "grad_norm": 0.0007763570756651461,
      "learning_rate": 1.7985576507731742e-07,
      "loss": 0.0009,
      "step": 112360
    },
    {
      "epoch": 20430.909090909092,
      "grad_norm": 0.0006954288692213595,
      "learning_rate": 1.7976639723426979e-07,
      "loss": 0.001,
      "step": 112370
    },
    {
      "epoch": 20432.727272727272,
      "grad_norm": 0.1780613213777542,
      "learning_rate": 1.7967704673321915e-07,
      "loss": 0.0012,
      "step": 112380
    },
    {
      "epoch": 20434.545454545456,
      "grad_norm": 0.0009094229899346828,
      "learning_rate": 1.7958771357900443e-07,
      "loss": 0.001,
      "step": 112390
    },
    {
      "epoch": 20436.363636363636,
      "grad_norm": 0.0010472923750057817,
      "learning_rate": 1.7949839777646324e-07,
      "loss": 0.0008,
      "step": 112400
    },
    {
      "epoch": 20438.18181818182,
      "grad_norm": 0.26839157938957214,
      "learning_rate": 1.794090993304324e-07,
      "loss": 0.0015,
      "step": 112410
    },
    {
      "epoch": 20440.0,
      "grad_norm": 0.1809518188238144,
      "learning_rate": 1.793198182457476e-07,
      "loss": 0.0009,
      "step": 112420
    },
    {
      "epoch": 20441.81818181818,
      "grad_norm": 0.0006887607160024345,
      "learning_rate": 1.7923055452724418e-07,
      "loss": 0.0009,
      "step": 112430
    },
    {
      "epoch": 20443.636363636364,
      "grad_norm": 0.20581993460655212,
      "learning_rate": 1.791413081797559e-07,
      "loss": 0.001,
      "step": 112440
    },
    {
      "epoch": 20445.454545454544,
      "grad_norm": 0.0005962278228253126,
      "learning_rate": 1.790520792081157e-07,
      "loss": 0.001,
      "step": 112450
    },
    {
      "epoch": 20447.272727272728,
      "grad_norm": 0.0006636922480538487,
      "learning_rate": 1.789628676171559e-07,
      "loss": 0.0012,
      "step": 112460
    },
    {
      "epoch": 20449.090909090908,
      "grad_norm": 0.000581136264372617,
      "learning_rate": 1.7887367341170777e-07,
      "loss": 0.0012,
      "step": 112470
    },
    {
      "epoch": 20450.909090909092,
      "grad_norm": 0.1733207404613495,
      "learning_rate": 1.787844965966015e-07,
      "loss": 0.0012,
      "step": 112480
    },
    {
      "epoch": 20452.727272727272,
      "grad_norm": 0.17750093340873718,
      "learning_rate": 1.7869533717666625e-07,
      "loss": 0.001,
      "step": 112490
    },
    {
      "epoch": 20454.545454545456,
      "grad_norm": 0.0023554859217256308,
      "learning_rate": 1.7860619515673032e-07,
      "loss": 0.001,
      "step": 112500
    },
    {
      "epoch": 20454.545454545456,
      "eval_loss": 5.077014446258545,
      "eval_runtime": 0.9522,
      "eval_samples_per_second": 10.502,
      "eval_steps_per_second": 5.251,
      "step": 112500
    },
    {
      "epoch": 20456.363636363636,
      "grad_norm": 0.0005771775031462312,
      "learning_rate": 1.785170705416214e-07,
      "loss": 0.0011,
      "step": 112510
    },
    {
      "epoch": 20458.18181818182,
      "grad_norm": 0.0006112794508226216,
      "learning_rate": 1.784279633361659e-07,
      "loss": 0.001,
      "step": 112520
    },
    {
      "epoch": 20460.0,
      "grad_norm": 0.21654509007930756,
      "learning_rate": 1.78338873545189e-07,
      "loss": 0.0012,
      "step": 112530
    },
    {
      "epoch": 20461.81818181818,
      "grad_norm": 0.0007077682530507445,
      "learning_rate": 1.7824980117351562e-07,
      "loss": 0.001,
      "step": 112540
    },
    {
      "epoch": 20463.636363636364,
      "grad_norm": 0.011086644604802132,
      "learning_rate": 1.7816074622596966e-07,
      "loss": 0.0012,
      "step": 112550
    },
    {
      "epoch": 20465.454545454544,
      "grad_norm": 0.21523061394691467,
      "learning_rate": 1.7807170870737316e-07,
      "loss": 0.0009,
      "step": 112560
    },
    {
      "epoch": 20467.272727272728,
      "grad_norm": 0.006350597366690636,
      "learning_rate": 1.7798268862254822e-07,
      "loss": 0.0013,
      "step": 112570
    },
    {
      "epoch": 20469.090909090908,
      "grad_norm": 0.4137742817401886,
      "learning_rate": 1.778936859763158e-07,
      "loss": 0.001,
      "step": 112580
    },
    {
      "epoch": 20470.909090909092,
      "grad_norm": 0.20888887345790863,
      "learning_rate": 1.7780470077349563e-07,
      "loss": 0.001,
      "step": 112590
    },
    {
      "epoch": 20472.727272727272,
      "grad_norm": 0.27869534492492676,
      "learning_rate": 1.7771573301890663e-07,
      "loss": 0.001,
      "step": 112600
    },
    {
      "epoch": 20474.545454545456,
      "grad_norm": 0.2870844304561615,
      "learning_rate": 1.7762678271736659e-07,
      "loss": 0.0011,
      "step": 112610
    },
    {
      "epoch": 20476.363636363636,
      "grad_norm": 0.15233148634433746,
      "learning_rate": 1.7753784987369285e-07,
      "loss": 0.0011,
      "step": 112620
    },
    {
      "epoch": 20478.18181818182,
      "grad_norm": 0.001383132068440318,
      "learning_rate": 1.7744893449270143e-07,
      "loss": 0.0009,
      "step": 112630
    },
    {
      "epoch": 20480.0,
      "grad_norm": 0.0006145276711322367,
      "learning_rate": 1.7736003657920724e-07,
      "loss": 0.0012,
      "step": 112640
    },
    {
      "epoch": 20481.81818181818,
      "grad_norm": 0.27216702699661255,
      "learning_rate": 1.7727115613802464e-07,
      "loss": 0.0011,
      "step": 112650
    },
    {
      "epoch": 20483.636363636364,
      "grad_norm": 0.0011765703093260527,
      "learning_rate": 1.771822931739672e-07,
      "loss": 0.0011,
      "step": 112660
    },
    {
      "epoch": 20485.454545454544,
      "grad_norm": 0.17248176038265228,
      "learning_rate": 1.7709344769184658e-07,
      "loss": 0.0012,
      "step": 112670
    },
    {
      "epoch": 20487.272727272728,
      "grad_norm": 0.17427244782447815,
      "learning_rate": 1.7700461969647468e-07,
      "loss": 0.001,
      "step": 112680
    },
    {
      "epoch": 20489.090909090908,
      "grad_norm": 0.0006115275318734348,
      "learning_rate": 1.7691580919266147e-07,
      "loss": 0.0009,
      "step": 112690
    },
    {
      "epoch": 20490.909090909092,
      "grad_norm": 0.21454903483390808,
      "learning_rate": 1.7682701618521683e-07,
      "loss": 0.0009,
      "step": 112700
    },
    {
      "epoch": 20492.727272727272,
      "grad_norm": 0.20085780322551727,
      "learning_rate": 1.767382406789491e-07,
      "loss": 0.0011,
      "step": 112710
    },
    {
      "epoch": 20494.545454545456,
      "grad_norm": 0.0005633358960039914,
      "learning_rate": 1.7664948267866565e-07,
      "loss": 0.0013,
      "step": 112720
    },
    {
      "epoch": 20496.363636363636,
      "grad_norm": 0.1756010800600052,
      "learning_rate": 1.7656074218917327e-07,
      "loss": 0.0009,
      "step": 112730
    },
    {
      "epoch": 20498.18181818182,
      "grad_norm": 0.19782041013240814,
      "learning_rate": 1.7647201921527799e-07,
      "loss": 0.0012,
      "step": 112740
    },
    {
      "epoch": 20500.0,
      "grad_norm": 0.1773158311843872,
      "learning_rate": 1.7638331376178384e-07,
      "loss": 0.001,
      "step": 112750
    },
    {
      "epoch": 20501.81818181818,
      "grad_norm": 0.0004265807510819286,
      "learning_rate": 1.7629462583349508e-07,
      "loss": 0.0012,
      "step": 112760
    },
    {
      "epoch": 20503.636363636364,
      "grad_norm": 0.17702274024486542,
      "learning_rate": 1.7620595543521427e-07,
      "loss": 0.0012,
      "step": 112770
    },
    {
      "epoch": 20505.454545454544,
      "grad_norm": 0.18000146746635437,
      "learning_rate": 1.7611730257174356e-07,
      "loss": 0.001,
      "step": 112780
    },
    {
      "epoch": 20507.272727272728,
      "grad_norm": 0.0009551270632073283,
      "learning_rate": 1.7602866724788367e-07,
      "loss": 0.001,
      "step": 112790
    },
    {
      "epoch": 20509.090909090908,
      "grad_norm": 0.0016940546920523047,
      "learning_rate": 1.7594004946843454e-07,
      "loss": 0.0012,
      "step": 112800
    },
    {
      "epoch": 20510.909090909092,
      "grad_norm": 0.17065712809562683,
      "learning_rate": 1.7585144923819545e-07,
      "loss": 0.0012,
      "step": 112810
    },
    {
      "epoch": 20512.727272727272,
      "grad_norm": 0.20588645339012146,
      "learning_rate": 1.7576286656196427e-07,
      "loss": 0.0009,
      "step": 112820
    },
    {
      "epoch": 20514.545454545456,
      "grad_norm": 0.17747898399829865,
      "learning_rate": 1.7567430144453797e-07,
      "loss": 0.0013,
      "step": 112830
    },
    {
      "epoch": 20516.363636363636,
      "grad_norm": 0.2871097922325134,
      "learning_rate": 1.7558575389071316e-07,
      "loss": 0.001,
      "step": 112840
    },
    {
      "epoch": 20518.18181818182,
      "grad_norm": 0.0007043852237984538,
      "learning_rate": 1.7549722390528477e-07,
      "loss": 0.0009,
      "step": 112850
    },
    {
      "epoch": 20520.0,
      "grad_norm": 0.0012344748247414827,
      "learning_rate": 1.75408711493047e-07,
      "loss": 0.0012,
      "step": 112860
    },
    {
      "epoch": 20521.81818181818,
      "grad_norm": 0.17201468348503113,
      "learning_rate": 1.7532021665879337e-07,
      "loss": 0.0012,
      "step": 112870
    },
    {
      "epoch": 20523.636363636364,
      "grad_norm": 0.0006235924665816128,
      "learning_rate": 1.7523173940731607e-07,
      "loss": 0.001,
      "step": 112880
    },
    {
      "epoch": 20525.454545454544,
      "grad_norm": 0.0007787755457684398,
      "learning_rate": 1.7514327974340675e-07,
      "loss": 0.0007,
      "step": 112890
    },
    {
      "epoch": 20527.272727272728,
      "grad_norm": 0.17456017434597015,
      "learning_rate": 1.750548376718558e-07,
      "loss": 0.0014,
      "step": 112900
    },
    {
      "epoch": 20529.090909090908,
      "grad_norm": 0.0012187955435365438,
      "learning_rate": 1.7496641319745243e-07,
      "loss": 0.0009,
      "step": 112910
    },
    {
      "epoch": 20530.909090909092,
      "grad_norm": 0.17722539603710175,
      "learning_rate": 1.7487800632498545e-07,
      "loss": 0.0012,
      "step": 112920
    },
    {
      "epoch": 20532.727272727272,
      "grad_norm": 0.0006230480503290892,
      "learning_rate": 1.7478961705924284e-07,
      "loss": 0.0012,
      "step": 112930
    },
    {
      "epoch": 20534.545454545456,
      "grad_norm": 0.19432565569877625,
      "learning_rate": 1.7470124540501046e-07,
      "loss": 0.0011,
      "step": 112940
    },
    {
      "epoch": 20536.363636363636,
      "grad_norm": 0.0008086534799076617,
      "learning_rate": 1.7461289136707457e-07,
      "loss": 0.001,
      "step": 112950
    },
    {
      "epoch": 20538.18181818182,
      "grad_norm": 0.2747761011123657,
      "learning_rate": 1.7452455495021962e-07,
      "loss": 0.0013,
      "step": 112960
    },
    {
      "epoch": 20540.0,
      "grad_norm": 0.00042029438191093504,
      "learning_rate": 1.744362361592297e-07,
      "loss": 0.0008,
      "step": 112970
    },
    {
      "epoch": 20541.81818181818,
      "grad_norm": 0.17381638288497925,
      "learning_rate": 1.7434793499888745e-07,
      "loss": 0.0009,
      "step": 112980
    },
    {
      "epoch": 20543.636363636364,
      "grad_norm": 0.2033933848142624,
      "learning_rate": 1.742596514739746e-07,
      "loss": 0.0013,
      "step": 112990
    },
    {
      "epoch": 20545.454545454544,
      "grad_norm": 0.20137113332748413,
      "learning_rate": 1.741713855892724e-07,
      "loss": 0.0012,
      "step": 113000
    },
    {
      "epoch": 20545.454545454544,
      "eval_loss": 5.15730619430542,
      "eval_runtime": 0.9501,
      "eval_samples_per_second": 10.526,
      "eval_steps_per_second": 5.263,
      "step": 113000
    },
    {
      "epoch": 20547.272727272728,
      "grad_norm": 0.004869916941970587,
      "learning_rate": 1.740831373495607e-07,
      "loss": 0.001,
      "step": 113010
    },
    {
      "epoch": 20549.090909090908,
      "grad_norm": 0.0006699090008623898,
      "learning_rate": 1.7399490675961832e-07,
      "loss": 0.0009,
      "step": 113020
    },
    {
      "epoch": 20550.909090909092,
      "grad_norm": 0.0006087633082643151,
      "learning_rate": 1.7390669382422362e-07,
      "loss": 0.0009,
      "step": 113030
    },
    {
      "epoch": 20552.727272727272,
      "grad_norm": 0.17785994708538055,
      "learning_rate": 1.7381849854815357e-07,
      "loss": 0.0012,
      "step": 113040
    },
    {
      "epoch": 20554.545454545456,
      "grad_norm": 0.18812407553195953,
      "learning_rate": 1.7373032093618412e-07,
      "loss": 0.001,
      "step": 113050
    },
    {
      "epoch": 20556.363636363636,
      "grad_norm": 0.2724486291408539,
      "learning_rate": 1.7364216099309082e-07,
      "loss": 0.0012,
      "step": 113060
    },
    {
      "epoch": 20558.18181818182,
      "grad_norm": 0.2104891985654831,
      "learning_rate": 1.7355401872364754e-07,
      "loss": 0.001,
      "step": 113070
    },
    {
      "epoch": 20560.0,
      "grad_norm": 0.0009953310946002603,
      "learning_rate": 1.734658941326279e-07,
      "loss": 0.001,
      "step": 113080
    },
    {
      "epoch": 20561.81818181818,
      "grad_norm": 0.26769471168518066,
      "learning_rate": 1.733777872248041e-07,
      "loss": 0.0012,
      "step": 113090
    },
    {
      "epoch": 20563.636363636364,
      "grad_norm": 0.23043197393417358,
      "learning_rate": 1.7328969800494726e-07,
      "loss": 0.0012,
      "step": 113100
    },
    {
      "epoch": 20565.454545454544,
      "grad_norm": 0.21548102796077728,
      "learning_rate": 1.7320162647782816e-07,
      "loss": 0.001,
      "step": 113110
    },
    {
      "epoch": 20567.272727272728,
      "grad_norm": 0.14653170108795166,
      "learning_rate": 1.7311357264821603e-07,
      "loss": 0.0009,
      "step": 113120
    },
    {
      "epoch": 20569.090909090908,
      "grad_norm": 0.20905734598636627,
      "learning_rate": 1.7302553652087925e-07,
      "loss": 0.0011,
      "step": 113130
    },
    {
      "epoch": 20570.909090909092,
      "grad_norm": 0.0007141851237975061,
      "learning_rate": 1.7293751810058565e-07,
      "loss": 0.001,
      "step": 113140
    },
    {
      "epoch": 20572.727272727272,
      "grad_norm": 0.00045223478809930384,
      "learning_rate": 1.7284951739210146e-07,
      "loss": 0.001,
      "step": 113150
    },
    {
      "epoch": 20574.545454545456,
      "grad_norm": 0.0004851226694881916,
      "learning_rate": 1.727615344001926e-07,
      "loss": 0.0014,
      "step": 113160
    },
    {
      "epoch": 20576.363636363636,
      "grad_norm": 0.2266935110092163,
      "learning_rate": 1.726735691296236e-07,
      "loss": 0.0012,
      "step": 113170
    },
    {
      "epoch": 20578.18181818182,
      "grad_norm": 0.008085668087005615,
      "learning_rate": 1.72585621585158e-07,
      "loss": 0.001,
      "step": 113180
    },
    {
      "epoch": 20580.0,
      "grad_norm": 0.0009274935582652688,
      "learning_rate": 1.7249769177155877e-07,
      "loss": 0.0009,
      "step": 113190
    },
    {
      "epoch": 20581.81818181818,
      "grad_norm": 0.27134189009666443,
      "learning_rate": 1.7240977969358754e-07,
      "loss": 0.0011,
      "step": 113200
    },
    {
      "epoch": 20583.636363636364,
      "grad_norm": 0.1494481861591339,
      "learning_rate": 1.7232188535600507e-07,
      "loss": 0.0007,
      "step": 113210
    },
    {
      "epoch": 20585.454545454544,
      "grad_norm": 0.19788961112499237,
      "learning_rate": 1.722340087635714e-07,
      "loss": 0.0015,
      "step": 113220
    },
    {
      "epoch": 20587.272727272728,
      "grad_norm": 0.18735361099243164,
      "learning_rate": 1.721461499210453e-07,
      "loss": 0.0008,
      "step": 113230
    },
    {
      "epoch": 20589.090909090908,
      "grad_norm": 0.18066348135471344,
      "learning_rate": 1.7205830883318456e-07,
      "loss": 0.0011,
      "step": 113240
    },
    {
      "epoch": 20590.909090909092,
      "grad_norm": 0.0006295828497968614,
      "learning_rate": 1.7197048550474641e-07,
      "loss": 0.0012,
      "step": 113250
    },
    {
      "epoch": 20592.727272727272,
      "grad_norm": 0.002721131546422839,
      "learning_rate": 1.718826799404866e-07,
      "loss": 0.001,
      "step": 113260
    },
    {
      "epoch": 20594.545454545456,
      "grad_norm": 0.17896783351898193,
      "learning_rate": 1.7179489214516042e-07,
      "loss": 0.0012,
      "step": 113270
    },
    {
      "epoch": 20596.363636363636,
      "grad_norm": 0.1787564754486084,
      "learning_rate": 1.7170712212352184e-07,
      "loss": 0.0011,
      "step": 113280
    },
    {
      "epoch": 20598.18181818182,
      "grad_norm": 0.0005982046714052558,
      "learning_rate": 1.7161936988032382e-07,
      "loss": 0.0011,
      "step": 113290
    },
    {
      "epoch": 20600.0,
      "grad_norm": 0.0007125519914552569,
      "learning_rate": 1.715316354203188e-07,
      "loss": 0.0012,
      "step": 113300
    },
    {
      "epoch": 20601.81818181818,
      "grad_norm": 0.0006228415877558291,
      "learning_rate": 1.7144391874825782e-07,
      "loss": 0.0012,
      "step": 113310
    },
    {
      "epoch": 20603.636363636364,
      "grad_norm": 0.0008265538490377367,
      "learning_rate": 1.71356219868891e-07,
      "loss": 0.001,
      "step": 113320
    },
    {
      "epoch": 20605.454545454544,
      "grad_norm": 0.19099830090999603,
      "learning_rate": 1.712685387869678e-07,
      "loss": 0.0011,
      "step": 113330
    },
    {
      "epoch": 20607.272727272728,
      "grad_norm": 0.0009886282496154308,
      "learning_rate": 1.711808755072363e-07,
      "loss": 0.0009,
      "step": 113340
    },
    {
      "epoch": 20609.090909090908,
      "grad_norm": 0.2676621079444885,
      "learning_rate": 1.7109323003444414e-07,
      "loss": 0.0013,
      "step": 113350
    },
    {
      "epoch": 20610.909090909092,
      "grad_norm": 0.17848384380340576,
      "learning_rate": 1.710056023733375e-07,
      "loss": 0.0009,
      "step": 113360
    },
    {
      "epoch": 20612.727272727272,
      "grad_norm": 0.0006945127970539033,
      "learning_rate": 1.7091799252866168e-07,
      "loss": 0.001,
      "step": 113370
    },
    {
      "epoch": 20614.545454545456,
      "grad_norm": 0.19166308641433716,
      "learning_rate": 1.7083040050516134e-07,
      "loss": 0.0015,
      "step": 113380
    },
    {
      "epoch": 20616.363636363636,
      "grad_norm": 0.0006625296664424241,
      "learning_rate": 1.7074282630757996e-07,
      "loss": 0.0007,
      "step": 113390
    },
    {
      "epoch": 20618.18181818182,
      "grad_norm": 0.20002876222133636,
      "learning_rate": 1.7065526994065972e-07,
      "loss": 0.0013,
      "step": 113400
    },
    {
      "epoch": 20620.0,
      "grad_norm": 0.27963006496429443,
      "learning_rate": 1.705677314091426e-07,
      "loss": 0.0013,
      "step": 113410
    },
    {
      "epoch": 20621.81818181818,
      "grad_norm": 0.16120213270187378,
      "learning_rate": 1.7048021071776897e-07,
      "loss": 0.001,
      "step": 113420
    },
    {
      "epoch": 20623.636363636364,
      "grad_norm": 0.001648630015552044,
      "learning_rate": 1.703927078712783e-07,
      "loss": 0.0012,
      "step": 113430
    },
    {
      "epoch": 20625.454545454544,
      "grad_norm": 0.006992358714342117,
      "learning_rate": 1.7030522287440958e-07,
      "loss": 0.0013,
      "step": 113440
    },
    {
      "epoch": 20627.272727272728,
      "grad_norm": 0.17969556152820587,
      "learning_rate": 1.702177557319001e-07,
      "loss": 0.0012,
      "step": 113450
    },
    {
      "epoch": 20629.090909090908,
      "grad_norm": 0.00046680361265316606,
      "learning_rate": 1.7013030644848698e-07,
      "loss": 0.0006,
      "step": 113460
    },
    {
      "epoch": 20630.909090909092,
      "grad_norm": 0.21743302047252655,
      "learning_rate": 1.7004287502890575e-07,
      "loss": 0.0012,
      "step": 113470
    },
    {
      "epoch": 20632.727272727272,
      "grad_norm": 0.001167014241218567,
      "learning_rate": 1.6995546147789103e-07,
      "loss": 0.0012,
      "step": 113480
    },
    {
      "epoch": 20634.545454545456,
      "grad_norm": 0.21398307383060455,
      "learning_rate": 1.6986806580017693e-07,
      "loss": 0.001,
      "step": 113490
    },
    {
      "epoch": 20636.363636363636,
      "grad_norm": 0.0013155019842088223,
      "learning_rate": 1.697806880004962e-07,
      "loss": 0.0008,
      "step": 113500
    },
    {
      "epoch": 20636.363636363636,
      "eval_loss": 5.022145748138428,
      "eval_runtime": 0.949,
      "eval_samples_per_second": 10.537,
      "eval_steps_per_second": 5.269,
      "step": 113500
    },
    {
      "epoch": 20638.18181818182,
      "grad_norm": 0.20028269290924072,
      "learning_rate": 1.6969332808358056e-07,
      "loss": 0.0013,
      "step": 113510
    },
    {
      "epoch": 20640.0,
      "grad_norm": 0.21327681839466095,
      "learning_rate": 1.6960598605416116e-07,
      "loss": 0.001,
      "step": 113520
    },
    {
      "epoch": 20641.81818181818,
      "grad_norm": 0.20565418899059296,
      "learning_rate": 1.6951866191696768e-07,
      "loss": 0.001,
      "step": 113530
    },
    {
      "epoch": 20643.636363636364,
      "grad_norm": 0.04588077962398529,
      "learning_rate": 1.694313556767294e-07,
      "loss": 0.0014,
      "step": 113540
    },
    {
      "epoch": 20645.454545454544,
      "grad_norm": 0.2124849408864975,
      "learning_rate": 1.6934406733817413e-07,
      "loss": 0.0007,
      "step": 113550
    },
    {
      "epoch": 20647.272727272728,
      "grad_norm": 0.18893983960151672,
      "learning_rate": 1.6925679690602873e-07,
      "loss": 0.0012,
      "step": 113560
    },
    {
      "epoch": 20649.090909090908,
      "grad_norm": 0.18241675198078156,
      "learning_rate": 1.6916954438501969e-07,
      "loss": 0.0011,
      "step": 113570
    },
    {
      "epoch": 20650.909090909092,
      "grad_norm": 0.0003547499072737992,
      "learning_rate": 1.6908230977987182e-07,
      "loss": 0.0008,
      "step": 113580
    },
    {
      "epoch": 20652.727272727272,
      "grad_norm": 0.21670372784137726,
      "learning_rate": 1.6899509309530908e-07,
      "loss": 0.0013,
      "step": 113590
    },
    {
      "epoch": 20654.545454545456,
      "grad_norm": 0.0005313941510394216,
      "learning_rate": 1.6890789433605506e-07,
      "loss": 0.0009,
      "step": 113600
    },
    {
      "epoch": 20656.363636363636,
      "grad_norm": 0.17626561224460602,
      "learning_rate": 1.6882071350683163e-07,
      "loss": 0.0016,
      "step": 113610
    },
    {
      "epoch": 20658.18181818182,
      "grad_norm": 0.00041807600064203143,
      "learning_rate": 1.6873355061236e-07,
      "loss": 0.0007,
      "step": 113620
    },
    {
      "epoch": 20660.0,
      "grad_norm": 0.18803396821022034,
      "learning_rate": 1.6864640565736054e-07,
      "loss": 0.0012,
      "step": 113630
    },
    {
      "epoch": 20661.81818181818,
      "grad_norm": 0.18122993409633636,
      "learning_rate": 1.685592786465524e-07,
      "loss": 0.0012,
      "step": 113640
    },
    {
      "epoch": 20663.636363636364,
      "grad_norm": 0.19426089525222778,
      "learning_rate": 1.68472169584654e-07,
      "loss": 0.001,
      "step": 113650
    },
    {
      "epoch": 20665.454545454544,
      "grad_norm": 0.20607437193393707,
      "learning_rate": 1.6838507847638266e-07,
      "loss": 0.001,
      "step": 113660
    },
    {
      "epoch": 20667.272727272728,
      "grad_norm": 0.19454844295978546,
      "learning_rate": 1.6829800532645445e-07,
      "loss": 0.001,
      "step": 113670
    },
    {
      "epoch": 20669.090909090908,
      "grad_norm": 0.006049502640962601,
      "learning_rate": 1.6821095013958513e-07,
      "loss": 0.0012,
      "step": 113680
    },
    {
      "epoch": 20670.909090909092,
      "grad_norm": 0.0006590498378500342,
      "learning_rate": 1.6812391292048894e-07,
      "loss": 0.0009,
      "step": 113690
    },
    {
      "epoch": 20672.727272727272,
      "grad_norm": 0.2060653418302536,
      "learning_rate": 1.6803689367387918e-07,
      "loss": 0.0012,
      "step": 113700
    },
    {
      "epoch": 20674.545454545456,
      "grad_norm": 0.2824321687221527,
      "learning_rate": 1.6794989240446855e-07,
      "loss": 0.0014,
      "step": 113710
    },
    {
      "epoch": 20676.363636363636,
      "grad_norm": 0.0008801012299954891,
      "learning_rate": 1.6786290911696828e-07,
      "loss": 0.0007,
      "step": 113720
    },
    {
      "epoch": 20678.18181818182,
      "grad_norm": 0.0003966278163716197,
      "learning_rate": 1.6777594381608934e-07,
      "loss": 0.001,
      "step": 113730
    },
    {
      "epoch": 20680.0,
      "grad_norm": 0.17960858345031738,
      "learning_rate": 1.676889965065406e-07,
      "loss": 0.0012,
      "step": 113740
    },
    {
      "epoch": 20681.81818181818,
      "grad_norm": 0.0005136459367349744,
      "learning_rate": 1.6760206719303105e-07,
      "loss": 0.0009,
      "step": 113750
    },
    {
      "epoch": 20683.636363636364,
      "grad_norm": 0.0008037525694817305,
      "learning_rate": 1.6751515588026828e-07,
      "loss": 0.0014,
      "step": 113760
    },
    {
      "epoch": 20685.454545454544,
      "grad_norm": 0.17128261923789978,
      "learning_rate": 1.6742826257295885e-07,
      "loss": 0.0009,
      "step": 113770
    },
    {
      "epoch": 20687.272727272728,
      "grad_norm": 0.1873532086610794,
      "learning_rate": 1.673413872758082e-07,
      "loss": 0.0011,
      "step": 113780
    },
    {
      "epoch": 20689.090909090908,
      "grad_norm": 0.0007766559137962759,
      "learning_rate": 1.6725452999352135e-07,
      "loss": 0.001,
      "step": 113790
    },
    {
      "epoch": 20690.909090909092,
      "grad_norm": 0.1890631765127182,
      "learning_rate": 1.6716769073080178e-07,
      "loss": 0.0012,
      "step": 113800
    },
    {
      "epoch": 20692.727272727272,
      "grad_norm": 0.178010031580925,
      "learning_rate": 1.6708086949235205e-07,
      "loss": 0.0009,
      "step": 113810
    },
    {
      "epoch": 20694.545454545456,
      "grad_norm": 0.0012352893827483058,
      "learning_rate": 1.6699406628287422e-07,
      "loss": 0.0012,
      "step": 113820
    },
    {
      "epoch": 20696.363636363636,
      "grad_norm": 0.0013556571211665869,
      "learning_rate": 1.6690728110706876e-07,
      "loss": 0.0012,
      "step": 113830
    },
    {
      "epoch": 20698.18181818182,
      "grad_norm": 0.0009198358748108149,
      "learning_rate": 1.6682051396963593e-07,
      "loss": 0.0009,
      "step": 113840
    },
    {
      "epoch": 20700.0,
      "grad_norm": 0.0004984951810911298,
      "learning_rate": 1.667337648752738e-07,
      "loss": 0.0012,
      "step": 113850
    },
    {
      "epoch": 20701.81818181818,
      "grad_norm": 0.0007745689945295453,
      "learning_rate": 1.666470338286806e-07,
      "loss": 0.001,
      "step": 113860
    },
    {
      "epoch": 20703.636363636364,
      "grad_norm": 0.22349470853805542,
      "learning_rate": 1.665603208345534e-07,
      "loss": 0.001,
      "step": 113870
    },
    {
      "epoch": 20705.454545454544,
      "grad_norm": 0.1792433261871338,
      "learning_rate": 1.6647362589758785e-07,
      "loss": 0.0012,
      "step": 113880
    },
    {
      "epoch": 20707.272727272728,
      "grad_norm": 0.0005879038944840431,
      "learning_rate": 1.6638694902247863e-07,
      "loss": 0.0009,
      "step": 113890
    },
    {
      "epoch": 20709.090909090908,
      "grad_norm": 0.18929331004619598,
      "learning_rate": 1.6630029021392007e-07,
      "loss": 0.0012,
      "step": 113900
    },
    {
      "epoch": 20710.909090909092,
      "grad_norm": 0.20293015241622925,
      "learning_rate": 1.6621364947660472e-07,
      "loss": 0.0012,
      "step": 113910
    },
    {
      "epoch": 20712.727272727272,
      "grad_norm": 0.18682098388671875,
      "learning_rate": 1.6612702681522506e-07,
      "loss": 0.0011,
      "step": 113920
    },
    {
      "epoch": 20714.545454545456,
      "grad_norm": 0.17720088362693787,
      "learning_rate": 1.660404222344714e-07,
      "loss": 0.0009,
      "step": 113930
    },
    {
      "epoch": 20716.363636363636,
      "grad_norm": 0.21204979717731476,
      "learning_rate": 1.659538357390341e-07,
      "loss": 0.0012,
      "step": 113940
    },
    {
      "epoch": 20718.18181818182,
      "grad_norm": 0.1855594664812088,
      "learning_rate": 1.6586726733360234e-07,
      "loss": 0.0012,
      "step": 113950
    },
    {
      "epoch": 20720.0,
      "grad_norm": 0.2096361517906189,
      "learning_rate": 1.6578071702286396e-07,
      "loss": 0.0011,
      "step": 113960
    },
    {
      "epoch": 20721.81818181818,
      "grad_norm": 0.0009445755276829004,
      "learning_rate": 1.6569418481150593e-07,
      "loss": 0.001,
      "step": 113970
    },
    {
      "epoch": 20723.636363636364,
      "grad_norm": 0.0014620812144130468,
      "learning_rate": 1.6560767070421462e-07,
      "loss": 0.0011,
      "step": 113980
    },
    {
      "epoch": 20725.454545454544,
      "grad_norm": 0.0004872404388152063,
      "learning_rate": 1.655211747056749e-07,
      "loss": 0.001,
      "step": 113990
    },
    {
      "epoch": 20727.272727272728,
      "grad_norm": 0.0011092772474512458,
      "learning_rate": 1.6543469682057104e-07,
      "loss": 0.0011,
      "step": 114000
    },
    {
      "epoch": 20727.272727272728,
      "eval_loss": 5.18845272064209,
      "eval_runtime": 0.9495,
      "eval_samples_per_second": 10.531,
      "eval_steps_per_second": 5.266,
      "step": 114000
    },
    {
      "epoch": 20729.090909090908,
      "grad_norm": 0.0005473798373714089,
      "learning_rate": 1.6534823705358592e-07,
      "loss": 0.0012,
      "step": 114010
    },
    {
      "epoch": 20730.909090909092,
      "grad_norm": 0.28756245970726013,
      "learning_rate": 1.6526179540940182e-07,
      "loss": 0.0012,
      "step": 114020
    },
    {
      "epoch": 20732.727272727272,
      "grad_norm": 0.0006229421705938876,
      "learning_rate": 1.6517537189270043e-07,
      "loss": 0.0012,
      "step": 114030
    },
    {
      "epoch": 20734.545454545456,
      "grad_norm": 0.18361535668373108,
      "learning_rate": 1.6508896650816107e-07,
      "loss": 0.001,
      "step": 114040
    },
    {
      "epoch": 20736.363636363636,
      "grad_norm": 0.0013448772951960564,
      "learning_rate": 1.650025792604634e-07,
      "loss": 0.001,
      "step": 114050
    },
    {
      "epoch": 20738.18181818182,
      "grad_norm": 0.26848796010017395,
      "learning_rate": 1.6491621015428587e-07,
      "loss": 0.0014,
      "step": 114060
    },
    {
      "epoch": 20740.0,
      "grad_norm": 0.0004901232314296067,
      "learning_rate": 1.6482985919430543e-07,
      "loss": 0.0009,
      "step": 114070
    },
    {
      "epoch": 20741.81818181818,
      "grad_norm": 0.19901323318481445,
      "learning_rate": 1.6474352638519846e-07,
      "loss": 0.0012,
      "step": 114080
    },
    {
      "epoch": 20743.636363636364,
      "grad_norm": 0.0005030995816923678,
      "learning_rate": 1.6465721173164e-07,
      "loss": 0.0009,
      "step": 114090
    },
    {
      "epoch": 20745.454545454544,
      "grad_norm": 0.1771223545074463,
      "learning_rate": 1.6457091523830457e-07,
      "loss": 0.0013,
      "step": 114100
    },
    {
      "epoch": 20747.272727272728,
      "grad_norm": 0.0007357532740570605,
      "learning_rate": 1.644846369098658e-07,
      "loss": 0.0009,
      "step": 114110
    },
    {
      "epoch": 20749.090909090908,
      "grad_norm": 0.17211422324180603,
      "learning_rate": 1.6439837675099537e-07,
      "loss": 0.001,
      "step": 114120
    },
    {
      "epoch": 20750.909090909092,
      "grad_norm": 0.1775856465101242,
      "learning_rate": 1.643121347663649e-07,
      "loss": 0.0012,
      "step": 114130
    },
    {
      "epoch": 20752.727272727272,
      "grad_norm": 0.0004332889511715621,
      "learning_rate": 1.6422591096064507e-07,
      "loss": 0.001,
      "step": 114140
    },
    {
      "epoch": 20754.545454545456,
      "grad_norm": 0.26719850301742554,
      "learning_rate": 1.6413970533850497e-07,
      "loss": 0.0015,
      "step": 114150
    },
    {
      "epoch": 20756.363636363636,
      "grad_norm": 0.000706010905560106,
      "learning_rate": 1.6405351790461286e-07,
      "loss": 0.0009,
      "step": 114160
    },
    {
      "epoch": 20758.18181818182,
      "grad_norm": 0.2139579951763153,
      "learning_rate": 1.639673486636365e-07,
      "loss": 0.0015,
      "step": 114170
    },
    {
      "epoch": 20760.0,
      "grad_norm": 0.01786794513463974,
      "learning_rate": 1.638811976202421e-07,
      "loss": 0.0008,
      "step": 114180
    },
    {
      "epoch": 20761.81818181818,
      "grad_norm": 0.26974812150001526,
      "learning_rate": 1.6379506477909515e-07,
      "loss": 0.0012,
      "step": 114190
    },
    {
      "epoch": 20763.636363636364,
      "grad_norm": 0.17296826839447021,
      "learning_rate": 1.6370895014485998e-07,
      "loss": 0.0013,
      "step": 114200
    },
    {
      "epoch": 20765.454545454544,
      "grad_norm": 0.0008666242938488722,
      "learning_rate": 1.6362285372220015e-07,
      "loss": 0.0007,
      "step": 114210
    },
    {
      "epoch": 20767.272727272728,
      "grad_norm": 0.20074798166751862,
      "learning_rate": 1.6353677551577845e-07,
      "loss": 0.0013,
      "step": 114220
    },
    {
      "epoch": 20769.090909090908,
      "grad_norm": 0.2050086110830307,
      "learning_rate": 1.6345071553025586e-07,
      "loss": 0.001,
      "step": 114230
    },
    {
      "epoch": 20770.909090909092,
      "grad_norm": 0.2682291567325592,
      "learning_rate": 1.6336467377029307e-07,
      "loss": 0.001,
      "step": 114240
    },
    {
      "epoch": 20772.727272727272,
      "grad_norm": 0.0014772057766094804,
      "learning_rate": 1.6327865024054983e-07,
      "loss": 0.0009,
      "step": 114250
    },
    {
      "epoch": 20774.545454545456,
      "grad_norm": 0.0012718080542981625,
      "learning_rate": 1.6319264494568452e-07,
      "loss": 0.0009,
      "step": 114260
    },
    {
      "epoch": 20776.363636363636,
      "grad_norm": 0.20305046439170837,
      "learning_rate": 1.6310665789035466e-07,
      "loss": 0.0015,
      "step": 114270
    },
    {
      "epoch": 20778.18181818182,
      "grad_norm": 0.21728070080280304,
      "learning_rate": 1.630206890792167e-07,
      "loss": 0.001,
      "step": 114280
    },
    {
      "epoch": 20780.0,
      "grad_norm": 0.17950701713562012,
      "learning_rate": 1.6293473851692629e-07,
      "loss": 0.0011,
      "step": 114290
    },
    {
      "epoch": 20781.81818181818,
      "grad_norm": 0.0006415711250156164,
      "learning_rate": 1.6284880620813846e-07,
      "loss": 0.0012,
      "step": 114300
    },
    {
      "epoch": 20783.636363636364,
      "grad_norm": 0.0004952083108946681,
      "learning_rate": 1.6276289215750598e-07,
      "loss": 0.0009,
      "step": 114310
    },
    {
      "epoch": 20785.454545454544,
      "grad_norm": 0.0008781394571997225,
      "learning_rate": 1.6267699636968203e-07,
      "loss": 0.0012,
      "step": 114320
    },
    {
      "epoch": 20787.272727272728,
      "grad_norm": 0.01749269850552082,
      "learning_rate": 1.6259111884931814e-07,
      "loss": 0.0015,
      "step": 114330
    },
    {
      "epoch": 20789.090909090908,
      "grad_norm": 0.16827571392059326,
      "learning_rate": 1.6250525960106493e-07,
      "loss": 0.0009,
      "step": 114340
    },
    {
      "epoch": 20790.909090909092,
      "grad_norm": 0.0005332114524208009,
      "learning_rate": 1.6241941862957198e-07,
      "loss": 0.0012,
      "step": 114350
    },
    {
      "epoch": 20792.727272727272,
      "grad_norm": 0.18419180810451508,
      "learning_rate": 1.6233359593948775e-07,
      "loss": 0.0009,
      "step": 114360
    },
    {
      "epoch": 20794.545454545456,
      "grad_norm": 0.00069914769846946,
      "learning_rate": 1.6224779153546037e-07,
      "loss": 0.0012,
      "step": 114370
    },
    {
      "epoch": 20796.363636363636,
      "grad_norm": 0.000612816191278398,
      "learning_rate": 1.6216200542213614e-07,
      "loss": 0.001,
      "step": 114380
    },
    {
      "epoch": 20798.18181818182,
      "grad_norm": 0.0006195895839482546,
      "learning_rate": 1.6207623760416072e-07,
      "loss": 0.001,
      "step": 114390
    },
    {
      "epoch": 20800.0,
      "grad_norm": 0.21995532512664795,
      "learning_rate": 1.6199048808617894e-07,
      "loss": 0.0012,
      "step": 114400
    },
    {
      "epoch": 20801.81818181818,
      "grad_norm": 0.2809891700744629,
      "learning_rate": 1.6190475687283482e-07,
      "loss": 0.0013,
      "step": 114410
    },
    {
      "epoch": 20803.636363636364,
      "grad_norm": 0.0005728849209845066,
      "learning_rate": 1.6181904396877037e-07,
      "loss": 0.0009,
      "step": 114420
    },
    {
      "epoch": 20805.454545454544,
      "grad_norm": 0.16699083149433136,
      "learning_rate": 1.6173334937862765e-07,
      "loss": 0.001,
      "step": 114430
    },
    {
      "epoch": 20807.272727272728,
      "grad_norm": 0.17523598670959473,
      "learning_rate": 1.6164767310704757e-07,
      "loss": 0.0012,
      "step": 114440
    },
    {
      "epoch": 20809.090909090908,
      "grad_norm": 0.27487313747406006,
      "learning_rate": 1.6156201515866968e-07,
      "loss": 0.0013,
      "step": 114450
    },
    {
      "epoch": 20810.909090909092,
      "grad_norm": 0.17504559457302094,
      "learning_rate": 1.614763755381327e-07,
      "loss": 0.0009,
      "step": 114460
    },
    {
      "epoch": 20812.727272727272,
      "grad_norm": 0.0005721318884752691,
      "learning_rate": 1.6139075425007425e-07,
      "loss": 0.0012,
      "step": 114470
    },
    {
      "epoch": 20814.545454545456,
      "grad_norm": 0.0005990641075186431,
      "learning_rate": 1.6130515129913143e-07,
      "loss": 0.0009,
      "step": 114480
    },
    {
      "epoch": 20816.363636363636,
      "grad_norm": 0.17392422258853912,
      "learning_rate": 1.6121956668993975e-07,
      "loss": 0.001,
      "step": 114490
    },
    {
      "epoch": 20818.18181818182,
      "grad_norm": 0.0005511175259016454,
      "learning_rate": 1.6113400042713388e-07,
      "loss": 0.001,
      "step": 114500
    },
    {
      "epoch": 20818.18181818182,
      "eval_loss": 5.202549457550049,
      "eval_runtime": 0.951,
      "eval_samples_per_second": 10.516,
      "eval_steps_per_second": 5.258,
      "step": 114500
    },
    {
      "epoch": 20820.0,
      "grad_norm": 0.17625591158866882,
      "learning_rate": 1.610484525153477e-07,
      "loss": 0.0012,
      "step": 114510
    },
    {
      "epoch": 20821.81818181818,
      "grad_norm": 0.011310886591672897,
      "learning_rate": 1.6096292295921427e-07,
      "loss": 0.0013,
      "step": 114520
    },
    {
      "epoch": 20823.636363636364,
      "grad_norm": 0.21122683584690094,
      "learning_rate": 1.6087741176336512e-07,
      "loss": 0.0012,
      "step": 114530
    },
    {
      "epoch": 20825.454545454544,
      "grad_norm": 0.16733551025390625,
      "learning_rate": 1.60791918932431e-07,
      "loss": 0.001,
      "step": 114540
    },
    {
      "epoch": 20827.272727272728,
      "grad_norm": 0.00040906594949774444,
      "learning_rate": 1.6070644447104165e-07,
      "loss": 0.0008,
      "step": 114550
    },
    {
      "epoch": 20829.090909090908,
      "grad_norm": 0.2060728669166565,
      "learning_rate": 1.606209883838261e-07,
      "loss": 0.0012,
      "step": 114560
    },
    {
      "epoch": 20830.909090909092,
      "grad_norm": 0.17780844867229462,
      "learning_rate": 1.605355506754121e-07,
      "loss": 0.0012,
      "step": 114570
    },
    {
      "epoch": 20832.727272727272,
      "grad_norm": 0.005126301199197769,
      "learning_rate": 1.6045013135042617e-07,
      "loss": 0.0009,
      "step": 114580
    },
    {
      "epoch": 20834.545454545456,
      "grad_norm": 0.2741210162639618,
      "learning_rate": 1.6036473041349435e-07,
      "loss": 0.0013,
      "step": 114590
    },
    {
      "epoch": 20836.363636363636,
      "grad_norm": 0.002932335250079632,
      "learning_rate": 1.6027934786924185e-07,
      "loss": 0.0008,
      "step": 114600
    },
    {
      "epoch": 20838.18181818182,
      "grad_norm": 0.16627636551856995,
      "learning_rate": 1.6019398372229176e-07,
      "loss": 0.0013,
      "step": 114610
    },
    {
      "epoch": 20840.0,
      "grad_norm": 0.21234959363937378,
      "learning_rate": 1.6010863797726741e-07,
      "loss": 0.0012,
      "step": 114620
    },
    {
      "epoch": 20841.81818181818,
      "grad_norm": 0.0005643694894388318,
      "learning_rate": 1.600233106387904e-07,
      "loss": 0.0012,
      "step": 114630
    },
    {
      "epoch": 20843.636363636364,
      "grad_norm": 0.17730112373828888,
      "learning_rate": 1.5993800171148175e-07,
      "loss": 0.0012,
      "step": 114640
    },
    {
      "epoch": 20845.454545454544,
      "grad_norm": 0.14752228558063507,
      "learning_rate": 1.598527111999612e-07,
      "loss": 0.0008,
      "step": 114650
    },
    {
      "epoch": 20847.272727272728,
      "grad_norm": 0.19168789684772491,
      "learning_rate": 1.5976743910884737e-07,
      "loss": 0.0012,
      "step": 114660
    },
    {
      "epoch": 20849.090909090908,
      "grad_norm": 0.0006960750324651599,
      "learning_rate": 1.5968218544275852e-07,
      "loss": 0.0009,
      "step": 114670
    },
    {
      "epoch": 20850.909090909092,
      "grad_norm": 0.012465808540582657,
      "learning_rate": 1.5959695020631126e-07,
      "loss": 0.0012,
      "step": 114680
    },
    {
      "epoch": 20852.727272727272,
      "grad_norm": 0.17950975894927979,
      "learning_rate": 1.5951173340412134e-07,
      "loss": 0.0009,
      "step": 114690
    },
    {
      "epoch": 20854.545454545456,
      "grad_norm": 0.017607321962714195,
      "learning_rate": 1.5942653504080388e-07,
      "loss": 0.0014,
      "step": 114700
    },
    {
      "epoch": 20856.363636363636,
      "grad_norm": 0.17317059636116028,
      "learning_rate": 1.5934135512097247e-07,
      "loss": 0.001,
      "step": 114710
    },
    {
      "epoch": 20858.18181818182,
      "grad_norm": 0.0009218562045134604,
      "learning_rate": 1.5925619364924013e-07,
      "loss": 0.0008,
      "step": 114720
    },
    {
      "epoch": 20860.0,
      "grad_norm": 0.20549173653125763,
      "learning_rate": 1.5917105063021873e-07,
      "loss": 0.0012,
      "step": 114730
    },
    {
      "epoch": 20861.81818181818,
      "grad_norm": 0.21199564635753632,
      "learning_rate": 1.590859260685189e-07,
      "loss": 0.0011,
      "step": 114740
    },
    {
      "epoch": 20863.636363636364,
      "grad_norm": 0.0004920052015222609,
      "learning_rate": 1.590008199687508e-07,
      "loss": 0.001,
      "step": 114750
    },
    {
      "epoch": 20865.454545454544,
      "grad_norm": 0.2772252559661865,
      "learning_rate": 1.5891573233552314e-07,
      "loss": 0.0014,
      "step": 114760
    },
    {
      "epoch": 20867.272727272728,
      "grad_norm": 0.0005956673994660378,
      "learning_rate": 1.5883066317344352e-07,
      "loss": 0.0009,
      "step": 114770
    },
    {
      "epoch": 20869.090909090908,
      "grad_norm": 0.000516968488227576,
      "learning_rate": 1.587456124871191e-07,
      "loss": 0.001,
      "step": 114780
    },
    {
      "epoch": 20870.909090909092,
      "grad_norm": 0.0007899044430814683,
      "learning_rate": 1.5866058028115597e-07,
      "loss": 0.0007,
      "step": 114790
    },
    {
      "epoch": 20872.727272727272,
      "grad_norm": 0.0007329014479182661,
      "learning_rate": 1.5857556656015836e-07,
      "loss": 0.0014,
      "step": 114800
    },
    {
      "epoch": 20874.545454545456,
      "grad_norm": 0.0007376663270406425,
      "learning_rate": 1.5849057132873062e-07,
      "loss": 0.0009,
      "step": 114810
    },
    {
      "epoch": 20876.363636363636,
      "grad_norm": 0.0008070589974522591,
      "learning_rate": 1.5840559459147522e-07,
      "loss": 0.0011,
      "step": 114820
    },
    {
      "epoch": 20878.18181818182,
      "grad_norm": 0.2746184766292572,
      "learning_rate": 1.5832063635299437e-07,
      "loss": 0.0015,
      "step": 114830
    },
    {
      "epoch": 20880.0,
      "grad_norm": 0.001124313217587769,
      "learning_rate": 1.5823569661788877e-07,
      "loss": 0.0009,
      "step": 114840
    },
    {
      "epoch": 20881.81818181818,
      "grad_norm": 0.2714589834213257,
      "learning_rate": 1.581507753907581e-07,
      "loss": 0.0013,
      "step": 114850
    },
    {
      "epoch": 20883.636363636364,
      "grad_norm": 0.00045459577813744545,
      "learning_rate": 1.5806587267620148e-07,
      "loss": 0.0009,
      "step": 114860
    },
    {
      "epoch": 20885.454545454544,
      "grad_norm": 0.0006979628233239055,
      "learning_rate": 1.5798098847881663e-07,
      "loss": 0.0012,
      "step": 114870
    },
    {
      "epoch": 20887.272727272728,
      "grad_norm": 0.0004229126498103142,
      "learning_rate": 1.578961228032002e-07,
      "loss": 0.0009,
      "step": 114880
    },
    {
      "epoch": 20889.090909090908,
      "grad_norm": 0.17201028764247894,
      "learning_rate": 1.5781127565394837e-07,
      "loss": 0.0013,
      "step": 114890
    },
    {
      "epoch": 20890.909090909092,
      "grad_norm": 0.21077807247638702,
      "learning_rate": 1.5772644703565564e-07,
      "loss": 0.001,
      "step": 114900
    },
    {
      "epoch": 20892.727272727272,
      "grad_norm": 0.26646411418914795,
      "learning_rate": 1.5764163695291615e-07,
      "loss": 0.0012,
      "step": 114910
    },
    {
      "epoch": 20894.545454545456,
      "grad_norm": 0.21088650822639465,
      "learning_rate": 1.5755684541032254e-07,
      "loss": 0.0009,
      "step": 114920
    },
    {
      "epoch": 20896.363636363636,
      "grad_norm": 0.2783992290496826,
      "learning_rate": 1.5747207241246652e-07,
      "loss": 0.0015,
      "step": 114930
    },
    {
      "epoch": 20898.18181818182,
      "grad_norm": 0.18093182146549225,
      "learning_rate": 1.5738731796393918e-07,
      "loss": 0.0009,
      "step": 114940
    },
    {
      "epoch": 20900.0,
      "grad_norm": 0.1675296425819397,
      "learning_rate": 1.5730258206933024e-07,
      "loss": 0.001,
      "step": 114950
    },
    {
      "epoch": 20901.81818181818,
      "grad_norm": 0.1743950992822647,
      "learning_rate": 1.5721786473322823e-07,
      "loss": 0.0012,
      "step": 114960
    },
    {
      "epoch": 20903.636363636364,
      "grad_norm": 0.001078056520782411,
      "learning_rate": 1.5713316596022135e-07,
      "loss": 0.0008,
      "step": 114970
    },
    {
      "epoch": 20905.454545454544,
      "grad_norm": 0.2011842429637909,
      "learning_rate": 1.5704848575489625e-07,
      "loss": 0.0012,
      "step": 114980
    },
    {
      "epoch": 20907.272727272728,
      "grad_norm": 0.0009646079852245748,
      "learning_rate": 1.5696382412183851e-07,
      "loss": 0.001,
      "step": 114990
    },
    {
      "epoch": 20909.090909090908,
      "grad_norm": 0.28056034445762634,
      "learning_rate": 1.5687918106563325e-07,
      "loss": 0.0012,
      "step": 115000
    },
    {
      "epoch": 20909.090909090908,
      "eval_loss": 5.046817779541016,
      "eval_runtime": 0.9494,
      "eval_samples_per_second": 10.533,
      "eval_steps_per_second": 5.266,
      "step": 115000
    },
    {
      "epoch": 20910.909090909092,
      "grad_norm": 0.0008320949273183942,
      "learning_rate": 1.5679455659086393e-07,
      "loss": 0.001,
      "step": 115010
    },
    {
      "epoch": 20912.727272727272,
      "grad_norm": 0.18923568725585938,
      "learning_rate": 1.567099507021137e-07,
      "loss": 0.0012,
      "step": 115020
    },
    {
      "epoch": 20914.545454545456,
      "grad_norm": 0.0005389212165027857,
      "learning_rate": 1.5662536340396405e-07,
      "loss": 0.0009,
      "step": 115030
    },
    {
      "epoch": 20916.363636363636,
      "grad_norm": 0.18927428126335144,
      "learning_rate": 1.5654079470099562e-07,
      "loss": 0.0011,
      "step": 115040
    },
    {
      "epoch": 20918.18181818182,
      "grad_norm": 0.18246152997016907,
      "learning_rate": 1.5645624459778855e-07,
      "loss": 0.0011,
      "step": 115050
    },
    {
      "epoch": 20920.0,
      "grad_norm": 0.3081190884113312,
      "learning_rate": 1.5637171309892134e-07,
      "loss": 0.0011,
      "step": 115060
    },
    {
      "epoch": 20921.81818181818,
      "grad_norm": 0.17291656136512756,
      "learning_rate": 1.5628720020897156e-07,
      "loss": 0.001,
      "step": 115070
    },
    {
      "epoch": 20923.636363636364,
      "grad_norm": 0.0004863346694037318,
      "learning_rate": 1.5620270593251633e-07,
      "loss": 0.0008,
      "step": 115080
    },
    {
      "epoch": 20925.454545454544,
      "grad_norm": 0.15786418318748474,
      "learning_rate": 1.5611823027413108e-07,
      "loss": 0.0017,
      "step": 115090
    },
    {
      "epoch": 20927.272727272728,
      "grad_norm": 0.0008521040435880423,
      "learning_rate": 1.5603377323839068e-07,
      "loss": 0.0007,
      "step": 115100
    },
    {
      "epoch": 20929.090909090908,
      "grad_norm": 0.22585944831371307,
      "learning_rate": 1.5594933482986884e-07,
      "loss": 0.0013,
      "step": 115110
    },
    {
      "epoch": 20930.909090909092,
      "grad_norm": 0.0007297922857105732,
      "learning_rate": 1.5586491505313799e-07,
      "loss": 0.0009,
      "step": 115120
    },
    {
      "epoch": 20932.727272727272,
      "grad_norm": 0.0005635794368572533,
      "learning_rate": 1.5578051391277013e-07,
      "loss": 0.0013,
      "step": 115130
    },
    {
      "epoch": 20934.545454545456,
      "grad_norm": 0.01844918355345726,
      "learning_rate": 1.5569613141333587e-07,
      "loss": 0.0013,
      "step": 115140
    },
    {
      "epoch": 20936.363636363636,
      "grad_norm": 0.2101888507604599,
      "learning_rate": 1.556117675594047e-07,
      "loss": 0.0009,
      "step": 115150
    },
    {
      "epoch": 20938.18181818182,
      "grad_norm": 0.1970859169960022,
      "learning_rate": 1.555274223555455e-07,
      "loss": 0.001,
      "step": 115160
    },
    {
      "epoch": 20940.0,
      "grad_norm": 0.0017211444210261106,
      "learning_rate": 1.5544309580632587e-07,
      "loss": 0.0011,
      "step": 115170
    },
    {
      "epoch": 20941.81818181818,
      "grad_norm": 0.20068226754665375,
      "learning_rate": 1.5535878791631225e-07,
      "loss": 0.0009,
      "step": 115180
    },
    {
      "epoch": 20943.636363636364,
      "grad_norm": 0.18338190019130707,
      "learning_rate": 1.5527449869007054e-07,
      "loss": 0.0012,
      "step": 115190
    },
    {
      "epoch": 20945.454545454544,
      "grad_norm": 0.00040569694829173386,
      "learning_rate": 1.551902281321651e-07,
      "loss": 0.0012,
      "step": 115200
    },
    {
      "epoch": 20947.272727272728,
      "grad_norm": 0.000635028351098299,
      "learning_rate": 1.5510597624715977e-07,
      "loss": 0.001,
      "step": 115210
    },
    {
      "epoch": 20949.090909090908,
      "grad_norm": 0.17156539857387543,
      "learning_rate": 1.5502174303961707e-07,
      "loss": 0.0013,
      "step": 115220
    },
    {
      "epoch": 20950.909090909092,
      "grad_norm": 0.2166016846895218,
      "learning_rate": 1.549375285140984e-07,
      "loss": 0.0011,
      "step": 115230
    },
    {
      "epoch": 20952.727272727272,
      "grad_norm": 0.0004788846126757562,
      "learning_rate": 1.548533326751647e-07,
      "loss": 0.0009,
      "step": 115240
    },
    {
      "epoch": 20954.545454545456,
      "grad_norm": 0.21786822378635406,
      "learning_rate": 1.547691555273753e-07,
      "loss": 0.0013,
      "step": 115250
    },
    {
      "epoch": 20956.363636363636,
      "grad_norm": 0.0004609034804161638,
      "learning_rate": 1.5468499707528852e-07,
      "loss": 0.001,
      "step": 115260
    },
    {
      "epoch": 20958.18181818182,
      "grad_norm": 0.20233798027038574,
      "learning_rate": 1.546008573234624e-07,
      "loss": 0.0012,
      "step": 115270
    },
    {
      "epoch": 20960.0,
      "grad_norm": 0.18123674392700195,
      "learning_rate": 1.5451673627645295e-07,
      "loss": 0.0012,
      "step": 115280
    },
    {
      "epoch": 20961.81818181818,
      "grad_norm": 0.0006424383609555662,
      "learning_rate": 1.5443263393881618e-07,
      "loss": 0.001,
      "step": 115290
    },
    {
      "epoch": 20963.636363636364,
      "grad_norm": 0.2141140103340149,
      "learning_rate": 1.5434855031510625e-07,
      "loss": 0.0013,
      "step": 115300
    },
    {
      "epoch": 20965.454545454544,
      "grad_norm": 0.0005083858268335462,
      "learning_rate": 1.5426448540987657e-07,
      "loss": 0.0009,
      "step": 115310
    },
    {
      "epoch": 20967.272727272728,
      "grad_norm": 0.0007411307888105512,
      "learning_rate": 1.5418043922768e-07,
      "loss": 0.0008,
      "step": 115320
    },
    {
      "epoch": 20969.090909090908,
      "grad_norm": 0.001116186729632318,
      "learning_rate": 1.5409641177306764e-07,
      "loss": 0.0012,
      "step": 115330
    },
    {
      "epoch": 20970.909090909092,
      "grad_norm": 0.011577531695365906,
      "learning_rate": 1.5401240305059e-07,
      "loss": 0.0012,
      "step": 115340
    },
    {
      "epoch": 20972.727272727272,
      "grad_norm": 0.18268214166164398,
      "learning_rate": 1.5392841306479666e-07,
      "loss": 0.0011,
      "step": 115350
    },
    {
      "epoch": 20974.545454545456,
      "grad_norm": 0.1885662078857422,
      "learning_rate": 1.5384444182023592e-07,
      "loss": 0.0011,
      "step": 115360
    },
    {
      "epoch": 20976.363636363636,
      "grad_norm": 0.2042078673839569,
      "learning_rate": 1.5376048932145503e-07,
      "loss": 0.0013,
      "step": 115370
    },
    {
      "epoch": 20978.18181818182,
      "grad_norm": 0.19402791559696198,
      "learning_rate": 1.5367655557300064e-07,
      "loss": 0.0009,
      "step": 115380
    },
    {
      "epoch": 20980.0,
      "grad_norm": 0.1980733722448349,
      "learning_rate": 1.535926405794179e-07,
      "loss": 0.001,
      "step": 115390
    },
    {
      "epoch": 20981.81818181818,
      "grad_norm": 0.0005854174378328025,
      "learning_rate": 1.535087443452514e-07,
      "loss": 0.0012,
      "step": 115400
    },
    {
      "epoch": 20983.636363636364,
      "grad_norm": 0.17855995893478394,
      "learning_rate": 1.534248668750443e-07,
      "loss": 0.0012,
      "step": 115410
    },
    {
      "epoch": 20985.454545454544,
      "grad_norm": 0.05021674931049347,
      "learning_rate": 1.5334100817333883e-07,
      "loss": 0.001,
      "step": 115420
    },
    {
      "epoch": 20987.272727272728,
      "grad_norm": 0.20596720278263092,
      "learning_rate": 1.5325716824467654e-07,
      "loss": 0.001,
      "step": 115430
    },
    {
      "epoch": 20989.090909090908,
      "grad_norm": 0.19990889728069305,
      "learning_rate": 1.5317334709359757e-07,
      "loss": 0.001,
      "step": 115440
    },
    {
      "epoch": 20990.909090909092,
      "grad_norm": 0.17922183871269226,
      "learning_rate": 1.530895447246411e-07,
      "loss": 0.0011,
      "step": 115450
    },
    {
      "epoch": 20992.727272727272,
      "grad_norm": 0.2731863856315613,
      "learning_rate": 1.530057611423456e-07,
      "loss": 0.0012,
      "step": 115460
    },
    {
      "epoch": 20994.545454545456,
      "grad_norm": 0.000722104508895427,
      "learning_rate": 1.5292199635124808e-07,
      "loss": 0.0012,
      "step": 115470
    },
    {
      "epoch": 20996.363636363636,
      "grad_norm": 0.0007265218300744891,
      "learning_rate": 1.5283825035588498e-07,
      "loss": 0.0009,
      "step": 115480
    },
    {
      "epoch": 20998.18181818182,
      "grad_norm": 0.045223213732242584,
      "learning_rate": 1.527545231607914e-07,
      "loss": 0.0013,
      "step": 115490
    },
    {
      "epoch": 21000.0,
      "grad_norm": 0.18102946877479553,
      "learning_rate": 1.5267081477050131e-07,
      "loss": 0.0009,
      "step": 115500
    },
    {
      "epoch": 21000.0,
      "eval_loss": 5.179594039916992,
      "eval_runtime": 0.9537,
      "eval_samples_per_second": 10.486,
      "eval_steps_per_second": 5.243,
      "step": 115500
    },
    {
      "epoch": 21001.81818181818,
      "grad_norm": 0.0006336749647744,
      "learning_rate": 1.5258712518954826e-07,
      "loss": 0.001,
      "step": 115510
    },
    {
      "epoch": 21003.636363636364,
      "grad_norm": 0.19869007170200348,
      "learning_rate": 1.5250345442246415e-07,
      "loss": 0.0011,
      "step": 115520
    },
    {
      "epoch": 21005.454545454544,
      "grad_norm": 0.21385014057159424,
      "learning_rate": 1.5241980247378005e-07,
      "loss": 0.0012,
      "step": 115530
    },
    {
      "epoch": 21007.272727272728,
      "grad_norm": 0.0004887845134362578,
      "learning_rate": 1.5233616934802628e-07,
      "loss": 0.0008,
      "step": 115540
    },
    {
      "epoch": 21009.090909090908,
      "grad_norm": 0.001053071697242558,
      "learning_rate": 1.5225255504973183e-07,
      "loss": 0.0012,
      "step": 115550
    },
    {
      "epoch": 21010.909090909092,
      "grad_norm": 0.199032261967659,
      "learning_rate": 1.5216895958342457e-07,
      "loss": 0.0012,
      "step": 115560
    },
    {
      "epoch": 21012.727272727272,
      "grad_norm": 0.2174927294254303,
      "learning_rate": 1.5208538295363195e-07,
      "loss": 0.0011,
      "step": 115570
    },
    {
      "epoch": 21014.545454545456,
      "grad_norm": 0.2660345137119293,
      "learning_rate": 1.5200182516487953e-07,
      "loss": 0.0009,
      "step": 115580
    },
    {
      "epoch": 21016.363636363636,
      "grad_norm": 0.21612492203712463,
      "learning_rate": 1.519182862216929e-07,
      "loss": 0.0012,
      "step": 115590
    },
    {
      "epoch": 21018.18181818182,
      "grad_norm": 0.18926073610782623,
      "learning_rate": 1.5183476612859536e-07,
      "loss": 0.0013,
      "step": 115600
    },
    {
      "epoch": 21020.0,
      "grad_norm": 0.17270271480083466,
      "learning_rate": 1.5175126489011026e-07,
      "loss": 0.0007,
      "step": 115610
    },
    {
      "epoch": 21021.81818181818,
      "grad_norm": 0.2125585377216339,
      "learning_rate": 1.5166778251075963e-07,
      "loss": 0.0012,
      "step": 115620
    },
    {
      "epoch": 21023.636363636364,
      "grad_norm": 0.0006759838433936238,
      "learning_rate": 1.515843189950643e-07,
      "loss": 0.001,
      "step": 115630
    },
    {
      "epoch": 21025.454545454544,
      "grad_norm": 0.19974204897880554,
      "learning_rate": 1.5150087434754394e-07,
      "loss": 0.001,
      "step": 115640
    },
    {
      "epoch": 21027.272727272728,
      "grad_norm": 0.01737287826836109,
      "learning_rate": 1.5141744857271778e-07,
      "loss": 0.0012,
      "step": 115650
    },
    {
      "epoch": 21029.090909090908,
      "grad_norm": 0.017406636849045753,
      "learning_rate": 1.5133404167510348e-07,
      "loss": 0.0012,
      "step": 115660
    },
    {
      "epoch": 21030.909090909092,
      "grad_norm": 0.0005303609650582075,
      "learning_rate": 1.5125065365921778e-07,
      "loss": 0.0007,
      "step": 115670
    },
    {
      "epoch": 21032.727272727272,
      "grad_norm": 0.00046829364146105945,
      "learning_rate": 1.5116728452957682e-07,
      "loss": 0.0014,
      "step": 115680
    },
    {
      "epoch": 21034.545454545456,
      "grad_norm": 0.000542685913387686,
      "learning_rate": 1.51083934290695e-07,
      "loss": 0.001,
      "step": 115690
    },
    {
      "epoch": 21036.363636363636,
      "grad_norm": 0.0005482838023453951,
      "learning_rate": 1.5100060294708644e-07,
      "loss": 0.0011,
      "step": 115700
    },
    {
      "epoch": 21038.18181818182,
      "grad_norm": 0.1712757796049118,
      "learning_rate": 1.5091729050326375e-07,
      "loss": 0.0013,
      "step": 115710
    },
    {
      "epoch": 21040.0,
      "grad_norm": 0.18078261613845825,
      "learning_rate": 1.508339969637384e-07,
      "loss": 0.0011,
      "step": 115720
    },
    {
      "epoch": 21041.81818181818,
      "grad_norm": 0.0006988957175053656,
      "learning_rate": 1.507507223330215e-07,
      "loss": 0.001,
      "step": 115730
    },
    {
      "epoch": 21043.636363636364,
      "grad_norm": 0.0006224128301255405,
      "learning_rate": 1.5066746661562252e-07,
      "loss": 0.0012,
      "step": 115740
    },
    {
      "epoch": 21045.454545454544,
      "grad_norm": 0.15472495555877686,
      "learning_rate": 1.5058422981604997e-07,
      "loss": 0.001,
      "step": 115750
    },
    {
      "epoch": 21047.272727272728,
      "grad_norm": 0.00041536937351338565,
      "learning_rate": 1.5050101193881177e-07,
      "loss": 0.0009,
      "step": 115760
    },
    {
      "epoch": 21049.090909090908,
      "grad_norm": 0.1777852177619934,
      "learning_rate": 1.504178129884142e-07,
      "loss": 0.0012,
      "step": 115770
    },
    {
      "epoch": 21050.909090909092,
      "grad_norm": 0.2115776091814041,
      "learning_rate": 1.5033463296936332e-07,
      "loss": 0.0012,
      "step": 115780
    },
    {
      "epoch": 21052.727272727272,
      "grad_norm": 0.19903844594955444,
      "learning_rate": 1.5025147188616304e-07,
      "loss": 0.0007,
      "step": 115790
    },
    {
      "epoch": 21054.545454545456,
      "grad_norm": 0.0017039533704519272,
      "learning_rate": 1.5016832974331723e-07,
      "loss": 0.0015,
      "step": 115800
    },
    {
      "epoch": 21056.363636363636,
      "grad_norm": 0.21155279874801636,
      "learning_rate": 1.5008520654532854e-07,
      "loss": 0.0009,
      "step": 115810
    },
    {
      "epoch": 21058.18181818182,
      "grad_norm": 0.16339479386806488,
      "learning_rate": 1.500021022966982e-07,
      "loss": 0.0012,
      "step": 115820
    },
    {
      "epoch": 21060.0,
      "grad_norm": 0.19877773523330688,
      "learning_rate": 1.4991901700192654e-07,
      "loss": 0.001,
      "step": 115830
    },
    {
      "epoch": 21061.81818181818,
      "grad_norm": 0.19715601205825806,
      "learning_rate": 1.4983595066551336e-07,
      "loss": 0.0011,
      "step": 115840
    },
    {
      "epoch": 21063.636363636364,
      "grad_norm": 0.0010208789026364684,
      "learning_rate": 1.4975290329195677e-07,
      "loss": 0.0011,
      "step": 115850
    },
    {
      "epoch": 21065.454545454544,
      "grad_norm": 0.0007370131206698716,
      "learning_rate": 1.4966987488575427e-07,
      "loss": 0.0007,
      "step": 115860
    },
    {
      "epoch": 21067.272727272728,
      "grad_norm": 0.21003679931163788,
      "learning_rate": 1.4958686545140198e-07,
      "loss": 0.0015,
      "step": 115870
    },
    {
      "epoch": 21069.090909090908,
      "grad_norm": 0.0005327798426151276,
      "learning_rate": 1.4950387499339535e-07,
      "loss": 0.0009,
      "step": 115880
    },
    {
      "epoch": 21070.909090909092,
      "grad_norm": 0.1760762333869934,
      "learning_rate": 1.4942090351622883e-07,
      "loss": 0.0012,
      "step": 115890
    },
    {
      "epoch": 21072.727272727272,
      "grad_norm": 0.00089539639884606,
      "learning_rate": 1.4933795102439555e-07,
      "loss": 0.0009,
      "step": 115900
    },
    {
      "epoch": 21074.545454545456,
      "grad_norm": 0.0005652804975397885,
      "learning_rate": 1.492550175223875e-07,
      "loss": 0.0013,
      "step": 115910
    },
    {
      "epoch": 21076.363636363636,
      "grad_norm": 0.21462495625019073,
      "learning_rate": 1.491721030146963e-07,
      "loss": 0.0009,
      "step": 115920
    },
    {
      "epoch": 21078.18181818182,
      "grad_norm": 0.20419012010097504,
      "learning_rate": 1.490892075058119e-07,
      "loss": 0.0012,
      "step": 115930
    },
    {
      "epoch": 21080.0,
      "grad_norm": 0.00078321568435058,
      "learning_rate": 1.4900633100022342e-07,
      "loss": 0.001,
      "step": 115940
    },
    {
      "epoch": 21081.81818181818,
      "grad_norm": 0.0027029879856854677,
      "learning_rate": 1.489234735024188e-07,
      "loss": 0.0012,
      "step": 115950
    },
    {
      "epoch": 21083.636363636364,
      "grad_norm": 0.1963985115289688,
      "learning_rate": 1.4884063501688537e-07,
      "loss": 0.0009,
      "step": 115960
    },
    {
      "epoch": 21085.454545454544,
      "grad_norm": 0.0005750508862547576,
      "learning_rate": 1.4875781554810947e-07,
      "loss": 0.0009,
      "step": 115970
    },
    {
      "epoch": 21087.272727272728,
      "grad_norm": 0.0005905741709284484,
      "learning_rate": 1.4867501510057545e-07,
      "loss": 0.0013,
      "step": 115980
    },
    {
      "epoch": 21089.090909090908,
      "grad_norm": 0.011715223081409931,
      "learning_rate": 1.485922336787676e-07,
      "loss": 0.0013,
      "step": 115990
    },
    {
      "epoch": 21090.909090909092,
      "grad_norm": 0.1774330586194992,
      "learning_rate": 1.4850947128716911e-07,
      "loss": 0.0009,
      "step": 116000
    },
    {
      "epoch": 21090.909090909092,
      "eval_loss": 5.152581214904785,
      "eval_runtime": 0.9524,
      "eval_samples_per_second": 10.5,
      "eval_steps_per_second": 5.25,
      "step": 116000
    },
    {
      "epoch": 21092.727272727272,
      "grad_norm": 0.159366637468338,
      "learning_rate": 1.484267279302618e-07,
      "loss": 0.0011,
      "step": 116010
    },
    {
      "epoch": 21094.545454545456,
      "grad_norm": 0.21732085943222046,
      "learning_rate": 1.4834400361252624e-07,
      "loss": 0.0011,
      "step": 116020
    },
    {
      "epoch": 21096.363636363636,
      "grad_norm": 0.0004641418345272541,
      "learning_rate": 1.482612983384427e-07,
      "loss": 0.0011,
      "step": 116030
    },
    {
      "epoch": 21098.18181818182,
      "grad_norm": 0.17933954298496246,
      "learning_rate": 1.4817861211248993e-07,
      "loss": 0.0012,
      "step": 116040
    },
    {
      "epoch": 21100.0,
      "grad_norm": 0.26803842186927795,
      "learning_rate": 1.4809594493914568e-07,
      "loss": 0.001,
      "step": 116050
    },
    {
      "epoch": 21101.81818181818,
      "grad_norm": 0.0011338891927152872,
      "learning_rate": 1.4801329682288656e-07,
      "loss": 0.0011,
      "step": 116060
    },
    {
      "epoch": 21103.636363636364,
      "grad_norm": 0.0007204430876299739,
      "learning_rate": 1.479306677681884e-07,
      "loss": 0.0008,
      "step": 116070
    },
    {
      "epoch": 21105.454545454544,
      "grad_norm": 0.0007228452013805509,
      "learning_rate": 1.4784805777952626e-07,
      "loss": 0.0014,
      "step": 116080
    },
    {
      "epoch": 21107.272727272728,
      "grad_norm": 0.0007751338416710496,
      "learning_rate": 1.4776546686137348e-07,
      "loss": 0.0009,
      "step": 116090
    },
    {
      "epoch": 21109.090909090908,
      "grad_norm": 0.0006073178956285119,
      "learning_rate": 1.4768289501820263e-07,
      "loss": 0.0011,
      "step": 116100
    },
    {
      "epoch": 21110.909090909092,
      "grad_norm": 0.000421158445533365,
      "learning_rate": 1.476003422544856e-07,
      "loss": 0.001,
      "step": 116110
    },
    {
      "epoch": 21112.727272727272,
      "grad_norm": 0.16873285174369812,
      "learning_rate": 1.4751780857469288e-07,
      "loss": 0.0011,
      "step": 116120
    },
    {
      "epoch": 21114.545454545456,
      "grad_norm": 0.2119402140378952,
      "learning_rate": 1.474352939832939e-07,
      "loss": 0.0011,
      "step": 116130
    },
    {
      "epoch": 21116.363636363636,
      "grad_norm": 0.17591078579425812,
      "learning_rate": 1.4735279848475719e-07,
      "loss": 0.0012,
      "step": 116140
    },
    {
      "epoch": 21118.18181818182,
      "grad_norm": 0.2631635069847107,
      "learning_rate": 1.4727032208355022e-07,
      "loss": 0.0011,
      "step": 116150
    },
    {
      "epoch": 21120.0,
      "grad_norm": 0.0006630784482695162,
      "learning_rate": 1.4718786478413982e-07,
      "loss": 0.0009,
      "step": 116160
    },
    {
      "epoch": 21121.81818181818,
      "grad_norm": 0.0009493596735410392,
      "learning_rate": 1.4710542659099075e-07,
      "loss": 0.0012,
      "step": 116170
    },
    {
      "epoch": 21123.636363636364,
      "grad_norm": 0.00044419747428037226,
      "learning_rate": 1.4702300750856772e-07,
      "loss": 0.0009,
      "step": 116180
    },
    {
      "epoch": 21125.454545454544,
      "grad_norm": 0.2785319685935974,
      "learning_rate": 1.469406075413342e-07,
      "loss": 0.0015,
      "step": 116190
    },
    {
      "epoch": 21127.272727272728,
      "grad_norm": 0.20003801584243774,
      "learning_rate": 1.4685822669375236e-07,
      "loss": 0.0007,
      "step": 116200
    },
    {
      "epoch": 21129.090909090908,
      "grad_norm": 0.18663761019706726,
      "learning_rate": 1.467758649702835e-07,
      "loss": 0.001,
      "step": 116210
    },
    {
      "epoch": 21130.909090909092,
      "grad_norm": 0.0008566548931412399,
      "learning_rate": 1.4669352237538763e-07,
      "loss": 0.0012,
      "step": 116220
    },
    {
      "epoch": 21132.727272727272,
      "grad_norm": 0.0005768949631601572,
      "learning_rate": 1.466111989135243e-07,
      "loss": 0.0009,
      "step": 116230
    },
    {
      "epoch": 21134.545454545456,
      "grad_norm": 0.17715412378311157,
      "learning_rate": 1.4652889458915153e-07,
      "loss": 0.0013,
      "step": 116240
    },
    {
      "epoch": 21136.363636363636,
      "grad_norm": 0.17670324444770813,
      "learning_rate": 1.4644660940672627e-07,
      "loss": 0.001,
      "step": 116250
    },
    {
      "epoch": 21138.18181818182,
      "grad_norm": 0.21553905308246613,
      "learning_rate": 1.4636434337070474e-07,
      "loss": 0.0011,
      "step": 116260
    },
    {
      "epoch": 21140.0,
      "grad_norm": 0.0008404285763390362,
      "learning_rate": 1.4628209648554217e-07,
      "loss": 0.0011,
      "step": 116270
    },
    {
      "epoch": 21141.81818181818,
      "grad_norm": 0.001332989428192377,
      "learning_rate": 1.4619986875569246e-07,
      "loss": 0.001,
      "step": 116280
    },
    {
      "epoch": 21143.636363636364,
      "grad_norm": 0.19380012154579163,
      "learning_rate": 1.4611766018560834e-07,
      "loss": 0.0012,
      "step": 116290
    },
    {
      "epoch": 21145.454545454544,
      "grad_norm": 0.20072928071022034,
      "learning_rate": 1.4603547077974214e-07,
      "loss": 0.0012,
      "step": 116300
    },
    {
      "epoch": 21147.272727272728,
      "grad_norm": 0.0009487977949902415,
      "learning_rate": 1.4595330054254457e-07,
      "loss": 0.001,
      "step": 116310
    },
    {
      "epoch": 21149.090909090908,
      "grad_norm": 0.17673619091510773,
      "learning_rate": 1.4587114947846552e-07,
      "loss": 0.0012,
      "step": 116320
    },
    {
      "epoch": 21150.909090909092,
      "grad_norm": 0.0007467520190402865,
      "learning_rate": 1.457890175919536e-07,
      "loss": 0.001,
      "step": 116330
    },
    {
      "epoch": 21152.727272727272,
      "grad_norm": 0.2145891934633255,
      "learning_rate": 1.4570690488745685e-07,
      "loss": 0.0013,
      "step": 116340
    },
    {
      "epoch": 21154.545454545456,
      "grad_norm": 0.21392004191875458,
      "learning_rate": 1.456248113694223e-07,
      "loss": 0.001,
      "step": 116350
    },
    {
      "epoch": 21156.363636363636,
      "grad_norm": 0.17711642384529114,
      "learning_rate": 1.4554273704229492e-07,
      "loss": 0.001,
      "step": 116360
    },
    {
      "epoch": 21158.18181818182,
      "grad_norm": 0.0008191958768293262,
      "learning_rate": 1.4546068191051986e-07,
      "loss": 0.001,
      "step": 116370
    },
    {
      "epoch": 21160.0,
      "grad_norm": 0.20835134387016296,
      "learning_rate": 1.4537864597854077e-07,
      "loss": 0.0011,
      "step": 116380
    },
    {
      "epoch": 21161.81818181818,
      "grad_norm": 0.0007880622870288789,
      "learning_rate": 1.4529662925080022e-07,
      "loss": 0.0009,
      "step": 116390
    },
    {
      "epoch": 21163.636363636364,
      "grad_norm": 0.29425501823425293,
      "learning_rate": 1.4521463173173965e-07,
      "loss": 0.0012,
      "step": 116400
    },
    {
      "epoch": 21165.454545454544,
      "grad_norm": 0.19830897450447083,
      "learning_rate": 1.4513265342579946e-07,
      "loss": 0.0014,
      "step": 116410
    },
    {
      "epoch": 21167.272727272728,
      "grad_norm": 0.0009790804469957948,
      "learning_rate": 1.450506943374194e-07,
      "loss": 0.0007,
      "step": 116420
    },
    {
      "epoch": 21169.090909090908,
      "grad_norm": 0.0003977577725891024,
      "learning_rate": 1.449687544710378e-07,
      "loss": 0.0012,
      "step": 116430
    },
    {
      "epoch": 21170.909090909092,
      "grad_norm": 0.20258480310440063,
      "learning_rate": 1.4488683383109184e-07,
      "loss": 0.0012,
      "step": 116440
    },
    {
      "epoch": 21172.727272727272,
      "grad_norm": 0.0008080413099378347,
      "learning_rate": 1.4480493242201807e-07,
      "loss": 0.001,
      "step": 116450
    },
    {
      "epoch": 21174.545454545456,
      "grad_norm": 0.21531958878040314,
      "learning_rate": 1.4472305024825187e-07,
      "loss": 0.0009,
      "step": 116460
    },
    {
      "epoch": 21176.363636363636,
      "grad_norm": 0.18142420053482056,
      "learning_rate": 1.4464118731422749e-07,
      "loss": 0.0012,
      "step": 116470
    },
    {
      "epoch": 21178.18181818182,
      "grad_norm": 0.0008308362448588014,
      "learning_rate": 1.44559343624378e-07,
      "loss": 0.001,
      "step": 116480
    },
    {
      "epoch": 21180.0,
      "grad_norm": 0.19649410247802734,
      "learning_rate": 1.444775191831355e-07,
      "loss": 0.0012,
      "step": 116490
    },
    {
      "epoch": 21181.81818181818,
      "grad_norm": 0.20032306015491486,
      "learning_rate": 1.4439571399493145e-07,
      "loss": 0.0012,
      "step": 116500
    },
    {
      "epoch": 21181.81818181818,
      "eval_loss": 5.131226539611816,
      "eval_runtime": 0.9501,
      "eval_samples_per_second": 10.525,
      "eval_steps_per_second": 5.263,
      "step": 116500
    },
    {
      "epoch": 21183.636363636364,
      "grad_norm": 0.0006778895622119308,
      "learning_rate": 1.4431392806419573e-07,
      "loss": 0.0008,
      "step": 116510
    },
    {
      "epoch": 21185.454545454544,
      "grad_norm": 0.017406607046723366,
      "learning_rate": 1.4423216139535733e-07,
      "loss": 0.0015,
      "step": 116520
    },
    {
      "epoch": 21187.272727272728,
      "grad_norm": 0.0006479303119704127,
      "learning_rate": 1.4415041399284434e-07,
      "loss": 0.0008,
      "step": 116530
    },
    {
      "epoch": 21189.090909090908,
      "grad_norm": 0.1979442983865738,
      "learning_rate": 1.4406868586108407e-07,
      "loss": 0.0013,
      "step": 116540
    },
    {
      "epoch": 21190.909090909092,
      "grad_norm": 0.16982924938201904,
      "learning_rate": 1.439869770045018e-07,
      "loss": 0.001,
      "step": 116550
    },
    {
      "epoch": 21192.727272727272,
      "grad_norm": 0.20295488834381104,
      "learning_rate": 1.4390528742752273e-07,
      "loss": 0.0011,
      "step": 116560
    },
    {
      "epoch": 21194.545454545456,
      "grad_norm": 0.0007900821510702372,
      "learning_rate": 1.4382361713457082e-07,
      "loss": 0.0009,
      "step": 116570
    },
    {
      "epoch": 21196.363636363636,
      "grad_norm": 0.0005694280262105167,
      "learning_rate": 1.4374196613006874e-07,
      "loss": 0.0014,
      "step": 116580
    },
    {
      "epoch": 21198.18181818182,
      "grad_norm": 0.19768860936164856,
      "learning_rate": 1.4366033441843822e-07,
      "loss": 0.0009,
      "step": 116590
    },
    {
      "epoch": 21200.0,
      "grad_norm": 0.0004022649663966149,
      "learning_rate": 1.4357872200409987e-07,
      "loss": 0.001,
      "step": 116600
    },
    {
      "epoch": 21201.81818181818,
      "grad_norm": 0.00044869453995488584,
      "learning_rate": 1.4349712889147353e-07,
      "loss": 0.0009,
      "step": 116610
    },
    {
      "epoch": 21203.636363636364,
      "grad_norm": 0.20541709661483765,
      "learning_rate": 1.4341555508497778e-07,
      "loss": 0.001,
      "step": 116620
    },
    {
      "epoch": 21205.454545454544,
      "grad_norm": 0.010895650833845139,
      "learning_rate": 1.4333400058902995e-07,
      "loss": 0.0015,
      "step": 116630
    },
    {
      "epoch": 21207.272727272728,
      "grad_norm": 0.26886287331581116,
      "learning_rate": 1.432524654080467e-07,
      "loss": 0.001,
      "step": 116640
    },
    {
      "epoch": 21209.090909090908,
      "grad_norm": 0.0004780488961841911,
      "learning_rate": 1.4317094954644378e-07,
      "loss": 0.0009,
      "step": 116650
    },
    {
      "epoch": 21210.909090909092,
      "grad_norm": 0.17372293770313263,
      "learning_rate": 1.4308945300863528e-07,
      "loss": 0.0012,
      "step": 116660
    },
    {
      "epoch": 21212.727272727272,
      "grad_norm": 0.0005848748842254281,
      "learning_rate": 1.4300797579903473e-07,
      "loss": 0.001,
      "step": 116670
    },
    {
      "epoch": 21214.545454545456,
      "grad_norm": 0.17967256903648376,
      "learning_rate": 1.4292651792205425e-07,
      "loss": 0.0009,
      "step": 116680
    },
    {
      "epoch": 21216.363636363636,
      "grad_norm": 0.005241083912551403,
      "learning_rate": 1.4284507938210543e-07,
      "loss": 0.0015,
      "step": 116690
    },
    {
      "epoch": 21218.18181818182,
      "grad_norm": 0.0003629573911894113,
      "learning_rate": 1.4276366018359842e-07,
      "loss": 0.0008,
      "step": 116700
    },
    {
      "epoch": 21220.0,
      "grad_norm": 0.000987570034340024,
      "learning_rate": 1.4268226033094215e-07,
      "loss": 0.0012,
      "step": 116710
    },
    {
      "epoch": 21221.81818181818,
      "grad_norm": 0.19951683282852173,
      "learning_rate": 1.4260087982854503e-07,
      "loss": 0.0012,
      "step": 116720
    },
    {
      "epoch": 21223.636363636364,
      "grad_norm": 0.17937380075454712,
      "learning_rate": 1.4251951868081436e-07,
      "loss": 0.001,
      "step": 116730
    },
    {
      "epoch": 21225.454545454544,
      "grad_norm": 0.0010461683850735426,
      "learning_rate": 1.4243817689215566e-07,
      "loss": 0.0009,
      "step": 116740
    },
    {
      "epoch": 21227.272727272728,
      "grad_norm": 0.012118330225348473,
      "learning_rate": 1.4235685446697433e-07,
      "loss": 0.0015,
      "step": 116750
    },
    {
      "epoch": 21229.090909090908,
      "grad_norm": 0.40810298919677734,
      "learning_rate": 1.42275551409674e-07,
      "loss": 0.001,
      "step": 116760
    },
    {
      "epoch": 21230.909090909092,
      "grad_norm": 0.0013478670734912157,
      "learning_rate": 1.4219426772465797e-07,
      "loss": 0.0012,
      "step": 116770
    },
    {
      "epoch": 21232.727272727272,
      "grad_norm": 0.21508918702602386,
      "learning_rate": 1.4211300341632792e-07,
      "loss": 0.0011,
      "step": 116780
    },
    {
      "epoch": 21234.545454545456,
      "grad_norm": 0.000511303311213851,
      "learning_rate": 1.4203175848908437e-07,
      "loss": 0.0012,
      "step": 116790
    },
    {
      "epoch": 21236.363636363636,
      "grad_norm": 0.0016260277479887009,
      "learning_rate": 1.4195053294732757e-07,
      "loss": 0.0011,
      "step": 116800
    },
    {
      "epoch": 21238.18181818182,
      "grad_norm": 0.14838756620883942,
      "learning_rate": 1.4186932679545592e-07,
      "loss": 0.0011,
      "step": 116810
    },
    {
      "epoch": 21240.0,
      "grad_norm": 0.0006042876048013568,
      "learning_rate": 1.4178814003786704e-07,
      "loss": 0.0011,
      "step": 116820
    },
    {
      "epoch": 21241.81818181818,
      "grad_norm": 0.20583197474479675,
      "learning_rate": 1.4170697267895775e-07,
      "loss": 0.0012,
      "step": 116830
    },
    {
      "epoch": 21243.636363636364,
      "grad_norm": 0.24780195951461792,
      "learning_rate": 1.4162582472312335e-07,
      "loss": 0.0011,
      "step": 116840
    },
    {
      "epoch": 21245.454545454544,
      "grad_norm": 0.0008050983888097107,
      "learning_rate": 1.4154469617475862e-07,
      "loss": 0.0007,
      "step": 116850
    },
    {
      "epoch": 21247.272727272728,
      "grad_norm": 0.19545887410640717,
      "learning_rate": 1.414635870382569e-07,
      "loss": 0.0015,
      "step": 116860
    },
    {
      "epoch": 21249.090909090908,
      "grad_norm": 0.0005732013378292322,
      "learning_rate": 1.4138249731801039e-07,
      "loss": 0.0008,
      "step": 116870
    },
    {
      "epoch": 21250.909090909092,
      "grad_norm": 0.005962253548204899,
      "learning_rate": 1.4130142701841075e-07,
      "loss": 0.0012,
      "step": 116880
    },
    {
      "epoch": 21252.727272727272,
      "grad_norm": 0.21427302062511444,
      "learning_rate": 1.4122037614384814e-07,
      "loss": 0.0012,
      "step": 116890
    },
    {
      "epoch": 21254.545454545456,
      "grad_norm": 0.005369854625314474,
      "learning_rate": 1.4113934469871164e-07,
      "loss": 0.0009,
      "step": 116900
    },
    {
      "epoch": 21256.363636363636,
      "grad_norm": 0.17657029628753662,
      "learning_rate": 1.4105833268738963e-07,
      "loss": 0.0011,
      "step": 116910
    },
    {
      "epoch": 21258.18181818182,
      "grad_norm": 0.006054926197975874,
      "learning_rate": 1.4097734011426953e-07,
      "loss": 0.0013,
      "step": 116920
    },
    {
      "epoch": 21260.0,
      "grad_norm": 0.20649650692939758,
      "learning_rate": 1.4089636698373686e-07,
      "loss": 0.0011,
      "step": 116930
    },
    {
      "epoch": 21261.81818181818,
      "grad_norm": 0.0010902215726673603,
      "learning_rate": 1.4081541330017704e-07,
      "loss": 0.0012,
      "step": 116940
    },
    {
      "epoch": 21263.636363636364,
      "grad_norm": 0.0007272164220921695,
      "learning_rate": 1.4073447906797374e-07,
      "loss": 0.0008,
      "step": 116950
    },
    {
      "epoch": 21265.454545454544,
      "grad_norm": 0.26187631487846375,
      "learning_rate": 1.4065356429151032e-07,
      "loss": 0.0012,
      "step": 116960
    },
    {
      "epoch": 21267.272727272728,
      "grad_norm": 0.1713956594467163,
      "learning_rate": 1.405726689751684e-07,
      "loss": 0.0011,
      "step": 116970
    },
    {
      "epoch": 21269.090909090908,
      "grad_norm": 0.0006328039453364909,
      "learning_rate": 1.4049179312332866e-07,
      "loss": 0.0009,
      "step": 116980
    },
    {
      "epoch": 21270.909090909092,
      "grad_norm": 0.2676103115081787,
      "learning_rate": 1.4041093674037118e-07,
      "loss": 0.0012,
      "step": 116990
    },
    {
      "epoch": 21272.727272727272,
      "grad_norm": 0.22881631553173065,
      "learning_rate": 1.4033009983067452e-07,
      "loss": 0.0012,
      "step": 117000
    },
    {
      "epoch": 21272.727272727272,
      "eval_loss": 5.1841301918029785,
      "eval_runtime": 0.9493,
      "eval_samples_per_second": 10.534,
      "eval_steps_per_second": 5.267,
      "step": 117000
    },
    {
      "epoch": 21274.545454545456,
      "grad_norm": 0.21681660413742065,
      "learning_rate": 1.402492823986162e-07,
      "loss": 0.0011,
      "step": 117010
    },
    {
      "epoch": 21276.363636363636,
      "grad_norm": 0.0007901261560618877,
      "learning_rate": 1.401684844485731e-07,
      "loss": 0.0011,
      "step": 117020
    },
    {
      "epoch": 21278.18181818182,
      "grad_norm": 0.00047848306712694466,
      "learning_rate": 1.400877059849207e-07,
      "loss": 0.0009,
      "step": 117030
    },
    {
      "epoch": 21280.0,
      "grad_norm": 0.1787574738264084,
      "learning_rate": 1.400069470120332e-07,
      "loss": 0.0012,
      "step": 117040
    },
    {
      "epoch": 21281.81818181818,
      "grad_norm": 0.19086498022079468,
      "learning_rate": 1.3992620753428447e-07,
      "loss": 0.0012,
      "step": 117050
    },
    {
      "epoch": 21283.636363636364,
      "grad_norm": 0.15784169733524323,
      "learning_rate": 1.3984548755604652e-07,
      "loss": 0.001,
      "step": 117060
    },
    {
      "epoch": 21285.454545454544,
      "grad_norm": 0.0006652449956163764,
      "learning_rate": 1.3976478708169098e-07,
      "loss": 0.0009,
      "step": 117070
    },
    {
      "epoch": 21287.272727272728,
      "grad_norm": 0.2129768580198288,
      "learning_rate": 1.3968410611558806e-07,
      "loss": 0.0012,
      "step": 117080
    },
    {
      "epoch": 21289.090909090908,
      "grad_norm": 0.0004448647960089147,
      "learning_rate": 1.3960344466210665e-07,
      "loss": 0.001,
      "step": 117090
    },
    {
      "epoch": 21290.909090909092,
      "grad_norm": 0.00043336788075976074,
      "learning_rate": 1.395228027256154e-07,
      "loss": 0.0012,
      "step": 117100
    },
    {
      "epoch": 21292.727272727272,
      "grad_norm": 0.2060677707195282,
      "learning_rate": 1.394421803104811e-07,
      "loss": 0.0009,
      "step": 117110
    },
    {
      "epoch": 21294.545454545456,
      "grad_norm": 0.27559834718704224,
      "learning_rate": 1.3936157742106974e-07,
      "loss": 0.0015,
      "step": 117120
    },
    {
      "epoch": 21296.363636363636,
      "grad_norm": 0.26744869351387024,
      "learning_rate": 1.392809940617466e-07,
      "loss": 0.0011,
      "step": 117130
    },
    {
      "epoch": 21298.18181818182,
      "grad_norm": 0.00048732024151831865,
      "learning_rate": 1.3920043023687528e-07,
      "loss": 0.0009,
      "step": 117140
    },
    {
      "epoch": 21300.0,
      "grad_norm": 0.21429100632667542,
      "learning_rate": 1.391198859508189e-07,
      "loss": 0.0012,
      "step": 117150
    },
    {
      "epoch": 21301.81818181818,
      "grad_norm": 0.20111516118049622,
      "learning_rate": 1.3903936120793925e-07,
      "loss": 0.001,
      "step": 117160
    },
    {
      "epoch": 21303.636363636364,
      "grad_norm": 0.1722758710384369,
      "learning_rate": 1.3895885601259684e-07,
      "loss": 0.0012,
      "step": 117170
    },
    {
      "epoch": 21305.454545454544,
      "grad_norm": 0.0018107605865225196,
      "learning_rate": 1.3887837036915168e-07,
      "loss": 0.0015,
      "step": 117180
    },
    {
      "epoch": 21307.272727272728,
      "grad_norm": 0.0004398593446239829,
      "learning_rate": 1.3879790428196225e-07,
      "loss": 0.0006,
      "step": 117190
    },
    {
      "epoch": 21309.090909090908,
      "grad_norm": 0.18041180074214935,
      "learning_rate": 1.3871745775538597e-07,
      "loss": 0.0013,
      "step": 117200
    },
    {
      "epoch": 21310.909090909092,
      "grad_norm": 0.0004982692771591246,
      "learning_rate": 1.386370307937797e-07,
      "loss": 0.001,
      "step": 117210
    },
    {
      "epoch": 21312.727272727272,
      "grad_norm": 0.0004792503023054451,
      "learning_rate": 1.385566234014987e-07,
      "loss": 0.0012,
      "step": 117220
    },
    {
      "epoch": 21314.545454545456,
      "grad_norm": 0.1758977472782135,
      "learning_rate": 1.3847623558289727e-07,
      "loss": 0.0009,
      "step": 117230
    },
    {
      "epoch": 21316.363636363636,
      "grad_norm": 0.18421368300914764,
      "learning_rate": 1.3839586734232907e-07,
      "loss": 0.0012,
      "step": 117240
    },
    {
      "epoch": 21318.18181818182,
      "grad_norm": 0.0005740414489991963,
      "learning_rate": 1.3831551868414597e-07,
      "loss": 0.001,
      "step": 117250
    },
    {
      "epoch": 21320.0,
      "grad_norm": 0.0004271254874765873,
      "learning_rate": 1.3823518961269958e-07,
      "loss": 0.0012,
      "step": 117260
    },
    {
      "epoch": 21321.81818181818,
      "grad_norm": 0.0007660567644052207,
      "learning_rate": 1.3815488013233984e-07,
      "loss": 0.0012,
      "step": 117270
    },
    {
      "epoch": 21323.636363636364,
      "grad_norm": 0.21789059042930603,
      "learning_rate": 1.380745902474158e-07,
      "loss": 0.0012,
      "step": 117280
    },
    {
      "epoch": 21325.454545454544,
      "grad_norm": 0.000712706008926034,
      "learning_rate": 1.3799431996227567e-07,
      "loss": 0.0009,
      "step": 117290
    },
    {
      "epoch": 21327.272727272728,
      "grad_norm": 0.18772707879543304,
      "learning_rate": 1.3791406928126635e-07,
      "loss": 0.0011,
      "step": 117300
    },
    {
      "epoch": 21329.090909090908,
      "grad_norm": 0.0014976714737713337,
      "learning_rate": 1.3783383820873357e-07,
      "loss": 0.001,
      "step": 117310
    },
    {
      "epoch": 21330.909090909092,
      "grad_norm": 0.1768125742673874,
      "learning_rate": 1.3775362674902252e-07,
      "loss": 0.0012,
      "step": 117320
    },
    {
      "epoch": 21332.727272727272,
      "grad_norm": 0.2038886696100235,
      "learning_rate": 1.3767343490647664e-07,
      "loss": 0.001,
      "step": 117330
    },
    {
      "epoch": 21334.545454545456,
      "grad_norm": 0.0005132256192155182,
      "learning_rate": 1.3759326268543897e-07,
      "loss": 0.0009,
      "step": 117340
    },
    {
      "epoch": 21336.363636363636,
      "grad_norm": 0.2797410488128662,
      "learning_rate": 1.3751311009025107e-07,
      "loss": 0.0014,
      "step": 117350
    },
    {
      "epoch": 21338.18181818182,
      "grad_norm": 0.01087976060807705,
      "learning_rate": 1.374329771252533e-07,
      "loss": 0.0012,
      "step": 117360
    },
    {
      "epoch": 21340.0,
      "grad_norm": 0.0004107510612811893,
      "learning_rate": 1.3735286379478555e-07,
      "loss": 0.0009,
      "step": 117370
    },
    {
      "epoch": 21341.81818181818,
      "grad_norm": 0.00041216801037080586,
      "learning_rate": 1.3727277010318622e-07,
      "loss": 0.001,
      "step": 117380
    },
    {
      "epoch": 21343.636363636364,
      "grad_norm": 0.0009133440908044577,
      "learning_rate": 1.3719269605479238e-07,
      "loss": 0.0012,
      "step": 117390
    },
    {
      "epoch": 21345.454545454544,
      "grad_norm": 0.0004794323758687824,
      "learning_rate": 1.3711264165394087e-07,
      "loss": 0.0009,
      "step": 117400
    },
    {
      "epoch": 21347.272727272728,
      "grad_norm": 0.2144668698310852,
      "learning_rate": 1.3703260690496675e-07,
      "loss": 0.0013,
      "step": 117410
    },
    {
      "epoch": 21349.090909090908,
      "grad_norm": 0.2145911306142807,
      "learning_rate": 1.3695259181220403e-07,
      "loss": 0.001,
      "step": 117420
    },
    {
      "epoch": 21350.909090909092,
      "grad_norm": 0.012141222134232521,
      "learning_rate": 1.368725963799862e-07,
      "loss": 0.001,
      "step": 117430
    },
    {
      "epoch": 21352.727272727272,
      "grad_norm": 0.0009802517015486956,
      "learning_rate": 1.3679262061264514e-07,
      "loss": 0.0007,
      "step": 117440
    },
    {
      "epoch": 21354.545454545456,
      "grad_norm": 0.27722153067588806,
      "learning_rate": 1.3671266451451207e-07,
      "loss": 0.0016,
      "step": 117450
    },
    {
      "epoch": 21356.363636363636,
      "grad_norm": 0.19808661937713623,
      "learning_rate": 1.3663272808991683e-07,
      "loss": 0.0009,
      "step": 117460
    },
    {
      "epoch": 21358.18181818182,
      "grad_norm": 0.0005251404945738614,
      "learning_rate": 1.3655281134318813e-07,
      "loss": 0.0009,
      "step": 117470
    },
    {
      "epoch": 21360.0,
      "grad_norm": 0.17755234241485596,
      "learning_rate": 1.3647291427865416e-07,
      "loss": 0.0013,
      "step": 117480
    },
    {
      "epoch": 21361.81818181818,
      "grad_norm": 0.0008605052134953439,
      "learning_rate": 1.3639303690064147e-07,
      "loss": 0.0012,
      "step": 117490
    },
    {
      "epoch": 21363.636363636364,
      "grad_norm": 0.28186720609664917,
      "learning_rate": 1.3631317921347562e-07,
      "loss": 0.0011,
      "step": 117500
    },
    {
      "epoch": 21363.636363636364,
      "eval_loss": 5.09755277633667,
      "eval_runtime": 0.9506,
      "eval_samples_per_second": 10.519,
      "eval_steps_per_second": 5.26,
      "step": 117500
    },
    {
      "epoch": 21365.454545454544,
      "grad_norm": 0.0012066676281392574,
      "learning_rate": 1.362333412214816e-07,
      "loss": 0.0009,
      "step": 117510
    },
    {
      "epoch": 21367.272727272728,
      "grad_norm": 0.0008103781729005277,
      "learning_rate": 1.3615352292898265e-07,
      "loss": 0.0013,
      "step": 117520
    },
    {
      "epoch": 21369.090909090908,
      "grad_norm": 0.0005544866435229778,
      "learning_rate": 1.360737243403015e-07,
      "loss": 0.0009,
      "step": 117530
    },
    {
      "epoch": 21370.909090909092,
      "grad_norm": 0.0014349231496453285,
      "learning_rate": 1.359939454597595e-07,
      "loss": 0.0012,
      "step": 117540
    },
    {
      "epoch": 21372.727272727272,
      "grad_norm": 0.20495206117630005,
      "learning_rate": 1.3591418629167682e-07,
      "loss": 0.0012,
      "step": 117550
    },
    {
      "epoch": 21374.545454545456,
      "grad_norm": 0.0007648242171853781,
      "learning_rate": 1.358344468403731e-07,
      "loss": 0.001,
      "step": 117560
    },
    {
      "epoch": 21376.363636363636,
      "grad_norm": 0.2152811735868454,
      "learning_rate": 1.3575472711016633e-07,
      "loss": 0.001,
      "step": 117570
    },
    {
      "epoch": 21378.18181818182,
      "grad_norm": 0.20662330090999603,
      "learning_rate": 1.3567502710537365e-07,
      "loss": 0.001,
      "step": 117580
    },
    {
      "epoch": 21380.0,
      "grad_norm": 0.002130009001120925,
      "learning_rate": 1.3559534683031133e-07,
      "loss": 0.001,
      "step": 117590
    },
    {
      "epoch": 21381.81818181818,
      "grad_norm": 0.20894183218479156,
      "learning_rate": 1.3551568628929432e-07,
      "loss": 0.0011,
      "step": 117600
    },
    {
      "epoch": 21383.636363636364,
      "grad_norm": 0.00035619206028059125,
      "learning_rate": 1.354360454866364e-07,
      "loss": 0.0012,
      "step": 117610
    },
    {
      "epoch": 21385.454545454544,
      "grad_norm": 0.17713415622711182,
      "learning_rate": 1.3535642442665075e-07,
      "loss": 0.001,
      "step": 117620
    },
    {
      "epoch": 21387.272727272728,
      "grad_norm": 0.0005145527538843453,
      "learning_rate": 1.3527682311364884e-07,
      "loss": 0.0009,
      "step": 117630
    },
    {
      "epoch": 21389.090909090908,
      "grad_norm": 0.1732887178659439,
      "learning_rate": 1.351972415519418e-07,
      "loss": 0.0012,
      "step": 117640
    },
    {
      "epoch": 21390.909090909092,
      "grad_norm": 0.0013869836693629622,
      "learning_rate": 1.351176797458391e-07,
      "loss": 0.0012,
      "step": 117650
    },
    {
      "epoch": 21392.727272727272,
      "grad_norm": 0.20849643647670746,
      "learning_rate": 1.3503813769964923e-07,
      "loss": 0.0011,
      "step": 117660
    },
    {
      "epoch": 21394.545454545456,
      "grad_norm": 0.0006269713630899787,
      "learning_rate": 1.3495861541768e-07,
      "loss": 0.0009,
      "step": 117670
    },
    {
      "epoch": 21396.363636363636,
      "grad_norm": 0.0015519955195486546,
      "learning_rate": 1.348791129042377e-07,
      "loss": 0.0012,
      "step": 117680
    },
    {
      "epoch": 21398.18181818182,
      "grad_norm": 0.00040846443152986467,
      "learning_rate": 1.3479963016362765e-07,
      "loss": 0.001,
      "step": 117690
    },
    {
      "epoch": 21400.0,
      "grad_norm": 0.006839384324848652,
      "learning_rate": 1.3472016720015444e-07,
      "loss": 0.0012,
      "step": 117700
    },
    {
      "epoch": 21401.81818181818,
      "grad_norm": 0.1866569072008133,
      "learning_rate": 1.3464072401812105e-07,
      "loss": 0.001,
      "step": 117710
    },
    {
      "epoch": 21403.636363636364,
      "grad_norm": 0.0006485050544142723,
      "learning_rate": 1.3456130062183003e-07,
      "loss": 0.001,
      "step": 117720
    },
    {
      "epoch": 21405.454545454544,
      "grad_norm": 0.0006802613497711718,
      "learning_rate": 1.34481897015582e-07,
      "loss": 0.001,
      "step": 117730
    },
    {
      "epoch": 21407.272727272728,
      "grad_norm": 0.0004625182773452252,
      "learning_rate": 1.344025132036772e-07,
      "loss": 0.0012,
      "step": 117740
    },
    {
      "epoch": 21409.090909090908,
      "grad_norm": 0.17486344277858734,
      "learning_rate": 1.3432314919041477e-07,
      "loss": 0.001,
      "step": 117750
    },
    {
      "epoch": 21410.909090909092,
      "grad_norm": 0.21581512689590454,
      "learning_rate": 1.3424380498009253e-07,
      "loss": 0.0012,
      "step": 117760
    },
    {
      "epoch": 21412.727272727272,
      "grad_norm": 0.21178825199604034,
      "learning_rate": 1.3416448057700703e-07,
      "loss": 0.0011,
      "step": 117770
    },
    {
      "epoch": 21414.545454545456,
      "grad_norm": 0.005860207136720419,
      "learning_rate": 1.3408517598545444e-07,
      "loss": 0.001,
      "step": 117780
    },
    {
      "epoch": 21416.363636363636,
      "grad_norm": 0.28118449449539185,
      "learning_rate": 1.340058912097292e-07,
      "loss": 0.0012,
      "step": 117790
    },
    {
      "epoch": 21418.18181818182,
      "grad_norm": 0.00043570087291300297,
      "learning_rate": 1.3392662625412486e-07,
      "loss": 0.0009,
      "step": 117800
    },
    {
      "epoch": 21420.0,
      "grad_norm": 0.0012063960311934352,
      "learning_rate": 1.3384738112293415e-07,
      "loss": 0.0012,
      "step": 117810
    },
    {
      "epoch": 21421.81818181818,
      "grad_norm": 0.18491801619529724,
      "learning_rate": 1.337681558204483e-07,
      "loss": 0.001,
      "step": 117820
    },
    {
      "epoch": 21423.636363636364,
      "grad_norm": 0.2006746232509613,
      "learning_rate": 1.3368895035095794e-07,
      "loss": 0.0012,
      "step": 117830
    },
    {
      "epoch": 21425.454545454544,
      "grad_norm": 0.2022414654493332,
      "learning_rate": 1.3360976471875225e-07,
      "loss": 0.0009,
      "step": 117840
    },
    {
      "epoch": 21427.272727272728,
      "grad_norm": 0.19951431453227997,
      "learning_rate": 1.335305989281193e-07,
      "loss": 0.0011,
      "step": 117850
    },
    {
      "epoch": 21429.090909090908,
      "grad_norm": 0.0014633185928687453,
      "learning_rate": 1.334514529833466e-07,
      "loss": 0.001,
      "step": 117860
    },
    {
      "epoch": 21430.909090909092,
      "grad_norm": 0.0006867483607493341,
      "learning_rate": 1.3337232688872007e-07,
      "loss": 0.0012,
      "step": 117870
    },
    {
      "epoch": 21432.727272727272,
      "grad_norm": 0.0005035650683566928,
      "learning_rate": 1.332932206485245e-07,
      "loss": 0.001,
      "step": 117880
    },
    {
      "epoch": 21434.545454545456,
      "grad_norm": 0.0009149200050160289,
      "learning_rate": 1.3321413426704426e-07,
      "loss": 0.0011,
      "step": 117890
    },
    {
      "epoch": 21436.363636363636,
      "grad_norm": 0.21677960455417633,
      "learning_rate": 1.3313506774856175e-07,
      "loss": 0.0012,
      "step": 117900
    },
    {
      "epoch": 21438.18181818182,
      "grad_norm": 0.0004539492365438491,
      "learning_rate": 1.330560210973593e-07,
      "loss": 0.0008,
      "step": 117910
    },
    {
      "epoch": 21440.0,
      "grad_norm": 0.001991256605833769,
      "learning_rate": 1.3297699431771702e-07,
      "loss": 0.0011,
      "step": 117920
    },
    {
      "epoch": 21441.81818181818,
      "grad_norm": 0.0010222624987363815,
      "learning_rate": 1.3289798741391483e-07,
      "loss": 0.0012,
      "step": 117930
    },
    {
      "epoch": 21443.636363636364,
      "grad_norm": 0.0008588616037741303,
      "learning_rate": 1.328190003902314e-07,
      "loss": 0.0007,
      "step": 117940
    },
    {
      "epoch": 21445.454545454544,
      "grad_norm": 0.0009041553130373359,
      "learning_rate": 1.3274003325094417e-07,
      "loss": 0.0013,
      "step": 117950
    },
    {
      "epoch": 21447.272727272728,
      "grad_norm": 0.18220728635787964,
      "learning_rate": 1.3266108600032928e-07,
      "loss": 0.0012,
      "step": 117960
    },
    {
      "epoch": 21449.090909090908,
      "grad_norm": 0.0006641207146458328,
      "learning_rate": 1.3258215864266238e-07,
      "loss": 0.001,
      "step": 117970
    },
    {
      "epoch": 21450.909090909092,
      "grad_norm": 0.0008104065200313926,
      "learning_rate": 1.3250325118221756e-07,
      "loss": 0.0012,
      "step": 117980
    },
    {
      "epoch": 21452.727272727272,
      "grad_norm": 0.0013450378319248557,
      "learning_rate": 1.3242436362326803e-07,
      "loss": 0.0011,
      "step": 117990
    },
    {
      "epoch": 21454.545454545456,
      "grad_norm": 0.19544048607349396,
      "learning_rate": 1.3234549597008572e-07,
      "loss": 0.0014,
      "step": 118000
    },
    {
      "epoch": 21454.545454545456,
      "eval_loss": 5.233287811279297,
      "eval_runtime": 0.9471,
      "eval_samples_per_second": 10.559,
      "eval_steps_per_second": 5.279,
      "step": 118000
    },
    {
      "epoch": 21456.363636363636,
      "grad_norm": 0.16525888442993164,
      "learning_rate": 1.3226664822694177e-07,
      "loss": 0.0013,
      "step": 118010
    },
    {
      "epoch": 21458.18181818182,
      "grad_norm": 0.0010915650054812431,
      "learning_rate": 1.3218782039810633e-07,
      "loss": 0.0007,
      "step": 118020
    },
    {
      "epoch": 21460.0,
      "grad_norm": 0.00095404963940382,
      "learning_rate": 1.3210901248784802e-07,
      "loss": 0.0012,
      "step": 118030
    },
    {
      "epoch": 21461.81818181818,
      "grad_norm": 0.0014125533634796739,
      "learning_rate": 1.3203022450043455e-07,
      "loss": 0.0009,
      "step": 118040
    },
    {
      "epoch": 21463.636363636364,
      "grad_norm": 0.00068087741965428,
      "learning_rate": 1.3195145644013283e-07,
      "loss": 0.001,
      "step": 118050
    },
    {
      "epoch": 21465.454545454544,
      "grad_norm": 0.0005881183315068483,
      "learning_rate": 1.3187270831120846e-07,
      "loss": 0.0012,
      "step": 118060
    },
    {
      "epoch": 21467.272727272728,
      "grad_norm": 0.0008161348523572087,
      "learning_rate": 1.3179398011792586e-07,
      "loss": 0.0011,
      "step": 118070
    },
    {
      "epoch": 21469.090909090908,
      "grad_norm": 0.27561405301094055,
      "learning_rate": 1.3171527186454839e-07,
      "loss": 0.0011,
      "step": 118080
    },
    {
      "epoch": 21470.909090909092,
      "grad_norm": 0.18464316427707672,
      "learning_rate": 1.3163658355533862e-07,
      "loss": 0.001,
      "step": 118090
    },
    {
      "epoch": 21472.727272727272,
      "grad_norm": 0.0006442542653530836,
      "learning_rate": 1.3155791519455812e-07,
      "loss": 0.0012,
      "step": 118100
    },
    {
      "epoch": 21474.545454545456,
      "grad_norm": 0.04313923045992851,
      "learning_rate": 1.3147926678646649e-07,
      "loss": 0.0012,
      "step": 118110
    },
    {
      "epoch": 21476.363636363636,
      "grad_norm": 0.19908635318279266,
      "learning_rate": 1.3140063833532323e-07,
      "loss": 0.001,
      "step": 118120
    },
    {
      "epoch": 21478.18181818182,
      "grad_norm": 0.2035558819770813,
      "learning_rate": 1.313220298453865e-07,
      "loss": 0.001,
      "step": 118130
    },
    {
      "epoch": 21480.0,
      "grad_norm": 0.29777008295059204,
      "learning_rate": 1.312434413209131e-07,
      "loss": 0.0011,
      "step": 118140
    },
    {
      "epoch": 21481.81818181818,
      "grad_norm": 0.19859091937541962,
      "learning_rate": 1.3116487276615883e-07,
      "loss": 0.0012,
      "step": 118150
    },
    {
      "epoch": 21483.636363636364,
      "grad_norm": 0.21517382562160492,
      "learning_rate": 1.3108632418537885e-07,
      "loss": 0.0011,
      "step": 118160
    },
    {
      "epoch": 21485.454545454544,
      "grad_norm": 0.00206604297272861,
      "learning_rate": 1.310077955828267e-07,
      "loss": 0.001,
      "step": 118170
    },
    {
      "epoch": 21487.272727272728,
      "grad_norm": 0.20125076174736023,
      "learning_rate": 1.30929286962755e-07,
      "loss": 0.0012,
      "step": 118180
    },
    {
      "epoch": 21489.090909090908,
      "grad_norm": 0.2775135040283203,
      "learning_rate": 1.3085079832941526e-07,
      "loss": 0.0012,
      "step": 118190
    },
    {
      "epoch": 21490.909090909092,
      "grad_norm": 0.0007528100977651775,
      "learning_rate": 1.3077232968705805e-07,
      "loss": 0.0009,
      "step": 118200
    },
    {
      "epoch": 21492.727272727272,
      "grad_norm": 0.19262883067131042,
      "learning_rate": 1.3069388103993306e-07,
      "loss": 0.001,
      "step": 118210
    },
    {
      "epoch": 21494.545454545456,
      "grad_norm": 0.17580263316631317,
      "learning_rate": 1.3061545239228804e-07,
      "loss": 0.001,
      "step": 118220
    },
    {
      "epoch": 21496.363636363636,
      "grad_norm": 0.18572495877742767,
      "learning_rate": 1.305370437483706e-07,
      "loss": 0.001,
      "step": 118230
    },
    {
      "epoch": 21498.18181818182,
      "grad_norm": 0.0005540886195376515,
      "learning_rate": 1.3045865511242693e-07,
      "loss": 0.001,
      "step": 118240
    },
    {
      "epoch": 21500.0,
      "grad_norm": 0.0006703703547827899,
      "learning_rate": 1.3038028648870202e-07,
      "loss": 0.0012,
      "step": 118250
    },
    {
      "epoch": 21501.81818181818,
      "grad_norm": 0.29534101486206055,
      "learning_rate": 1.3030193788143988e-07,
      "loss": 0.0011,
      "step": 118260
    },
    {
      "epoch": 21503.636363636364,
      "grad_norm": 0.2862103581428528,
      "learning_rate": 1.3022360929488324e-07,
      "loss": 0.0014,
      "step": 118270
    },
    {
      "epoch": 21505.454545454544,
      "grad_norm": 0.000622329767793417,
      "learning_rate": 1.3014530073327402e-07,
      "loss": 0.0008,
      "step": 118280
    },
    {
      "epoch": 21507.272727272728,
      "grad_norm": 0.0005076396046206355,
      "learning_rate": 1.3006701220085336e-07,
      "loss": 0.001,
      "step": 118290
    },
    {
      "epoch": 21509.090909090908,
      "grad_norm": 0.28886228799819946,
      "learning_rate": 1.2998874370186024e-07,
      "loss": 0.0012,
      "step": 118300
    },
    {
      "epoch": 21510.909090909092,
      "grad_norm": 0.20168821513652802,
      "learning_rate": 1.2991049524053356e-07,
      "loss": 0.0012,
      "step": 118310
    },
    {
      "epoch": 21512.727272727272,
      "grad_norm": 0.2005499005317688,
      "learning_rate": 1.298322668211109e-07,
      "loss": 0.0012,
      "step": 118320
    },
    {
      "epoch": 21514.545454545456,
      "grad_norm": 0.0006306097493506968,
      "learning_rate": 1.297540584478286e-07,
      "loss": 0.0007,
      "step": 118330
    },
    {
      "epoch": 21516.363636363636,
      "grad_norm": 0.21222667396068573,
      "learning_rate": 1.2967587012492194e-07,
      "loss": 0.0014,
      "step": 118340
    },
    {
      "epoch": 21518.18181818182,
      "grad_norm": 0.0013222454581409693,
      "learning_rate": 1.29597701856625e-07,
      "loss": 0.0009,
      "step": 118350
    },
    {
      "epoch": 21520.0,
      "grad_norm": 0.21633201837539673,
      "learning_rate": 1.2951955364717116e-07,
      "loss": 0.0012,
      "step": 118360
    },
    {
      "epoch": 21521.81818181818,
      "grad_norm": 0.1957380771636963,
      "learning_rate": 1.2944142550079236e-07,
      "loss": 0.0012,
      "step": 118370
    },
    {
      "epoch": 21523.636363636364,
      "grad_norm": 0.21640074253082275,
      "learning_rate": 1.2936331742171942e-07,
      "loss": 0.0009,
      "step": 118380
    },
    {
      "epoch": 21525.454545454544,
      "grad_norm": 0.19204603135585785,
      "learning_rate": 1.292852294141824e-07,
      "loss": 0.0014,
      "step": 118390
    },
    {
      "epoch": 21527.272727272728,
      "grad_norm": 0.18395043909549713,
      "learning_rate": 1.2920716148241033e-07,
      "loss": 0.0009,
      "step": 118400
    },
    {
      "epoch": 21529.090909090908,
      "grad_norm": 0.20249097049236298,
      "learning_rate": 1.2912911363063046e-07,
      "loss": 0.001,
      "step": 118410
    },
    {
      "epoch": 21530.909090909092,
      "grad_norm": 0.012033957988023758,
      "learning_rate": 1.290510858630695e-07,
      "loss": 0.0012,
      "step": 118420
    },
    {
      "epoch": 21532.727272727272,
      "grad_norm": 0.001048857462592423,
      "learning_rate": 1.289730781839533e-07,
      "loss": 0.0012,
      "step": 118430
    },
    {
      "epoch": 21534.545454545456,
      "grad_norm": 0.18370476365089417,
      "learning_rate": 1.2889509059750604e-07,
      "loss": 0.0011,
      "step": 118440
    },
    {
      "epoch": 21536.363636363636,
      "grad_norm": 0.20167508721351624,
      "learning_rate": 1.2881712310795118e-07,
      "loss": 0.0012,
      "step": 118450
    },
    {
      "epoch": 21538.18181818182,
      "grad_norm": 0.0009658835479058325,
      "learning_rate": 1.2873917571951076e-07,
      "loss": 0.0009,
      "step": 118460
    },
    {
      "epoch": 21540.0,
      "grad_norm": 0.20508331060409546,
      "learning_rate": 1.2866124843640613e-07,
      "loss": 0.0012,
      "step": 118470
    },
    {
      "epoch": 21541.81818181818,
      "grad_norm": 0.1766284704208374,
      "learning_rate": 1.285833412628577e-07,
      "loss": 0.0012,
      "step": 118480
    },
    {
      "epoch": 21543.636363636364,
      "grad_norm": 0.1975783258676529,
      "learning_rate": 1.2850545420308383e-07,
      "loss": 0.0011,
      "step": 118490
    },
    {
      "epoch": 21545.454545454544,
      "grad_norm": 0.1992296576499939,
      "learning_rate": 1.284275872613028e-07,
      "loss": 0.0012,
      "step": 118500
    },
    {
      "epoch": 21545.454545454544,
      "eval_loss": 5.210745811462402,
      "eval_runtime": 0.9511,
      "eval_samples_per_second": 10.514,
      "eval_steps_per_second": 5.257,
      "step": 118500
    },
    {
      "epoch": 21547.272727272728,
      "grad_norm": 0.0003989829565398395,
      "learning_rate": 1.283497404417315e-07,
      "loss": 0.0009,
      "step": 118510
    },
    {
      "epoch": 21549.090909090908,
      "grad_norm": 0.21655087172985077,
      "learning_rate": 1.282719137485856e-07,
      "loss": 0.001,
      "step": 118520
    },
    {
      "epoch": 21550.909090909092,
      "grad_norm": 0.16918547451496124,
      "learning_rate": 1.281941071860797e-07,
      "loss": 0.0012,
      "step": 118530
    },
    {
      "epoch": 21552.727272727272,
      "grad_norm": 0.21305973827838898,
      "learning_rate": 1.2811632075842716e-07,
      "loss": 0.001,
      "step": 118540
    },
    {
      "epoch": 21554.545454545456,
      "grad_norm": 0.0005768347764387727,
      "learning_rate": 1.280385544698408e-07,
      "loss": 0.0009,
      "step": 118550
    },
    {
      "epoch": 21556.363636363636,
      "grad_norm": 0.18812797963619232,
      "learning_rate": 1.2796080832453181e-07,
      "loss": 0.0012,
      "step": 118560
    },
    {
      "epoch": 21558.18181818182,
      "grad_norm": 0.2020455300807953,
      "learning_rate": 1.2788308232671036e-07,
      "loss": 0.0012,
      "step": 118570
    },
    {
      "epoch": 21560.0,
      "grad_norm": 0.1979110985994339,
      "learning_rate": 1.2780537648058575e-07,
      "loss": 0.001,
      "step": 118580
    },
    {
      "epoch": 21561.81818181818,
      "grad_norm": 0.21693705022335052,
      "learning_rate": 1.277276907903664e-07,
      "loss": 0.0012,
      "step": 118590
    },
    {
      "epoch": 21563.636363636364,
      "grad_norm": 0.17033392190933228,
      "learning_rate": 1.276500252602587e-07,
      "loss": 0.0012,
      "step": 118600
    },
    {
      "epoch": 21565.454545454544,
      "grad_norm": 0.0005988084012642503,
      "learning_rate": 1.2757237989446907e-07,
      "loss": 0.0009,
      "step": 118610
    },
    {
      "epoch": 21567.272727272728,
      "grad_norm": 0.2103777974843979,
      "learning_rate": 1.2749475469720195e-07,
      "loss": 0.0011,
      "step": 118620
    },
    {
      "epoch": 21569.090909090908,
      "grad_norm": 0.000611528696026653,
      "learning_rate": 1.2741714967266136e-07,
      "loss": 0.001,
      "step": 118630
    },
    {
      "epoch": 21570.909090909092,
      "grad_norm": 0.2197045087814331,
      "learning_rate": 1.2733956482504993e-07,
      "loss": 0.0012,
      "step": 118640
    },
    {
      "epoch": 21572.727272727272,
      "grad_norm": 0.2843979299068451,
      "learning_rate": 1.2726200015856893e-07,
      "loss": 0.001,
      "step": 118650
    },
    {
      "epoch": 21574.545454545456,
      "grad_norm": 0.0004415461444295943,
      "learning_rate": 1.2718445567741899e-07,
      "loss": 0.0013,
      "step": 118660
    },
    {
      "epoch": 21576.363636363636,
      "grad_norm": 0.21652530133724213,
      "learning_rate": 1.2710693138579975e-07,
      "loss": 0.0014,
      "step": 118670
    },
    {
      "epoch": 21578.18181818182,
      "grad_norm": 0.2689533233642578,
      "learning_rate": 1.2702942728790894e-07,
      "loss": 0.0009,
      "step": 118680
    },
    {
      "epoch": 21580.0,
      "grad_norm": 0.0006523379706777632,
      "learning_rate": 1.2695194338794412e-07,
      "loss": 0.0009,
      "step": 118690
    },
    {
      "epoch": 21581.81818181818,
      "grad_norm": 0.00058660440845415,
      "learning_rate": 1.2687447969010113e-07,
      "loss": 0.0012,
      "step": 118700
    },
    {
      "epoch": 21583.636363636364,
      "grad_norm": 0.20318210124969482,
      "learning_rate": 1.2679703619857525e-07,
      "loss": 0.001,
      "step": 118710
    },
    {
      "epoch": 21585.454545454544,
      "grad_norm": 0.0007181091350503266,
      "learning_rate": 1.2671961291756012e-07,
      "loss": 0.0009,
      "step": 118720
    },
    {
      "epoch": 21587.272727272728,
      "grad_norm": 0.27104729413986206,
      "learning_rate": 1.2664220985124852e-07,
      "loss": 0.0013,
      "step": 118730
    },
    {
      "epoch": 21589.090909090908,
      "grad_norm": 0.18510860204696655,
      "learning_rate": 1.2656482700383235e-07,
      "loss": 0.0008,
      "step": 118740
    },
    {
      "epoch": 21590.909090909092,
      "grad_norm": 0.0013579856604337692,
      "learning_rate": 1.2648746437950208e-07,
      "loss": 0.001,
      "step": 118750
    },
    {
      "epoch": 21592.727272727272,
      "grad_norm": 0.0005381134687922895,
      "learning_rate": 1.2641012198244717e-07,
      "loss": 0.0009,
      "step": 118760
    },
    {
      "epoch": 21594.545454545456,
      "grad_norm": 0.15588393807411194,
      "learning_rate": 1.2633279981685608e-07,
      "loss": 0.0014,
      "step": 118770
    },
    {
      "epoch": 21596.363636363636,
      "grad_norm": 0.0010653596837073565,
      "learning_rate": 1.2625549788691646e-07,
      "loss": 0.0009,
      "step": 118780
    },
    {
      "epoch": 21598.18181818182,
      "grad_norm": 0.000517246953677386,
      "learning_rate": 1.2617821619681395e-07,
      "loss": 0.001,
      "step": 118790
    },
    {
      "epoch": 21600.0,
      "grad_norm": 0.0006713402108289301,
      "learning_rate": 1.2610095475073413e-07,
      "loss": 0.0012,
      "step": 118800
    },
    {
      "epoch": 21601.81818181818,
      "grad_norm": 0.006429845467209816,
      "learning_rate": 1.2602371355286074e-07,
      "loss": 0.0012,
      "step": 118810
    },
    {
      "epoch": 21603.636363636364,
      "grad_norm": 0.000830949516966939,
      "learning_rate": 1.2594649260737693e-07,
      "loss": 0.001,
      "step": 118820
    },
    {
      "epoch": 21605.454545454544,
      "grad_norm": 0.18986213207244873,
      "learning_rate": 1.258692919184645e-07,
      "loss": 0.0012,
      "step": 118830
    },
    {
      "epoch": 21607.272727272728,
      "grad_norm": 0.2889115810394287,
      "learning_rate": 1.25792111490304e-07,
      "loss": 0.0011,
      "step": 118840
    },
    {
      "epoch": 21609.090909090908,
      "grad_norm": 0.011903165839612484,
      "learning_rate": 1.2571495132707539e-07,
      "loss": 0.0012,
      "step": 118850
    },
    {
      "epoch": 21610.909090909092,
      "grad_norm": 0.18090946972370148,
      "learning_rate": 1.2563781143295703e-07,
      "loss": 0.0012,
      "step": 118860
    },
    {
      "epoch": 21612.727272727272,
      "grad_norm": 0.22254614531993866,
      "learning_rate": 1.2556069181212626e-07,
      "loss": 0.0011,
      "step": 118870
    },
    {
      "epoch": 21614.545454545456,
      "grad_norm": 0.000939356570597738,
      "learning_rate": 1.2548359246875967e-07,
      "loss": 0.001,
      "step": 118880
    },
    {
      "epoch": 21616.363636363636,
      "grad_norm": 0.17788277566432953,
      "learning_rate": 1.254065134070323e-07,
      "loss": 0.001,
      "step": 118890
    },
    {
      "epoch": 21618.18181818182,
      "grad_norm": 0.17749850451946259,
      "learning_rate": 1.2532945463111856e-07,
      "loss": 0.0012,
      "step": 118900
    },
    {
      "epoch": 21620.0,
      "grad_norm": 0.21691489219665527,
      "learning_rate": 1.2525241614519134e-07,
      "loss": 0.001,
      "step": 118910
    },
    {
      "epoch": 21621.81818181818,
      "grad_norm": 0.0008185595506802201,
      "learning_rate": 1.2517539795342247e-07,
      "loss": 0.0012,
      "step": 118920
    },
    {
      "epoch": 21623.636363636364,
      "grad_norm": 0.00044376408914104104,
      "learning_rate": 1.2509840005998306e-07,
      "loss": 0.0009,
      "step": 118930
    },
    {
      "epoch": 21625.454545454544,
      "grad_norm": 0.0005989490309730172,
      "learning_rate": 1.2502142246904284e-07,
      "loss": 0.0012,
      "step": 118940
    },
    {
      "epoch": 21627.272727272728,
      "grad_norm": 0.20154322683811188,
      "learning_rate": 1.249444651847702e-07,
      "loss": 0.001,
      "step": 118950
    },
    {
      "epoch": 21629.090909090908,
      "grad_norm": 0.0007095957989804447,
      "learning_rate": 1.2486752821133312e-07,
      "loss": 0.001,
      "step": 118960
    },
    {
      "epoch": 21630.909090909092,
      "grad_norm": 0.2085743099451065,
      "learning_rate": 1.2479061155289778e-07,
      "loss": 0.001,
      "step": 118970
    },
    {
      "epoch": 21632.727272727272,
      "grad_norm": 0.2222054898738861,
      "learning_rate": 1.2471371521362945e-07,
      "loss": 0.0013,
      "step": 118980
    },
    {
      "epoch": 21634.545454545456,
      "grad_norm": 0.0004846867232117802,
      "learning_rate": 1.2463683919769268e-07,
      "loss": 0.0007,
      "step": 118990
    },
    {
      "epoch": 21636.363636363636,
      "grad_norm": 0.22107180953025818,
      "learning_rate": 1.245599835092504e-07,
      "loss": 0.0016,
      "step": 119000
    },
    {
      "epoch": 21636.363636363636,
      "eval_loss": 5.092164039611816,
      "eval_runtime": 0.9494,
      "eval_samples_per_second": 10.533,
      "eval_steps_per_second": 5.267,
      "step": 119000
    },
    {
      "epoch": 21638.18181818182,
      "grad_norm": 0.206999272108078,
      "learning_rate": 1.2448314815246484e-07,
      "loss": 0.0009,
      "step": 119010
    },
    {
      "epoch": 21640.0,
      "grad_norm": 0.0008193662506528199,
      "learning_rate": 1.244063331314969e-07,
      "loss": 0.001,
      "step": 119020
    },
    {
      "epoch": 21641.81818181818,
      "grad_norm": 0.0005229967646300793,
      "learning_rate": 1.2432953845050625e-07,
      "loss": 0.0012,
      "step": 119030
    },
    {
      "epoch": 21643.636363636364,
      "grad_norm": 0.21674926578998566,
      "learning_rate": 1.2425276411365198e-07,
      "loss": 0.0012,
      "step": 119040
    },
    {
      "epoch": 21645.454545454544,
      "grad_norm": 0.0012808111496269703,
      "learning_rate": 1.241760101250916e-07,
      "loss": 0.001,
      "step": 119050
    },
    {
      "epoch": 21647.272727272728,
      "grad_norm": 0.2080969661474228,
      "learning_rate": 1.2409927648898145e-07,
      "loss": 0.0011,
      "step": 119060
    },
    {
      "epoch": 21649.090909090908,
      "grad_norm": 0.0013385182246565819,
      "learning_rate": 1.2402256320947728e-07,
      "loss": 0.001,
      "step": 119070
    },
    {
      "epoch": 21650.909090909092,
      "grad_norm": 0.0005090025952085853,
      "learning_rate": 1.2394587029073323e-07,
      "loss": 0.0011,
      "step": 119080
    },
    {
      "epoch": 21652.727272727272,
      "grad_norm": 0.2182055413722992,
      "learning_rate": 1.2386919773690274e-07,
      "loss": 0.001,
      "step": 119090
    },
    {
      "epoch": 21654.545454545456,
      "grad_norm": 0.19691036641597748,
      "learning_rate": 1.2379254555213786e-07,
      "loss": 0.0014,
      "step": 119100
    },
    {
      "epoch": 21656.363636363636,
      "grad_norm": 0.22085703909397125,
      "learning_rate": 1.237159137405895e-07,
      "loss": 0.0009,
      "step": 119110
    },
    {
      "epoch": 21658.18181818182,
      "grad_norm": 0.20287799835205078,
      "learning_rate": 1.2363930230640779e-07,
      "loss": 0.0012,
      "step": 119120
    },
    {
      "epoch": 21660.0,
      "grad_norm": 0.2801071107387543,
      "learning_rate": 1.2356271125374152e-07,
      "loss": 0.001,
      "step": 119130
    },
    {
      "epoch": 21661.81818181818,
      "grad_norm": 0.2011338770389557,
      "learning_rate": 1.2348614058673822e-07,
      "loss": 0.0011,
      "step": 119140
    },
    {
      "epoch": 21663.636363636364,
      "grad_norm": 0.16768421232700348,
      "learning_rate": 1.2340959030954484e-07,
      "loss": 0.0012,
      "step": 119150
    },
    {
      "epoch": 21665.454545454544,
      "grad_norm": 0.20699891448020935,
      "learning_rate": 1.233330604263067e-07,
      "loss": 0.0009,
      "step": 119160
    },
    {
      "epoch": 21667.272727272728,
      "grad_norm": 0.18078787624835968,
      "learning_rate": 1.232565509411681e-07,
      "loss": 0.0013,
      "step": 119170
    },
    {
      "epoch": 21669.090909090908,
      "grad_norm": 0.1808527410030365,
      "learning_rate": 1.2318006185827268e-07,
      "loss": 0.0011,
      "step": 119180
    },
    {
      "epoch": 21670.909090909092,
      "grad_norm": 0.20671714842319489,
      "learning_rate": 1.2310359318176228e-07,
      "loss": 0.0012,
      "step": 119190
    },
    {
      "epoch": 21672.727272727272,
      "grad_norm": 0.0004699096898548305,
      "learning_rate": 1.2302714491577833e-07,
      "loss": 0.0011,
      "step": 119200
    },
    {
      "epoch": 21674.545454545456,
      "grad_norm": 0.20288163423538208,
      "learning_rate": 1.229507170644607e-07,
      "loss": 0.001,
      "step": 119210
    },
    {
      "epoch": 21676.363636363636,
      "grad_norm": 0.0006558184395544231,
      "learning_rate": 1.2287430963194807e-07,
      "loss": 0.0011,
      "step": 119220
    },
    {
      "epoch": 21678.18181818182,
      "grad_norm": 0.20742550492286682,
      "learning_rate": 1.227979226223786e-07,
      "loss": 0.001,
      "step": 119230
    },
    {
      "epoch": 21680.0,
      "grad_norm": 0.28307685256004333,
      "learning_rate": 1.2272155603988875e-07,
      "loss": 0.001,
      "step": 119240
    },
    {
      "epoch": 21681.81818181818,
      "grad_norm": 0.00042486030724830925,
      "learning_rate": 1.22645209888614e-07,
      "loss": 0.0012,
      "step": 119250
    },
    {
      "epoch": 21683.636363636364,
      "grad_norm": 0.20831207931041718,
      "learning_rate": 1.2256888417268907e-07,
      "loss": 0.0011,
      "step": 119260
    },
    {
      "epoch": 21685.454545454544,
      "grad_norm": 0.0006406784523278475,
      "learning_rate": 1.2249257889624703e-07,
      "loss": 0.0008,
      "step": 119270
    },
    {
      "epoch": 21687.272727272728,
      "grad_norm": 0.0006106838118284941,
      "learning_rate": 1.2241629406342046e-07,
      "loss": 0.0012,
      "step": 119280
    },
    {
      "epoch": 21689.090909090908,
      "grad_norm": 0.001012887223623693,
      "learning_rate": 1.2234002967834035e-07,
      "loss": 0.0011,
      "step": 119290
    },
    {
      "epoch": 21690.909090909092,
      "grad_norm": 0.0006479278090409935,
      "learning_rate": 1.2226378574513653e-07,
      "loss": 0.001,
      "step": 119300
    },
    {
      "epoch": 21692.727272727272,
      "grad_norm": 0.20617663860321045,
      "learning_rate": 1.2218756226793826e-07,
      "loss": 0.0013,
      "step": 119310
    },
    {
      "epoch": 21694.545454545456,
      "grad_norm": 0.28236204385757446,
      "learning_rate": 1.2211135925087324e-07,
      "loss": 0.001,
      "step": 119320
    },
    {
      "epoch": 21696.363636363636,
      "grad_norm": 0.27930131554603577,
      "learning_rate": 1.22035176698068e-07,
      "loss": 0.001,
      "step": 119330
    },
    {
      "epoch": 21698.18181818182,
      "grad_norm": 0.2171928584575653,
      "learning_rate": 1.219590146136485e-07,
      "loss": 0.001,
      "step": 119340
    },
    {
      "epoch": 21700.0,
      "grad_norm": 0.0009532389813102782,
      "learning_rate": 1.21882873001739e-07,
      "loss": 0.001,
      "step": 119350
    },
    {
      "epoch": 21701.81818181818,
      "grad_norm": 0.17100396752357483,
      "learning_rate": 1.2180675186646282e-07,
      "loss": 0.001,
      "step": 119360
    },
    {
      "epoch": 21703.636363636364,
      "grad_norm": 0.00071366858901456,
      "learning_rate": 1.217306512119425e-07,
      "loss": 0.0012,
      "step": 119370
    },
    {
      "epoch": 21705.454545454544,
      "grad_norm": 0.002796264598146081,
      "learning_rate": 1.2165457104229894e-07,
      "loss": 0.0008,
      "step": 119380
    },
    {
      "epoch": 21707.272727272728,
      "grad_norm": 0.1864013820886612,
      "learning_rate": 1.2157851136165242e-07,
      "loss": 0.0013,
      "step": 119390
    },
    {
      "epoch": 21709.090909090908,
      "grad_norm": 0.0006150725530460477,
      "learning_rate": 1.2150247217412185e-07,
      "loss": 0.0011,
      "step": 119400
    },
    {
      "epoch": 21710.909090909092,
      "grad_norm": 0.18276727199554443,
      "learning_rate": 1.2142645348382484e-07,
      "loss": 0.0012,
      "step": 119410
    },
    {
      "epoch": 21712.727272727272,
      "grad_norm": 0.20655880868434906,
      "learning_rate": 1.2135045529487847e-07,
      "loss": 0.0013,
      "step": 119420
    },
    {
      "epoch": 21714.545454545456,
      "grad_norm": 0.20326147973537445,
      "learning_rate": 1.212744776113982e-07,
      "loss": 0.001,
      "step": 119430
    },
    {
      "epoch": 21716.363636363636,
      "grad_norm": 0.0007078716880641878,
      "learning_rate": 1.211985204374984e-07,
      "loss": 0.0007,
      "step": 119440
    },
    {
      "epoch": 21718.18181818182,
      "grad_norm": 0.0011238849256187677,
      "learning_rate": 1.2112258377729274e-07,
      "loss": 0.0012,
      "step": 119450
    },
    {
      "epoch": 21720.0,
      "grad_norm": 0.18892590701580048,
      "learning_rate": 1.2104666763489325e-07,
      "loss": 0.0012,
      "step": 119460
    },
    {
      "epoch": 21721.81818181818,
      "grad_norm": 0.20195139944553375,
      "learning_rate": 1.2097077201441136e-07,
      "loss": 0.0012,
      "step": 119470
    },
    {
      "epoch": 21723.636363636364,
      "grad_norm": 0.001100319903343916,
      "learning_rate": 1.2089489691995702e-07,
      "loss": 0.001,
      "step": 119480
    },
    {
      "epoch": 21725.454545454544,
      "grad_norm": 0.006289864424616098,
      "learning_rate": 1.2081904235563905e-07,
      "loss": 0.0013,
      "step": 119490
    },
    {
      "epoch": 21727.272727272728,
      "grad_norm": 0.0005862978287041187,
      "learning_rate": 1.2074320832556556e-07,
      "loss": 0.0009,
      "step": 119500
    },
    {
      "epoch": 21727.272727272728,
      "eval_loss": 5.122559070587158,
      "eval_runtime": 0.9494,
      "eval_samples_per_second": 10.532,
      "eval_steps_per_second": 5.266,
      "step": 119500
    },
    {
      "epoch": 21729.090909090908,
      "grad_norm": 0.18816573917865753,
      "learning_rate": 1.2066739483384314e-07,
      "loss": 0.0012,
      "step": 119510
    },
    {
      "epoch": 21730.909090909092,
      "grad_norm": 0.0006225753459148109,
      "learning_rate": 1.2059160188457724e-07,
      "loss": 0.001,
      "step": 119520
    },
    {
      "epoch": 21732.727272727272,
      "grad_norm": 0.19497795403003693,
      "learning_rate": 1.205158294818727e-07,
      "loss": 0.0013,
      "step": 119530
    },
    {
      "epoch": 21734.545454545456,
      "grad_norm": 0.30804720520973206,
      "learning_rate": 1.2044007762983276e-07,
      "loss": 0.0009,
      "step": 119540
    },
    {
      "epoch": 21736.363636363636,
      "grad_norm": 0.0007003509672358632,
      "learning_rate": 1.203643463325596e-07,
      "loss": 0.0009,
      "step": 119550
    },
    {
      "epoch": 21738.18181818182,
      "grad_norm": 0.23249518871307373,
      "learning_rate": 1.202886355941546e-07,
      "loss": 0.0014,
      "step": 119560
    },
    {
      "epoch": 21740.0,
      "grad_norm": 0.004973819479346275,
      "learning_rate": 1.202129454187175e-07,
      "loss": 0.001,
      "step": 119570
    },
    {
      "epoch": 21741.81818181818,
      "grad_norm": 0.3163057267665863,
      "learning_rate": 1.2013727581034782e-07,
      "loss": 0.0012,
      "step": 119580
    },
    {
      "epoch": 21743.636363636364,
      "grad_norm": 0.20171292126178741,
      "learning_rate": 1.2006162677314264e-07,
      "loss": 0.0009,
      "step": 119590
    },
    {
      "epoch": 21745.454545454544,
      "grad_norm": 0.0006658605998381972,
      "learning_rate": 1.199859983111991e-07,
      "loss": 0.0011,
      "step": 119600
    },
    {
      "epoch": 21747.272727272728,
      "grad_norm": 0.00043738100794143975,
      "learning_rate": 1.199103904286129e-07,
      "loss": 0.001,
      "step": 119610
    },
    {
      "epoch": 21749.090909090908,
      "grad_norm": 0.23298591375350952,
      "learning_rate": 1.198348031294783e-07,
      "loss": 0.0011,
      "step": 119620
    },
    {
      "epoch": 21750.909090909092,
      "grad_norm": 0.2052289992570877,
      "learning_rate": 1.1975923641788864e-07,
      "loss": 0.0012,
      "step": 119630
    },
    {
      "epoch": 21752.727272727272,
      "grad_norm": 0.2013728767633438,
      "learning_rate": 1.196836902979364e-07,
      "loss": 0.001,
      "step": 119640
    },
    {
      "epoch": 21754.545454545456,
      "grad_norm": 0.21646220982074738,
      "learning_rate": 1.1960816477371249e-07,
      "loss": 0.0009,
      "step": 119650
    },
    {
      "epoch": 21756.363636363636,
      "grad_norm": 0.22033272683620453,
      "learning_rate": 1.1953265984930715e-07,
      "loss": 0.0013,
      "step": 119660
    },
    {
      "epoch": 21758.18181818182,
      "grad_norm": 0.18043343722820282,
      "learning_rate": 1.194571755288092e-07,
      "loss": 0.001,
      "step": 119670
    },
    {
      "epoch": 21760.0,
      "grad_norm": 0.0009221933432854712,
      "learning_rate": 1.1938171181630623e-07,
      "loss": 0.001,
      "step": 119680
    },
    {
      "epoch": 21761.81818181818,
      "grad_norm": 0.005798029713332653,
      "learning_rate": 1.1930626871588524e-07,
      "loss": 0.0012,
      "step": 119690
    },
    {
      "epoch": 21763.636363636364,
      "grad_norm": 0.20789581537246704,
      "learning_rate": 1.192308462316317e-07,
      "loss": 0.001,
      "step": 119700
    },
    {
      "epoch": 21765.454545454544,
      "grad_norm": 0.19508303701877594,
      "learning_rate": 1.1915544436762982e-07,
      "loss": 0.001,
      "step": 119710
    },
    {
      "epoch": 21767.272727272728,
      "grad_norm": 0.0007298643467947841,
      "learning_rate": 1.1908006312796326e-07,
      "loss": 0.001,
      "step": 119720
    },
    {
      "epoch": 21769.090909090908,
      "grad_norm": 0.0008627453353255987,
      "learning_rate": 1.1900470251671412e-07,
      "loss": 0.0012,
      "step": 119730
    },
    {
      "epoch": 21770.909090909092,
      "grad_norm": 0.20001983642578125,
      "learning_rate": 1.1892936253796331e-07,
      "loss": 0.0011,
      "step": 119740
    },
    {
      "epoch": 21772.727272727272,
      "grad_norm": 0.19861504435539246,
      "learning_rate": 1.1885404319579107e-07,
      "loss": 0.0014,
      "step": 119750
    },
    {
      "epoch": 21774.545454545456,
      "grad_norm": 0.0005656974972225726,
      "learning_rate": 1.18778744494276e-07,
      "loss": 0.001,
      "step": 119760
    },
    {
      "epoch": 21776.363636363636,
      "grad_norm": 0.0005971089703962207,
      "learning_rate": 1.1870346643749629e-07,
      "loss": 0.0008,
      "step": 119770
    },
    {
      "epoch": 21778.18181818182,
      "grad_norm": 0.0005020518437959254,
      "learning_rate": 1.1862820902952796e-07,
      "loss": 0.0011,
      "step": 119780
    },
    {
      "epoch": 21780.0,
      "grad_norm": 0.21032558381557465,
      "learning_rate": 1.1855297227444689e-07,
      "loss": 0.0013,
      "step": 119790
    },
    {
      "epoch": 21781.81818181818,
      "grad_norm": 0.21613942086696625,
      "learning_rate": 1.1847775617632744e-07,
      "loss": 0.001,
      "step": 119800
    },
    {
      "epoch": 21783.636363636364,
      "grad_norm": 0.0007306351908482611,
      "learning_rate": 1.1840256073924288e-07,
      "loss": 0.0009,
      "step": 119810
    },
    {
      "epoch": 21785.454545454544,
      "grad_norm": 0.0003891076776199043,
      "learning_rate": 1.1832738596726516e-07,
      "loss": 0.0014,
      "step": 119820
    },
    {
      "epoch": 21787.272727272728,
      "grad_norm": 0.19015777111053467,
      "learning_rate": 1.182522318644656e-07,
      "loss": 0.001,
      "step": 119830
    },
    {
      "epoch": 21789.090909090908,
      "grad_norm": 0.21975968778133392,
      "learning_rate": 1.181770984349138e-07,
      "loss": 0.0011,
      "step": 119840
    },
    {
      "epoch": 21790.909090909092,
      "grad_norm": 0.0005732194404117763,
      "learning_rate": 1.1810198568267903e-07,
      "loss": 0.001,
      "step": 119850
    },
    {
      "epoch": 21792.727272727272,
      "grad_norm": 0.0027082108426839113,
      "learning_rate": 1.1802689361182839e-07,
      "loss": 0.0009,
      "step": 119860
    },
    {
      "epoch": 21794.545454545456,
      "grad_norm": 0.22991539537906647,
      "learning_rate": 1.1795182222642869e-07,
      "loss": 0.0011,
      "step": 119870
    },
    {
      "epoch": 21796.363636363636,
      "grad_norm": 0.0008655148558318615,
      "learning_rate": 1.1787677153054549e-07,
      "loss": 0.0012,
      "step": 119880
    },
    {
      "epoch": 21798.18181818182,
      "grad_norm": 0.00548024196177721,
      "learning_rate": 1.1780174152824296e-07,
      "loss": 0.0012,
      "step": 119890
    },
    {
      "epoch": 21800.0,
      "grad_norm": 0.012288647703826427,
      "learning_rate": 1.177267322235842e-07,
      "loss": 0.0009,
      "step": 119900
    },
    {
      "epoch": 21801.81818181818,
      "grad_norm": 0.001065464224666357,
      "learning_rate": 1.1765174362063152e-07,
      "loss": 0.0007,
      "step": 119910
    },
    {
      "epoch": 21803.636363636364,
      "grad_norm": 0.204660564661026,
      "learning_rate": 1.1757677572344577e-07,
      "loss": 0.0013,
      "step": 119920
    },
    {
      "epoch": 21805.454545454544,
      "grad_norm": 0.0012192229041829705,
      "learning_rate": 1.175018285360867e-07,
      "loss": 0.0011,
      "step": 119930
    },
    {
      "epoch": 21807.272727272728,
      "grad_norm": 0.00392614072188735,
      "learning_rate": 1.1742690206261291e-07,
      "loss": 0.001,
      "step": 119940
    },
    {
      "epoch": 21809.090909090908,
      "grad_norm": 0.2913883924484253,
      "learning_rate": 1.173519963070822e-07,
      "loss": 0.0014,
      "step": 119950
    },
    {
      "epoch": 21810.909090909092,
      "grad_norm": 0.19672322273254395,
      "learning_rate": 1.1727711127355116e-07,
      "loss": 0.0009,
      "step": 119960
    },
    {
      "epoch": 21812.727272727272,
      "grad_norm": 0.001508063287474215,
      "learning_rate": 1.1720224696607473e-07,
      "loss": 0.0012,
      "step": 119970
    },
    {
      "epoch": 21814.545454545456,
      "grad_norm": 0.0016793196555227041,
      "learning_rate": 1.171274033887073e-07,
      "loss": 0.0012,
      "step": 119980
    },
    {
      "epoch": 21816.363636363636,
      "grad_norm": 0.0006338476086966693,
      "learning_rate": 1.170525805455021e-07,
      "loss": 0.0013,
      "step": 119990
    },
    {
      "epoch": 21818.18181818182,
      "grad_norm": 0.20672225952148438,
      "learning_rate": 1.1697777844051104e-07,
      "loss": 0.001,
      "step": 120000
    },
    {
      "epoch": 21818.18181818182,
      "eval_loss": 5.212712287902832,
      "eval_runtime": 0.9477,
      "eval_samples_per_second": 10.552,
      "eval_steps_per_second": 5.276,
      "step": 120000
    },
    {
      "epoch": 21820.0,
      "grad_norm": 0.20621603727340698,
      "learning_rate": 1.1690299707778478e-07,
      "loss": 0.0011,
      "step": 120010
    },
    {
      "epoch": 21821.81818181818,
      "grad_norm": 0.20026054978370667,
      "learning_rate": 1.1682823646137336e-07,
      "loss": 0.0009,
      "step": 120020
    },
    {
      "epoch": 21823.636363636364,
      "grad_norm": 0.2184266299009323,
      "learning_rate": 1.1675349659532513e-07,
      "loss": 0.0015,
      "step": 120030
    },
    {
      "epoch": 21825.454545454544,
      "grad_norm": 0.0006673542666248977,
      "learning_rate": 1.166787774836877e-07,
      "loss": 0.0012,
      "step": 120040
    },
    {
      "epoch": 21827.272727272728,
      "grad_norm": 0.2903098464012146,
      "learning_rate": 1.1660407913050729e-07,
      "loss": 0.0015,
      "step": 120050
    },
    {
      "epoch": 21829.090909090908,
      "grad_norm": 0.000523669586982578,
      "learning_rate": 1.1652940153982915e-07,
      "loss": 0.0007,
      "step": 120060
    },
    {
      "epoch": 21830.909090909092,
      "grad_norm": 0.0006679618381895125,
      "learning_rate": 1.1645474471569761e-07,
      "loss": 0.0012,
      "step": 120070
    },
    {
      "epoch": 21832.727272727272,
      "grad_norm": 0.17230933904647827,
      "learning_rate": 1.1638010866215547e-07,
      "loss": 0.0012,
      "step": 120080
    },
    {
      "epoch": 21834.545454545456,
      "grad_norm": 0.15362179279327393,
      "learning_rate": 1.1630549338324453e-07,
      "loss": 0.001,
      "step": 120090
    },
    {
      "epoch": 21836.363636363636,
      "grad_norm": 0.20610898733139038,
      "learning_rate": 1.1623089888300569e-07,
      "loss": 0.0012,
      "step": 120100
    },
    {
      "epoch": 21838.18181818182,
      "grad_norm": 0.21806538105010986,
      "learning_rate": 1.1615632516547851e-07,
      "loss": 0.001,
      "step": 120110
    },
    {
      "epoch": 21840.0,
      "grad_norm": 0.1709400862455368,
      "learning_rate": 1.160817722347014e-07,
      "loss": 0.001,
      "step": 120120
    },
    {
      "epoch": 21841.81818181818,
      "grad_norm": 0.19800622761249542,
      "learning_rate": 1.1600724009471158e-07,
      "loss": 0.001,
      "step": 120130
    },
    {
      "epoch": 21843.636363636364,
      "grad_norm": 0.18931236863136292,
      "learning_rate": 1.1593272874954547e-07,
      "loss": 0.0012,
      "step": 120140
    },
    {
      "epoch": 21845.454545454544,
      "grad_norm": 0.0006016638362780213,
      "learning_rate": 1.1585823820323843e-07,
      "loss": 0.001,
      "step": 120150
    },
    {
      "epoch": 21847.272727272728,
      "grad_norm": 0.0003652866289485246,
      "learning_rate": 1.1578376845982384e-07,
      "loss": 0.0011,
      "step": 120160
    },
    {
      "epoch": 21849.090909090908,
      "grad_norm": 0.000867062306497246,
      "learning_rate": 1.157093195233349e-07,
      "loss": 0.0012,
      "step": 120170
    },
    {
      "epoch": 21850.909090909092,
      "grad_norm": 0.0007261750870384276,
      "learning_rate": 1.1563489139780342e-07,
      "loss": 0.0012,
      "step": 120180
    },
    {
      "epoch": 21852.727272727272,
      "grad_norm": 0.22210444509983063,
      "learning_rate": 1.1556048408725988e-07,
      "loss": 0.0012,
      "step": 120190
    },
    {
      "epoch": 21854.545454545456,
      "grad_norm": 0.0004699565179180354,
      "learning_rate": 1.1548609759573374e-07,
      "loss": 0.0012,
      "step": 120200
    },
    {
      "epoch": 21856.363636363636,
      "grad_norm": 0.0007870484841987491,
      "learning_rate": 1.1541173192725318e-07,
      "loss": 0.001,
      "step": 120210
    },
    {
      "epoch": 21858.18181818182,
      "grad_norm": 0.18964265286922455,
      "learning_rate": 1.1533738708584573e-07,
      "loss": 0.0012,
      "step": 120220
    },
    {
      "epoch": 21860.0,
      "grad_norm": 0.0006432572845369577,
      "learning_rate": 1.1526306307553735e-07,
      "loss": 0.0011,
      "step": 120230
    },
    {
      "epoch": 21861.81818181818,
      "grad_norm": 0.006523447576910257,
      "learning_rate": 1.1518875990035276e-07,
      "loss": 0.0012,
      "step": 120240
    },
    {
      "epoch": 21863.636363636364,
      "grad_norm": 0.20819927752017975,
      "learning_rate": 1.1511447756431602e-07,
      "loss": 0.0009,
      "step": 120250
    },
    {
      "epoch": 21865.454545454544,
      "grad_norm": 0.21882985532283783,
      "learning_rate": 1.1504021607144997e-07,
      "loss": 0.0012,
      "step": 120260
    },
    {
      "epoch": 21867.272727272728,
      "grad_norm": 0.0006373905926011503,
      "learning_rate": 1.1496597542577601e-07,
      "loss": 0.0009,
      "step": 120270
    },
    {
      "epoch": 21869.090909090908,
      "grad_norm": 0.00042670799302868545,
      "learning_rate": 1.1489175563131448e-07,
      "loss": 0.0012,
      "step": 120280
    },
    {
      "epoch": 21870.909090909092,
      "grad_norm": 0.20910917222499847,
      "learning_rate": 1.1481755669208493e-07,
      "loss": 0.001,
      "step": 120290
    },
    {
      "epoch": 21872.727272727272,
      "grad_norm": 0.0005372178857214749,
      "learning_rate": 1.1474337861210543e-07,
      "loss": 0.001,
      "step": 120300
    },
    {
      "epoch": 21874.545454545456,
      "grad_norm": 0.20117707550525665,
      "learning_rate": 1.1466922139539303e-07,
      "loss": 0.0013,
      "step": 120310
    },
    {
      "epoch": 21876.363636363636,
      "grad_norm": 0.1805991381406784,
      "learning_rate": 1.145950850459635e-07,
      "loss": 0.0009,
      "step": 120320
    },
    {
      "epoch": 21878.18181818182,
      "grad_norm": 0.18318700790405273,
      "learning_rate": 1.1452096956783181e-07,
      "loss": 0.0012,
      "step": 120330
    },
    {
      "epoch": 21880.0,
      "grad_norm": 0.1896192729473114,
      "learning_rate": 1.1444687496501188e-07,
      "loss": 0.0012,
      "step": 120340
    },
    {
      "epoch": 21881.81818181818,
      "grad_norm": 0.018145384266972542,
      "learning_rate": 1.1437280124151566e-07,
      "loss": 0.0012,
      "step": 120350
    },
    {
      "epoch": 21883.636363636364,
      "grad_norm": 0.28813064098358154,
      "learning_rate": 1.142987484013549e-07,
      "loss": 0.0012,
      "step": 120360
    },
    {
      "epoch": 21885.454545454544,
      "grad_norm": 0.0013982426607981324,
      "learning_rate": 1.1422471644853998e-07,
      "loss": 0.0007,
      "step": 120370
    },
    {
      "epoch": 21887.272727272728,
      "grad_norm": 0.18762627243995667,
      "learning_rate": 1.1415070538707989e-07,
      "loss": 0.0013,
      "step": 120380
    },
    {
      "epoch": 21889.090909090908,
      "grad_norm": 0.000844138499815017,
      "learning_rate": 1.140767152209826e-07,
      "loss": 0.0009,
      "step": 120390
    },
    {
      "epoch": 21890.909090909092,
      "grad_norm": 0.2169397622346878,
      "learning_rate": 1.1400274595425496e-07,
      "loss": 0.0012,
      "step": 120400
    },
    {
      "epoch": 21892.727272727272,
      "grad_norm": 0.0005501355626620352,
      "learning_rate": 1.1392879759090291e-07,
      "loss": 0.0012,
      "step": 120410
    },
    {
      "epoch": 21894.545454545456,
      "grad_norm": 0.018268181011080742,
      "learning_rate": 1.1385487013493095e-07,
      "loss": 0.001,
      "step": 120420
    },
    {
      "epoch": 21896.363636363636,
      "grad_norm": 0.1953682005405426,
      "learning_rate": 1.137809635903424e-07,
      "loss": 0.001,
      "step": 120430
    },
    {
      "epoch": 21898.18181818182,
      "grad_norm": 0.0014045486459508538,
      "learning_rate": 1.1370707796113981e-07,
      "loss": 0.001,
      "step": 120440
    },
    {
      "epoch": 21900.0,
      "grad_norm": 0.19792671501636505,
      "learning_rate": 1.1363321325132447e-07,
      "loss": 0.0012,
      "step": 120450
    },
    {
      "epoch": 21901.81818181818,
      "grad_norm": 0.0006022168090566993,
      "learning_rate": 1.1355936946489636e-07,
      "loss": 0.0012,
      "step": 120460
    },
    {
      "epoch": 21903.636363636364,
      "grad_norm": 0.0026858109049499035,
      "learning_rate": 1.1348554660585446e-07,
      "loss": 0.0009,
      "step": 120470
    },
    {
      "epoch": 21905.454545454544,
      "grad_norm": 0.20404796302318573,
      "learning_rate": 1.1341174467819637e-07,
      "loss": 0.0015,
      "step": 120480
    },
    {
      "epoch": 21907.272727272728,
      "grad_norm": 0.000430645770393312,
      "learning_rate": 1.1333796368591913e-07,
      "loss": 0.0008,
      "step": 120490
    },
    {
      "epoch": 21909.090909090908,
      "grad_norm": 0.2866239547729492,
      "learning_rate": 1.1326420363301808e-07,
      "loss": 0.0015,
      "step": 120500
    },
    {
      "epoch": 21909.090909090908,
      "eval_loss": 5.204200267791748,
      "eval_runtime": 0.9463,
      "eval_samples_per_second": 10.567,
      "eval_steps_per_second": 5.284,
      "step": 120500
    },
    {
      "epoch": 21910.909090909092,
      "grad_norm": 0.000996630871668458,
      "learning_rate": 1.1319046452348758e-07,
      "loss": 0.001,
      "step": 120510
    },
    {
      "epoch": 21912.727272727272,
      "grad_norm": 0.16916032135486603,
      "learning_rate": 1.1311674636132101e-07,
      "loss": 0.0014,
      "step": 120520
    },
    {
      "epoch": 21914.545454545456,
      "grad_norm": 0.0006782415439374745,
      "learning_rate": 1.1304304915051077e-07,
      "loss": 0.0007,
      "step": 120530
    },
    {
      "epoch": 21916.363636363636,
      "grad_norm": 0.25570279359817505,
      "learning_rate": 1.1296937289504738e-07,
      "loss": 0.0013,
      "step": 120540
    },
    {
      "epoch": 21918.18181818182,
      "grad_norm": 0.001052247709594667,
      "learning_rate": 1.1289571759892108e-07,
      "loss": 0.0009,
      "step": 120550
    },
    {
      "epoch": 21920.0,
      "grad_norm": 0.0005843662074767053,
      "learning_rate": 1.1282208326612036e-07,
      "loss": 0.0012,
      "step": 120560
    },
    {
      "epoch": 21921.81818181818,
      "grad_norm": 0.21368378400802612,
      "learning_rate": 1.1274846990063313e-07,
      "loss": 0.0012,
      "step": 120570
    },
    {
      "epoch": 21923.636363636364,
      "grad_norm": 0.20339158177375793,
      "learning_rate": 1.1267487750644567e-07,
      "loss": 0.001,
      "step": 120580
    },
    {
      "epoch": 21925.454545454544,
      "grad_norm": 0.15672259032726288,
      "learning_rate": 1.126013060875432e-07,
      "loss": 0.0012,
      "step": 120590
    },
    {
      "epoch": 21927.272727272728,
      "grad_norm": 0.0007301743607968092,
      "learning_rate": 1.1252775564791023e-07,
      "loss": 0.0009,
      "step": 120600
    },
    {
      "epoch": 21929.090909090908,
      "grad_norm": 0.26673996448516846,
      "learning_rate": 1.1245422619152967e-07,
      "loss": 0.0013,
      "step": 120610
    },
    {
      "epoch": 21930.909090909092,
      "grad_norm": 0.0007041872595436871,
      "learning_rate": 1.1238071772238334e-07,
      "loss": 0.001,
      "step": 120620
    },
    {
      "epoch": 21932.727272727272,
      "grad_norm": 0.0005038381204940379,
      "learning_rate": 1.123072302444521e-07,
      "loss": 0.001,
      "step": 120630
    },
    {
      "epoch": 21934.545454545456,
      "grad_norm": 0.1761629432439804,
      "learning_rate": 1.1223376376171572e-07,
      "loss": 0.0009,
      "step": 120640
    },
    {
      "epoch": 21936.363636363636,
      "grad_norm": 0.0005758816841989756,
      "learning_rate": 1.1216031827815275e-07,
      "loss": 0.0011,
      "step": 120650
    },
    {
      "epoch": 21938.18181818182,
      "grad_norm": 0.0015689583960920572,
      "learning_rate": 1.1208689379774039e-07,
      "loss": 0.001,
      "step": 120660
    },
    {
      "epoch": 21940.0,
      "grad_norm": 0.0003931077662855387,
      "learning_rate": 1.1201349032445483e-07,
      "loss": 0.0011,
      "step": 120670
    },
    {
      "epoch": 21941.81818181818,
      "grad_norm": 0.1775922030210495,
      "learning_rate": 1.1194010786227148e-07,
      "loss": 0.0011,
      "step": 120680
    },
    {
      "epoch": 21943.636363636364,
      "grad_norm": 0.000450105027994141,
      "learning_rate": 1.1186674641516414e-07,
      "loss": 0.0012,
      "step": 120690
    },
    {
      "epoch": 21945.454545454544,
      "grad_norm": 0.0013197498628869653,
      "learning_rate": 1.1179340598710546e-07,
      "loss": 0.0008,
      "step": 120700
    },
    {
      "epoch": 21947.272727272728,
      "grad_norm": 0.0009182398207485676,
      "learning_rate": 1.1172008658206733e-07,
      "loss": 0.0015,
      "step": 120710
    },
    {
      "epoch": 21949.090909090908,
      "grad_norm": 0.00046299261157400906,
      "learning_rate": 1.1164678820402057e-07,
      "loss": 0.001,
      "step": 120720
    },
    {
      "epoch": 21950.909090909092,
      "grad_norm": 0.1917761266231537,
      "learning_rate": 1.1157351085693401e-07,
      "loss": 0.0012,
      "step": 120730
    },
    {
      "epoch": 21952.727272727272,
      "grad_norm": 0.16757701337337494,
      "learning_rate": 1.1150025454477641e-07,
      "loss": 0.0012,
      "step": 120740
    },
    {
      "epoch": 21954.545454545456,
      "grad_norm": 0.21271003782749176,
      "learning_rate": 1.1142701927151454e-07,
      "loss": 0.001,
      "step": 120750
    },
    {
      "epoch": 21956.363636363636,
      "grad_norm": 0.0007660237606614828,
      "learning_rate": 1.1135380504111474e-07,
      "loss": 0.001,
      "step": 120760
    },
    {
      "epoch": 21958.18181818182,
      "grad_norm": 0.19916404783725739,
      "learning_rate": 1.1128061185754167e-07,
      "loss": 0.001,
      "step": 120770
    },
    {
      "epoch": 21960.0,
      "grad_norm": 0.0009342709672637284,
      "learning_rate": 1.1120743972475899e-07,
      "loss": 0.001,
      "step": 120780
    },
    {
      "epoch": 21961.81818181818,
      "grad_norm": 0.17480987310409546,
      "learning_rate": 1.1113428864672953e-07,
      "loss": 0.0012,
      "step": 120790
    },
    {
      "epoch": 21963.636363636364,
      "grad_norm": 0.213595911860466,
      "learning_rate": 1.1106115862741456e-07,
      "loss": 0.0012,
      "step": 120800
    },
    {
      "epoch": 21965.454545454544,
      "grad_norm": 0.17191794514656067,
      "learning_rate": 1.1098804967077424e-07,
      "loss": 0.001,
      "step": 120810
    },
    {
      "epoch": 21967.272727272728,
      "grad_norm": 0.16274532675743103,
      "learning_rate": 1.1091496178076804e-07,
      "loss": 0.0012,
      "step": 120820
    },
    {
      "epoch": 21969.090909090908,
      "grad_norm": 0.0017553127836436033,
      "learning_rate": 1.1084189496135366e-07,
      "loss": 0.0007,
      "step": 120830
    },
    {
      "epoch": 21970.909090909092,
      "grad_norm": 0.16691215336322784,
      "learning_rate": 1.1076884921648832e-07,
      "loss": 0.0012,
      "step": 120840
    },
    {
      "epoch": 21972.727272727272,
      "grad_norm": 0.0004176415386609733,
      "learning_rate": 1.1069582455012754e-07,
      "loss": 0.001,
      "step": 120850
    },
    {
      "epoch": 21974.545454545456,
      "grad_norm": 0.0006916737766005099,
      "learning_rate": 1.1062282096622578e-07,
      "loss": 0.001,
      "step": 120860
    },
    {
      "epoch": 21976.363636363636,
      "grad_norm": 0.1673288196325302,
      "learning_rate": 1.1054983846873684e-07,
      "loss": 0.0013,
      "step": 120870
    },
    {
      "epoch": 21978.18181818182,
      "grad_norm": 0.21213755011558533,
      "learning_rate": 1.104768770616128e-07,
      "loss": 0.001,
      "step": 120880
    },
    {
      "epoch": 21980.0,
      "grad_norm": 0.000823278387542814,
      "learning_rate": 1.1040393674880477e-07,
      "loss": 0.001,
      "step": 120890
    },
    {
      "epoch": 21981.81818181818,
      "grad_norm": 0.0050919633358716965,
      "learning_rate": 1.1033101753426282e-07,
      "loss": 0.001,
      "step": 120900
    },
    {
      "epoch": 21983.636363636364,
      "grad_norm": 0.017909785732626915,
      "learning_rate": 1.1025811942193624e-07,
      "loss": 0.0012,
      "step": 120910
    },
    {
      "epoch": 21985.454545454544,
      "grad_norm": 0.00043624499812722206,
      "learning_rate": 1.101852424157721e-07,
      "loss": 0.0009,
      "step": 120920
    },
    {
      "epoch": 21987.272727272728,
      "grad_norm": 0.1974484622478485,
      "learning_rate": 1.1011238651971743e-07,
      "loss": 0.0011,
      "step": 120930
    },
    {
      "epoch": 21989.090909090908,
      "grad_norm": 0.1714419424533844,
      "learning_rate": 1.1003955173771745e-07,
      "loss": 0.0011,
      "step": 120940
    },
    {
      "epoch": 21990.909090909092,
      "grad_norm": 0.1637415587902069,
      "learning_rate": 1.0996673807371676e-07,
      "loss": 0.0012,
      "step": 120950
    },
    {
      "epoch": 21992.727272727272,
      "grad_norm": 0.0008788209524936974,
      "learning_rate": 1.0989394553165831e-07,
      "loss": 0.0009,
      "step": 120960
    },
    {
      "epoch": 21994.545454545456,
      "grad_norm": 0.0014438405632972717,
      "learning_rate": 1.0982117411548403e-07,
      "loss": 0.0012,
      "step": 120970
    },
    {
      "epoch": 21996.363636363636,
      "grad_norm": 0.27389171719551086,
      "learning_rate": 1.0974842382913507e-07,
      "loss": 0.0012,
      "step": 120980
    },
    {
      "epoch": 21998.18181818182,
      "grad_norm": 0.21253159642219543,
      "learning_rate": 1.0967569467655102e-07,
      "loss": 0.0011,
      "step": 120990
    },
    {
      "epoch": 22000.0,
      "grad_norm": 0.006193214096128941,
      "learning_rate": 1.096029866616704e-07,
      "loss": 0.001,
      "step": 121000
    },
    {
      "epoch": 22000.0,
      "eval_loss": 5.1348137855529785,
      "eval_runtime": 0.9496,
      "eval_samples_per_second": 10.531,
      "eval_steps_per_second": 5.265,
      "step": 121000
    },
    {
      "epoch": 22001.81818181818,
      "grad_norm": 0.0004337205900810659,
      "learning_rate": 1.0953029978843082e-07,
      "loss": 0.0012,
      "step": 121010
    },
    {
      "epoch": 22003.636363636364,
      "grad_norm": 0.0005369054852053523,
      "learning_rate": 1.0945763406076835e-07,
      "loss": 0.0008,
      "step": 121020
    },
    {
      "epoch": 22005.454545454544,
      "grad_norm": 0.20464657247066498,
      "learning_rate": 1.0938498948261843e-07,
      "loss": 0.0014,
      "step": 121030
    },
    {
      "epoch": 22007.272727272728,
      "grad_norm": 0.17226406931877136,
      "learning_rate": 1.0931236605791494e-07,
      "loss": 0.0011,
      "step": 121040
    },
    {
      "epoch": 22009.090909090908,
      "grad_norm": 0.0008688175003044307,
      "learning_rate": 1.0923976379059058e-07,
      "loss": 0.001,
      "step": 121050
    },
    {
      "epoch": 22010.909090909092,
      "grad_norm": 0.26147088408470154,
      "learning_rate": 1.0916718268457736e-07,
      "loss": 0.0012,
      "step": 121060
    },
    {
      "epoch": 22012.727272727272,
      "grad_norm": 0.0007438436150550842,
      "learning_rate": 1.0909462274380576e-07,
      "loss": 0.001,
      "step": 121070
    },
    {
      "epoch": 22014.545454545456,
      "grad_norm": 0.20362892746925354,
      "learning_rate": 1.0902208397220497e-07,
      "loss": 0.0011,
      "step": 121080
    },
    {
      "epoch": 22016.363636363636,
      "grad_norm": 0.0005090183112770319,
      "learning_rate": 1.0894956637370361e-07,
      "loss": 0.001,
      "step": 121090
    },
    {
      "epoch": 22018.18181818182,
      "grad_norm": 0.19880928099155426,
      "learning_rate": 1.0887706995222862e-07,
      "loss": 0.0012,
      "step": 121100
    },
    {
      "epoch": 22020.0,
      "grad_norm": 0.0004322946479078382,
      "learning_rate": 1.0880459471170594e-07,
      "loss": 0.0011,
      "step": 121110
    },
    {
      "epoch": 22021.81818181818,
      "grad_norm": 0.17713408172130585,
      "learning_rate": 1.0873214065606062e-07,
      "loss": 0.0012,
      "step": 121120
    },
    {
      "epoch": 22023.636363636364,
      "grad_norm": 0.16427382826805115,
      "learning_rate": 1.0865970778921607e-07,
      "loss": 0.0012,
      "step": 121130
    },
    {
      "epoch": 22025.454545454544,
      "grad_norm": 0.001331002451479435,
      "learning_rate": 1.0858729611509515e-07,
      "loss": 0.0011,
      "step": 121140
    },
    {
      "epoch": 22027.272727272728,
      "grad_norm": 0.0009655536850914359,
      "learning_rate": 1.0851490563761911e-07,
      "loss": 0.001,
      "step": 121150
    },
    {
      "epoch": 22029.090909090908,
      "grad_norm": 0.0012716357596218586,
      "learning_rate": 1.0844253636070805e-07,
      "loss": 0.0011,
      "step": 121160
    },
    {
      "epoch": 22030.909090909092,
      "grad_norm": 0.0004654077929444611,
      "learning_rate": 1.0837018828828131e-07,
      "loss": 0.0012,
      "step": 121170
    },
    {
      "epoch": 22032.727272727272,
      "grad_norm": 0.00047496959450654685,
      "learning_rate": 1.0829786142425679e-07,
      "loss": 0.001,
      "step": 121180
    },
    {
      "epoch": 22034.545454545456,
      "grad_norm": 0.18635065853595734,
      "learning_rate": 1.082255557725511e-07,
      "loss": 0.0012,
      "step": 121190
    },
    {
      "epoch": 22036.363636363636,
      "grad_norm": 0.20671787858009338,
      "learning_rate": 1.0815327133708013e-07,
      "loss": 0.0009,
      "step": 121200
    },
    {
      "epoch": 22038.18181818182,
      "grad_norm": 0.18097521364688873,
      "learning_rate": 1.0808100812175836e-07,
      "loss": 0.0012,
      "step": 121210
    },
    {
      "epoch": 22040.0,
      "grad_norm": 0.0006583078647963703,
      "learning_rate": 1.0800876613049892e-07,
      "loss": 0.001,
      "step": 121220
    },
    {
      "epoch": 22041.81818181818,
      "grad_norm": 0.19171105325222015,
      "learning_rate": 1.079365453672143e-07,
      "loss": 0.001,
      "step": 121230
    },
    {
      "epoch": 22043.636363636364,
      "grad_norm": 0.0006547719822265208,
      "learning_rate": 1.0786434583581533e-07,
      "loss": 0.0012,
      "step": 121240
    },
    {
      "epoch": 22045.454545454544,
      "grad_norm": 0.0009572788258083165,
      "learning_rate": 1.0779216754021214e-07,
      "loss": 0.0011,
      "step": 121250
    },
    {
      "epoch": 22047.272727272728,
      "grad_norm": 0.21299585700035095,
      "learning_rate": 1.077200104843134e-07,
      "loss": 0.001,
      "step": 121260
    },
    {
      "epoch": 22049.090909090908,
      "grad_norm": 0.0011084015714004636,
      "learning_rate": 1.0764787467202658e-07,
      "loss": 0.0011,
      "step": 121270
    },
    {
      "epoch": 22050.909090909092,
      "grad_norm": 0.0005554230883717537,
      "learning_rate": 1.0757576010725838e-07,
      "loss": 0.001,
      "step": 121280
    },
    {
      "epoch": 22052.727272727272,
      "grad_norm": 0.0011361222714185715,
      "learning_rate": 1.0750366679391392e-07,
      "loss": 0.0008,
      "step": 121290
    },
    {
      "epoch": 22054.545454545456,
      "grad_norm": 0.00036443411954678595,
      "learning_rate": 1.0743159473589736e-07,
      "loss": 0.0015,
      "step": 121300
    },
    {
      "epoch": 22056.363636363636,
      "grad_norm": 0.006121794227510691,
      "learning_rate": 1.0735954393711188e-07,
      "loss": 0.0012,
      "step": 121310
    },
    {
      "epoch": 22058.18181818182,
      "grad_norm": 0.17031104862689972,
      "learning_rate": 1.0728751440145906e-07,
      "loss": 0.001,
      "step": 121320
    },
    {
      "epoch": 22060.0,
      "grad_norm": 0.0018964989576488733,
      "learning_rate": 1.0721550613283987e-07,
      "loss": 0.0011,
      "step": 121330
    },
    {
      "epoch": 22061.81818181818,
      "grad_norm": 0.21289074420928955,
      "learning_rate": 1.0714351913515379e-07,
      "loss": 0.001,
      "step": 121340
    },
    {
      "epoch": 22063.636363636364,
      "grad_norm": 0.0006621480570174754,
      "learning_rate": 1.07071553412299e-07,
      "loss": 0.0009,
      "step": 121350
    },
    {
      "epoch": 22065.454545454544,
      "grad_norm": 0.0006039310828782618,
      "learning_rate": 1.0699960896817311e-07,
      "loss": 0.0012,
      "step": 121360
    },
    {
      "epoch": 22067.272727272728,
      "grad_norm": 0.2034943401813507,
      "learning_rate": 1.06927685806672e-07,
      "loss": 0.0015,
      "step": 121370
    },
    {
      "epoch": 22069.090909090908,
      "grad_norm": 0.000482672534417361,
      "learning_rate": 1.0685578393169054e-07,
      "loss": 0.0007,
      "step": 121380
    },
    {
      "epoch": 22070.909090909092,
      "grad_norm": 0.20505453646183014,
      "learning_rate": 1.0678390334712272e-07,
      "loss": 0.0012,
      "step": 121390
    },
    {
      "epoch": 22072.727272727272,
      "grad_norm": 0.0006051576347090304,
      "learning_rate": 1.0671204405686107e-07,
      "loss": 0.0007,
      "step": 121400
    },
    {
      "epoch": 22074.545454545456,
      "grad_norm": 0.2046090066432953,
      "learning_rate": 1.0664020606479701e-07,
      "loss": 0.0013,
      "step": 121410
    },
    {
      "epoch": 22076.363636363636,
      "grad_norm": 0.21650059521198273,
      "learning_rate": 1.0656838937482099e-07,
      "loss": 0.0011,
      "step": 121420
    },
    {
      "epoch": 22078.18181818182,
      "grad_norm": 0.27349182963371277,
      "learning_rate": 1.0649659399082205e-07,
      "loss": 0.0012,
      "step": 121430
    },
    {
      "epoch": 22080.0,
      "grad_norm": 0.0012434301897883415,
      "learning_rate": 1.0642481991668839e-07,
      "loss": 0.0009,
      "step": 121440
    },
    {
      "epoch": 22081.81818181818,
      "grad_norm": 0.0006467943894676864,
      "learning_rate": 1.0635306715630682e-07,
      "loss": 0.0009,
      "step": 121450
    },
    {
      "epoch": 22083.636363636364,
      "grad_norm": 0.010606362484395504,
      "learning_rate": 1.062813357135629e-07,
      "loss": 0.0013,
      "step": 121460
    },
    {
      "epoch": 22085.454545454544,
      "grad_norm": 0.2765999734401703,
      "learning_rate": 1.0620962559234143e-07,
      "loss": 0.001,
      "step": 121470
    },
    {
      "epoch": 22087.272727272728,
      "grad_norm": 0.20748624205589294,
      "learning_rate": 1.0613793679652566e-07,
      "loss": 0.0012,
      "step": 121480
    },
    {
      "epoch": 22089.090909090908,
      "grad_norm": 0.17521491646766663,
      "learning_rate": 1.0606626932999774e-07,
      "loss": 0.001,
      "step": 121490
    },
    {
      "epoch": 22090.909090909092,
      "grad_norm": 0.1893061399459839,
      "learning_rate": 1.0599462319663904e-07,
      "loss": 0.0011,
      "step": 121500
    },
    {
      "epoch": 22090.909090909092,
      "eval_loss": 5.15630578994751,
      "eval_runtime": 0.951,
      "eval_samples_per_second": 10.515,
      "eval_steps_per_second": 5.258,
      "step": 121500
    },
    {
      "epoch": 22092.727272727272,
      "grad_norm": 0.00046276673674583435,
      "learning_rate": 1.0592299840032926e-07,
      "loss": 0.0009,
      "step": 121510
    },
    {
      "epoch": 22094.545454545456,
      "grad_norm": 0.0002949487534351647,
      "learning_rate": 1.0585139494494737e-07,
      "loss": 0.0013,
      "step": 121520
    },
    {
      "epoch": 22096.363636363636,
      "grad_norm": 0.2138429582118988,
      "learning_rate": 1.0577981283437093e-07,
      "loss": 0.001,
      "step": 121530
    },
    {
      "epoch": 22098.18181818182,
      "grad_norm": 0.005401664413511753,
      "learning_rate": 1.0570825207247624e-07,
      "loss": 0.0013,
      "step": 121540
    },
    {
      "epoch": 22100.0,
      "grad_norm": 0.26212364435195923,
      "learning_rate": 1.0563671266313895e-07,
      "loss": 0.001,
      "step": 121550
    },
    {
      "epoch": 22101.81818181818,
      "grad_norm": 0.006907811388373375,
      "learning_rate": 1.05565194610233e-07,
      "loss": 0.0012,
      "step": 121560
    },
    {
      "epoch": 22103.636363636364,
      "grad_norm": 0.00044894745224155486,
      "learning_rate": 1.054936979176313e-07,
      "loss": 0.001,
      "step": 121570
    },
    {
      "epoch": 22105.454545454544,
      "grad_norm": 0.16878151893615723,
      "learning_rate": 1.0542222258920597e-07,
      "loss": 0.0009,
      "step": 121580
    },
    {
      "epoch": 22107.272727272728,
      "grad_norm": 0.2127046138048172,
      "learning_rate": 1.0535076862882758e-07,
      "loss": 0.0011,
      "step": 121590
    },
    {
      "epoch": 22109.090909090908,
      "grad_norm": 0.0011488841846585274,
      "learning_rate": 1.0527933604036549e-07,
      "loss": 0.0011,
      "step": 121600
    },
    {
      "epoch": 22110.909090909092,
      "grad_norm": 0.17841047048568726,
      "learning_rate": 1.0520792482768837e-07,
      "loss": 0.0012,
      "step": 121610
    },
    {
      "epoch": 22112.727272727272,
      "grad_norm": 0.0009577438468113542,
      "learning_rate": 1.0513653499466313e-07,
      "loss": 0.0008,
      "step": 121620
    },
    {
      "epoch": 22114.545454545456,
      "grad_norm": 0.18649253249168396,
      "learning_rate": 1.0506516654515612e-07,
      "loss": 0.0015,
      "step": 121630
    },
    {
      "epoch": 22116.363636363636,
      "grad_norm": 0.0005613653338514268,
      "learning_rate": 1.0499381948303216e-07,
      "loss": 0.0009,
      "step": 121640
    },
    {
      "epoch": 22118.18181818182,
      "grad_norm": 0.20463287830352783,
      "learning_rate": 1.0492249381215478e-07,
      "loss": 0.0012,
      "step": 121650
    },
    {
      "epoch": 22120.0,
      "grad_norm": 0.1993258148431778,
      "learning_rate": 1.0485118953638688e-07,
      "loss": 0.001,
      "step": 121660
    },
    {
      "epoch": 22121.81818181818,
      "grad_norm": 0.2154003083705902,
      "learning_rate": 1.0477990665958975e-07,
      "loss": 0.001,
      "step": 121670
    },
    {
      "epoch": 22123.636363636364,
      "grad_norm": 0.2056608647108078,
      "learning_rate": 1.047086451856235e-07,
      "loss": 0.001,
      "step": 121680
    },
    {
      "epoch": 22125.454545454544,
      "grad_norm": 0.0005345348617993295,
      "learning_rate": 1.0463740511834756e-07,
      "loss": 0.0012,
      "step": 121690
    },
    {
      "epoch": 22127.272727272728,
      "grad_norm": 0.0005398115026764572,
      "learning_rate": 1.0456618646161952e-07,
      "loss": 0.0009,
      "step": 121700
    },
    {
      "epoch": 22129.090909090908,
      "grad_norm": 0.0011234720004722476,
      "learning_rate": 1.0449498921929667e-07,
      "loss": 0.0012,
      "step": 121710
    },
    {
      "epoch": 22130.909090909092,
      "grad_norm": 0.0004644299333449453,
      "learning_rate": 1.0442381339523404e-07,
      "loss": 0.001,
      "step": 121720
    },
    {
      "epoch": 22132.727272727272,
      "grad_norm": 0.0005645761266350746,
      "learning_rate": 1.0435265899328638e-07,
      "loss": 0.0012,
      "step": 121730
    },
    {
      "epoch": 22134.545454545456,
      "grad_norm": 0.2288278043270111,
      "learning_rate": 1.0428152601730717e-07,
      "loss": 0.001,
      "step": 121740
    },
    {
      "epoch": 22136.363636363636,
      "grad_norm": 0.2613205909729004,
      "learning_rate": 1.0421041447114836e-07,
      "loss": 0.0015,
      "step": 121750
    },
    {
      "epoch": 22138.18181818182,
      "grad_norm": 0.018181994557380676,
      "learning_rate": 1.0413932435866086e-07,
      "loss": 0.0009,
      "step": 121760
    },
    {
      "epoch": 22140.0,
      "grad_norm": 0.0026561464183032513,
      "learning_rate": 1.0406825568369477e-07,
      "loss": 0.0009,
      "step": 121770
    },
    {
      "epoch": 22141.81818181818,
      "grad_norm": 0.0004240193229634315,
      "learning_rate": 1.0399720845009857e-07,
      "loss": 0.001,
      "step": 121780
    },
    {
      "epoch": 22143.636363636364,
      "grad_norm": 0.00042286052484996617,
      "learning_rate": 1.0392618266171982e-07,
      "loss": 0.001,
      "step": 121790
    },
    {
      "epoch": 22145.454545454544,
      "grad_norm": 0.000968390260823071,
      "learning_rate": 1.038551783224047e-07,
      "loss": 0.0009,
      "step": 121800
    },
    {
      "epoch": 22147.272727272728,
      "grad_norm": 0.21463444828987122,
      "learning_rate": 1.0378419543599854e-07,
      "loss": 0.0013,
      "step": 121810
    },
    {
      "epoch": 22149.090909090908,
      "grad_norm": 0.0116737624630332,
      "learning_rate": 1.0371323400634552e-07,
      "loss": 0.0012,
      "step": 121820
    },
    {
      "epoch": 22150.909090909092,
      "grad_norm": 0.0005016071372665465,
      "learning_rate": 1.036422940372883e-07,
      "loss": 0.0009,
      "step": 121830
    },
    {
      "epoch": 22152.727272727272,
      "grad_norm": 0.21476683020591736,
      "learning_rate": 1.0357137553266848e-07,
      "loss": 0.0012,
      "step": 121840
    },
    {
      "epoch": 22154.545454545456,
      "grad_norm": 0.21488450467586517,
      "learning_rate": 1.0350047849632693e-07,
      "loss": 0.001,
      "step": 121850
    },
    {
      "epoch": 22156.363636363636,
      "grad_norm": 0.0006774127250537276,
      "learning_rate": 1.0342960293210279e-07,
      "loss": 0.0009,
      "step": 121860
    },
    {
      "epoch": 22158.18181818182,
      "grad_norm": 0.21778352558612823,
      "learning_rate": 1.0335874884383416e-07,
      "loss": 0.0013,
      "step": 121870
    },
    {
      "epoch": 22160.0,
      "grad_norm": 0.001147861941717565,
      "learning_rate": 1.0328791623535838e-07,
      "loss": 0.0011,
      "step": 121880
    },
    {
      "epoch": 22161.81818181818,
      "grad_norm": 0.17872150242328644,
      "learning_rate": 1.0321710511051107e-07,
      "loss": 0.001,
      "step": 121890
    },
    {
      "epoch": 22163.636363636364,
      "grad_norm": 0.21943879127502441,
      "learning_rate": 1.0314631547312729e-07,
      "loss": 0.0013,
      "step": 121900
    },
    {
      "epoch": 22165.454545454544,
      "grad_norm": 0.1660645753145218,
      "learning_rate": 1.0307554732704016e-07,
      "loss": 0.0008,
      "step": 121910
    },
    {
      "epoch": 22167.272727272728,
      "grad_norm": 0.0009706072742119431,
      "learning_rate": 1.030048006760823e-07,
      "loss": 0.001,
      "step": 121920
    },
    {
      "epoch": 22169.090909090908,
      "grad_norm": 0.19942176342010498,
      "learning_rate": 1.0293407552408501e-07,
      "loss": 0.0011,
      "step": 121930
    },
    {
      "epoch": 22170.909090909092,
      "grad_norm": 0.2714356482028961,
      "learning_rate": 1.0286337187487832e-07,
      "loss": 0.0012,
      "step": 121940
    },
    {
      "epoch": 22172.727272727272,
      "grad_norm": 0.00043078724411316216,
      "learning_rate": 1.0279268973229088e-07,
      "loss": 0.0009,
      "step": 121950
    },
    {
      "epoch": 22174.545454545456,
      "grad_norm": 0.17274902760982513,
      "learning_rate": 1.0272202910015082e-07,
      "loss": 0.0011,
      "step": 121960
    },
    {
      "epoch": 22176.363636363636,
      "grad_norm": 0.2070973515510559,
      "learning_rate": 1.0265138998228451e-07,
      "loss": 0.001,
      "step": 121970
    },
    {
      "epoch": 22178.18181818182,
      "grad_norm": 0.0003928231308236718,
      "learning_rate": 1.0258077238251733e-07,
      "loss": 0.001,
      "step": 121980
    },
    {
      "epoch": 22180.0,
      "grad_norm": 0.21367646753787994,
      "learning_rate": 1.0251017630467346e-07,
      "loss": 0.0012,
      "step": 121990
    },
    {
      "epoch": 22181.81818181818,
      "grad_norm": 0.20015168190002441,
      "learning_rate": 1.0243960175257605e-07,
      "loss": 0.0011,
      "step": 122000
    },
    {
      "epoch": 22181.81818181818,
      "eval_loss": 5.110434055328369,
      "eval_runtime": 0.95,
      "eval_samples_per_second": 10.526,
      "eval_steps_per_second": 5.263,
      "step": 122000
    },
    {
      "epoch": 22183.636363636364,
      "grad_norm": 0.0003993949794676155,
      "learning_rate": 1.023690487300472e-07,
      "loss": 0.0008,
      "step": 122010
    },
    {
      "epoch": 22185.454545454544,
      "grad_norm": 0.26273074746131897,
      "learning_rate": 1.0229851724090744e-07,
      "loss": 0.0014,
      "step": 122020
    },
    {
      "epoch": 22187.272727272728,
      "grad_norm": 0.16745123267173767,
      "learning_rate": 1.0222800728897624e-07,
      "loss": 0.0013,
      "step": 122030
    },
    {
      "epoch": 22189.090909090908,
      "grad_norm": 0.2125242054462433,
      "learning_rate": 1.0215751887807228e-07,
      "loss": 0.0011,
      "step": 122040
    },
    {
      "epoch": 22190.909090909092,
      "grad_norm": 0.1689523309469223,
      "learning_rate": 1.0208705201201274e-07,
      "loss": 0.001,
      "step": 122050
    },
    {
      "epoch": 22192.727272727272,
      "grad_norm": 0.0005229843081906438,
      "learning_rate": 1.020166066946136e-07,
      "loss": 0.001,
      "step": 122060
    },
    {
      "epoch": 22194.545454545456,
      "grad_norm": 0.0004946653498336673,
      "learning_rate": 1.0194618292968971e-07,
      "loss": 0.0011,
      "step": 122070
    },
    {
      "epoch": 22196.363636363636,
      "grad_norm": 0.0004793007974512875,
      "learning_rate": 1.0187578072105485e-07,
      "loss": 0.001,
      "step": 122080
    },
    {
      "epoch": 22198.18181818182,
      "grad_norm": 0.16620458662509918,
      "learning_rate": 1.0180540007252198e-07,
      "loss": 0.0012,
      "step": 122090
    },
    {
      "epoch": 22200.0,
      "grad_norm": 0.0008746329112909734,
      "learning_rate": 1.0173504098790186e-07,
      "loss": 0.001,
      "step": 122100
    },
    {
      "epoch": 22201.81818181818,
      "grad_norm": 0.19626560807228088,
      "learning_rate": 1.0166470347100514e-07,
      "loss": 0.0009,
      "step": 122110
    },
    {
      "epoch": 22203.636363636364,
      "grad_norm": 0.0005371184670366347,
      "learning_rate": 1.0159438752564087e-07,
      "loss": 0.0014,
      "step": 122120
    },
    {
      "epoch": 22205.454545454544,
      "grad_norm": 0.001428685151040554,
      "learning_rate": 1.0152409315561694e-07,
      "loss": 0.0009,
      "step": 122130
    },
    {
      "epoch": 22207.272727272728,
      "grad_norm": 0.0006563949282281101,
      "learning_rate": 1.0145382036473988e-07,
      "loss": 0.001,
      "step": 122140
    },
    {
      "epoch": 22209.090909090908,
      "grad_norm": 0.17562435567378998,
      "learning_rate": 1.0138356915681557e-07,
      "loss": 0.0013,
      "step": 122150
    },
    {
      "epoch": 22210.909090909092,
      "grad_norm": 0.19913926720619202,
      "learning_rate": 1.0131333953564824e-07,
      "loss": 0.001,
      "step": 122160
    },
    {
      "epoch": 22212.727272727272,
      "grad_norm": 0.16814525425434113,
      "learning_rate": 1.0124313150504116e-07,
      "loss": 0.0009,
      "step": 122170
    },
    {
      "epoch": 22214.545454545456,
      "grad_norm": 0.1591867059469223,
      "learning_rate": 1.011729450687962e-07,
      "loss": 0.0013,
      "step": 122180
    },
    {
      "epoch": 22216.363636363636,
      "grad_norm": 0.0005207557696849108,
      "learning_rate": 1.0110278023071445e-07,
      "loss": 0.0008,
      "step": 122190
    },
    {
      "epoch": 22218.18181818182,
      "grad_norm": 0.16946850717067719,
      "learning_rate": 1.0103263699459569e-07,
      "loss": 0.0013,
      "step": 122200
    },
    {
      "epoch": 22220.0,
      "grad_norm": 0.001085170079022646,
      "learning_rate": 1.0096251536423838e-07,
      "loss": 0.001,
      "step": 122210
    },
    {
      "epoch": 22221.81818181818,
      "grad_norm": 0.1755126714706421,
      "learning_rate": 1.0089241534343984e-07,
      "loss": 0.0012,
      "step": 122220
    },
    {
      "epoch": 22223.636363636364,
      "grad_norm": 0.0008310062112286687,
      "learning_rate": 1.008223369359964e-07,
      "loss": 0.0009,
      "step": 122230
    },
    {
      "epoch": 22225.454545454544,
      "grad_norm": 0.0007807983201928437,
      "learning_rate": 1.0075228014570308e-07,
      "loss": 0.001,
      "step": 122240
    },
    {
      "epoch": 22227.272727272728,
      "grad_norm": 0.0007345294579863548,
      "learning_rate": 1.0068224497635369e-07,
      "loss": 0.0012,
      "step": 122250
    },
    {
      "epoch": 22229.090909090908,
      "grad_norm": 0.0005108888144604862,
      "learning_rate": 1.006122314317408e-07,
      "loss": 0.001,
      "step": 122260
    },
    {
      "epoch": 22230.909090909092,
      "grad_norm": 0.010940364561975002,
      "learning_rate": 1.005422395156561e-07,
      "loss": 0.0011,
      "step": 122270
    },
    {
      "epoch": 22232.727272727272,
      "grad_norm": 0.00561909656971693,
      "learning_rate": 1.0047226923189022e-07,
      "loss": 0.0012,
      "step": 122280
    },
    {
      "epoch": 22234.545454545456,
      "grad_norm": 0.00046915371785871685,
      "learning_rate": 1.0040232058423181e-07,
      "loss": 0.0009,
      "step": 122290
    },
    {
      "epoch": 22236.363636363636,
      "grad_norm": 0.0006862173904664814,
      "learning_rate": 1.0033239357646911e-07,
      "loss": 0.001,
      "step": 122300
    },
    {
      "epoch": 22238.18181818182,
      "grad_norm": 0.2006741613149643,
      "learning_rate": 1.0026248821238914e-07,
      "loss": 0.0012,
      "step": 122310
    },
    {
      "epoch": 22240.0,
      "grad_norm": 0.20520205795764923,
      "learning_rate": 1.0019260449577738e-07,
      "loss": 0.0011,
      "step": 122320
    },
    {
      "epoch": 22241.81818181818,
      "grad_norm": 0.005603668745607138,
      "learning_rate": 1.0012274243041836e-07,
      "loss": 0.0011,
      "step": 122330
    },
    {
      "epoch": 22243.636363636364,
      "grad_norm": 0.22012728452682495,
      "learning_rate": 1.000529020200953e-07,
      "loss": 0.0014,
      "step": 122340
    },
    {
      "epoch": 22245.454545454544,
      "grad_norm": 0.17409929633140564,
      "learning_rate": 9.998308326859061e-08,
      "loss": 0.0013,
      "step": 122350
    },
    {
      "epoch": 22247.272727272728,
      "grad_norm": 0.1797320544719696,
      "learning_rate": 9.991328617968509e-08,
      "loss": 0.0007,
      "step": 122360
    },
    {
      "epoch": 22249.090909090908,
      "grad_norm": 0.1713428497314453,
      "learning_rate": 9.984351075715846e-08,
      "loss": 0.001,
      "step": 122370
    },
    {
      "epoch": 22250.909090909092,
      "grad_norm": 0.16893190145492554,
      "learning_rate": 9.977375700478946e-08,
      "loss": 0.001,
      "step": 122380
    },
    {
      "epoch": 22252.727272727272,
      "grad_norm": 0.0008336837636306882,
      "learning_rate": 9.970402492635582e-08,
      "loss": 0.0014,
      "step": 122390
    },
    {
      "epoch": 22254.545454545456,
      "grad_norm": 0.20767048001289368,
      "learning_rate": 9.963431452563331e-08,
      "loss": 0.0011,
      "step": 122400
    },
    {
      "epoch": 22256.363636363636,
      "grad_norm": 0.0006107264198362827,
      "learning_rate": 9.956462580639741e-08,
      "loss": 0.0006,
      "step": 122410
    },
    {
      "epoch": 22258.18181818182,
      "grad_norm": 0.0012161467457190156,
      "learning_rate": 9.949495877242181e-08,
      "loss": 0.0012,
      "step": 122420
    },
    {
      "epoch": 22260.0,
      "grad_norm": 0.0009435514803044498,
      "learning_rate": 9.94253134274795e-08,
      "loss": 0.0012,
      "step": 122430
    },
    {
      "epoch": 22261.81818181818,
      "grad_norm": 0.2191915512084961,
      "learning_rate": 9.935568977534203e-08,
      "loss": 0.0012,
      "step": 122440
    },
    {
      "epoch": 22263.636363636364,
      "grad_norm": 0.0005258608725853264,
      "learning_rate": 9.928608781977965e-08,
      "loss": 0.0012,
      "step": 122450
    },
    {
      "epoch": 22265.454545454544,
      "grad_norm": 0.006793239153921604,
      "learning_rate": 9.921650756456163e-08,
      "loss": 0.0009,
      "step": 122460
    },
    {
      "epoch": 22267.272727272728,
      "grad_norm": 0.0007538708741776645,
      "learning_rate": 9.914694901345644e-08,
      "loss": 0.0009,
      "step": 122470
    },
    {
      "epoch": 22269.090909090908,
      "grad_norm": 0.1789253056049347,
      "learning_rate": 9.90774121702303e-08,
      "loss": 0.0012,
      "step": 122480
    },
    {
      "epoch": 22270.909090909092,
      "grad_norm": 0.17371973395347595,
      "learning_rate": 9.900789703864931e-08,
      "loss": 0.0012,
      "step": 122490
    },
    {
      "epoch": 22272.727272727272,
      "grad_norm": 0.21791189908981323,
      "learning_rate": 9.893840362247807e-08,
      "loss": 0.0013,
      "step": 122500
    },
    {
      "epoch": 22272.727272727272,
      "eval_loss": 5.142516136169434,
      "eval_runtime": 0.9504,
      "eval_samples_per_second": 10.522,
      "eval_steps_per_second": 5.261,
      "step": 122500
    },
    {
      "epoch": 22274.545454545456,
      "grad_norm": 0.17632243037223816,
      "learning_rate": 9.886893192547984e-08,
      "loss": 0.0012,
      "step": 122510
    },
    {
      "epoch": 22276.363636363636,
      "grad_norm": 0.0006401477730832994,
      "learning_rate": 9.87994819514168e-08,
      "loss": 0.0009,
      "step": 122520
    },
    {
      "epoch": 22278.18181818182,
      "grad_norm": 0.0008631761884316802,
      "learning_rate": 9.87300537040498e-08,
      "loss": 0.001,
      "step": 122530
    },
    {
      "epoch": 22280.0,
      "grad_norm": 0.0008205870981328189,
      "learning_rate": 9.866064718713901e-08,
      "loss": 0.0012,
      "step": 122540
    },
    {
      "epoch": 22281.81818181818,
      "grad_norm": 0.0007192583288997412,
      "learning_rate": 9.859126240444282e-08,
      "loss": 0.001,
      "step": 122550
    },
    {
      "epoch": 22283.636363636364,
      "grad_norm": 0.21366283297538757,
      "learning_rate": 9.852189935971872e-08,
      "loss": 0.0012,
      "step": 122560
    },
    {
      "epoch": 22285.454545454544,
      "grad_norm": 0.0005878837546333671,
      "learning_rate": 9.845255805672303e-08,
      "loss": 0.0011,
      "step": 122570
    },
    {
      "epoch": 22287.272727272728,
      "grad_norm": 0.17712917923927307,
      "learning_rate": 9.838323849921121e-08,
      "loss": 0.001,
      "step": 122580
    },
    {
      "epoch": 22289.090909090908,
      "grad_norm": 0.00046497525181621313,
      "learning_rate": 9.831394069093663e-08,
      "loss": 0.001,
      "step": 122590
    },
    {
      "epoch": 22290.909090909092,
      "grad_norm": 0.0005337693146429956,
      "learning_rate": 9.824466463565245e-08,
      "loss": 0.001,
      "step": 122600
    },
    {
      "epoch": 22292.727272727272,
      "grad_norm": 0.000561143213417381,
      "learning_rate": 9.817541033711008e-08,
      "loss": 0.0012,
      "step": 122610
    },
    {
      "epoch": 22294.545454545456,
      "grad_norm": 0.005085504613816738,
      "learning_rate": 9.810617779906011e-08,
      "loss": 0.0011,
      "step": 122620
    },
    {
      "epoch": 22296.363636363636,
      "grad_norm": 0.17524011433124542,
      "learning_rate": 9.803696702525166e-08,
      "loss": 0.0008,
      "step": 122630
    },
    {
      "epoch": 22298.18181818182,
      "grad_norm": 0.21456876397132874,
      "learning_rate": 9.796777801943268e-08,
      "loss": 0.0012,
      "step": 122640
    },
    {
      "epoch": 22300.0,
      "grad_norm": 0.18163059651851654,
      "learning_rate": 9.789861078535011e-08,
      "loss": 0.0011,
      "step": 122650
    },
    {
      "epoch": 22301.81818181818,
      "grad_norm": 0.2752397954463959,
      "learning_rate": 9.782946532675001e-08,
      "loss": 0.0012,
      "step": 122660
    },
    {
      "epoch": 22303.636363636364,
      "grad_norm": 0.21728232502937317,
      "learning_rate": 9.776034164737629e-08,
      "loss": 0.0011,
      "step": 122670
    },
    {
      "epoch": 22305.454545454544,
      "grad_norm": 0.00047889663255773485,
      "learning_rate": 9.769123975097276e-08,
      "loss": 0.001,
      "step": 122680
    },
    {
      "epoch": 22307.272727272728,
      "grad_norm": 0.27601760625839233,
      "learning_rate": 9.762215964128123e-08,
      "loss": 0.0012,
      "step": 122690
    },
    {
      "epoch": 22309.090909090908,
      "grad_norm": 0.17365436255931854,
      "learning_rate": 9.755310132204297e-08,
      "loss": 0.001,
      "step": 122700
    },
    {
      "epoch": 22310.909090909092,
      "grad_norm": 0.0004419002798385918,
      "learning_rate": 9.748406479699766e-08,
      "loss": 0.001,
      "step": 122710
    },
    {
      "epoch": 22312.727272727272,
      "grad_norm": 0.1515365093946457,
      "learning_rate": 9.741505006988382e-08,
      "loss": 0.001,
      "step": 122720
    },
    {
      "epoch": 22314.545454545456,
      "grad_norm": 0.005631395615637302,
      "learning_rate": 9.734605714443905e-08,
      "loss": 0.001,
      "step": 122730
    },
    {
      "epoch": 22316.363636363636,
      "grad_norm": 0.0038486768025904894,
      "learning_rate": 9.72770860243996e-08,
      "loss": 0.0012,
      "step": 122740
    },
    {
      "epoch": 22318.18181818182,
      "grad_norm": 0.0005717869498766959,
      "learning_rate": 9.720813671350032e-08,
      "loss": 0.0008,
      "step": 122750
    },
    {
      "epoch": 22320.0,
      "grad_norm": 0.17344903945922852,
      "learning_rate": 9.71392092154753e-08,
      "loss": 0.0012,
      "step": 122760
    },
    {
      "epoch": 22321.81818181818,
      "grad_norm": 0.18867282569408417,
      "learning_rate": 9.707030353405743e-08,
      "loss": 0.0012,
      "step": 122770
    },
    {
      "epoch": 22323.636363636364,
      "grad_norm": 0.16663150489330292,
      "learning_rate": 9.700141967297787e-08,
      "loss": 0.0009,
      "step": 122780
    },
    {
      "epoch": 22325.454545454544,
      "grad_norm": 0.0005692550330422819,
      "learning_rate": 9.693255763596719e-08,
      "loss": 0.0013,
      "step": 122790
    },
    {
      "epoch": 22327.272727272728,
      "grad_norm": 0.20508214831352234,
      "learning_rate": 9.686371742675442e-08,
      "loss": 0.0012,
      "step": 122800
    },
    {
      "epoch": 22329.090909090908,
      "grad_norm": 0.0012220009230077267,
      "learning_rate": 9.679489904906774e-08,
      "loss": 0.0009,
      "step": 122810
    },
    {
      "epoch": 22330.909090909092,
      "grad_norm": 0.21654486656188965,
      "learning_rate": 9.672610250663387e-08,
      "loss": 0.0012,
      "step": 122820
    },
    {
      "epoch": 22332.727272727272,
      "grad_norm": 0.20558004081249237,
      "learning_rate": 9.665732780317825e-08,
      "loss": 0.0012,
      "step": 122830
    },
    {
      "epoch": 22334.545454545456,
      "grad_norm": 0.21836479008197784,
      "learning_rate": 9.65885749424255e-08,
      "loss": 0.0009,
      "step": 122840
    },
    {
      "epoch": 22336.363636363636,
      "grad_norm": 0.000570220872759819,
      "learning_rate": 9.651984392809914e-08,
      "loss": 0.0009,
      "step": 122850
    },
    {
      "epoch": 22338.18181818182,
      "grad_norm": 0.17297986149787903,
      "learning_rate": 9.645113476392064e-08,
      "loss": 0.0013,
      "step": 122860
    },
    {
      "epoch": 22340.0,
      "grad_norm": 0.0007059968775138259,
      "learning_rate": 9.63824474536114e-08,
      "loss": 0.001,
      "step": 122870
    },
    {
      "epoch": 22341.81818181818,
      "grad_norm": 0.0013544248649850488,
      "learning_rate": 9.63137820008908e-08,
      "loss": 0.0012,
      "step": 122880
    },
    {
      "epoch": 22343.636363636364,
      "grad_norm": 0.26989203691482544,
      "learning_rate": 9.624513840947763e-08,
      "loss": 0.0008,
      "step": 122890
    },
    {
      "epoch": 22345.454545454544,
      "grad_norm": 0.2042304426431656,
      "learning_rate": 9.617651668308913e-08,
      "loss": 0.0013,
      "step": 122900
    },
    {
      "epoch": 22347.272727272728,
      "grad_norm": 0.28679370880126953,
      "learning_rate": 9.610791682544123e-08,
      "loss": 0.0012,
      "step": 122910
    },
    {
      "epoch": 22349.090909090908,
      "grad_norm": 0.0009843555744737387,
      "learning_rate": 9.603933884024928e-08,
      "loss": 0.0009,
      "step": 122920
    },
    {
      "epoch": 22350.909090909092,
      "grad_norm": 0.2155369520187378,
      "learning_rate": 9.597078273122682e-08,
      "loss": 0.0012,
      "step": 122930
    },
    {
      "epoch": 22352.727272727272,
      "grad_norm": 0.1801576018333435,
      "learning_rate": 9.590224850208645e-08,
      "loss": 0.0008,
      "step": 122940
    },
    {
      "epoch": 22354.545454545456,
      "grad_norm": 0.20460781455039978,
      "learning_rate": 9.583373615653978e-08,
      "loss": 0.0014,
      "step": 122950
    },
    {
      "epoch": 22356.363636363636,
      "grad_norm": 0.000539794797077775,
      "learning_rate": 9.576524569829691e-08,
      "loss": 0.0009,
      "step": 122960
    },
    {
      "epoch": 22358.18181818182,
      "grad_norm": 0.17187686264514923,
      "learning_rate": 9.569677713106671e-08,
      "loss": 0.0011,
      "step": 122970
    },
    {
      "epoch": 22360.0,
      "grad_norm": 0.16774682700634003,
      "learning_rate": 9.562833045855745e-08,
      "loss": 0.0011,
      "step": 122980
    },
    {
      "epoch": 22361.81818181818,
      "grad_norm": 0.0006590232369489968,
      "learning_rate": 9.555990568447536e-08,
      "loss": 0.0012,
      "step": 122990
    },
    {
      "epoch": 22363.636363636364,
      "grad_norm": 0.004941231571137905,
      "learning_rate": 9.549150281252632e-08,
      "loss": 0.001,
      "step": 123000
    },
    {
      "epoch": 22363.636363636364,
      "eval_loss": 5.146514415740967,
      "eval_runtime": 0.9478,
      "eval_samples_per_second": 10.551,
      "eval_steps_per_second": 5.275,
      "step": 123000
    },
    {
      "epoch": 22365.454545454544,
      "grad_norm": 0.0023287134245038033,
      "learning_rate": 9.542312184641444e-08,
      "loss": 0.0008,
      "step": 123010
    },
    {
      "epoch": 22367.272727272728,
      "grad_norm": 0.000572109711356461,
      "learning_rate": 9.535476278984272e-08,
      "loss": 0.0012,
      "step": 123020
    },
    {
      "epoch": 22369.090909090908,
      "grad_norm": 0.0009170937119051814,
      "learning_rate": 9.52864256465134e-08,
      "loss": 0.0012,
      "step": 123030
    },
    {
      "epoch": 22370.909090909092,
      "grad_norm": 0.00044210630585439503,
      "learning_rate": 9.5218110420127e-08,
      "loss": 0.0012,
      "step": 123040
    },
    {
      "epoch": 22372.727272727272,
      "grad_norm": 0.1846400946378708,
      "learning_rate": 9.514981711438302e-08,
      "loss": 0.0012,
      "step": 123050
    },
    {
      "epoch": 22374.545454545456,
      "grad_norm": 0.21951979398727417,
      "learning_rate": 9.50815457329801e-08,
      "loss": 0.0009,
      "step": 123060
    },
    {
      "epoch": 22376.363636363636,
      "grad_norm": 0.1614106148481369,
      "learning_rate": 9.501329627961507e-08,
      "loss": 0.001,
      "step": 123070
    },
    {
      "epoch": 22378.18181818182,
      "grad_norm": 0.27866241335868835,
      "learning_rate": 9.494506875798431e-08,
      "loss": 0.0013,
      "step": 123080
    },
    {
      "epoch": 22380.0,
      "grad_norm": 0.21543873846530914,
      "learning_rate": 9.48768631717824e-08,
      "loss": 0.0009,
      "step": 123090
    },
    {
      "epoch": 22381.81818181818,
      "grad_norm": 0.0014453502371907234,
      "learning_rate": 9.480867952470284e-08,
      "loss": 0.0011,
      "step": 123100
    },
    {
      "epoch": 22383.636363636364,
      "grad_norm": 0.20770761370658875,
      "learning_rate": 9.47405178204384e-08,
      "loss": 0.001,
      "step": 123110
    },
    {
      "epoch": 22385.454545454544,
      "grad_norm": 0.2708789110183716,
      "learning_rate": 9.467237806268008e-08,
      "loss": 0.0012,
      "step": 123120
    },
    {
      "epoch": 22387.272727272728,
      "grad_norm": 0.21655501425266266,
      "learning_rate": 9.460426025511791e-08,
      "loss": 0.0012,
      "step": 123130
    },
    {
      "epoch": 22389.090909090908,
      "grad_norm": 0.1703498363494873,
      "learning_rate": 9.453616440144096e-08,
      "loss": 0.0009,
      "step": 123140
    },
    {
      "epoch": 22390.909090909092,
      "grad_norm": 0.0006206075777299702,
      "learning_rate": 9.446809050533678e-08,
      "loss": 0.0012,
      "step": 123150
    },
    {
      "epoch": 22392.727272727272,
      "grad_norm": 0.0006120550096966326,
      "learning_rate": 9.440003857049173e-08,
      "loss": 0.001,
      "step": 123160
    },
    {
      "epoch": 22394.545454545456,
      "grad_norm": 0.0006883180467411876,
      "learning_rate": 9.433200860059132e-08,
      "loss": 0.0013,
      "step": 123170
    },
    {
      "epoch": 22396.363636363636,
      "grad_norm": 0.16957339644432068,
      "learning_rate": 9.426400059931955e-08,
      "loss": 0.0009,
      "step": 123180
    },
    {
      "epoch": 22398.18181818182,
      "grad_norm": 0.0005187824135646224,
      "learning_rate": 9.419601457035942e-08,
      "loss": 0.001,
      "step": 123190
    },
    {
      "epoch": 22400.0,
      "grad_norm": 0.27738040685653687,
      "learning_rate": 9.412805051739265e-08,
      "loss": 0.0012,
      "step": 123200
    },
    {
      "epoch": 22401.81818181818,
      "grad_norm": 0.22909723222255707,
      "learning_rate": 9.406010844409956e-08,
      "loss": 0.0012,
      "step": 123210
    },
    {
      "epoch": 22403.636363636364,
      "grad_norm": 0.0004378290323074907,
      "learning_rate": 9.39921883541599e-08,
      "loss": 0.001,
      "step": 123220
    },
    {
      "epoch": 22405.454545454544,
      "grad_norm": 0.20710989832878113,
      "learning_rate": 9.392429025125154e-08,
      "loss": 0.0009,
      "step": 123230
    },
    {
      "epoch": 22407.272727272728,
      "grad_norm": 0.16293811798095703,
      "learning_rate": 9.385641413905137e-08,
      "loss": 0.0012,
      "step": 123240
    },
    {
      "epoch": 22409.090909090908,
      "grad_norm": 0.18363046646118164,
      "learning_rate": 9.378856002123547e-08,
      "loss": 0.001,
      "step": 123250
    },
    {
      "epoch": 22410.909090909092,
      "grad_norm": 0.0033655299339443445,
      "learning_rate": 9.372072790147817e-08,
      "loss": 0.0012,
      "step": 123260
    },
    {
      "epoch": 22412.727272727272,
      "grad_norm": 0.0008544324664399028,
      "learning_rate": 9.365291778345303e-08,
      "loss": 0.0009,
      "step": 123270
    },
    {
      "epoch": 22414.545454545456,
      "grad_norm": 0.2094779759645462,
      "learning_rate": 9.35851296708322e-08,
      "loss": 0.0012,
      "step": 123280
    },
    {
      "epoch": 22416.363636363636,
      "grad_norm": 0.0012547110673040152,
      "learning_rate": 9.351736356728656e-08,
      "loss": 0.0011,
      "step": 123290
    },
    {
      "epoch": 22418.18181818182,
      "grad_norm": 0.0005234958953224123,
      "learning_rate": 9.344961947648622e-08,
      "loss": 0.001,
      "step": 123300
    },
    {
      "epoch": 22420.0,
      "grad_norm": 0.1999312788248062,
      "learning_rate": 9.338189740209957e-08,
      "loss": 0.0013,
      "step": 123310
    },
    {
      "epoch": 22421.81818181818,
      "grad_norm": 0.0008795796893537045,
      "learning_rate": 9.3314197347794e-08,
      "loss": 0.001,
      "step": 123320
    },
    {
      "epoch": 22423.636363636364,
      "grad_norm": 0.269042432308197,
      "learning_rate": 9.324651931723598e-08,
      "loss": 0.0012,
      "step": 123330
    },
    {
      "epoch": 22425.454545454544,
      "grad_norm": 0.0013119907816872,
      "learning_rate": 9.317886331409047e-08,
      "loss": 0.001,
      "step": 123340
    },
    {
      "epoch": 22427.272727272728,
      "grad_norm": 0.2902248203754425,
      "learning_rate": 9.31112293420212e-08,
      "loss": 0.0012,
      "step": 123350
    },
    {
      "epoch": 22429.090909090908,
      "grad_norm": 0.17215856909751892,
      "learning_rate": 9.304361740469102e-08,
      "loss": 0.0009,
      "step": 123360
    },
    {
      "epoch": 22430.909090909092,
      "grad_norm": 0.0009208547999151051,
      "learning_rate": 9.297602750576117e-08,
      "loss": 0.0012,
      "step": 123370
    },
    {
      "epoch": 22432.727272727272,
      "grad_norm": 0.18123193085193634,
      "learning_rate": 9.290845964889222e-08,
      "loss": 0.0013,
      "step": 123380
    },
    {
      "epoch": 22434.545454545456,
      "grad_norm": 0.2167350798845291,
      "learning_rate": 9.284091383774312e-08,
      "loss": 0.0009,
      "step": 123390
    },
    {
      "epoch": 22436.363636363636,
      "grad_norm": 0.000677655276376754,
      "learning_rate": 9.277339007597157e-08,
      "loss": 0.0011,
      "step": 123400
    },
    {
      "epoch": 22438.18181818182,
      "grad_norm": 0.0011016433127224445,
      "learning_rate": 9.270588836723464e-08,
      "loss": 0.0009,
      "step": 123410
    },
    {
      "epoch": 22440.0,
      "grad_norm": 0.0005995342507958412,
      "learning_rate": 9.263840871518758e-08,
      "loss": 0.0012,
      "step": 123420
    },
    {
      "epoch": 22441.81818181818,
      "grad_norm": 0.20838959515094757,
      "learning_rate": 9.257095112348462e-08,
      "loss": 0.001,
      "step": 123430
    },
    {
      "epoch": 22443.636363636364,
      "grad_norm": 0.17241300642490387,
      "learning_rate": 9.250351559577919e-08,
      "loss": 0.0015,
      "step": 123440
    },
    {
      "epoch": 22445.454545454544,
      "grad_norm": 0.0004922440275549889,
      "learning_rate": 9.243610213572284e-08,
      "loss": 0.0009,
      "step": 123450
    },
    {
      "epoch": 22447.272727272728,
      "grad_norm": 0.004954505246132612,
      "learning_rate": 9.236871074696661e-08,
      "loss": 0.0011,
      "step": 123460
    },
    {
      "epoch": 22449.090909090908,
      "grad_norm": 0.000982998637482524,
      "learning_rate": 9.230134143315992e-08,
      "loss": 0.0009,
      "step": 123470
    },
    {
      "epoch": 22450.909090909092,
      "grad_norm": 0.17804382741451263,
      "learning_rate": 9.223399419795092e-08,
      "loss": 0.001,
      "step": 123480
    },
    {
      "epoch": 22452.727272727272,
      "grad_norm": 0.0016679614782333374,
      "learning_rate": 9.216666904498699e-08,
      "loss": 0.0013,
      "step": 123490
    },
    {
      "epoch": 22454.545454545456,
      "grad_norm": 0.22025929391384125,
      "learning_rate": 9.209936597791407e-08,
      "loss": 0.0011,
      "step": 123500
    },
    {
      "epoch": 22454.545454545456,
      "eval_loss": 5.166658401489258,
      "eval_runtime": 0.9501,
      "eval_samples_per_second": 10.525,
      "eval_steps_per_second": 5.263,
      "step": 123500
    },
    {
      "epoch": 22456.363636363636,
      "grad_norm": 0.1892269402742386,
      "learning_rate": 9.203208500037663e-08,
      "loss": 0.001,
      "step": 123510
    },
    {
      "epoch": 22458.18181818182,
      "grad_norm": 0.0010711813811212778,
      "learning_rate": 9.196482611601852e-08,
      "loss": 0.0009,
      "step": 123520
    },
    {
      "epoch": 22460.0,
      "grad_norm": 0.0009682661038823426,
      "learning_rate": 9.189758932848196e-08,
      "loss": 0.0012,
      "step": 123530
    },
    {
      "epoch": 22461.81818181818,
      "grad_norm": 0.2151629626750946,
      "learning_rate": 9.183037464140803e-08,
      "loss": 0.0009,
      "step": 123540
    },
    {
      "epoch": 22463.636363636364,
      "grad_norm": 0.20779524743556976,
      "learning_rate": 9.176318205843687e-08,
      "loss": 0.0014,
      "step": 123550
    },
    {
      "epoch": 22465.454545454544,
      "grad_norm": 0.16867780685424805,
      "learning_rate": 9.169601158320706e-08,
      "loss": 0.0013,
      "step": 123560
    },
    {
      "epoch": 22467.272727272728,
      "grad_norm": 0.0005692039849236608,
      "learning_rate": 9.16288632193563e-08,
      "loss": 0.0008,
      "step": 123570
    },
    {
      "epoch": 22469.090909090908,
      "grad_norm": 0.001429365947842598,
      "learning_rate": 9.1561736970521e-08,
      "loss": 0.0012,
      "step": 123580
    },
    {
      "epoch": 22470.909090909092,
      "grad_norm": 0.1720573604106903,
      "learning_rate": 9.149463284033604e-08,
      "loss": 0.0012,
      "step": 123590
    },
    {
      "epoch": 22472.727272727272,
      "grad_norm": 0.0006861945730634034,
      "learning_rate": 9.142755083243575e-08,
      "loss": 0.0011,
      "step": 123600
    },
    {
      "epoch": 22474.545454545456,
      "grad_norm": 0.0006012425874359906,
      "learning_rate": 9.136049095045273e-08,
      "loss": 0.0011,
      "step": 123610
    },
    {
      "epoch": 22476.363636363636,
      "grad_norm": 0.000919180631171912,
      "learning_rate": 9.129345319801846e-08,
      "loss": 0.001,
      "step": 123620
    },
    {
      "epoch": 22478.18181818182,
      "grad_norm": 0.005378056317567825,
      "learning_rate": 9.122643757876353e-08,
      "loss": 0.0015,
      "step": 123630
    },
    {
      "epoch": 22480.0,
      "grad_norm": 0.17518356442451477,
      "learning_rate": 9.115944409631687e-08,
      "loss": 0.0009,
      "step": 123640
    },
    {
      "epoch": 22481.81818181818,
      "grad_norm": 0.00409684469923377,
      "learning_rate": 9.109247275430687e-08,
      "loss": 0.001,
      "step": 123650
    },
    {
      "epoch": 22483.636363636364,
      "grad_norm": 0.0005061028641648591,
      "learning_rate": 9.102552355635979e-08,
      "loss": 0.001,
      "step": 123660
    },
    {
      "epoch": 22485.454545454544,
      "grad_norm": 0.19979935884475708,
      "learning_rate": 9.095859650610144e-08,
      "loss": 0.0011,
      "step": 123670
    },
    {
      "epoch": 22487.272727272728,
      "grad_norm": 0.21799920499324799,
      "learning_rate": 9.089169160715632e-08,
      "loss": 0.0012,
      "step": 123680
    },
    {
      "epoch": 22489.090909090908,
      "grad_norm": 0.000992216751910746,
      "learning_rate": 9.082480886314759e-08,
      "loss": 0.001,
      "step": 123690
    },
    {
      "epoch": 22490.909090909092,
      "grad_norm": 0.0006949843373149633,
      "learning_rate": 9.075794827769695e-08,
      "loss": 0.0012,
      "step": 123700
    },
    {
      "epoch": 22492.727272727272,
      "grad_norm": 0.25928977131843567,
      "learning_rate": 9.069110985442557e-08,
      "loss": 0.001,
      "step": 123710
    },
    {
      "epoch": 22494.545454545456,
      "grad_norm": 0.0005103364819660783,
      "learning_rate": 9.062429359695278e-08,
      "loss": 0.0009,
      "step": 123720
    },
    {
      "epoch": 22496.363636363636,
      "grad_norm": 0.0007913045701570809,
      "learning_rate": 9.055749950889696e-08,
      "loss": 0.0012,
      "step": 123730
    },
    {
      "epoch": 22498.18181818182,
      "grad_norm": 0.16599923372268677,
      "learning_rate": 9.049072759387549e-08,
      "loss": 0.0012,
      "step": 123740
    },
    {
      "epoch": 22500.0,
      "grad_norm": 0.0005022216937504709,
      "learning_rate": 9.042397785550404e-08,
      "loss": 0.001,
      "step": 123750
    },
    {
      "epoch": 22501.81818181818,
      "grad_norm": 0.0013811788521707058,
      "learning_rate": 9.035725029739787e-08,
      "loss": 0.0012,
      "step": 123760
    },
    {
      "epoch": 22503.636363636364,
      "grad_norm": 0.17988033592700958,
      "learning_rate": 9.029054492317001e-08,
      "loss": 0.0011,
      "step": 123770
    },
    {
      "epoch": 22505.454545454544,
      "grad_norm": 0.1719837635755539,
      "learning_rate": 9.022386173643304e-08,
      "loss": 0.001,
      "step": 123780
    },
    {
      "epoch": 22507.272727272728,
      "grad_norm": 0.2604747712612152,
      "learning_rate": 9.015720074079836e-08,
      "loss": 0.0013,
      "step": 123790
    },
    {
      "epoch": 22509.090909090908,
      "grad_norm": 0.0005549321649596095,
      "learning_rate": 9.009056193987569e-08,
      "loss": 0.0009,
      "step": 123800
    },
    {
      "epoch": 22510.909090909092,
      "grad_norm": 0.000851071032229811,
      "learning_rate": 9.002394533727381e-08,
      "loss": 0.0009,
      "step": 123810
    },
    {
      "epoch": 22512.727272727272,
      "grad_norm": 0.2124805599451065,
      "learning_rate": 8.995735093660045e-08,
      "loss": 0.0012,
      "step": 123820
    },
    {
      "epoch": 22514.545454545456,
      "grad_norm": 0.001432337099686265,
      "learning_rate": 8.989077874146179e-08,
      "loss": 0.0012,
      "step": 123830
    },
    {
      "epoch": 22516.363636363636,
      "grad_norm": 0.16537411510944366,
      "learning_rate": 8.98242287554633e-08,
      "loss": 0.0012,
      "step": 123840
    },
    {
      "epoch": 22518.18181818182,
      "grad_norm": 0.0005717157036997378,
      "learning_rate": 8.975770098220847e-08,
      "loss": 0.001,
      "step": 123850
    },
    {
      "epoch": 22520.0,
      "grad_norm": 0.0010768574429675937,
      "learning_rate": 8.969119542530035e-08,
      "loss": 0.0012,
      "step": 123860
    },
    {
      "epoch": 22521.81818181818,
      "grad_norm": 0.2661675810813904,
      "learning_rate": 8.962471208834054e-08,
      "loss": 0.0012,
      "step": 123870
    },
    {
      "epoch": 22523.636363636364,
      "grad_norm": 0.17090342938899994,
      "learning_rate": 8.955825097492936e-08,
      "loss": 0.001,
      "step": 123880
    },
    {
      "epoch": 22525.454545454544,
      "grad_norm": 0.0014648443320766091,
      "learning_rate": 8.949181208866579e-08,
      "loss": 0.0009,
      "step": 123890
    },
    {
      "epoch": 22527.272727272728,
      "grad_norm": 0.0004805967619176954,
      "learning_rate": 8.942539543314798e-08,
      "loss": 0.0011,
      "step": 123900
    },
    {
      "epoch": 22529.090909090908,
      "grad_norm": 0.0005666262586601079,
      "learning_rate": 8.935900101197263e-08,
      "loss": 0.001,
      "step": 123910
    },
    {
      "epoch": 22530.909090909092,
      "grad_norm": 0.000619089521933347,
      "learning_rate": 8.929262882873523e-08,
      "loss": 0.0009,
      "step": 123920
    },
    {
      "epoch": 22532.727272727272,
      "grad_norm": 0.28137433528900146,
      "learning_rate": 8.922627888703e-08,
      "loss": 0.0015,
      "step": 123930
    },
    {
      "epoch": 22534.545454545456,
      "grad_norm": 0.0006378476973623037,
      "learning_rate": 8.915995119045016e-08,
      "loss": 0.0008,
      "step": 123940
    },
    {
      "epoch": 22536.363636363636,
      "grad_norm": 0.0010325696785002947,
      "learning_rate": 8.909364574258793e-08,
      "loss": 0.0011,
      "step": 123950
    },
    {
      "epoch": 22538.18181818182,
      "grad_norm": 0.0010261188726872206,
      "learning_rate": 8.902736254703347e-08,
      "loss": 0.001,
      "step": 123960
    },
    {
      "epoch": 22540.0,
      "grad_norm": 0.1695988029241562,
      "learning_rate": 8.896110160737663e-08,
      "loss": 0.0012,
      "step": 123970
    },
    {
      "epoch": 22541.81818181818,
      "grad_norm": 0.2127104252576828,
      "learning_rate": 8.889486292720577e-08,
      "loss": 0.0012,
      "step": 123980
    },
    {
      "epoch": 22543.636363636364,
      "grad_norm": 0.16384944319725037,
      "learning_rate": 8.882864651010796e-08,
      "loss": 0.0009,
      "step": 123990
    },
    {
      "epoch": 22545.454545454544,
      "grad_norm": 0.000400951539631933,
      "learning_rate": 8.876245235966883e-08,
      "loss": 0.001,
      "step": 124000
    },
    {
      "epoch": 22545.454545454544,
      "eval_loss": 5.186188697814941,
      "eval_runtime": 0.9464,
      "eval_samples_per_second": 10.567,
      "eval_steps_per_second": 5.283,
      "step": 124000
    },
    {
      "epoch": 22547.272727272728,
      "grad_norm": 0.16980542242527008,
      "learning_rate": 8.869628047947347e-08,
      "loss": 0.0012,
      "step": 124010
    },
    {
      "epoch": 22549.090909090908,
      "grad_norm": 0.17017285525798798,
      "learning_rate": 8.863013087310501e-08,
      "loss": 0.001,
      "step": 124020
    },
    {
      "epoch": 22550.909090909092,
      "grad_norm": 0.000525460229255259,
      "learning_rate": 8.856400354414611e-08,
      "loss": 0.001,
      "step": 124030
    },
    {
      "epoch": 22552.727272727272,
      "grad_norm": 0.16883127391338348,
      "learning_rate": 8.849789849617745e-08,
      "loss": 0.0013,
      "step": 124040
    },
    {
      "epoch": 22554.545454545456,
      "grad_norm": 0.20544955134391785,
      "learning_rate": 8.843181573277903e-08,
      "loss": 0.001,
      "step": 124050
    },
    {
      "epoch": 22556.363636363636,
      "grad_norm": 0.2171415388584137,
      "learning_rate": 8.836575525752964e-08,
      "loss": 0.0009,
      "step": 124060
    },
    {
      "epoch": 22558.18181818182,
      "grad_norm": 0.0005351332947611809,
      "learning_rate": 8.829971707400668e-08,
      "loss": 0.001,
      "step": 124070
    },
    {
      "epoch": 22560.0,
      "grad_norm": 0.2134477198123932,
      "learning_rate": 8.823370118578627e-08,
      "loss": 0.0012,
      "step": 124080
    },
    {
      "epoch": 22561.81818181818,
      "grad_norm": 0.0019397857831791043,
      "learning_rate": 8.816770759644359e-08,
      "loss": 0.0009,
      "step": 124090
    },
    {
      "epoch": 22563.636363636364,
      "grad_norm": 0.2735864520072937,
      "learning_rate": 8.810173630955248e-08,
      "loss": 0.0013,
      "step": 124100
    },
    {
      "epoch": 22565.454545454544,
      "grad_norm": 0.3019349277019501,
      "learning_rate": 8.803578732868543e-08,
      "loss": 0.0012,
      "step": 124110
    },
    {
      "epoch": 22567.272727272728,
      "grad_norm": 0.0012212255969643593,
      "learning_rate": 8.796986065741386e-08,
      "loss": 0.0007,
      "step": 124120
    },
    {
      "epoch": 22569.090909090908,
      "grad_norm": 0.0006748984451405704,
      "learning_rate": 8.790395629930803e-08,
      "loss": 0.0011,
      "step": 124130
    },
    {
      "epoch": 22570.909090909092,
      "grad_norm": 0.0023636710830032825,
      "learning_rate": 8.783807425793722e-08,
      "loss": 0.0011,
      "step": 124140
    },
    {
      "epoch": 22572.727272727272,
      "grad_norm": 0.2629846930503845,
      "learning_rate": 8.777221453686867e-08,
      "loss": 0.0011,
      "step": 124150
    },
    {
      "epoch": 22574.545454545456,
      "grad_norm": 0.2045956552028656,
      "learning_rate": 8.770637713966922e-08,
      "loss": 0.001,
      "step": 124160
    },
    {
      "epoch": 22576.363636363636,
      "grad_norm": 0.21666261553764343,
      "learning_rate": 8.764056206990445e-08,
      "loss": 0.001,
      "step": 124170
    },
    {
      "epoch": 22578.18181818182,
      "grad_norm": 0.29761484265327454,
      "learning_rate": 8.757476933113828e-08,
      "loss": 0.0013,
      "step": 124180
    },
    {
      "epoch": 22580.0,
      "grad_norm": 0.0010098398197442293,
      "learning_rate": 8.750899892693375e-08,
      "loss": 0.001,
      "step": 124190
    },
    {
      "epoch": 22581.81818181818,
      "grad_norm": 0.1680925339460373,
      "learning_rate": 8.744325086085247e-08,
      "loss": 0.0011,
      "step": 124200
    },
    {
      "epoch": 22583.636363636364,
      "grad_norm": 0.0006601439090445638,
      "learning_rate": 8.737752513645502e-08,
      "loss": 0.001,
      "step": 124210
    },
    {
      "epoch": 22585.454545454544,
      "grad_norm": 0.010588487610220909,
      "learning_rate": 8.731182175730106e-08,
      "loss": 0.001,
      "step": 124220
    },
    {
      "epoch": 22587.272727272728,
      "grad_norm": 0.21525061130523682,
      "learning_rate": 8.724614072694819e-08,
      "loss": 0.0012,
      "step": 124230
    },
    {
      "epoch": 22589.090909090908,
      "grad_norm": 0.20499897003173828,
      "learning_rate": 8.718048204895351e-08,
      "loss": 0.0012,
      "step": 124240
    },
    {
      "epoch": 22590.909090909092,
      "grad_norm": 0.0010464910883456469,
      "learning_rate": 8.711484572687295e-08,
      "loss": 0.0012,
      "step": 124250
    },
    {
      "epoch": 22592.727272727272,
      "grad_norm": 0.29184553027153015,
      "learning_rate": 8.704923176426071e-08,
      "loss": 0.0009,
      "step": 124260
    },
    {
      "epoch": 22594.545454545456,
      "grad_norm": 0.26978784799575806,
      "learning_rate": 8.698364016467024e-08,
      "loss": 0.0012,
      "step": 124270
    },
    {
      "epoch": 22596.363636363636,
      "grad_norm": 0.20691150426864624,
      "learning_rate": 8.691807093165333e-08,
      "loss": 0.001,
      "step": 124280
    },
    {
      "epoch": 22598.18181818182,
      "grad_norm": 0.21658000349998474,
      "learning_rate": 8.685252406876114e-08,
      "loss": 0.0012,
      "step": 124290
    },
    {
      "epoch": 22600.0,
      "grad_norm": 0.21681161224842072,
      "learning_rate": 8.678699957954322e-08,
      "loss": 0.0011,
      "step": 124300
    },
    {
      "epoch": 22601.81818181818,
      "grad_norm": 0.1817639321088791,
      "learning_rate": 8.672149746754775e-08,
      "loss": 0.0011,
      "step": 124310
    },
    {
      "epoch": 22603.636363636364,
      "grad_norm": 0.20647670328617096,
      "learning_rate": 8.665601773632225e-08,
      "loss": 0.001,
      "step": 124320
    },
    {
      "epoch": 22605.454545454544,
      "grad_norm": 0.0006631318246945739,
      "learning_rate": 8.659056038941281e-08,
      "loss": 0.001,
      "step": 124330
    },
    {
      "epoch": 22607.272727272728,
      "grad_norm": 0.2710528075695038,
      "learning_rate": 8.652512543036377e-08,
      "loss": 0.0013,
      "step": 124340
    },
    {
      "epoch": 22609.090909090908,
      "grad_norm": 0.0005511222989298403,
      "learning_rate": 8.645971286271903e-08,
      "loss": 0.0009,
      "step": 124350
    },
    {
      "epoch": 22610.909090909092,
      "grad_norm": 0.27952757477760315,
      "learning_rate": 8.639432269002101e-08,
      "loss": 0.0012,
      "step": 124360
    },
    {
      "epoch": 22612.727272727272,
      "grad_norm": 0.0004350266535766423,
      "learning_rate": 8.632895491581071e-08,
      "loss": 0.001,
      "step": 124370
    },
    {
      "epoch": 22614.545454545456,
      "grad_norm": 0.000839590560644865,
      "learning_rate": 8.626360954362816e-08,
      "loss": 0.001,
      "step": 124380
    },
    {
      "epoch": 22616.363636363636,
      "grad_norm": 0.21742214262485504,
      "learning_rate": 8.619828657701189e-08,
      "loss": 0.001,
      "step": 124390
    },
    {
      "epoch": 22618.18181818182,
      "grad_norm": 0.2671022117137909,
      "learning_rate": 8.61329860194997e-08,
      "loss": 0.0013,
      "step": 124400
    },
    {
      "epoch": 22620.0,
      "grad_norm": 0.001906037563458085,
      "learning_rate": 8.606770787462775e-08,
      "loss": 0.0009,
      "step": 124410
    },
    {
      "epoch": 22621.81818181818,
      "grad_norm": 0.2770311236381531,
      "learning_rate": 8.6002452145931e-08,
      "loss": 0.0012,
      "step": 124420
    },
    {
      "epoch": 22623.636363636364,
      "grad_norm": 0.16282841563224792,
      "learning_rate": 8.59372188369435e-08,
      "loss": 0.001,
      "step": 124430
    },
    {
      "epoch": 22625.454545454544,
      "grad_norm": 0.0009546020883135498,
      "learning_rate": 8.587200795119792e-08,
      "loss": 0.0013,
      "step": 124440
    },
    {
      "epoch": 22627.272727272728,
      "grad_norm": 0.0007121281232684851,
      "learning_rate": 8.580681949222568e-08,
      "loss": 0.0009,
      "step": 124450
    },
    {
      "epoch": 22629.090909090908,
      "grad_norm": 0.0005787465488538146,
      "learning_rate": 8.574165346355694e-08,
      "loss": 0.0012,
      "step": 124460
    },
    {
      "epoch": 22630.909090909092,
      "grad_norm": 0.26591920852661133,
      "learning_rate": 8.56765098687206e-08,
      "loss": 0.0012,
      "step": 124470
    },
    {
      "epoch": 22632.727272727272,
      "grad_norm": 0.0010828366503119469,
      "learning_rate": 8.56113887112448e-08,
      "loss": 0.001,
      "step": 124480
    },
    {
      "epoch": 22634.545454545456,
      "grad_norm": 0.17890359461307526,
      "learning_rate": 8.554628999465591e-08,
      "loss": 0.0014,
      "step": 124490
    },
    {
      "epoch": 22636.363636363636,
      "grad_norm": 0.17305561900138855,
      "learning_rate": 8.548121372247919e-08,
      "loss": 0.0007,
      "step": 124500
    },
    {
      "epoch": 22636.363636363636,
      "eval_loss": 5.176058769226074,
      "eval_runtime": 0.9542,
      "eval_samples_per_second": 10.48,
      "eval_steps_per_second": 5.24,
      "step": 124500
    },
    {
      "epoch": 22638.18181818182,
      "grad_norm": 0.01114446111023426,
      "learning_rate": 8.541615989823892e-08,
      "loss": 0.0014,
      "step": 124510
    },
    {
      "epoch": 22640.0,
      "grad_norm": 0.21882973611354828,
      "learning_rate": 8.535112852545829e-08,
      "loss": 0.0009,
      "step": 124520
    },
    {
      "epoch": 22641.81818181818,
      "grad_norm": 0.17354953289031982,
      "learning_rate": 8.528611960765852e-08,
      "loss": 0.0011,
      "step": 124530
    },
    {
      "epoch": 22643.636363636364,
      "grad_norm": 0.0005130885983817279,
      "learning_rate": 8.522113314836049e-08,
      "loss": 0.0012,
      "step": 124540
    },
    {
      "epoch": 22645.454545454544,
      "grad_norm": 0.0007052047294564545,
      "learning_rate": 8.515616915108326e-08,
      "loss": 0.0008,
      "step": 124550
    },
    {
      "epoch": 22647.272727272728,
      "grad_norm": 0.0009484121692366898,
      "learning_rate": 8.509122761934518e-08,
      "loss": 0.0013,
      "step": 124560
    },
    {
      "epoch": 22649.090909090908,
      "grad_norm": 0.28106358647346497,
      "learning_rate": 8.502630855666287e-08,
      "loss": 0.0015,
      "step": 124570
    },
    {
      "epoch": 22650.909090909092,
      "grad_norm": 0.172156423330307,
      "learning_rate": 8.4961411966552e-08,
      "loss": 0.0009,
      "step": 124580
    },
    {
      "epoch": 22652.727272727272,
      "grad_norm": 0.0004202748532406986,
      "learning_rate": 8.489653785252709e-08,
      "loss": 0.001,
      "step": 124590
    },
    {
      "epoch": 22654.545454545456,
      "grad_norm": 0.2164231389760971,
      "learning_rate": 8.483168621810132e-08,
      "loss": 0.0011,
      "step": 124600
    },
    {
      "epoch": 22656.363636363636,
      "grad_norm": 0.0017972220666706562,
      "learning_rate": 8.476685706678654e-08,
      "loss": 0.0011,
      "step": 124610
    },
    {
      "epoch": 22658.18181818182,
      "grad_norm": 0.21463888883590698,
      "learning_rate": 8.47020504020936e-08,
      "loss": 0.001,
      "step": 124620
    },
    {
      "epoch": 22660.0,
      "grad_norm": 0.001214682124555111,
      "learning_rate": 8.463726622753226e-08,
      "loss": 0.0011,
      "step": 124630
    },
    {
      "epoch": 22661.81818181818,
      "grad_norm": 0.0005640042363665998,
      "learning_rate": 8.457250454661064e-08,
      "loss": 0.0012,
      "step": 124640
    },
    {
      "epoch": 22663.636363636364,
      "grad_norm": 0.17051099240779877,
      "learning_rate": 8.450776536283593e-08,
      "loss": 0.0012,
      "step": 124650
    },
    {
      "epoch": 22665.454545454544,
      "grad_norm": 0.17100489139556885,
      "learning_rate": 8.444304867971386e-08,
      "loss": 0.0009,
      "step": 124660
    },
    {
      "epoch": 22667.272727272728,
      "grad_norm": 0.2128058224916458,
      "learning_rate": 8.437835450074938e-08,
      "loss": 0.001,
      "step": 124670
    },
    {
      "epoch": 22669.090909090908,
      "grad_norm": 0.20361003279685974,
      "learning_rate": 8.431368282944584e-08,
      "loss": 0.0012,
      "step": 124680
    },
    {
      "epoch": 22670.909090909092,
      "grad_norm": 0.21558307111263275,
      "learning_rate": 8.424903366930531e-08,
      "loss": 0.001,
      "step": 124690
    },
    {
      "epoch": 22672.727272727272,
      "grad_norm": 0.2071574628353119,
      "learning_rate": 8.418440702382896e-08,
      "loss": 0.0009,
      "step": 124700
    },
    {
      "epoch": 22674.545454545456,
      "grad_norm": 0.2794772684574127,
      "learning_rate": 8.411980289651687e-08,
      "loss": 0.0014,
      "step": 124710
    },
    {
      "epoch": 22676.363636363636,
      "grad_norm": 0.0006161399651318789,
      "learning_rate": 8.405522129086712e-08,
      "loss": 0.0007,
      "step": 124720
    },
    {
      "epoch": 22678.18181818182,
      "grad_norm": 0.0011491833720356226,
      "learning_rate": 8.399066221037743e-08,
      "loss": 0.0012,
      "step": 124730
    },
    {
      "epoch": 22680.0,
      "grad_norm": 0.2054523378610611,
      "learning_rate": 8.392612565854373e-08,
      "loss": 0.0012,
      "step": 124740
    },
    {
      "epoch": 22681.81818181818,
      "grad_norm": 0.0005686604417860508,
      "learning_rate": 8.386161163886119e-08,
      "loss": 0.0012,
      "step": 124750
    },
    {
      "epoch": 22683.636363636364,
      "grad_norm": 0.1745457947254181,
      "learning_rate": 8.379712015482332e-08,
      "loss": 0.0009,
      "step": 124760
    },
    {
      "epoch": 22685.454545454544,
      "grad_norm": 0.16837193071842194,
      "learning_rate": 8.373265120992251e-08,
      "loss": 0.001,
      "step": 124770
    },
    {
      "epoch": 22687.272727272728,
      "grad_norm": 0.16980372369289398,
      "learning_rate": 8.366820480765035e-08,
      "loss": 0.0012,
      "step": 124780
    },
    {
      "epoch": 22689.090909090908,
      "grad_norm": 0.0005639246082864702,
      "learning_rate": 8.360378095149672e-08,
      "loss": 0.001,
      "step": 124790
    },
    {
      "epoch": 22690.909090909092,
      "grad_norm": 0.22829033434391022,
      "learning_rate": 8.353937964495028e-08,
      "loss": 0.0012,
      "step": 124800
    },
    {
      "epoch": 22692.727272727272,
      "grad_norm": 0.21750597655773163,
      "learning_rate": 8.347500089149889e-08,
      "loss": 0.001,
      "step": 124810
    },
    {
      "epoch": 22694.545454545456,
      "grad_norm": 0.2721765637397766,
      "learning_rate": 8.341064469462872e-08,
      "loss": 0.001,
      "step": 124820
    },
    {
      "epoch": 22696.363636363636,
      "grad_norm": 0.2186388224363327,
      "learning_rate": 8.334631105782514e-08,
      "loss": 0.0012,
      "step": 124830
    },
    {
      "epoch": 22698.18181818182,
      "grad_norm": 0.21439716219902039,
      "learning_rate": 8.328199998457197e-08,
      "loss": 0.0011,
      "step": 124840
    },
    {
      "epoch": 22700.0,
      "grad_norm": 0.0007055545574985445,
      "learning_rate": 8.321771147835182e-08,
      "loss": 0.001,
      "step": 124850
    },
    {
      "epoch": 22701.81818181818,
      "grad_norm": 0.2589975595474243,
      "learning_rate": 8.315344554264642e-08,
      "loss": 0.001,
      "step": 124860
    },
    {
      "epoch": 22703.636363636364,
      "grad_norm": 0.20382346212863922,
      "learning_rate": 8.30892021809359e-08,
      "loss": 0.001,
      "step": 124870
    },
    {
      "epoch": 22705.454545454544,
      "grad_norm": 0.0010497078765183687,
      "learning_rate": 8.302498139669923e-08,
      "loss": 0.0012,
      "step": 124880
    },
    {
      "epoch": 22707.272727272728,
      "grad_norm": 0.0008845081320032477,
      "learning_rate": 8.296078319341443e-08,
      "loss": 0.0008,
      "step": 124890
    },
    {
      "epoch": 22709.090909090908,
      "grad_norm": 0.1832876354455948,
      "learning_rate": 8.289660757455802e-08,
      "loss": 0.0012,
      "step": 124900
    },
    {
      "epoch": 22710.909090909092,
      "grad_norm": 0.1668771207332611,
      "learning_rate": 8.283245454360527e-08,
      "loss": 0.0011,
      "step": 124910
    },
    {
      "epoch": 22712.727272727272,
      "grad_norm": 0.26846054196357727,
      "learning_rate": 8.276832410403051e-08,
      "loss": 0.0012,
      "step": 124920
    },
    {
      "epoch": 22714.545454545456,
      "grad_norm": 0.2165103703737259,
      "learning_rate": 8.270421625930646e-08,
      "loss": 0.0007,
      "step": 124930
    },
    {
      "epoch": 22716.363636363636,
      "grad_norm": 0.0007336502894759178,
      "learning_rate": 8.264013101290512e-08,
      "loss": 0.0013,
      "step": 124940
    },
    {
      "epoch": 22718.18181818182,
      "grad_norm": 0.0012518150033429265,
      "learning_rate": 8.257606836829677e-08,
      "loss": 0.0009,
      "step": 124950
    },
    {
      "epoch": 22720.0,
      "grad_norm": 0.16188228130340576,
      "learning_rate": 8.251202832895066e-08,
      "loss": 0.0012,
      "step": 124960
    },
    {
      "epoch": 22721.81818181818,
      "grad_norm": 0.00648494902998209,
      "learning_rate": 8.244801089833497e-08,
      "loss": 0.0012,
      "step": 124970
    },
    {
      "epoch": 22723.636363636364,
      "grad_norm": 0.17908677458763123,
      "learning_rate": 8.238401607991646e-08,
      "loss": 0.0012,
      "step": 124980
    },
    {
      "epoch": 22725.454545454544,
      "grad_norm": 0.27680522203445435,
      "learning_rate": 8.232004387716052e-08,
      "loss": 0.0009,
      "step": 124990
    },
    {
      "epoch": 22727.272727272728,
      "grad_norm": 0.26201513409614563,
      "learning_rate": 8.225609429353186e-08,
      "loss": 0.0011,
      "step": 125000
    },
    {
      "epoch": 22727.272727272728,
      "eval_loss": 5.2057085037231445,
      "eval_runtime": 0.9494,
      "eval_samples_per_second": 10.533,
      "eval_steps_per_second": 5.266,
      "step": 125000
    },
    {
      "epoch": 22729.090909090908,
      "grad_norm": 0.0006880441796965897,
      "learning_rate": 8.219216733249329e-08,
      "loss": 0.0009,
      "step": 125010
    },
    {
      "epoch": 22730.909090909092,
      "grad_norm": 0.010840094648301601,
      "learning_rate": 8.212826299750696e-08,
      "loss": 0.001,
      "step": 125020
    },
    {
      "epoch": 22732.727272727272,
      "grad_norm": 0.0006908640498295426,
      "learning_rate": 8.206438129203353e-08,
      "loss": 0.0012,
      "step": 125030
    },
    {
      "epoch": 22734.545454545456,
      "grad_norm": 0.0003321756958030164,
      "learning_rate": 8.20005222195323e-08,
      "loss": 0.0011,
      "step": 125040
    },
    {
      "epoch": 22736.363636363636,
      "grad_norm": 0.17339392006397247,
      "learning_rate": 8.193668578346169e-08,
      "loss": 0.0009,
      "step": 125050
    },
    {
      "epoch": 22738.18181818182,
      "grad_norm": 0.17703577876091003,
      "learning_rate": 8.187287198727866e-08,
      "loss": 0.0011,
      "step": 125060
    },
    {
      "epoch": 22740.0,
      "grad_norm": 0.0006842771545052528,
      "learning_rate": 8.180908083443883e-08,
      "loss": 0.0011,
      "step": 125070
    },
    {
      "epoch": 22741.81818181818,
      "grad_norm": 0.17887331545352936,
      "learning_rate": 8.174531232839704e-08,
      "loss": 0.001,
      "step": 125080
    },
    {
      "epoch": 22743.636363636364,
      "grad_norm": 0.21575979888439178,
      "learning_rate": 8.168156647260648e-08,
      "loss": 0.0012,
      "step": 125090
    },
    {
      "epoch": 22745.454545454544,
      "grad_norm": 0.21747106313705444,
      "learning_rate": 8.161784327051919e-08,
      "loss": 0.0011,
      "step": 125100
    },
    {
      "epoch": 22747.272727272728,
      "grad_norm": 0.21717101335525513,
      "learning_rate": 8.15541427255862e-08,
      "loss": 0.001,
      "step": 125110
    },
    {
      "epoch": 22749.090909090908,
      "grad_norm": 0.28400978446006775,
      "learning_rate": 8.149046484125694e-08,
      "loss": 0.0011,
      "step": 125120
    },
    {
      "epoch": 22750.909090909092,
      "grad_norm": 0.2123764008283615,
      "learning_rate": 8.142680962098014e-08,
      "loss": 0.0012,
      "step": 125130
    },
    {
      "epoch": 22752.727272727272,
      "grad_norm": 0.2073066234588623,
      "learning_rate": 8.136317706820278e-08,
      "loss": 0.0009,
      "step": 125140
    },
    {
      "epoch": 22754.545454545456,
      "grad_norm": 0.0004951817099936306,
      "learning_rate": 8.129956718637082e-08,
      "loss": 0.0011,
      "step": 125150
    },
    {
      "epoch": 22756.363636363636,
      "grad_norm": 0.0008333638543263078,
      "learning_rate": 8.123597997892918e-08,
      "loss": 0.001,
      "step": 125160
    },
    {
      "epoch": 22758.18181818182,
      "grad_norm": 0.0004171607142779976,
      "learning_rate": 8.117241544932124e-08,
      "loss": 0.0012,
      "step": 125170
    },
    {
      "epoch": 22760.0,
      "grad_norm": 0.15438778698444366,
      "learning_rate": 8.110887360098923e-08,
      "loss": 0.0011,
      "step": 125180
    },
    {
      "epoch": 22761.81818181818,
      "grad_norm": 0.0004449900588952005,
      "learning_rate": 8.104535443737437e-08,
      "loss": 0.001,
      "step": 125190
    },
    {
      "epoch": 22763.636363636364,
      "grad_norm": 0.214758038520813,
      "learning_rate": 8.09818579619163e-08,
      "loss": 0.001,
      "step": 125200
    },
    {
      "epoch": 22765.454545454544,
      "grad_norm": 0.0005407147691585124,
      "learning_rate": 8.091838417805391e-08,
      "loss": 0.0012,
      "step": 125210
    },
    {
      "epoch": 22767.272727272728,
      "grad_norm": 0.2723425030708313,
      "learning_rate": 8.085493308922431e-08,
      "loss": 0.0014,
      "step": 125220
    },
    {
      "epoch": 22769.090909090908,
      "grad_norm": 0.03971735015511513,
      "learning_rate": 8.079150469886364e-08,
      "loss": 0.0009,
      "step": 125230
    },
    {
      "epoch": 22770.909090909092,
      "grad_norm": 0.1936606913805008,
      "learning_rate": 8.072809901040705e-08,
      "loss": 0.0008,
      "step": 125240
    },
    {
      "epoch": 22772.727272727272,
      "grad_norm": 0.16742031276226044,
      "learning_rate": 8.066471602728803e-08,
      "loss": 0.0012,
      "step": 125250
    },
    {
      "epoch": 22774.545454545456,
      "grad_norm": 0.0006025166367180645,
      "learning_rate": 8.060135575293902e-08,
      "loss": 0.001,
      "step": 125260
    },
    {
      "epoch": 22776.363636363636,
      "grad_norm": 0.0004983118851669133,
      "learning_rate": 8.053801819079137e-08,
      "loss": 0.001,
      "step": 125270
    },
    {
      "epoch": 22778.18181818182,
      "grad_norm": 0.18902207911014557,
      "learning_rate": 8.047470334427502e-08,
      "loss": 0.0013,
      "step": 125280
    },
    {
      "epoch": 22780.0,
      "grad_norm": 0.2055606096982956,
      "learning_rate": 8.041141121681866e-08,
      "loss": 0.001,
      "step": 125290
    },
    {
      "epoch": 22781.81818181818,
      "grad_norm": 0.0357782356441021,
      "learning_rate": 8.034814181184996e-08,
      "loss": 0.0012,
      "step": 125300
    },
    {
      "epoch": 22783.636363636364,
      "grad_norm": 0.29498064517974854,
      "learning_rate": 8.028489513279502e-08,
      "loss": 0.001,
      "step": 125310
    },
    {
      "epoch": 22785.454545454544,
      "grad_norm": 0.2136261910200119,
      "learning_rate": 8.022167118307921e-08,
      "loss": 0.0013,
      "step": 125320
    },
    {
      "epoch": 22787.272727272728,
      "grad_norm": 0.1800200641155243,
      "learning_rate": 8.015846996612625e-08,
      "loss": 0.0007,
      "step": 125330
    },
    {
      "epoch": 22789.090909090908,
      "grad_norm": 0.000657199474517256,
      "learning_rate": 8.009529148535853e-08,
      "loss": 0.001,
      "step": 125340
    },
    {
      "epoch": 22790.909090909092,
      "grad_norm": 0.0005725885857827961,
      "learning_rate": 8.003213574419776e-08,
      "loss": 0.0008,
      "step": 125350
    },
    {
      "epoch": 22792.727272727272,
      "grad_norm": 0.20563417673110962,
      "learning_rate": 7.996900274606395e-08,
      "loss": 0.0015,
      "step": 125360
    },
    {
      "epoch": 22794.545454545456,
      "grad_norm": 0.20827090740203857,
      "learning_rate": 7.990589249437591e-08,
      "loss": 0.001,
      "step": 125370
    },
    {
      "epoch": 22796.363636363636,
      "grad_norm": 0.0004133176407776773,
      "learning_rate": 7.984280499255147e-08,
      "loss": 0.001,
      "step": 125380
    },
    {
      "epoch": 22798.18181818182,
      "grad_norm": 0.0006016316474415362,
      "learning_rate": 7.977974024400702e-08,
      "loss": 0.001,
      "step": 125390
    },
    {
      "epoch": 22800.0,
      "grad_norm": 0.2714396119117737,
      "learning_rate": 7.971669825215787e-08,
      "loss": 0.0011,
      "step": 125400
    },
    {
      "epoch": 22801.81818181818,
      "grad_norm": 0.0005772653385065496,
      "learning_rate": 7.965367902041797e-08,
      "loss": 0.001,
      "step": 125410
    },
    {
      "epoch": 22803.636363636364,
      "grad_norm": 0.0005658093723468482,
      "learning_rate": 7.959068255219991e-08,
      "loss": 0.0012,
      "step": 125420
    },
    {
      "epoch": 22805.454545454544,
      "grad_norm": 0.22112902998924255,
      "learning_rate": 7.952770885091548e-08,
      "loss": 0.001,
      "step": 125430
    },
    {
      "epoch": 22807.272727272728,
      "grad_norm": 0.0006162848439998925,
      "learning_rate": 7.946475791997481e-08,
      "loss": 0.001,
      "step": 125440
    },
    {
      "epoch": 22809.090909090908,
      "grad_norm": 0.0009652088047005236,
      "learning_rate": 7.940182976278692e-08,
      "loss": 0.0012,
      "step": 125450
    },
    {
      "epoch": 22810.909090909092,
      "grad_norm": 0.20399650931358337,
      "learning_rate": 7.933892438275985e-08,
      "loss": 0.0012,
      "step": 125460
    },
    {
      "epoch": 22812.727272727272,
      "grad_norm": 0.21689409017562866,
      "learning_rate": 7.92760417833e-08,
      "loss": 0.001,
      "step": 125470
    },
    {
      "epoch": 22814.545454545456,
      "grad_norm": 0.16147994995117188,
      "learning_rate": 7.921318196781268e-08,
      "loss": 0.0012,
      "step": 125480
    },
    {
      "epoch": 22816.363636363636,
      "grad_norm": 0.0006372083444148302,
      "learning_rate": 7.915034493970218e-08,
      "loss": 0.001,
      "step": 125490
    },
    {
      "epoch": 22818.18181818182,
      "grad_norm": 0.000477712310384959,
      "learning_rate": 7.908753070237124e-08,
      "loss": 0.001,
      "step": 125500
    },
    {
      "epoch": 22818.18181818182,
      "eval_loss": 5.092019557952881,
      "eval_runtime": 0.9486,
      "eval_samples_per_second": 10.542,
      "eval_steps_per_second": 5.271,
      "step": 125500
    },
    {
      "epoch": 22820.0,
      "grad_norm": 0.0005167862982489169,
      "learning_rate": 7.902473925922182e-08,
      "loss": 0.0012,
      "step": 125510
    },
    {
      "epoch": 22821.81818181818,
      "grad_norm": 0.0006423876038752496,
      "learning_rate": 7.896197061365389e-08,
      "loss": 0.0012,
      "step": 125520
    },
    {
      "epoch": 22823.636363636364,
      "grad_norm": 0.0007673522341065109,
      "learning_rate": 7.889922476906685e-08,
      "loss": 0.001,
      "step": 125530
    },
    {
      "epoch": 22825.454545454544,
      "grad_norm": 0.0012699835933744907,
      "learning_rate": 7.883650172885881e-08,
      "loss": 0.001,
      "step": 125540
    },
    {
      "epoch": 22827.272727272728,
      "grad_norm": 0.0004500666109379381,
      "learning_rate": 7.877380149642626e-08,
      "loss": 0.001,
      "step": 125550
    },
    {
      "epoch": 22829.090909090908,
      "grad_norm": 0.17778295278549194,
      "learning_rate": 7.871112407516472e-08,
      "loss": 0.0013,
      "step": 125560
    },
    {
      "epoch": 22830.909090909092,
      "grad_norm": 0.2804650366306305,
      "learning_rate": 7.864846946846854e-08,
      "loss": 0.001,
      "step": 125570
    },
    {
      "epoch": 22832.727272727272,
      "grad_norm": 0.15796059370040894,
      "learning_rate": 7.858583767973071e-08,
      "loss": 0.001,
      "step": 125580
    },
    {
      "epoch": 22834.545454545456,
      "grad_norm": 0.18278683722019196,
      "learning_rate": 7.852322871234285e-08,
      "loss": 0.001,
      "step": 125590
    },
    {
      "epoch": 22836.363636363636,
      "grad_norm": 0.28365495800971985,
      "learning_rate": 7.84606425696957e-08,
      "loss": 0.0013,
      "step": 125600
    },
    {
      "epoch": 22838.18181818182,
      "grad_norm": 0.20091156661510468,
      "learning_rate": 7.839807925517833e-08,
      "loss": 0.001,
      "step": 125610
    },
    {
      "epoch": 22840.0,
      "grad_norm": 0.20746546983718872,
      "learning_rate": 7.833553877217914e-08,
      "loss": 0.001,
      "step": 125620
    },
    {
      "epoch": 22841.81818181818,
      "grad_norm": 0.0011906488798558712,
      "learning_rate": 7.827302112408469e-08,
      "loss": 0.001,
      "step": 125630
    },
    {
      "epoch": 22843.636363636364,
      "grad_norm": 0.0008361036889255047,
      "learning_rate": 7.82105263142806e-08,
      "loss": 0.0015,
      "step": 125640
    },
    {
      "epoch": 22845.454545454544,
      "grad_norm": 0.009873749688267708,
      "learning_rate": 7.814805434615135e-08,
      "loss": 0.0009,
      "step": 125650
    },
    {
      "epoch": 22847.272727272728,
      "grad_norm": 0.2755753993988037,
      "learning_rate": 7.808560522308005e-08,
      "loss": 0.0011,
      "step": 125660
    },
    {
      "epoch": 22849.090909090908,
      "grad_norm": 0.0006305922870524228,
      "learning_rate": 7.802317894844834e-08,
      "loss": 0.0009,
      "step": 125670
    },
    {
      "epoch": 22850.909090909092,
      "grad_norm": 0.1977660059928894,
      "learning_rate": 7.796077552563724e-08,
      "loss": 0.0012,
      "step": 125680
    },
    {
      "epoch": 22852.727272727272,
      "grad_norm": 0.18237969279289246,
      "learning_rate": 7.78983949580258e-08,
      "loss": 0.001,
      "step": 125690
    },
    {
      "epoch": 22854.545454545456,
      "grad_norm": 0.0006470863008871675,
      "learning_rate": 7.783603724899257e-08,
      "loss": 0.001,
      "step": 125700
    },
    {
      "epoch": 22856.363636363636,
      "grad_norm": 0.0006186947575770319,
      "learning_rate": 7.777370240191406e-08,
      "loss": 0.0009,
      "step": 125710
    },
    {
      "epoch": 22858.18181818182,
      "grad_norm": 0.0011838258942589164,
      "learning_rate": 7.771139042016617e-08,
      "loss": 0.0012,
      "step": 125720
    },
    {
      "epoch": 22860.0,
      "grad_norm": 0.2019851803779602,
      "learning_rate": 7.764910130712348e-08,
      "loss": 0.0012,
      "step": 125730
    },
    {
      "epoch": 22861.81818181818,
      "grad_norm": 0.20581063628196716,
      "learning_rate": 7.758683506615915e-08,
      "loss": 0.001,
      "step": 125740
    },
    {
      "epoch": 22863.636363636364,
      "grad_norm": 0.0006710713496431708,
      "learning_rate": 7.75245917006449e-08,
      "loss": 0.001,
      "step": 125750
    },
    {
      "epoch": 22865.454545454544,
      "grad_norm": 0.21381817758083344,
      "learning_rate": 7.746237121395183e-08,
      "loss": 0.0013,
      "step": 125760
    },
    {
      "epoch": 22867.272727272728,
      "grad_norm": 0.1978202760219574,
      "learning_rate": 7.740017360944923e-08,
      "loss": 0.0009,
      "step": 125770
    },
    {
      "epoch": 22869.090909090908,
      "grad_norm": 0.0009535022545605898,
      "learning_rate": 7.733799889050546e-08,
      "loss": 0.001,
      "step": 125780
    },
    {
      "epoch": 22870.909090909092,
      "grad_norm": 0.0004167633014731109,
      "learning_rate": 7.727584706048735e-08,
      "loss": 0.0012,
      "step": 125790
    },
    {
      "epoch": 22872.727272727272,
      "grad_norm": 0.0005465997965075076,
      "learning_rate": 7.721371812276079e-08,
      "loss": 0.0009,
      "step": 125800
    },
    {
      "epoch": 22874.545454545456,
      "grad_norm": 0.21499375998973846,
      "learning_rate": 7.715161208069054e-08,
      "loss": 0.0012,
      "step": 125810
    },
    {
      "epoch": 22876.363636363636,
      "grad_norm": 0.17589762806892395,
      "learning_rate": 7.70895289376397e-08,
      "loss": 0.0012,
      "step": 125820
    },
    {
      "epoch": 22878.18181818182,
      "grad_norm": 0.17375704646110535,
      "learning_rate": 7.702746869697019e-08,
      "loss": 0.0012,
      "step": 125830
    },
    {
      "epoch": 22880.0,
      "grad_norm": 0.0006744393613189459,
      "learning_rate": 7.696543136204319e-08,
      "loss": 0.001,
      "step": 125840
    },
    {
      "epoch": 22881.81818181818,
      "grad_norm": 0.17030446231365204,
      "learning_rate": 7.690341693621805e-08,
      "loss": 0.0009,
      "step": 125850
    },
    {
      "epoch": 22883.636363636364,
      "grad_norm": 0.20428240299224854,
      "learning_rate": 7.684142542285305e-08,
      "loss": 0.0015,
      "step": 125860
    },
    {
      "epoch": 22885.454545454544,
      "grad_norm": 0.21332086622714996,
      "learning_rate": 7.677945682530551e-08,
      "loss": 0.0007,
      "step": 125870
    },
    {
      "epoch": 22887.272727272728,
      "grad_norm": 0.17235711216926575,
      "learning_rate": 7.671751114693103e-08,
      "loss": 0.0012,
      "step": 125880
    },
    {
      "epoch": 22889.090909090908,
      "grad_norm": 0.15171900391578674,
      "learning_rate": 7.665558839108466e-08,
      "loss": 0.0012,
      "step": 125890
    },
    {
      "epoch": 22890.909090909092,
      "grad_norm": 0.0005342649528756738,
      "learning_rate": 7.659368856111925e-08,
      "loss": 0.0012,
      "step": 125900
    },
    {
      "epoch": 22892.727272727272,
      "grad_norm": 0.000545893853995949,
      "learning_rate": 7.653181166038714e-08,
      "loss": 0.0011,
      "step": 125910
    },
    {
      "epoch": 22894.545454545456,
      "grad_norm": 0.0005928975879214704,
      "learning_rate": 7.646995769223941e-08,
      "loss": 0.001,
      "step": 125920
    },
    {
      "epoch": 22896.363636363636,
      "grad_norm": 0.0006718408549204469,
      "learning_rate": 7.640812666002555e-08,
      "loss": 0.0012,
      "step": 125930
    },
    {
      "epoch": 22898.18181818182,
      "grad_norm": 0.0006218106718733907,
      "learning_rate": 7.63463185670939e-08,
      "loss": 0.001,
      "step": 125940
    },
    {
      "epoch": 22900.0,
      "grad_norm": 0.0008538403199054301,
      "learning_rate": 7.628453341679181e-08,
      "loss": 0.0012,
      "step": 125950
    },
    {
      "epoch": 22901.81818181818,
      "grad_norm": 0.19952884316444397,
      "learning_rate": 7.622277121246512e-08,
      "loss": 0.0009,
      "step": 125960
    },
    {
      "epoch": 22903.636363636364,
      "grad_norm": 0.17606976628303528,
      "learning_rate": 7.616103195745848e-08,
      "loss": 0.0013,
      "step": 125970
    },
    {
      "epoch": 22905.454545454544,
      "grad_norm": 0.0009881771402433515,
      "learning_rate": 7.609931565511524e-08,
      "loss": 0.0009,
      "step": 125980
    },
    {
      "epoch": 22907.272727272728,
      "grad_norm": 0.19449259340763092,
      "learning_rate": 7.603762230877775e-08,
      "loss": 0.0012,
      "step": 125990
    },
    {
      "epoch": 22909.090909090908,
      "grad_norm": 0.20832648873329163,
      "learning_rate": 7.597595192178702e-08,
      "loss": 0.0012,
      "step": 126000
    },
    {
      "epoch": 22909.090909090908,
      "eval_loss": 5.0987725257873535,
      "eval_runtime": 0.9509,
      "eval_samples_per_second": 10.516,
      "eval_steps_per_second": 5.258,
      "step": 126000
    },
    {
      "epoch": 22910.909090909092,
      "grad_norm": 0.0005014277994632721,
      "learning_rate": 7.591430449748265e-08,
      "loss": 0.001,
      "step": 126010
    },
    {
      "epoch": 22912.727272727272,
      "grad_norm": 0.18111881613731384,
      "learning_rate": 7.585268003920303e-08,
      "loss": 0.001,
      "step": 126020
    },
    {
      "epoch": 22914.545454545456,
      "grad_norm": 0.1809207648038864,
      "learning_rate": 7.579107855028561e-08,
      "loss": 0.001,
      "step": 126030
    },
    {
      "epoch": 22916.363636363636,
      "grad_norm": 0.2068459689617157,
      "learning_rate": 7.57295000340662e-08,
      "loss": 0.001,
      "step": 126040
    },
    {
      "epoch": 22918.18181818182,
      "grad_norm": 0.0005888956948183477,
      "learning_rate": 7.566794449387959e-08,
      "loss": 0.001,
      "step": 126050
    },
    {
      "epoch": 22920.0,
      "grad_norm": 0.21903705596923828,
      "learning_rate": 7.560641193305911e-08,
      "loss": 0.0014,
      "step": 126060
    },
    {
      "epoch": 22921.81818181818,
      "grad_norm": 0.2819335460662842,
      "learning_rate": 7.554490235493721e-08,
      "loss": 0.001,
      "step": 126070
    },
    {
      "epoch": 22923.636363636364,
      "grad_norm": 0.0007255091913975775,
      "learning_rate": 7.5483415762845e-08,
      "loss": 0.0012,
      "step": 126080
    },
    {
      "epoch": 22925.454545454544,
      "grad_norm": 0.0007684478769078851,
      "learning_rate": 7.542195216011188e-08,
      "loss": 0.0011,
      "step": 126090
    },
    {
      "epoch": 22927.272727272728,
      "grad_norm": 0.20661067962646484,
      "learning_rate": 7.536051155006657e-08,
      "loss": 0.0012,
      "step": 126100
    },
    {
      "epoch": 22929.090909090908,
      "grad_norm": 0.19554226100444794,
      "learning_rate": 7.52990939360364e-08,
      "loss": 0.001,
      "step": 126110
    },
    {
      "epoch": 22930.909090909092,
      "grad_norm": 0.28586235642433167,
      "learning_rate": 7.523769932134738e-08,
      "loss": 0.0012,
      "step": 126120
    },
    {
      "epoch": 22932.727272727272,
      "grad_norm": 0.20198141038417816,
      "learning_rate": 7.517632770932414e-08,
      "loss": 0.0009,
      "step": 126130
    },
    {
      "epoch": 22934.545454545456,
      "grad_norm": 0.2037578821182251,
      "learning_rate": 7.511497910329023e-08,
      "loss": 0.0013,
      "step": 126140
    },
    {
      "epoch": 22936.363636363636,
      "grad_norm": 0.0006231639999896288,
      "learning_rate": 7.505365350656811e-08,
      "loss": 0.0007,
      "step": 126150
    },
    {
      "epoch": 22938.18181818182,
      "grad_norm": 0.0007344134501181543,
      "learning_rate": 7.499235092247869e-08,
      "loss": 0.0012,
      "step": 126160
    },
    {
      "epoch": 22940.0,
      "grad_norm": 0.03670479729771614,
      "learning_rate": 7.493107135434168e-08,
      "loss": 0.0012,
      "step": 126170
    },
    {
      "epoch": 22941.81818181818,
      "grad_norm": 0.17993293702602386,
      "learning_rate": 7.486981480547567e-08,
      "loss": 0.001,
      "step": 126180
    },
    {
      "epoch": 22943.636363636364,
      "grad_norm": 0.0011477734660729766,
      "learning_rate": 7.48085812791982e-08,
      "loss": 0.001,
      "step": 126190
    },
    {
      "epoch": 22945.454545454544,
      "grad_norm": 0.0008926538284868002,
      "learning_rate": 7.47473707788251e-08,
      "loss": 0.0012,
      "step": 126200
    },
    {
      "epoch": 22947.272727272728,
      "grad_norm": 0.0004964153049513698,
      "learning_rate": 7.468618330767113e-08,
      "loss": 0.0012,
      "step": 126210
    },
    {
      "epoch": 22949.090909090908,
      "grad_norm": 0.0010576836066320539,
      "learning_rate": 7.462501886905004e-08,
      "loss": 0.0009,
      "step": 126220
    },
    {
      "epoch": 22950.909090909092,
      "grad_norm": 0.0005301850032992661,
      "learning_rate": 7.456387746627402e-08,
      "loss": 0.0012,
      "step": 126230
    },
    {
      "epoch": 22952.727272727272,
      "grad_norm": 0.0006719704833813012,
      "learning_rate": 7.450275910265413e-08,
      "loss": 0.0009,
      "step": 126240
    },
    {
      "epoch": 22954.545454545456,
      "grad_norm": 0.20506982505321503,
      "learning_rate": 7.444166378150013e-08,
      "loss": 0.0011,
      "step": 126250
    },
    {
      "epoch": 22956.363636363636,
      "grad_norm": 0.0005096279201097786,
      "learning_rate": 7.43805915061207e-08,
      "loss": 0.0011,
      "step": 126260
    },
    {
      "epoch": 22958.18181818182,
      "grad_norm": 0.001276384457014501,
      "learning_rate": 7.431954227982329e-08,
      "loss": 0.0009,
      "step": 126270
    },
    {
      "epoch": 22960.0,
      "grad_norm": 0.005932066589593887,
      "learning_rate": 7.42585161059136e-08,
      "loss": 0.0013,
      "step": 126280
    },
    {
      "epoch": 22961.81818181818,
      "grad_norm": 0.2049228399991989,
      "learning_rate": 7.419751298769667e-08,
      "loss": 0.0012,
      "step": 126290
    },
    {
      "epoch": 22963.636363636364,
      "grad_norm": 0.2759299874305725,
      "learning_rate": 7.413653292847616e-08,
      "loss": 0.0012,
      "step": 126300
    },
    {
      "epoch": 22965.454545454544,
      "grad_norm": 0.018018070608377457,
      "learning_rate": 7.407557593155433e-08,
      "loss": 0.0009,
      "step": 126310
    },
    {
      "epoch": 22967.272727272728,
      "grad_norm": 0.14831051230430603,
      "learning_rate": 7.401464200023228e-08,
      "loss": 0.0011,
      "step": 126320
    },
    {
      "epoch": 22969.090909090908,
      "grad_norm": 0.26241475343704224,
      "learning_rate": 7.395373113780962e-08,
      "loss": 0.0009,
      "step": 126330
    },
    {
      "epoch": 22970.909090909092,
      "grad_norm": 0.21718914806842804,
      "learning_rate": 7.389284334758517e-08,
      "loss": 0.0012,
      "step": 126340
    },
    {
      "epoch": 22972.727272727272,
      "grad_norm": 0.27601316571235657,
      "learning_rate": 7.383197863285628e-08,
      "loss": 0.0013,
      "step": 126350
    },
    {
      "epoch": 22974.545454545456,
      "grad_norm": 0.2023104429244995,
      "learning_rate": 7.377113699691879e-08,
      "loss": 0.001,
      "step": 126360
    },
    {
      "epoch": 22976.363636363636,
      "grad_norm": 0.2674206793308258,
      "learning_rate": 7.371031844306769e-08,
      "loss": 0.0012,
      "step": 126370
    },
    {
      "epoch": 22978.18181818182,
      "grad_norm": 0.20031513273715973,
      "learning_rate": 7.364952297459664e-08,
      "loss": 0.0008,
      "step": 126380
    },
    {
      "epoch": 22980.0,
      "grad_norm": 0.0012841947609558702,
      "learning_rate": 7.358875059479791e-08,
      "loss": 0.001,
      "step": 126390
    },
    {
      "epoch": 22981.81818181818,
      "grad_norm": 0.000543388887308538,
      "learning_rate": 7.352800130696252e-08,
      "loss": 0.0012,
      "step": 126400
    },
    {
      "epoch": 22983.636363636364,
      "grad_norm": 0.20021967589855194,
      "learning_rate": 7.346727511438028e-08,
      "loss": 0.0008,
      "step": 126410
    },
    {
      "epoch": 22985.454545454544,
      "grad_norm": 0.1772746741771698,
      "learning_rate": 7.34065720203399e-08,
      "loss": 0.0015,
      "step": 126420
    },
    {
      "epoch": 22987.272727272728,
      "grad_norm": 0.0066237496212124825,
      "learning_rate": 7.334589202812863e-08,
      "loss": 0.001,
      "step": 126430
    },
    {
      "epoch": 22989.090909090908,
      "grad_norm": 0.20599773526191711,
      "learning_rate": 7.32852351410324e-08,
      "loss": 0.001,
      "step": 126440
    },
    {
      "epoch": 22990.909090909092,
      "grad_norm": 0.0005479842657223344,
      "learning_rate": 7.322460136233621e-08,
      "loss": 0.0008,
      "step": 126450
    },
    {
      "epoch": 22992.727272727272,
      "grad_norm": 0.24879637360572815,
      "learning_rate": 7.316399069532386e-08,
      "loss": 0.0012,
      "step": 126460
    },
    {
      "epoch": 22994.545454545456,
      "grad_norm": 0.16562189161777496,
      "learning_rate": 7.310340314327718e-08,
      "loss": 0.001,
      "step": 126470
    },
    {
      "epoch": 22996.363636363636,
      "grad_norm": 0.17728503048419952,
      "learning_rate": 7.304283870947747e-08,
      "loss": 0.0011,
      "step": 126480
    },
    {
      "epoch": 22998.18181818182,
      "grad_norm": 0.00549459271132946,
      "learning_rate": 7.298229739720468e-08,
      "loss": 0.0013,
      "step": 126490
    },
    {
      "epoch": 23000.0,
      "grad_norm": 0.0005871432949788868,
      "learning_rate": 7.292177920973724e-08,
      "loss": 0.0009,
      "step": 126500
    },
    {
      "epoch": 23000.0,
      "eval_loss": 5.138632297515869,
      "eval_runtime": 0.9495,
      "eval_samples_per_second": 10.532,
      "eval_steps_per_second": 5.266,
      "step": 126500
    },
    {
      "epoch": 23001.81818181818,
      "grad_norm": 0.21944020688533783,
      "learning_rate": 7.286128415035248e-08,
      "loss": 0.0012,
      "step": 126510
    },
    {
      "epoch": 23003.636363636364,
      "grad_norm": 0.005994535982608795,
      "learning_rate": 7.280081222232637e-08,
      "loss": 0.001,
      "step": 126520
    },
    {
      "epoch": 23005.454545454544,
      "grad_norm": 0.011944111436605453,
      "learning_rate": 7.274036342893392e-08,
      "loss": 0.0012,
      "step": 126530
    },
    {
      "epoch": 23007.272727272728,
      "grad_norm": 0.17439217865467072,
      "learning_rate": 7.267993777344856e-08,
      "loss": 0.0009,
      "step": 126540
    },
    {
      "epoch": 23009.090909090908,
      "grad_norm": 0.0007943461532704532,
      "learning_rate": 7.26195352591425e-08,
      "loss": 0.001,
      "step": 126550
    },
    {
      "epoch": 23010.909090909092,
      "grad_norm": 0.19930501282215118,
      "learning_rate": 7.255915588928691e-08,
      "loss": 0.0012,
      "step": 126560
    },
    {
      "epoch": 23012.727272727272,
      "grad_norm": 0.00047097899368964136,
      "learning_rate": 7.249879966715173e-08,
      "loss": 0.0013,
      "step": 126570
    },
    {
      "epoch": 23014.545454545456,
      "grad_norm": 0.1701468974351883,
      "learning_rate": 7.243846659600528e-08,
      "loss": 0.0007,
      "step": 126580
    },
    {
      "epoch": 23016.363636363636,
      "grad_norm": 0.1500643789768219,
      "learning_rate": 7.237815667911502e-08,
      "loss": 0.0012,
      "step": 126590
    },
    {
      "epoch": 23018.18181818182,
      "grad_norm": 0.2928980886936188,
      "learning_rate": 7.23178699197467e-08,
      "loss": 0.0014,
      "step": 126600
    },
    {
      "epoch": 23020.0,
      "grad_norm": 0.00034949195105582476,
      "learning_rate": 7.22576063211654e-08,
      "loss": 0.0009,
      "step": 126610
    },
    {
      "epoch": 23021.81818181818,
      "grad_norm": 0.2182210087776184,
      "learning_rate": 7.219736588663455e-08,
      "loss": 0.001,
      "step": 126620
    },
    {
      "epoch": 23023.636363636364,
      "grad_norm": 0.0008013789774850011,
      "learning_rate": 7.213714861941628e-08,
      "loss": 0.0012,
      "step": 126630
    },
    {
      "epoch": 23025.454545454544,
      "grad_norm": 0.0005493455100804567,
      "learning_rate": 7.207695452277179e-08,
      "loss": 0.001,
      "step": 126640
    },
    {
      "epoch": 23027.272727272728,
      "grad_norm": 0.0014446263667196035,
      "learning_rate": 7.201678359996099e-08,
      "loss": 0.0009,
      "step": 126650
    },
    {
      "epoch": 23029.090909090908,
      "grad_norm": 0.0004485036770347506,
      "learning_rate": 7.195663585424194e-08,
      "loss": 0.0012,
      "step": 126660
    },
    {
      "epoch": 23030.909090909092,
      "grad_norm": 0.19676657021045685,
      "learning_rate": 7.189651128887231e-08,
      "loss": 0.0012,
      "step": 126670
    },
    {
      "epoch": 23032.727272727272,
      "grad_norm": 0.0013355013215914369,
      "learning_rate": 7.183640990710776e-08,
      "loss": 0.0009,
      "step": 126680
    },
    {
      "epoch": 23034.545454545456,
      "grad_norm": 0.0005674078711308539,
      "learning_rate": 7.177633171220337e-08,
      "loss": 0.0011,
      "step": 126690
    },
    {
      "epoch": 23036.363636363636,
      "grad_norm": 0.0004096602788195014,
      "learning_rate": 7.171627670741243e-08,
      "loss": 0.001,
      "step": 126700
    },
    {
      "epoch": 23038.18181818182,
      "grad_norm": 0.001217407057993114,
      "learning_rate": 7.16562448959871e-08,
      "loss": 0.001,
      "step": 126710
    },
    {
      "epoch": 23040.0,
      "grad_norm": 0.20751185715198517,
      "learning_rate": 7.159623628117855e-08,
      "loss": 0.0012,
      "step": 126720
    },
    {
      "epoch": 23041.81818181818,
      "grad_norm": 0.0006622405489906669,
      "learning_rate": 7.153625086623643e-08,
      "loss": 0.001,
      "step": 126730
    },
    {
      "epoch": 23043.636363636364,
      "grad_norm": 0.0021839190740138292,
      "learning_rate": 7.147628865440908e-08,
      "loss": 0.0012,
      "step": 126740
    },
    {
      "epoch": 23045.454545454544,
      "grad_norm": 0.0007398446323350072,
      "learning_rate": 7.141634964894388e-08,
      "loss": 0.0007,
      "step": 126750
    },
    {
      "epoch": 23047.272727272728,
      "grad_norm": 0.1739589273929596,
      "learning_rate": 7.135643385308676e-08,
      "loss": 0.0013,
      "step": 126760
    },
    {
      "epoch": 23049.090909090908,
      "grad_norm": 0.17655451595783234,
      "learning_rate": 7.129654127008222e-08,
      "loss": 0.001,
      "step": 126770
    },
    {
      "epoch": 23050.909090909092,
      "grad_norm": 0.27113234996795654,
      "learning_rate": 7.123667190317395e-08,
      "loss": 0.001,
      "step": 126780
    },
    {
      "epoch": 23052.727272727272,
      "grad_norm": 0.00063126947497949,
      "learning_rate": 7.117682575560386e-08,
      "loss": 0.0012,
      "step": 126790
    },
    {
      "epoch": 23054.545454545456,
      "grad_norm": 0.17204631865024567,
      "learning_rate": 7.111700283061317e-08,
      "loss": 0.0013,
      "step": 126800
    },
    {
      "epoch": 23056.363636363636,
      "grad_norm": 0.218398779630661,
      "learning_rate": 7.105720313144143e-08,
      "loss": 0.0011,
      "step": 126810
    },
    {
      "epoch": 23058.18181818182,
      "grad_norm": 0.1864328384399414,
      "learning_rate": 7.09974266613268e-08,
      "loss": 0.0011,
      "step": 126820
    },
    {
      "epoch": 23060.0,
      "grad_norm": 0.0006184927187860012,
      "learning_rate": 7.093767342350671e-08,
      "loss": 0.001,
      "step": 126830
    },
    {
      "epoch": 23061.81818181818,
      "grad_norm": 0.0005718201282434165,
      "learning_rate": 7.087794342121723e-08,
      "loss": 0.001,
      "step": 126840
    },
    {
      "epoch": 23063.636363636364,
      "grad_norm": 0.00071193918120116,
      "learning_rate": 7.081823665769243e-08,
      "loss": 0.001,
      "step": 126850
    },
    {
      "epoch": 23065.454545454544,
      "grad_norm": 0.00044182961573824286,
      "learning_rate": 7.075855313616613e-08,
      "loss": 0.001,
      "step": 126860
    },
    {
      "epoch": 23067.272727272728,
      "grad_norm": 0.0013320697471499443,
      "learning_rate": 7.069889285987024e-08,
      "loss": 0.0012,
      "step": 126870
    },
    {
      "epoch": 23069.090909090908,
      "grad_norm": 0.0010799056617543101,
      "learning_rate": 7.063925583203573e-08,
      "loss": 0.0012,
      "step": 126880
    },
    {
      "epoch": 23070.909090909092,
      "grad_norm": 0.2758798599243164,
      "learning_rate": 7.057964205589217e-08,
      "loss": 0.001,
      "step": 126890
    },
    {
      "epoch": 23072.727272727272,
      "grad_norm": 0.0008561992435716093,
      "learning_rate": 7.052005153466778e-08,
      "loss": 0.0013,
      "step": 126900
    },
    {
      "epoch": 23074.545454545456,
      "grad_norm": 0.2583194375038147,
      "learning_rate": 7.046048427158984e-08,
      "loss": 0.0013,
      "step": 126910
    },
    {
      "epoch": 23076.363636363636,
      "grad_norm": 0.1770590990781784,
      "learning_rate": 7.040094026988402e-08,
      "loss": 0.001,
      "step": 126920
    },
    {
      "epoch": 23078.18181818182,
      "grad_norm": 0.2034233957529068,
      "learning_rate": 7.034141953277484e-08,
      "loss": 0.001,
      "step": 126930
    },
    {
      "epoch": 23080.0,
      "grad_norm": 0.21803826093673706,
      "learning_rate": 7.028192206348577e-08,
      "loss": 0.001,
      "step": 126940
    },
    {
      "epoch": 23081.81818181818,
      "grad_norm": 0.2762163579463959,
      "learning_rate": 7.022244786523878e-08,
      "loss": 0.0012,
      "step": 126950
    },
    {
      "epoch": 23083.636363636364,
      "grad_norm": 0.005218584090471268,
      "learning_rate": 7.016299694125449e-08,
      "loss": 0.0009,
      "step": 126960
    },
    {
      "epoch": 23085.454545454544,
      "grad_norm": 0.16993598639965057,
      "learning_rate": 7.010356929475264e-08,
      "loss": 0.0012,
      "step": 126970
    },
    {
      "epoch": 23087.272727272728,
      "grad_norm": 0.2016526162624359,
      "learning_rate": 7.004416492895132e-08,
      "loss": 0.0011,
      "step": 126980
    },
    {
      "epoch": 23089.090909090908,
      "grad_norm": 0.0006637113401666284,
      "learning_rate": 6.998478384706768e-08,
      "loss": 0.001,
      "step": 126990
    },
    {
      "epoch": 23090.909090909092,
      "grad_norm": 0.0005767957773059607,
      "learning_rate": 6.992542605231738e-08,
      "loss": 0.0012,
      "step": 127000
    },
    {
      "epoch": 23090.909090909092,
      "eval_loss": 5.134684085845947,
      "eval_runtime": 0.946,
      "eval_samples_per_second": 10.57,
      "eval_steps_per_second": 5.285,
      "step": 127000
    },
    {
      "epoch": 23092.727272727272,
      "grad_norm": 0.036779630929231644,
      "learning_rate": 6.986609154791479e-08,
      "loss": 0.0012,
      "step": 127010
    },
    {
      "epoch": 23094.545454545456,
      "grad_norm": 0.17223194241523743,
      "learning_rate": 6.980678033707333e-08,
      "loss": 0.0012,
      "step": 127020
    },
    {
      "epoch": 23096.363636363636,
      "grad_norm": 0.0004188980965409428,
      "learning_rate": 6.974749242300487e-08,
      "loss": 0.001,
      "step": 127030
    },
    {
      "epoch": 23098.18181818182,
      "grad_norm": 0.19986005127429962,
      "learning_rate": 6.968822780891992e-08,
      "loss": 0.0009,
      "step": 127040
    },
    {
      "epoch": 23100.0,
      "grad_norm": 0.2753101885318756,
      "learning_rate": 6.962898649802822e-08,
      "loss": 0.001,
      "step": 127050
    },
    {
      "epoch": 23101.81818181818,
      "grad_norm": 0.00045401096576824784,
      "learning_rate": 6.956976849353768e-08,
      "loss": 0.0012,
      "step": 127060
    },
    {
      "epoch": 23103.636363636364,
      "grad_norm": 0.007262545637786388,
      "learning_rate": 6.951057379865538e-08,
      "loss": 0.001,
      "step": 127070
    },
    {
      "epoch": 23105.454545454544,
      "grad_norm": 0.0006042629829607904,
      "learning_rate": 6.945140241658686e-08,
      "loss": 0.0007,
      "step": 127080
    },
    {
      "epoch": 23107.272727272728,
      "grad_norm": 0.17568255960941315,
      "learning_rate": 6.939225435053647e-08,
      "loss": 0.0013,
      "step": 127090
    },
    {
      "epoch": 23109.090909090908,
      "grad_norm": 0.17194153368473053,
      "learning_rate": 6.933312960370747e-08,
      "loss": 0.0012,
      "step": 127100
    },
    {
      "epoch": 23110.909090909092,
      "grad_norm": 0.19723942875862122,
      "learning_rate": 6.927402817930167e-08,
      "loss": 0.0011,
      "step": 127110
    },
    {
      "epoch": 23112.727272727272,
      "grad_norm": 0.0006107510416768491,
      "learning_rate": 6.921495008051942e-08,
      "loss": 0.0012,
      "step": 127120
    },
    {
      "epoch": 23114.545454545456,
      "grad_norm": 0.000773870327975601,
      "learning_rate": 6.915589531056042e-08,
      "loss": 0.001,
      "step": 127130
    },
    {
      "epoch": 23116.363636363636,
      "grad_norm": 0.23046991229057312,
      "learning_rate": 6.909686387262253e-08,
      "loss": 0.0012,
      "step": 127140
    },
    {
      "epoch": 23118.18181818182,
      "grad_norm": 0.0004944692482240498,
      "learning_rate": 6.903785576990251e-08,
      "loss": 0.0009,
      "step": 127150
    },
    {
      "epoch": 23120.0,
      "grad_norm": 0.2698753774166107,
      "learning_rate": 6.897887100559608e-08,
      "loss": 0.0012,
      "step": 127160
    },
    {
      "epoch": 23121.81818181818,
      "grad_norm": 0.14997194707393646,
      "learning_rate": 6.891990958289722e-08,
      "loss": 0.0011,
      "step": 127170
    },
    {
      "epoch": 23123.636363636364,
      "grad_norm": 0.0013071093708276749,
      "learning_rate": 6.886097150499925e-08,
      "loss": 0.001,
      "step": 127180
    },
    {
      "epoch": 23125.454545454544,
      "grad_norm": 0.21498654782772064,
      "learning_rate": 6.880205677509382e-08,
      "loss": 0.001,
      "step": 127190
    },
    {
      "epoch": 23127.272727272728,
      "grad_norm": 0.0007295291870832443,
      "learning_rate": 6.874316539637126e-08,
      "loss": 0.0009,
      "step": 127200
    },
    {
      "epoch": 23129.090909090908,
      "grad_norm": 0.017199285328388214,
      "learning_rate": 6.868429737202103e-08,
      "loss": 0.0013,
      "step": 127210
    },
    {
      "epoch": 23130.909090909092,
      "grad_norm": 0.0006254374166019261,
      "learning_rate": 6.86254527052309e-08,
      "loss": 0.001,
      "step": 127220
    },
    {
      "epoch": 23132.727272727272,
      "grad_norm": 0.0018147916998714209,
      "learning_rate": 6.85666313991875e-08,
      "loss": 0.0009,
      "step": 127230
    },
    {
      "epoch": 23134.545454545456,
      "grad_norm": 0.0009844450978562236,
      "learning_rate": 6.85078334570765e-08,
      "loss": 0.001,
      "step": 127240
    },
    {
      "epoch": 23136.363636363636,
      "grad_norm": 0.17539404332637787,
      "learning_rate": 6.84490588820818e-08,
      "loss": 0.0012,
      "step": 127250
    },
    {
      "epoch": 23138.18181818182,
      "grad_norm": 0.14557288587093353,
      "learning_rate": 6.839030767738652e-08,
      "loss": 0.0011,
      "step": 127260
    },
    {
      "epoch": 23140.0,
      "grad_norm": 0.011536804959177971,
      "learning_rate": 6.833157984617216e-08,
      "loss": 0.001,
      "step": 127270
    },
    {
      "epoch": 23141.81818181818,
      "grad_norm": 0.003883026074618101,
      "learning_rate": 6.827287539161896e-08,
      "loss": 0.001,
      "step": 127280
    },
    {
      "epoch": 23143.636363636364,
      "grad_norm": 0.21337512135505676,
      "learning_rate": 6.821419431690628e-08,
      "loss": 0.001,
      "step": 127290
    },
    {
      "epoch": 23145.454545454544,
      "grad_norm": 0.0006874714745208621,
      "learning_rate": 6.815553662521184e-08,
      "loss": 0.0012,
      "step": 127300
    },
    {
      "epoch": 23147.272727272728,
      "grad_norm": 0.00032628412009216845,
      "learning_rate": 6.8096902319712e-08,
      "loss": 0.0009,
      "step": 127310
    },
    {
      "epoch": 23149.090909090908,
      "grad_norm": 0.1744399070739746,
      "learning_rate": 6.803829140358236e-08,
      "loss": 0.0013,
      "step": 127320
    },
    {
      "epoch": 23150.909090909092,
      "grad_norm": 0.00037790887290611863,
      "learning_rate": 6.797970387999685e-08,
      "loss": 0.0011,
      "step": 127330
    },
    {
      "epoch": 23152.727272727272,
      "grad_norm": 0.2057144045829773,
      "learning_rate": 6.792113975212805e-08,
      "loss": 0.0012,
      "step": 127340
    },
    {
      "epoch": 23154.545454545456,
      "grad_norm": 0.17341531813144684,
      "learning_rate": 6.786259902314767e-08,
      "loss": 0.0009,
      "step": 127350
    },
    {
      "epoch": 23156.363636363636,
      "grad_norm": 0.28174567222595215,
      "learning_rate": 6.780408169622582e-08,
      "loss": 0.0015,
      "step": 127360
    },
    {
      "epoch": 23158.18181818182,
      "grad_norm": 0.0014758555917069316,
      "learning_rate": 6.77455877745316e-08,
      "loss": 0.0007,
      "step": 127370
    },
    {
      "epoch": 23160.0,
      "grad_norm": 0.0006677723722532392,
      "learning_rate": 6.76871172612326e-08,
      "loss": 0.0012,
      "step": 127380
    },
    {
      "epoch": 23161.81818181818,
      "grad_norm": 0.00036730532883666456,
      "learning_rate": 6.762867015949514e-08,
      "loss": 0.001,
      "step": 127390
    },
    {
      "epoch": 23163.636363636364,
      "grad_norm": 0.00046116538578644395,
      "learning_rate": 6.757024647248455e-08,
      "loss": 0.0009,
      "step": 127400
    },
    {
      "epoch": 23165.454545454544,
      "grad_norm": 0.0009897516574710608,
      "learning_rate": 6.75118462033647e-08,
      "loss": 0.0015,
      "step": 127410
    },
    {
      "epoch": 23167.272727272728,
      "grad_norm": 0.0003954795829486102,
      "learning_rate": 6.745346935529805e-08,
      "loss": 0.001,
      "step": 127420
    },
    {
      "epoch": 23169.090909090908,
      "grad_norm": 0.0063463919796049595,
      "learning_rate": 6.73951159314462e-08,
      "loss": 0.0013,
      "step": 127430
    },
    {
      "epoch": 23170.909090909092,
      "grad_norm": 0.0005398235516622663,
      "learning_rate": 6.7336785934969e-08,
      "loss": 0.0007,
      "step": 127440
    },
    {
      "epoch": 23172.727272727272,
      "grad_norm": 0.00049159349873662,
      "learning_rate": 6.727847936902542e-08,
      "loss": 0.001,
      "step": 127450
    },
    {
      "epoch": 23174.545454545456,
      "grad_norm": 0.21470610797405243,
      "learning_rate": 6.7220196236773e-08,
      "loss": 0.0012,
      "step": 127460
    },
    {
      "epoch": 23176.363636363636,
      "grad_norm": 0.26139771938323975,
      "learning_rate": 6.716193654136787e-08,
      "loss": 0.0013,
      "step": 127470
    },
    {
      "epoch": 23178.18181818182,
      "grad_norm": 0.00048347373376600444,
      "learning_rate": 6.710370028596518e-08,
      "loss": 0.001,
      "step": 127480
    },
    {
      "epoch": 23180.0,
      "grad_norm": 0.19870775938034058,
      "learning_rate": 6.704548747371868e-08,
      "loss": 0.0012,
      "step": 127490
    },
    {
      "epoch": 23181.81818181818,
      "grad_norm": 0.0016213078051805496,
      "learning_rate": 6.698729810778064e-08,
      "loss": 0.0012,
      "step": 127500
    },
    {
      "epoch": 23181.81818181818,
      "eval_loss": 5.177286624908447,
      "eval_runtime": 0.9486,
      "eval_samples_per_second": 10.542,
      "eval_steps_per_second": 5.271,
      "step": 127500
    },
    {
      "epoch": 23183.636363636364,
      "grad_norm": 0.008570992387831211,
      "learning_rate": 6.692913219130253e-08,
      "loss": 0.001,
      "step": 127510
    },
    {
      "epoch": 23185.454545454544,
      "grad_norm": 0.0005905693396925926,
      "learning_rate": 6.687098972743416e-08,
      "loss": 0.001,
      "step": 127520
    },
    {
      "epoch": 23187.272727272728,
      "grad_norm": 0.20159977674484253,
      "learning_rate": 6.681287071932407e-08,
      "loss": 0.0011,
      "step": 127530
    },
    {
      "epoch": 23189.090909090908,
      "grad_norm": 0.2033083736896515,
      "learning_rate": 6.675477517011985e-08,
      "loss": 0.0012,
      "step": 127540
    },
    {
      "epoch": 23190.909090909092,
      "grad_norm": 0.20219212770462036,
      "learning_rate": 6.669670308296743e-08,
      "loss": 0.0009,
      "step": 127550
    },
    {
      "epoch": 23192.727272727272,
      "grad_norm": 0.0005482341512106359,
      "learning_rate": 6.663865446101192e-08,
      "loss": 0.0012,
      "step": 127560
    },
    {
      "epoch": 23194.545454545456,
      "grad_norm": 0.0005382685922086239,
      "learning_rate": 6.658062930739666e-08,
      "loss": 0.0012,
      "step": 127570
    },
    {
      "epoch": 23196.363636363636,
      "grad_norm": 0.0006812253268435597,
      "learning_rate": 6.652262762526395e-08,
      "loss": 0.0007,
      "step": 127580
    },
    {
      "epoch": 23198.18181818182,
      "grad_norm": 0.000547122850548476,
      "learning_rate": 6.646464941775499e-08,
      "loss": 0.0012,
      "step": 127590
    },
    {
      "epoch": 23200.0,
      "grad_norm": 0.17338363826274872,
      "learning_rate": 6.640669468800947e-08,
      "loss": 0.0012,
      "step": 127600
    },
    {
      "epoch": 23201.81818181818,
      "grad_norm": 0.18203239142894745,
      "learning_rate": 6.634876343916579e-08,
      "loss": 0.0012,
      "step": 127610
    },
    {
      "epoch": 23203.636363636364,
      "grad_norm": 0.16574716567993164,
      "learning_rate": 6.629085567436132e-08,
      "loss": 0.0012,
      "step": 127620
    },
    {
      "epoch": 23205.454545454544,
      "grad_norm": 0.22417804598808289,
      "learning_rate": 6.623297139673184e-08,
      "loss": 0.0009,
      "step": 127630
    },
    {
      "epoch": 23207.272727272728,
      "grad_norm": 0.16593222320079803,
      "learning_rate": 6.617511060941233e-08,
      "loss": 0.001,
      "step": 127640
    },
    {
      "epoch": 23209.090909090908,
      "grad_norm": 0.0014623412862420082,
      "learning_rate": 6.611727331553585e-08,
      "loss": 0.0011,
      "step": 127650
    },
    {
      "epoch": 23210.909090909092,
      "grad_norm": 0.0003829213383141905,
      "learning_rate": 6.60594595182346e-08,
      "loss": 0.0012,
      "step": 127660
    },
    {
      "epoch": 23212.727272727272,
      "grad_norm": 0.0005619692383334041,
      "learning_rate": 6.600166922063966e-08,
      "loss": 0.0012,
      "step": 127670
    },
    {
      "epoch": 23214.545454545456,
      "grad_norm": 0.1692880243062973,
      "learning_rate": 6.594390242588044e-08,
      "loss": 0.0011,
      "step": 127680
    },
    {
      "epoch": 23216.363636363636,
      "grad_norm": 0.00046917007421143353,
      "learning_rate": 6.588615913708523e-08,
      "loss": 0.0006,
      "step": 127690
    },
    {
      "epoch": 23218.18181818182,
      "grad_norm": 0.21878167986869812,
      "learning_rate": 6.582843935738119e-08,
      "loss": 0.0014,
      "step": 127700
    },
    {
      "epoch": 23220.0,
      "grad_norm": 0.2736065685749054,
      "learning_rate": 6.577074308989405e-08,
      "loss": 0.001,
      "step": 127710
    },
    {
      "epoch": 23221.81818181818,
      "grad_norm": 0.22144067287445068,
      "learning_rate": 6.57130703377482e-08,
      "loss": 0.0012,
      "step": 127720
    },
    {
      "epoch": 23223.636363636364,
      "grad_norm": 0.17427927255630493,
      "learning_rate": 6.565542110406702e-08,
      "loss": 0.001,
      "step": 127730
    },
    {
      "epoch": 23225.454545454544,
      "grad_norm": 0.006202489137649536,
      "learning_rate": 6.55977953919723e-08,
      "loss": 0.0013,
      "step": 127740
    },
    {
      "epoch": 23227.272727272728,
      "grad_norm": 0.21974892914295197,
      "learning_rate": 6.554019320458493e-08,
      "loss": 0.001,
      "step": 127750
    },
    {
      "epoch": 23229.090909090908,
      "grad_norm": 0.0010765750193968415,
      "learning_rate": 6.548261454502412e-08,
      "loss": 0.0009,
      "step": 127760
    },
    {
      "epoch": 23230.909090909092,
      "grad_norm": 0.2933530807495117,
      "learning_rate": 6.542505941640802e-08,
      "loss": 0.0012,
      "step": 127770
    },
    {
      "epoch": 23232.727272727272,
      "grad_norm": 0.23453561961650848,
      "learning_rate": 6.536752782185362e-08,
      "loss": 0.0012,
      "step": 127780
    },
    {
      "epoch": 23234.545454545456,
      "grad_norm": 0.17146779596805573,
      "learning_rate": 6.531001976447636e-08,
      "loss": 0.0009,
      "step": 127790
    },
    {
      "epoch": 23236.363636363636,
      "grad_norm": 0.2174816131591797,
      "learning_rate": 6.52525352473905e-08,
      "loss": 0.001,
      "step": 127800
    },
    {
      "epoch": 23238.18181818182,
      "grad_norm": 0.001593010383658111,
      "learning_rate": 6.519507427370923e-08,
      "loss": 0.001,
      "step": 127810
    },
    {
      "epoch": 23240.0,
      "grad_norm": 0.16706453263759613,
      "learning_rate": 6.513763684654417e-08,
      "loss": 0.0012,
      "step": 127820
    },
    {
      "epoch": 23241.81818181818,
      "grad_norm": 0.1416611224412918,
      "learning_rate": 6.5080222969006e-08,
      "loss": 0.0009,
      "step": 127830
    },
    {
      "epoch": 23243.636363636364,
      "grad_norm": 0.16527731716632843,
      "learning_rate": 6.50228326442036e-08,
      "loss": 0.0014,
      "step": 127840
    },
    {
      "epoch": 23245.454545454544,
      "grad_norm": 0.01189395785331726,
      "learning_rate": 6.496546587524509e-08,
      "loss": 0.001,
      "step": 127850
    },
    {
      "epoch": 23247.272727272728,
      "grad_norm": 0.2053147554397583,
      "learning_rate": 6.490812266523716e-08,
      "loss": 0.0012,
      "step": 127860
    },
    {
      "epoch": 23249.090909090908,
      "grad_norm": 0.43318456411361694,
      "learning_rate": 6.485080301728518e-08,
      "loss": 0.0011,
      "step": 127870
    },
    {
      "epoch": 23250.909090909092,
      "grad_norm": 0.00039161581662483513,
      "learning_rate": 6.47935069344931e-08,
      "loss": 0.0009,
      "step": 127880
    },
    {
      "epoch": 23252.727272727272,
      "grad_norm": 0.19952301681041718,
      "learning_rate": 6.473623441996389e-08,
      "loss": 0.001,
      "step": 127890
    },
    {
      "epoch": 23254.545454545456,
      "grad_norm": 0.2794550657272339,
      "learning_rate": 6.467898547679913e-08,
      "loss": 0.0013,
      "step": 127900
    },
    {
      "epoch": 23256.363636363636,
      "grad_norm": 0.16807332634925842,
      "learning_rate": 6.462176010809894e-08,
      "loss": 0.0009,
      "step": 127910
    },
    {
      "epoch": 23258.18181818182,
      "grad_norm": 0.0005981025751680136,
      "learning_rate": 6.456455831696233e-08,
      "loss": 0.0011,
      "step": 127920
    },
    {
      "epoch": 23260.0,
      "grad_norm": 0.0004935028264299035,
      "learning_rate": 6.45073801064871e-08,
      "loss": 0.0012,
      "step": 127930
    },
    {
      "epoch": 23261.81818181818,
      "grad_norm": 0.0012699241051450372,
      "learning_rate": 6.44502254797698e-08,
      "loss": 0.0011,
      "step": 127940
    },
    {
      "epoch": 23263.636363636364,
      "grad_norm": 0.0005452049081213772,
      "learning_rate": 6.439309443990531e-08,
      "loss": 0.0009,
      "step": 127950
    },
    {
      "epoch": 23265.454545454544,
      "grad_norm": 0.20415380597114563,
      "learning_rate": 6.433598698998765e-08,
      "loss": 0.0015,
      "step": 127960
    },
    {
      "epoch": 23267.272727272728,
      "grad_norm": 0.20264726877212524,
      "learning_rate": 6.427890313310957e-08,
      "loss": 0.001,
      "step": 127970
    },
    {
      "epoch": 23269.090909090908,
      "grad_norm": 0.16635701060295105,
      "learning_rate": 6.422184287236226e-08,
      "loss": 0.001,
      "step": 127980
    },
    {
      "epoch": 23270.909090909092,
      "grad_norm": 0.0007403537165373564,
      "learning_rate": 6.416480621083581e-08,
      "loss": 0.0009,
      "step": 127990
    },
    {
      "epoch": 23272.727272727272,
      "grad_norm": 0.0025122256483882666,
      "learning_rate": 6.410779315161886e-08,
      "loss": 0.0013,
      "step": 128000
    },
    {
      "epoch": 23272.727272727272,
      "eval_loss": 5.219371795654297,
      "eval_runtime": 0.9476,
      "eval_samples_per_second": 10.553,
      "eval_steps_per_second": 5.276,
      "step": 128000
    },
    {
      "epoch": 23274.545454545456,
      "grad_norm": 0.16729025542736053,
      "learning_rate": 6.405080369779897e-08,
      "loss": 0.0013,
      "step": 128010
    },
    {
      "epoch": 23276.363636363636,
      "grad_norm": 0.2185453474521637,
      "learning_rate": 6.399383785246271e-08,
      "loss": 0.0009,
      "step": 128020
    },
    {
      "epoch": 23278.18181818182,
      "grad_norm": 0.0004443070210982114,
      "learning_rate": 6.393689561869443e-08,
      "loss": 0.0009,
      "step": 128030
    },
    {
      "epoch": 23280.0,
      "grad_norm": 0.0006911625969223678,
      "learning_rate": 6.387997699957815e-08,
      "loss": 0.0012,
      "step": 128040
    },
    {
      "epoch": 23281.81818181818,
      "grad_norm": 0.001030996791087091,
      "learning_rate": 6.382308199819625e-08,
      "loss": 0.0009,
      "step": 128050
    },
    {
      "epoch": 23283.636363636364,
      "grad_norm": 0.0015072159003466368,
      "learning_rate": 6.376621061762977e-08,
      "loss": 0.0012,
      "step": 128060
    },
    {
      "epoch": 23285.454545454544,
      "grad_norm": 0.00039311402360908687,
      "learning_rate": 6.37093628609584e-08,
      "loss": 0.0012,
      "step": 128070
    },
    {
      "epoch": 23287.272727272728,
      "grad_norm": 0.20696260035037994,
      "learning_rate": 6.365253873126097e-08,
      "loss": 0.0012,
      "step": 128080
    },
    {
      "epoch": 23289.090909090908,
      "grad_norm": 0.000575224170461297,
      "learning_rate": 6.359573823161457e-08,
      "loss": 0.0009,
      "step": 128090
    },
    {
      "epoch": 23290.909090909092,
      "grad_norm": 0.2056964486837387,
      "learning_rate": 6.353896136509524e-08,
      "loss": 0.0012,
      "step": 128100
    },
    {
      "epoch": 23292.727272727272,
      "grad_norm": 0.0007869863184168935,
      "learning_rate": 6.348220813477757e-08,
      "loss": 0.001,
      "step": 128110
    },
    {
      "epoch": 23294.545454545456,
      "grad_norm": 0.0005232879193499684,
      "learning_rate": 6.342547854373504e-08,
      "loss": 0.001,
      "step": 128120
    },
    {
      "epoch": 23296.363636363636,
      "grad_norm": 0.0004941187216900289,
      "learning_rate": 6.336877259504003e-08,
      "loss": 0.0016,
      "step": 128130
    },
    {
      "epoch": 23298.18181818182,
      "grad_norm": 0.0011936586815863848,
      "learning_rate": 6.331209029176299e-08,
      "loss": 0.0007,
      "step": 128140
    },
    {
      "epoch": 23300.0,
      "grad_norm": 0.0005135575775057077,
      "learning_rate": 6.325543163697372e-08,
      "loss": 0.0012,
      "step": 128150
    },
    {
      "epoch": 23301.81818181818,
      "grad_norm": 0.00044921855442225933,
      "learning_rate": 6.319879663374066e-08,
      "loss": 0.0007,
      "step": 128160
    },
    {
      "epoch": 23303.636363636364,
      "grad_norm": 0.0005850856541655958,
      "learning_rate": 6.314218528513066e-08,
      "loss": 0.0014,
      "step": 128170
    },
    {
      "epoch": 23305.454545454544,
      "grad_norm": 0.17931298911571503,
      "learning_rate": 6.308559759420955e-08,
      "loss": 0.0013,
      "step": 128180
    },
    {
      "epoch": 23307.272727272728,
      "grad_norm": 0.0005330279236659408,
      "learning_rate": 6.30290335640416e-08,
      "loss": 0.0009,
      "step": 128190
    },
    {
      "epoch": 23309.090909090908,
      "grad_norm": 0.16095736622810364,
      "learning_rate": 6.297249319769016e-08,
      "loss": 0.0011,
      "step": 128200
    },
    {
      "epoch": 23310.909090909092,
      "grad_norm": 0.20734046399593353,
      "learning_rate": 6.291597649821729e-08,
      "loss": 0.0011,
      "step": 128210
    },
    {
      "epoch": 23312.727272727272,
      "grad_norm": 0.0009910974185913801,
      "learning_rate": 6.28594834686832e-08,
      "loss": 0.0009,
      "step": 128220
    },
    {
      "epoch": 23314.545454545456,
      "grad_norm": 0.2700573205947876,
      "learning_rate": 6.280301411214745e-08,
      "loss": 0.0013,
      "step": 128230
    },
    {
      "epoch": 23316.363636363636,
      "grad_norm": 0.17123784124851227,
      "learning_rate": 6.274656843166821e-08,
      "loss": 0.0009,
      "step": 128240
    },
    {
      "epoch": 23318.18181818182,
      "grad_norm": 0.0005375545588321984,
      "learning_rate": 6.269014643030212e-08,
      "loss": 0.001,
      "step": 128250
    },
    {
      "epoch": 23320.0,
      "grad_norm": 0.21519935131072998,
      "learning_rate": 6.263374811110467e-08,
      "loss": 0.0012,
      "step": 128260
    },
    {
      "epoch": 23321.81818181818,
      "grad_norm": 0.0007648455793969333,
      "learning_rate": 6.257737347712999e-08,
      "loss": 0.0011,
      "step": 128270
    },
    {
      "epoch": 23323.636363636364,
      "grad_norm": 0.1762632578611374,
      "learning_rate": 6.25210225314312e-08,
      "loss": 0.0012,
      "step": 128280
    },
    {
      "epoch": 23325.454545454544,
      "grad_norm": 0.0003876310947816819,
      "learning_rate": 6.246469527705978e-08,
      "loss": 0.0007,
      "step": 128290
    },
    {
      "epoch": 23327.272727272728,
      "grad_norm": 0.001575428294017911,
      "learning_rate": 6.240839171706608e-08,
      "loss": 0.0015,
      "step": 128300
    },
    {
      "epoch": 23329.090909090908,
      "grad_norm": 0.0015825965674594045,
      "learning_rate": 6.235211185449918e-08,
      "loss": 0.0009,
      "step": 128310
    },
    {
      "epoch": 23330.909090909092,
      "grad_norm": 0.20750854909420013,
      "learning_rate": 6.22958556924072e-08,
      "loss": 0.0012,
      "step": 128320
    },
    {
      "epoch": 23332.727272727272,
      "grad_norm": 0.16661101579666138,
      "learning_rate": 6.223962323383609e-08,
      "loss": 0.0009,
      "step": 128330
    },
    {
      "epoch": 23334.545454545456,
      "grad_norm": 0.26539406180381775,
      "learning_rate": 6.21834144818314e-08,
      "loss": 0.0015,
      "step": 128340
    },
    {
      "epoch": 23336.363636363636,
      "grad_norm": 0.0009094047709368169,
      "learning_rate": 6.21272294394371e-08,
      "loss": 0.0006,
      "step": 128350
    },
    {
      "epoch": 23338.18181818182,
      "grad_norm": 0.21074916422367096,
      "learning_rate": 6.207106810969575e-08,
      "loss": 0.0013,
      "step": 128360
    },
    {
      "epoch": 23340.0,
      "grad_norm": 0.0007545204134657979,
      "learning_rate": 6.201493049564882e-08,
      "loss": 0.001,
      "step": 128370
    },
    {
      "epoch": 23341.81818181818,
      "grad_norm": 0.00046483089681714773,
      "learning_rate": 6.195881660033614e-08,
      "loss": 0.0012,
      "step": 128380
    },
    {
      "epoch": 23343.636363636364,
      "grad_norm": 0.000461781193735078,
      "learning_rate": 6.190272642679672e-08,
      "loss": 0.0007,
      "step": 128390
    },
    {
      "epoch": 23345.454545454544,
      "grad_norm": 0.1706327646970749,
      "learning_rate": 6.184665997806831e-08,
      "loss": 0.0013,
      "step": 128400
    },
    {
      "epoch": 23347.272727272728,
      "grad_norm": 0.0003671608865261078,
      "learning_rate": 6.17906172571866e-08,
      "loss": 0.0009,
      "step": 128410
    },
    {
      "epoch": 23349.090909090908,
      "grad_norm": 0.21654216945171356,
      "learning_rate": 6.173459826718691e-08,
      "loss": 0.0012,
      "step": 128420
    },
    {
      "epoch": 23350.909090909092,
      "grad_norm": 0.20148737728595734,
      "learning_rate": 6.167860301110284e-08,
      "loss": 0.001,
      "step": 128430
    },
    {
      "epoch": 23352.727272727272,
      "grad_norm": 0.21485424041748047,
      "learning_rate": 6.162263149196677e-08,
      "loss": 0.0013,
      "step": 128440
    },
    {
      "epoch": 23354.545454545456,
      "grad_norm": 0.0004835190193261951,
      "learning_rate": 6.156668371280982e-08,
      "loss": 0.0008,
      "step": 128450
    },
    {
      "epoch": 23356.363636363636,
      "grad_norm": 0.1718463897705078,
      "learning_rate": 6.151075967666164e-08,
      "loss": 0.0014,
      "step": 128460
    },
    {
      "epoch": 23358.18181818182,
      "grad_norm": 0.2008797526359558,
      "learning_rate": 6.145485938655093e-08,
      "loss": 0.0009,
      "step": 128470
    },
    {
      "epoch": 23360.0,
      "grad_norm": 0.0008085154113359749,
      "learning_rate": 6.139898284550488e-08,
      "loss": 0.001,
      "step": 128480
    },
    {
      "epoch": 23361.81818181818,
      "grad_norm": 0.16804498434066772,
      "learning_rate": 6.134313005654929e-08,
      "loss": 0.0011,
      "step": 128490
    },
    {
      "epoch": 23363.636363636364,
      "grad_norm": 0.2178669571876526,
      "learning_rate": 6.128730102270896e-08,
      "loss": 0.0012,
      "step": 128500
    },
    {
      "epoch": 23363.636363636364,
      "eval_loss": 5.215574741363525,
      "eval_runtime": 0.9505,
      "eval_samples_per_second": 10.521,
      "eval_steps_per_second": 5.26,
      "step": 128500
    },
    {
      "epoch": 23365.454545454544,
      "grad_norm": 0.21828623116016388,
      "learning_rate": 6.123149574700754e-08,
      "loss": 0.0012,
      "step": 128510
    },
    {
      "epoch": 23367.272727272728,
      "grad_norm": 0.2797001302242279,
      "learning_rate": 6.117571423246654e-08,
      "loss": 0.0014,
      "step": 128520
    },
    {
      "epoch": 23369.090909090908,
      "grad_norm": 0.3963576555252075,
      "learning_rate": 6.111995648210722e-08,
      "loss": 0.001,
      "step": 128530
    },
    {
      "epoch": 23370.909090909092,
      "grad_norm": 0.0004177778901066631,
      "learning_rate": 6.106422249894878e-08,
      "loss": 0.0008,
      "step": 128540
    },
    {
      "epoch": 23372.727272727272,
      "grad_norm": 0.16650111973285675,
      "learning_rate": 6.100851228600973e-08,
      "loss": 0.0013,
      "step": 128550
    },
    {
      "epoch": 23374.545454545456,
      "grad_norm": 0.15945881605148315,
      "learning_rate": 6.09528258463069e-08,
      "loss": 0.0009,
      "step": 128560
    },
    {
      "epoch": 23376.363636363636,
      "grad_norm": 0.18464116752147675,
      "learning_rate": 6.089716318285587e-08,
      "loss": 0.0012,
      "step": 128570
    },
    {
      "epoch": 23378.18181818182,
      "grad_norm": 0.00038574356585741043,
      "learning_rate": 6.084152429867113e-08,
      "loss": 0.0009,
      "step": 128580
    },
    {
      "epoch": 23380.0,
      "grad_norm": 0.2743976414203644,
      "learning_rate": 6.078590919676574e-08,
      "loss": 0.0012,
      "step": 128590
    },
    {
      "epoch": 23381.81818181818,
      "grad_norm": 0.19320787489414215,
      "learning_rate": 6.073031788015131e-08,
      "loss": 0.0011,
      "step": 128600
    },
    {
      "epoch": 23383.636363636364,
      "grad_norm": 0.0004744766338262707,
      "learning_rate": 6.067475035183861e-08,
      "loss": 0.001,
      "step": 128610
    },
    {
      "epoch": 23385.454545454544,
      "grad_norm": 0.0006036360282450914,
      "learning_rate": 6.061920661483665e-08,
      "loss": 0.0014,
      "step": 128620
    },
    {
      "epoch": 23387.272727272728,
      "grad_norm": 0.0015598753234371543,
      "learning_rate": 6.056368667215356e-08,
      "loss": 0.0009,
      "step": 128630
    },
    {
      "epoch": 23389.090909090908,
      "grad_norm": 0.0021058963611721992,
      "learning_rate": 6.050819052679584e-08,
      "loss": 0.001,
      "step": 128640
    },
    {
      "epoch": 23390.909090909092,
      "grad_norm": 0.1499309241771698,
      "learning_rate": 6.045271818176872e-08,
      "loss": 0.0011,
      "step": 128650
    },
    {
      "epoch": 23392.727272727272,
      "grad_norm": 0.0009756693616509438,
      "learning_rate": 6.039726964007657e-08,
      "loss": 0.001,
      "step": 128660
    },
    {
      "epoch": 23394.545454545456,
      "grad_norm": 0.0005993891390971839,
      "learning_rate": 6.034184490472194e-08,
      "loss": 0.0009,
      "step": 128670
    },
    {
      "epoch": 23396.363636363636,
      "grad_norm": 0.001181027153506875,
      "learning_rate": 6.02864439787063e-08,
      "loss": 0.0014,
      "step": 128680
    },
    {
      "epoch": 23398.18181818182,
      "grad_norm": 0.001721237669698894,
      "learning_rate": 6.023106686502988e-08,
      "loss": 0.0009,
      "step": 128690
    },
    {
      "epoch": 23400.0,
      "grad_norm": 0.1615680456161499,
      "learning_rate": 6.017571356669182e-08,
      "loss": 0.0012,
      "step": 128700
    },
    {
      "epoch": 23401.81818181818,
      "grad_norm": 0.0012622455833479762,
      "learning_rate": 6.012038408668935e-08,
      "loss": 0.0011,
      "step": 128710
    },
    {
      "epoch": 23403.636363636364,
      "grad_norm": 0.1577732414007187,
      "learning_rate": 6.006507842801906e-08,
      "loss": 0.001,
      "step": 128720
    },
    {
      "epoch": 23405.454545454544,
      "grad_norm": 0.00048687244998291135,
      "learning_rate": 6.000979659367578e-08,
      "loss": 0.0009,
      "step": 128730
    },
    {
      "epoch": 23407.272727272728,
      "grad_norm": 0.0007176441140472889,
      "learning_rate": 5.99545385866535e-08,
      "loss": 0.0012,
      "step": 128740
    },
    {
      "epoch": 23409.090909090908,
      "grad_norm": 0.0008845049305818975,
      "learning_rate": 5.989930440994451e-08,
      "loss": 0.0012,
      "step": 128750
    },
    {
      "epoch": 23410.909090909092,
      "grad_norm": 0.0004740221775136888,
      "learning_rate": 5.984409406653989e-08,
      "loss": 0.0012,
      "step": 128760
    },
    {
      "epoch": 23412.727272727272,
      "grad_norm": 0.0006640258361585438,
      "learning_rate": 5.978890755942978e-08,
      "loss": 0.0012,
      "step": 128770
    },
    {
      "epoch": 23414.545454545456,
      "grad_norm": 0.0007582969847135246,
      "learning_rate": 5.97337448916026e-08,
      "loss": 0.0009,
      "step": 128780
    },
    {
      "epoch": 23416.363636363636,
      "grad_norm": 0.16307465732097626,
      "learning_rate": 5.967860606604552e-08,
      "loss": 0.0013,
      "step": 128790
    },
    {
      "epoch": 23418.18181818182,
      "grad_norm": 0.17081105709075928,
      "learning_rate": 5.962349108574477e-08,
      "loss": 0.001,
      "step": 128800
    },
    {
      "epoch": 23420.0,
      "grad_norm": 0.20664569735527039,
      "learning_rate": 5.95683999536849e-08,
      "loss": 0.0011,
      "step": 128810
    },
    {
      "epoch": 23421.81818181818,
      "grad_norm": 0.2747593820095062,
      "learning_rate": 5.9513332672849414e-08,
      "loss": 0.0011,
      "step": 128820
    },
    {
      "epoch": 23423.636363636364,
      "grad_norm": 0.0007081845542415977,
      "learning_rate": 5.9458289246220485e-08,
      "loss": 0.0011,
      "step": 128830
    },
    {
      "epoch": 23425.454545454544,
      "grad_norm": 0.0009420799906365573,
      "learning_rate": 5.9403269676778725e-08,
      "loss": 0.0014,
      "step": 128840
    },
    {
      "epoch": 23427.272727272728,
      "grad_norm": 0.011869776993989944,
      "learning_rate": 5.9348273967503916e-08,
      "loss": 0.001,
      "step": 128850
    },
    {
      "epoch": 23429.090909090908,
      "grad_norm": 0.0008030444150790572,
      "learning_rate": 5.9293302121374234e-08,
      "loss": 0.0009,
      "step": 128860
    },
    {
      "epoch": 23430.909090909092,
      "grad_norm": 0.15992265939712524,
      "learning_rate": 5.923835414136646e-08,
      "loss": 0.001,
      "step": 128870
    },
    {
      "epoch": 23432.727272727272,
      "grad_norm": 0.16363030672073364,
      "learning_rate": 5.918343003045656e-08,
      "loss": 0.0014,
      "step": 128880
    },
    {
      "epoch": 23434.545454545456,
      "grad_norm": 0.20896919071674347,
      "learning_rate": 5.912852979161875e-08,
      "loss": 0.0009,
      "step": 128890
    },
    {
      "epoch": 23436.363636363636,
      "grad_norm": 0.0005437615909613669,
      "learning_rate": 5.9073653427826e-08,
      "loss": 0.001,
      "step": 128900
    },
    {
      "epoch": 23438.18181818182,
      "grad_norm": 0.0004865629889536649,
      "learning_rate": 5.901880094205036e-08,
      "loss": 0.001,
      "step": 128910
    },
    {
      "epoch": 23440.0,
      "grad_norm": 0.16879674792289734,
      "learning_rate": 5.8963972337262025e-08,
      "loss": 0.0012,
      "step": 128920
    },
    {
      "epoch": 23441.81818181818,
      "grad_norm": 0.20665565133094788,
      "learning_rate": 5.890916761643044e-08,
      "loss": 0.001,
      "step": 128930
    },
    {
      "epoch": 23443.636363636364,
      "grad_norm": 0.0006368512404151261,
      "learning_rate": 5.885438678252341e-08,
      "loss": 0.001,
      "step": 128940
    },
    {
      "epoch": 23445.454545454544,
      "grad_norm": 0.0006345732253976166,
      "learning_rate": 5.879962983850745e-08,
      "loss": 0.001,
      "step": 128950
    },
    {
      "epoch": 23447.272727272728,
      "grad_norm": 0.1986067146062851,
      "learning_rate": 5.874489678734812e-08,
      "loss": 0.0013,
      "step": 128960
    },
    {
      "epoch": 23449.090909090908,
      "grad_norm": 0.2534300982952118,
      "learning_rate": 5.869018763200928e-08,
      "loss": 0.001,
      "step": 128970
    },
    {
      "epoch": 23450.909090909092,
      "grad_norm": 0.2189958244562149,
      "learning_rate": 5.863550237545362e-08,
      "loss": 0.0012,
      "step": 128980
    },
    {
      "epoch": 23452.727272727272,
      "grad_norm": 0.00035465744440443814,
      "learning_rate": 5.85808410206427e-08,
      "loss": 0.0008,
      "step": 128990
    },
    {
      "epoch": 23454.545454545456,
      "grad_norm": 0.0022432266268879175,
      "learning_rate": 5.8526203570536504e-08,
      "loss": 0.0009,
      "step": 129000
    },
    {
      "epoch": 23454.545454545456,
      "eval_loss": 5.140946388244629,
      "eval_runtime": 0.9476,
      "eval_samples_per_second": 10.553,
      "eval_steps_per_second": 5.276,
      "step": 129000
    },
    {
      "epoch": 23456.363636363636,
      "grad_norm": 0.1986832469701767,
      "learning_rate": 5.8471590028094095e-08,
      "loss": 0.0013,
      "step": 129010
    },
    {
      "epoch": 23458.18181818182,
      "grad_norm": 0.21606403589248657,
      "learning_rate": 5.8417000396272896e-08,
      "loss": 0.0012,
      "step": 129020
    },
    {
      "epoch": 23460.0,
      "grad_norm": 0.01746242307126522,
      "learning_rate": 5.836243467802915e-08,
      "loss": 0.0011,
      "step": 129030
    },
    {
      "epoch": 23461.81818181818,
      "grad_norm": 0.0008238015579991043,
      "learning_rate": 5.8307892876317874e-08,
      "loss": 0.0012,
      "step": 129040
    },
    {
      "epoch": 23463.636363636364,
      "grad_norm": 0.2173115611076355,
      "learning_rate": 5.8253374994092775e-08,
      "loss": 0.0009,
      "step": 129050
    },
    {
      "epoch": 23465.454545454544,
      "grad_norm": 0.26587963104248047,
      "learning_rate": 5.819888103430598e-08,
      "loss": 0.0012,
      "step": 129060
    },
    {
      "epoch": 23467.272727272728,
      "grad_norm": 0.00630042003467679,
      "learning_rate": 5.814441099990891e-08,
      "loss": 0.0011,
      "step": 129070
    },
    {
      "epoch": 23469.090909090908,
      "grad_norm": 0.16989387571811676,
      "learning_rate": 5.8089964893851206e-08,
      "loss": 0.0009,
      "step": 129080
    },
    {
      "epoch": 23470.909090909092,
      "grad_norm": 0.0007962518138810992,
      "learning_rate": 5.803554271908123e-08,
      "loss": 0.0012,
      "step": 129090
    },
    {
      "epoch": 23472.727272727272,
      "grad_norm": 0.0005450157914310694,
      "learning_rate": 5.798114447854635e-08,
      "loss": 0.0011,
      "step": 129100
    },
    {
      "epoch": 23474.545454545456,
      "grad_norm": 0.1682845652103424,
      "learning_rate": 5.792677017519226e-08,
      "loss": 0.0008,
      "step": 129110
    },
    {
      "epoch": 23476.363636363636,
      "grad_norm": 0.2196177989244461,
      "learning_rate": 5.787241981196383e-08,
      "loss": 0.0012,
      "step": 129120
    },
    {
      "epoch": 23478.18181818182,
      "grad_norm": 0.20739705860614777,
      "learning_rate": 5.781809339180421e-08,
      "loss": 0.0012,
      "step": 129130
    },
    {
      "epoch": 23480.0,
      "grad_norm": 0.17183323204517365,
      "learning_rate": 5.776379091765532e-08,
      "loss": 0.001,
      "step": 129140
    },
    {
      "epoch": 23481.81818181818,
      "grad_norm": 0.00046726426808163524,
      "learning_rate": 5.770951239245803e-08,
      "loss": 0.0011,
      "step": 129150
    },
    {
      "epoch": 23483.636363636364,
      "grad_norm": 0.00046478601871058345,
      "learning_rate": 5.76552578191517e-08,
      "loss": 0.001,
      "step": 129160
    },
    {
      "epoch": 23485.454545454544,
      "grad_norm": 0.03692515939474106,
      "learning_rate": 5.7601027200674334e-08,
      "loss": 0.0014,
      "step": 129170
    },
    {
      "epoch": 23487.272727272728,
      "grad_norm": 0.21986831724643707,
      "learning_rate": 5.7546820539962905e-08,
      "loss": 0.0009,
      "step": 129180
    },
    {
      "epoch": 23489.090909090908,
      "grad_norm": 0.0005723271751776338,
      "learning_rate": 5.749263783995278e-08,
      "loss": 0.001,
      "step": 129190
    },
    {
      "epoch": 23490.909090909092,
      "grad_norm": 0.0005937317037023604,
      "learning_rate": 5.743847910357835e-08,
      "loss": 0.0012,
      "step": 129200
    },
    {
      "epoch": 23492.727272727272,
      "grad_norm": 0.2759007513523102,
      "learning_rate": 5.738434433377243e-08,
      "loss": 0.001,
      "step": 129210
    },
    {
      "epoch": 23494.545454545456,
      "grad_norm": 0.0005378458881750703,
      "learning_rate": 5.733023353346661e-08,
      "loss": 0.0011,
      "step": 129220
    },
    {
      "epoch": 23496.363636363636,
      "grad_norm": 0.0005160685395821929,
      "learning_rate": 5.727614670559133e-08,
      "loss": 0.001,
      "step": 129230
    },
    {
      "epoch": 23498.18181818182,
      "grad_norm": 0.2034643590450287,
      "learning_rate": 5.7222083853075585e-08,
      "loss": 0.0012,
      "step": 129240
    },
    {
      "epoch": 23500.0,
      "grad_norm": 0.16541996598243713,
      "learning_rate": 5.716804497884698e-08,
      "loss": 0.001,
      "step": 129250
    },
    {
      "epoch": 23501.81818181818,
      "grad_norm": 0.0009740666719153523,
      "learning_rate": 5.711403008583215e-08,
      "loss": 0.0011,
      "step": 129260
    },
    {
      "epoch": 23503.636363636364,
      "grad_norm": 0.18848389387130737,
      "learning_rate": 5.706003917695618e-08,
      "loss": 0.0012,
      "step": 129270
    },
    {
      "epoch": 23505.454545454544,
      "grad_norm": 0.00040019521838985384,
      "learning_rate": 5.70060722551427e-08,
      "loss": 0.0012,
      "step": 129280
    },
    {
      "epoch": 23507.272727272728,
      "grad_norm": 0.1630670577287674,
      "learning_rate": 5.6952129323314505e-08,
      "loss": 0.001,
      "step": 129290
    },
    {
      "epoch": 23509.090909090908,
      "grad_norm": 0.00047115053166635334,
      "learning_rate": 5.689821038439263e-08,
      "loss": 0.0011,
      "step": 129300
    },
    {
      "epoch": 23510.909090909092,
      "grad_norm": 0.0009326633298769593,
      "learning_rate": 5.68443154412972e-08,
      "loss": 0.0012,
      "step": 129310
    },
    {
      "epoch": 23512.727272727272,
      "grad_norm": 0.0006765715079382062,
      "learning_rate": 5.6790444496946756e-08,
      "loss": 0.001,
      "step": 129320
    },
    {
      "epoch": 23514.545454545456,
      "grad_norm": 0.0003934452834073454,
      "learning_rate": 5.673659755425858e-08,
      "loss": 0.001,
      "step": 129330
    },
    {
      "epoch": 23516.363636363636,
      "grad_norm": 0.0006021431763656437,
      "learning_rate": 5.6682774616148845e-08,
      "loss": 0.0012,
      "step": 129340
    },
    {
      "epoch": 23518.18181818182,
      "grad_norm": 0.222802996635437,
      "learning_rate": 5.6628975685532265e-08,
      "loss": 0.001,
      "step": 129350
    },
    {
      "epoch": 23520.0,
      "grad_norm": 0.16358627378940582,
      "learning_rate": 5.6575200765322074e-08,
      "loss": 0.001,
      "step": 129360
    },
    {
      "epoch": 23521.81818181818,
      "grad_norm": 0.1678525060415268,
      "learning_rate": 5.6521449858430716e-08,
      "loss": 0.0012,
      "step": 129370
    },
    {
      "epoch": 23523.636363636364,
      "grad_norm": 0.0005668981466442347,
      "learning_rate": 5.646772296776875e-08,
      "loss": 0.0009,
      "step": 129380
    },
    {
      "epoch": 23525.454545454544,
      "grad_norm": 0.2802416682243347,
      "learning_rate": 5.641402009624591e-08,
      "loss": 0.0014,
      "step": 129390
    },
    {
      "epoch": 23527.272727272728,
      "grad_norm": 0.01779749244451523,
      "learning_rate": 5.636034124677041e-08,
      "loss": 0.001,
      "step": 129400
    },
    {
      "epoch": 23529.090909090908,
      "grad_norm": 0.20077887177467346,
      "learning_rate": 5.630668642224906e-08,
      "loss": 0.001,
      "step": 129410
    },
    {
      "epoch": 23530.909090909092,
      "grad_norm": 0.00044761496246792376,
      "learning_rate": 5.625305562558763e-08,
      "loss": 0.0012,
      "step": 129420
    },
    {
      "epoch": 23532.727272727272,
      "grad_norm": 0.0022340714931488037,
      "learning_rate": 5.619944885969047e-08,
      "loss": 0.001,
      "step": 129430
    },
    {
      "epoch": 23534.545454545456,
      "grad_norm": 0.0014075887156650424,
      "learning_rate": 5.614586612746036e-08,
      "loss": 0.0009,
      "step": 129440
    },
    {
      "epoch": 23536.363636363636,
      "grad_norm": 0.01788662001490593,
      "learning_rate": 5.609230743179938e-08,
      "loss": 0.0017,
      "step": 129450
    },
    {
      "epoch": 23538.18181818182,
      "grad_norm": 0.20295406877994537,
      "learning_rate": 5.603877277560776e-08,
      "loss": 0.0007,
      "step": 129460
    },
    {
      "epoch": 23540.0,
      "grad_norm": 0.011442504823207855,
      "learning_rate": 5.598526216178451e-08,
      "loss": 0.001,
      "step": 129470
    },
    {
      "epoch": 23541.81818181818,
      "grad_norm": 0.000498730456456542,
      "learning_rate": 5.593177559322776e-08,
      "loss": 0.001,
      "step": 129480
    },
    {
      "epoch": 23543.636363636364,
      "grad_norm": 0.0005348986014723778,
      "learning_rate": 5.587831307283375e-08,
      "loss": 0.0009,
      "step": 129490
    },
    {
      "epoch": 23545.454545454544,
      "grad_norm": 0.2786892056465149,
      "learning_rate": 5.582487460349805e-08,
      "loss": 0.0014,
      "step": 129500
    },
    {
      "epoch": 23545.454545454544,
      "eval_loss": 5.180909156799316,
      "eval_runtime": 0.9501,
      "eval_samples_per_second": 10.525,
      "eval_steps_per_second": 5.263,
      "step": 129500
    },
    {
      "epoch": 23547.272727272728,
      "grad_norm": 0.000775297055952251,
      "learning_rate": 5.5771460188114184e-08,
      "loss": 0.0008,
      "step": 129510
    },
    {
      "epoch": 23549.090909090908,
      "grad_norm": 0.0005348852719180286,
      "learning_rate": 5.5718069829574885e-08,
      "loss": 0.0012,
      "step": 129520
    },
    {
      "epoch": 23550.909090909092,
      "grad_norm": 0.1955282986164093,
      "learning_rate": 5.566470353077163e-08,
      "loss": 0.0012,
      "step": 129530
    },
    {
      "epoch": 23552.727272727272,
      "grad_norm": 0.0008632122771814466,
      "learning_rate": 5.5611361294594314e-08,
      "loss": 0.0012,
      "step": 129540
    },
    {
      "epoch": 23554.545454545456,
      "grad_norm": 0.0029947555158287287,
      "learning_rate": 5.555804312393153e-08,
      "loss": 0.0007,
      "step": 129550
    },
    {
      "epoch": 23556.363636363636,
      "grad_norm": 0.1711561381816864,
      "learning_rate": 5.5504749021670904e-08,
      "loss": 0.0012,
      "step": 129560
    },
    {
      "epoch": 23558.18181818182,
      "grad_norm": 0.20250442624092102,
      "learning_rate": 5.5451478990698356e-08,
      "loss": 0.0011,
      "step": 129570
    },
    {
      "epoch": 23560.0,
      "grad_norm": 0.000741329335141927,
      "learning_rate": 5.539823303389884e-08,
      "loss": 0.0011,
      "step": 129580
    },
    {
      "epoch": 23561.81818181818,
      "grad_norm": 0.18026506900787354,
      "learning_rate": 5.534501115415574e-08,
      "loss": 0.0012,
      "step": 129590
    },
    {
      "epoch": 23563.636363636364,
      "grad_norm": 0.0008518948452547193,
      "learning_rate": 5.529181335435124e-08,
      "loss": 0.0009,
      "step": 129600
    },
    {
      "epoch": 23565.454545454544,
      "grad_norm": 0.01837892085313797,
      "learning_rate": 5.523863963736625e-08,
      "loss": 0.0015,
      "step": 129610
    },
    {
      "epoch": 23567.272727272728,
      "grad_norm": 0.17043474316596985,
      "learning_rate": 5.518549000608042e-08,
      "loss": 0.0009,
      "step": 129620
    },
    {
      "epoch": 23569.090909090908,
      "grad_norm": 0.16992641985416412,
      "learning_rate": 5.5132364463371886e-08,
      "loss": 0.0012,
      "step": 129630
    },
    {
      "epoch": 23570.909090909092,
      "grad_norm": 0.2925910949707031,
      "learning_rate": 5.507926301211774e-08,
      "loss": 0.0011,
      "step": 129640
    },
    {
      "epoch": 23572.727272727272,
      "grad_norm": 0.0006335127982310951,
      "learning_rate": 5.5026185655193625e-08,
      "loss": 0.0009,
      "step": 129650
    },
    {
      "epoch": 23574.545454545456,
      "grad_norm": 0.21665476262569427,
      "learning_rate": 5.497313239547374e-08,
      "loss": 0.0011,
      "step": 129660
    },
    {
      "epoch": 23576.363636363636,
      "grad_norm": 0.28112974762916565,
      "learning_rate": 5.4920103235831464e-08,
      "loss": 0.0014,
      "step": 129670
    },
    {
      "epoch": 23578.18181818182,
      "grad_norm": 0.26955217123031616,
      "learning_rate": 5.4867098179138206e-08,
      "loss": 0.0012,
      "step": 129680
    },
    {
      "epoch": 23580.0,
      "grad_norm": 0.000656545627862215,
      "learning_rate": 5.481411722826479e-08,
      "loss": 0.0009,
      "step": 129690
    },
    {
      "epoch": 23581.81818181818,
      "grad_norm": 0.20244701206684113,
      "learning_rate": 5.476116038607992e-08,
      "loss": 0.001,
      "step": 129700
    },
    {
      "epoch": 23583.636363636364,
      "grad_norm": 0.0007639203104190528,
      "learning_rate": 5.470822765545169e-08,
      "loss": 0.0007,
      "step": 129710
    },
    {
      "epoch": 23585.454545454544,
      "grad_norm": 0.0004608166927937418,
      "learning_rate": 5.46553190392467e-08,
      "loss": 0.0012,
      "step": 129720
    },
    {
      "epoch": 23587.272727272728,
      "grad_norm": 0.20781494677066803,
      "learning_rate": 5.4602434540330034e-08,
      "loss": 0.0013,
      "step": 129730
    },
    {
      "epoch": 23589.090909090908,
      "grad_norm": 0.0011163863819092512,
      "learning_rate": 5.4549574161565584e-08,
      "loss": 0.001,
      "step": 129740
    },
    {
      "epoch": 23590.909090909092,
      "grad_norm": 0.0009980606846511364,
      "learning_rate": 5.44967379058161e-08,
      "loss": 0.0011,
      "step": 129750
    },
    {
      "epoch": 23592.727272727272,
      "grad_norm": 0.17199432849884033,
      "learning_rate": 5.44439257759427e-08,
      "loss": 0.0013,
      "step": 129760
    },
    {
      "epoch": 23594.545454545456,
      "grad_norm": 0.0012186446692794561,
      "learning_rate": 5.4391137774805695e-08,
      "loss": 0.0008,
      "step": 129770
    },
    {
      "epoch": 23596.363636363636,
      "grad_norm": 0.0009746934520080686,
      "learning_rate": 5.4338373905263415e-08,
      "loss": 0.0012,
      "step": 129780
    },
    {
      "epoch": 23598.18181818182,
      "grad_norm": 0.0008538734400644898,
      "learning_rate": 5.428563417017334e-08,
      "loss": 0.001,
      "step": 129790
    },
    {
      "epoch": 23600.0,
      "grad_norm": 0.0009874725947156549,
      "learning_rate": 5.423291857239176e-08,
      "loss": 0.0012,
      "step": 129800
    },
    {
      "epoch": 23601.81818181818,
      "grad_norm": 0.0007924430537968874,
      "learning_rate": 5.4180227114773324e-08,
      "loss": 0.0012,
      "step": 129810
    },
    {
      "epoch": 23603.636363636364,
      "grad_norm": 0.0011340529890730977,
      "learning_rate": 5.412755980017131e-08,
      "loss": 0.0008,
      "step": 129820
    },
    {
      "epoch": 23605.454545454544,
      "grad_norm": 0.219192773103714,
      "learning_rate": 5.4074916631438206e-08,
      "loss": 0.0012,
      "step": 129830
    },
    {
      "epoch": 23607.272727272728,
      "grad_norm": 0.20956480503082275,
      "learning_rate": 5.402229761142463e-08,
      "loss": 0.0012,
      "step": 129840
    },
    {
      "epoch": 23609.090909090908,
      "grad_norm": 0.0003974141727667302,
      "learning_rate": 5.396970274298024e-08,
      "loss": 0.001,
      "step": 129850
    },
    {
      "epoch": 23610.909090909092,
      "grad_norm": 0.21755628287792206,
      "learning_rate": 5.3917132028953096e-08,
      "loss": 0.0012,
      "step": 129860
    },
    {
      "epoch": 23612.727272727272,
      "grad_norm": 0.0014130850322544575,
      "learning_rate": 5.386458547219025e-08,
      "loss": 0.0009,
      "step": 129870
    },
    {
      "epoch": 23614.545454545456,
      "grad_norm": 0.0005012150504626334,
      "learning_rate": 5.381206307553748e-08,
      "loss": 0.0012,
      "step": 129880
    },
    {
      "epoch": 23616.363636363636,
      "grad_norm": 0.16164201498031616,
      "learning_rate": 5.375956484183874e-08,
      "loss": 0.0012,
      "step": 129890
    },
    {
      "epoch": 23618.18181818182,
      "grad_norm": 0.0012830173363909125,
      "learning_rate": 5.37070907739372e-08,
      "loss": 0.001,
      "step": 129900
    },
    {
      "epoch": 23620.0,
      "grad_norm": 0.17636820673942566,
      "learning_rate": 5.365464087467475e-08,
      "loss": 0.0012,
      "step": 129910
    },
    {
      "epoch": 23621.81818181818,
      "grad_norm": 0.0013873710995540023,
      "learning_rate": 5.3602215146891506e-08,
      "loss": 0.0009,
      "step": 129920
    },
    {
      "epoch": 23623.636363636364,
      "grad_norm": 0.17616741359233856,
      "learning_rate": 5.354981359342659e-08,
      "loss": 0.0013,
      "step": 129930
    },
    {
      "epoch": 23625.454545454544,
      "grad_norm": 0.1820969581604004,
      "learning_rate": 5.349743621711783e-08,
      "loss": 0.0013,
      "step": 129940
    },
    {
      "epoch": 23627.272727272728,
      "grad_norm": 0.000518148357514292,
      "learning_rate": 5.3445083020801696e-08,
      "loss": 0.0009,
      "step": 129950
    },
    {
      "epoch": 23629.090909090908,
      "grad_norm": 0.433539479970932,
      "learning_rate": 5.3392754007313304e-08,
      "loss": 0.0012,
      "step": 129960
    },
    {
      "epoch": 23630.909090909092,
      "grad_norm": 0.0004050330608151853,
      "learning_rate": 5.334044917948638e-08,
      "loss": 0.0009,
      "step": 129970
    },
    {
      "epoch": 23632.727272727272,
      "grad_norm": 0.0007313770474866033,
      "learning_rate": 5.3288168540153553e-08,
      "loss": 0.0013,
      "step": 129980
    },
    {
      "epoch": 23634.545454545456,
      "grad_norm": 0.0006681811064481735,
      "learning_rate": 5.323591209214612e-08,
      "loss": 0.001,
      "step": 129990
    },
    {
      "epoch": 23636.363636363636,
      "grad_norm": 0.2080700546503067,
      "learning_rate": 5.318367983829392e-08,
      "loss": 0.001,
      "step": 130000
    },
    {
      "epoch": 23636.363636363636,
      "eval_loss": 5.1275458335876465,
      "eval_runtime": 0.9496,
      "eval_samples_per_second": 10.531,
      "eval_steps_per_second": 5.265,
      "step": 130000
    },
    {
      "epoch": 23638.18181818182,
      "grad_norm": 0.2191821038722992,
      "learning_rate": 5.313147178142541e-08,
      "loss": 0.001,
      "step": 130010
    },
    {
      "epoch": 23640.0,
      "grad_norm": 0.001457155798561871,
      "learning_rate": 5.307928792436811e-08,
      "loss": 0.0011,
      "step": 130020
    },
    {
      "epoch": 23641.81818181818,
      "grad_norm": 0.0005368514102883637,
      "learning_rate": 5.3027128269947875e-08,
      "loss": 0.0012,
      "step": 130030
    },
    {
      "epoch": 23643.636363636364,
      "grad_norm": 0.00040771678322926164,
      "learning_rate": 5.297499282098944e-08,
      "loss": 0.001,
      "step": 130040
    },
    {
      "epoch": 23645.454545454544,
      "grad_norm": 0.2022373229265213,
      "learning_rate": 5.292288158031594e-08,
      "loss": 0.001,
      "step": 130050
    },
    {
      "epoch": 23647.272727272728,
      "grad_norm": 0.18525506556034088,
      "learning_rate": 5.2870794550749555e-08,
      "loss": 0.0011,
      "step": 130060
    },
    {
      "epoch": 23649.090909090908,
      "grad_norm": 0.0005111320642754436,
      "learning_rate": 5.281873173511131e-08,
      "loss": 0.0011,
      "step": 130070
    },
    {
      "epoch": 23650.909090909092,
      "grad_norm": 0.0010983780957758427,
      "learning_rate": 5.276669313622012e-08,
      "loss": 0.0011,
      "step": 130080
    },
    {
      "epoch": 23652.727272727272,
      "grad_norm": 0.00044064727262593806,
      "learning_rate": 5.271467875689434e-08,
      "loss": 0.0011,
      "step": 130090
    },
    {
      "epoch": 23654.545454545456,
      "grad_norm": 0.0016147407004609704,
      "learning_rate": 5.266268859995082e-08,
      "loss": 0.001,
      "step": 130100
    },
    {
      "epoch": 23656.363636363636,
      "grad_norm": 0.00041927548591047525,
      "learning_rate": 5.261072266820499e-08,
      "loss": 0.0013,
      "step": 130110
    },
    {
      "epoch": 23658.18181818182,
      "grad_norm": 0.001242572907358408,
      "learning_rate": 5.2558780964471025e-08,
      "loss": 0.0011,
      "step": 130120
    },
    {
      "epoch": 23660.0,
      "grad_norm": 0.21997201442718506,
      "learning_rate": 5.2506863491561695e-08,
      "loss": 0.0012,
      "step": 130130
    },
    {
      "epoch": 23661.81818181818,
      "grad_norm": 0.0005355098401196301,
      "learning_rate": 5.245497025228873e-08,
      "loss": 0.0012,
      "step": 130140
    },
    {
      "epoch": 23663.636363636364,
      "grad_norm": 0.22232381999492645,
      "learning_rate": 5.240310124946223e-08,
      "loss": 0.0007,
      "step": 130150
    },
    {
      "epoch": 23665.454545454544,
      "grad_norm": 0.21702277660369873,
      "learning_rate": 5.235125648589106e-08,
      "loss": 0.0013,
      "step": 130160
    },
    {
      "epoch": 23667.272727272728,
      "grad_norm": 0.20441170036792755,
      "learning_rate": 5.2299435964382963e-08,
      "loss": 0.001,
      "step": 130170
    },
    {
      "epoch": 23669.090909090908,
      "grad_norm": 0.0004904614761471748,
      "learning_rate": 5.224763968774432e-08,
      "loss": 0.001,
      "step": 130180
    },
    {
      "epoch": 23670.909090909092,
      "grad_norm": 0.16679275035858154,
      "learning_rate": 5.219586765877998e-08,
      "loss": 0.0012,
      "step": 130190
    },
    {
      "epoch": 23672.727272727272,
      "grad_norm": 0.0006819504778832197,
      "learning_rate": 5.2144119880293544e-08,
      "loss": 0.0011,
      "step": 130200
    },
    {
      "epoch": 23674.545454545456,
      "grad_norm": 0.0026168921031057835,
      "learning_rate": 5.2092396355087555e-08,
      "loss": 0.0011,
      "step": 130210
    },
    {
      "epoch": 23676.363636363636,
      "grad_norm": 0.0008554703090339899,
      "learning_rate": 5.204069708596298e-08,
      "loss": 0.0009,
      "step": 130220
    },
    {
      "epoch": 23678.18181818182,
      "grad_norm": 0.0009636017493903637,
      "learning_rate": 5.1989022075719535e-08,
      "loss": 0.0012,
      "step": 130230
    },
    {
      "epoch": 23680.0,
      "grad_norm": 0.006051129195839167,
      "learning_rate": 5.193737132715559e-08,
      "loss": 0.0012,
      "step": 130240
    },
    {
      "epoch": 23681.81818181818,
      "grad_norm": 0.00038131451583467424,
      "learning_rate": 5.1885744843068294e-08,
      "loss": 0.0012,
      "step": 130250
    },
    {
      "epoch": 23683.636363636364,
      "grad_norm": 0.000703762867487967,
      "learning_rate": 5.183414262625363e-08,
      "loss": 0.0007,
      "step": 130260
    },
    {
      "epoch": 23685.454545454544,
      "grad_norm": 0.005365768913179636,
      "learning_rate": 5.17825646795057e-08,
      "loss": 0.0015,
      "step": 130270
    },
    {
      "epoch": 23687.272727272728,
      "grad_norm": 0.0005675500142388046,
      "learning_rate": 5.173101100561783e-08,
      "loss": 0.0008,
      "step": 130280
    },
    {
      "epoch": 23689.090909090908,
      "grad_norm": 0.0004493110754992813,
      "learning_rate": 5.167948160738206e-08,
      "loss": 0.0012,
      "step": 130290
    },
    {
      "epoch": 23690.909090909092,
      "grad_norm": 0.0009284866973757744,
      "learning_rate": 5.162797648758876e-08,
      "loss": 0.001,
      "step": 130300
    },
    {
      "epoch": 23692.727272727272,
      "grad_norm": 0.2174854278564453,
      "learning_rate": 5.1576495649027164e-08,
      "loss": 0.0014,
      "step": 130310
    },
    {
      "epoch": 23694.545454545456,
      "grad_norm": 0.000998457195237279,
      "learning_rate": 5.1525039094485025e-08,
      "loss": 0.0009,
      "step": 130320
    },
    {
      "epoch": 23696.363636363636,
      "grad_norm": 0.0006100488244555891,
      "learning_rate": 5.147360682674923e-08,
      "loss": 0.001,
      "step": 130330
    },
    {
      "epoch": 23698.18181818182,
      "grad_norm": 0.2081114798784256,
      "learning_rate": 5.142219884860483e-08,
      "loss": 0.0012,
      "step": 130340
    },
    {
      "epoch": 23700.0,
      "grad_norm": 0.1685919314622879,
      "learning_rate": 5.137081516283581e-08,
      "loss": 0.0012,
      "step": 130350
    },
    {
      "epoch": 23701.81818181818,
      "grad_norm": 0.17501750588417053,
      "learning_rate": 5.131945577222485e-08,
      "loss": 0.0014,
      "step": 130360
    },
    {
      "epoch": 23703.636363636364,
      "grad_norm": 0.20909757912158966,
      "learning_rate": 5.126812067955333e-08,
      "loss": 0.0009,
      "step": 130370
    },
    {
      "epoch": 23705.454545454544,
      "grad_norm": 0.26902857422828674,
      "learning_rate": 5.121680988760124e-08,
      "loss": 0.0014,
      "step": 130380
    },
    {
      "epoch": 23707.272727272728,
      "grad_norm": 0.1714329570531845,
      "learning_rate": 5.1165523399147256e-08,
      "loss": 0.001,
      "step": 130390
    },
    {
      "epoch": 23709.090909090908,
      "grad_norm": 0.2652871012687683,
      "learning_rate": 5.111426121696866e-08,
      "loss": 0.001,
      "step": 130400
    },
    {
      "epoch": 23710.909090909092,
      "grad_norm": 0.01776866987347603,
      "learning_rate": 5.106302334384172e-08,
      "loss": 0.001,
      "step": 130410
    },
    {
      "epoch": 23712.727272727272,
      "grad_norm": 0.27597007155418396,
      "learning_rate": 5.101180978254099e-08,
      "loss": 0.0012,
      "step": 130420
    },
    {
      "epoch": 23714.545454545456,
      "grad_norm": 0.0003362369316164404,
      "learning_rate": 5.096062053583994e-08,
      "loss": 0.0009,
      "step": 130430
    },
    {
      "epoch": 23716.363636363636,
      "grad_norm": 0.20392480492591858,
      "learning_rate": 5.090945560651072e-08,
      "loss": 0.0012,
      "step": 130440
    },
    {
      "epoch": 23718.18181818182,
      "grad_norm": 0.0004558404325507581,
      "learning_rate": 5.085831499732429e-08,
      "loss": 0.0009,
      "step": 130450
    },
    {
      "epoch": 23720.0,
      "grad_norm": 0.00045714309089817107,
      "learning_rate": 5.080719871104977e-08,
      "loss": 0.0012,
      "step": 130460
    },
    {
      "epoch": 23721.81818181818,
      "grad_norm": 0.0006039729923941195,
      "learning_rate": 5.075610675045566e-08,
      "loss": 0.0012,
      "step": 130470
    },
    {
      "epoch": 23723.636363636364,
      "grad_norm": 0.0005783928791061044,
      "learning_rate": 5.070503911830853e-08,
      "loss": 0.0009,
      "step": 130480
    },
    {
      "epoch": 23725.454545454544,
      "grad_norm": 0.20431795716285706,
      "learning_rate": 5.0653995817374115e-08,
      "loss": 0.0012,
      "step": 130490
    },
    {
      "epoch": 23727.272727272728,
      "grad_norm": 0.0005016503855586052,
      "learning_rate": 5.060297685041659e-08,
      "loss": 0.0009,
      "step": 130500
    },
    {
      "epoch": 23727.272727272728,
      "eval_loss": 5.170422554016113,
      "eval_runtime": 0.9542,
      "eval_samples_per_second": 10.48,
      "eval_steps_per_second": 5.24,
      "step": 130500
    },
    {
      "epoch": 23729.090909090908,
      "grad_norm": 0.0019473329884931445,
      "learning_rate": 5.0551982220198684e-08,
      "loss": 0.0012,
      "step": 130510
    },
    {
      "epoch": 23730.909090909092,
      "grad_norm": 0.0006323282141238451,
      "learning_rate": 5.050101192948214e-08,
      "loss": 0.001,
      "step": 130520
    },
    {
      "epoch": 23732.727272727272,
      "grad_norm": 0.0013559522340074182,
      "learning_rate": 5.045006598102724e-08,
      "loss": 0.001,
      "step": 130530
    },
    {
      "epoch": 23734.545454545456,
      "grad_norm": 0.16636301577091217,
      "learning_rate": 5.039914437759274e-08,
      "loss": 0.0014,
      "step": 130540
    },
    {
      "epoch": 23736.363636363636,
      "grad_norm": 0.0007905554957687855,
      "learning_rate": 5.034824712193636e-08,
      "loss": 0.001,
      "step": 130550
    },
    {
      "epoch": 23738.18181818182,
      "grad_norm": 0.000684175465721637,
      "learning_rate": 5.029737421681446e-08,
      "loss": 0.001,
      "step": 130560
    },
    {
      "epoch": 23740.0,
      "grad_norm": 0.00035588323953561485,
      "learning_rate": 5.0246525664982e-08,
      "loss": 0.0012,
      "step": 130570
    },
    {
      "epoch": 23741.81818181818,
      "grad_norm": 0.0005208241054788232,
      "learning_rate": 5.019570146919261e-08,
      "loss": 0.0012,
      "step": 130580
    },
    {
      "epoch": 23743.636363636364,
      "grad_norm": 0.2764973044395447,
      "learning_rate": 5.014490163219853e-08,
      "loss": 0.0011,
      "step": 130590
    },
    {
      "epoch": 23745.454545454544,
      "grad_norm": 0.1718081384897232,
      "learning_rate": 5.0094126156751016e-08,
      "loss": 0.001,
      "step": 130600
    },
    {
      "epoch": 23747.272727272728,
      "grad_norm": 0.2614462375640869,
      "learning_rate": 5.004337504559963e-08,
      "loss": 0.0012,
      "step": 130610
    },
    {
      "epoch": 23749.090909090908,
      "grad_norm": 0.21085849404335022,
      "learning_rate": 4.999264830149269e-08,
      "loss": 0.0009,
      "step": 130620
    },
    {
      "epoch": 23750.909090909092,
      "grad_norm": 0.0005978753324598074,
      "learning_rate": 4.994194592717732e-08,
      "loss": 0.0012,
      "step": 130630
    },
    {
      "epoch": 23752.727272727272,
      "grad_norm": 0.0003163498768117279,
      "learning_rate": 4.989126792539949e-08,
      "loss": 0.0009,
      "step": 130640
    },
    {
      "epoch": 23754.545454545456,
      "grad_norm": 0.16280563175678253,
      "learning_rate": 4.984061429890324e-08,
      "loss": 0.0011,
      "step": 130650
    },
    {
      "epoch": 23756.363636363636,
      "grad_norm": 0.1681271195411682,
      "learning_rate": 4.978998505043197e-08,
      "loss": 0.0012,
      "step": 130660
    },
    {
      "epoch": 23758.18181818182,
      "grad_norm": 0.0005196337588131428,
      "learning_rate": 4.9739380182727276e-08,
      "loss": 0.0009,
      "step": 130670
    },
    {
      "epoch": 23760.0,
      "grad_norm": 0.0012034375686198473,
      "learning_rate": 4.968879969852985e-08,
      "loss": 0.0012,
      "step": 130680
    },
    {
      "epoch": 23761.81818181818,
      "grad_norm": 0.006076541729271412,
      "learning_rate": 4.9638243600578665e-08,
      "loss": 0.0011,
      "step": 130690
    },
    {
      "epoch": 23763.636363636364,
      "grad_norm": 0.0013609135057777166,
      "learning_rate": 4.958771189161148e-08,
      "loss": 0.0009,
      "step": 130700
    },
    {
      "epoch": 23765.454545454544,
      "grad_norm": 0.001624082331545651,
      "learning_rate": 4.9537204574365e-08,
      "loss": 0.0009,
      "step": 130710
    },
    {
      "epoch": 23767.272727272728,
      "grad_norm": 0.16727599501609802,
      "learning_rate": 4.94867216515743e-08,
      "loss": 0.0013,
      "step": 130720
    },
    {
      "epoch": 23769.090909090908,
      "grad_norm": 0.001849872525781393,
      "learning_rate": 4.943626312597321e-08,
      "loss": 0.001,
      "step": 130730
    },
    {
      "epoch": 23770.909090909092,
      "grad_norm": 0.0006898637511767447,
      "learning_rate": 4.938582900029437e-08,
      "loss": 0.0009,
      "step": 130740
    },
    {
      "epoch": 23772.727272727272,
      "grad_norm": 0.0003964589268434793,
      "learning_rate": 4.9335419277268866e-08,
      "loss": 0.0013,
      "step": 130750
    },
    {
      "epoch": 23774.545454545456,
      "grad_norm": 0.15955471992492676,
      "learning_rate": 4.9285033959626795e-08,
      "loss": 0.0009,
      "step": 130760
    },
    {
      "epoch": 23776.363636363636,
      "grad_norm": 0.21788617968559265,
      "learning_rate": 4.92346730500966e-08,
      "loss": 0.0012,
      "step": 130770
    },
    {
      "epoch": 23778.18181818182,
      "grad_norm": 0.0014179504942148924,
      "learning_rate": 4.918433655140541e-08,
      "loss": 0.001,
      "step": 130780
    },
    {
      "epoch": 23780.0,
      "grad_norm": 0.0048470087349414825,
      "learning_rate": 4.913402446627946e-08,
      "loss": 0.0012,
      "step": 130790
    },
    {
      "epoch": 23781.81818181818,
      "grad_norm": 0.005676762200891972,
      "learning_rate": 4.908373679744315e-08,
      "loss": 0.0011,
      "step": 130800
    },
    {
      "epoch": 23783.636363636364,
      "grad_norm": 0.16796381771564484,
      "learning_rate": 4.903347354761977e-08,
      "loss": 0.001,
      "step": 130810
    },
    {
      "epoch": 23785.454545454544,
      "grad_norm": 0.21829867362976074,
      "learning_rate": 4.8983234719531306e-08,
      "loss": 0.0011,
      "step": 130820
    },
    {
      "epoch": 23787.272727272728,
      "grad_norm": 0.17216531932353973,
      "learning_rate": 4.8933020315898633e-08,
      "loss": 0.0011,
      "step": 130830
    },
    {
      "epoch": 23789.090909090908,
      "grad_norm": 0.0004368251538835466,
      "learning_rate": 4.8882830339440685e-08,
      "loss": 0.0011,
      "step": 130840
    },
    {
      "epoch": 23790.909090909092,
      "grad_norm": 0.0012193485163152218,
      "learning_rate": 4.883266479287579e-08,
      "loss": 0.0012,
      "step": 130850
    },
    {
      "epoch": 23792.727272727272,
      "grad_norm": 0.00041131602483801544,
      "learning_rate": 4.878252367892033e-08,
      "loss": 0.001,
      "step": 130860
    },
    {
      "epoch": 23794.545454545456,
      "grad_norm": 0.19270192086696625,
      "learning_rate": 4.873240700028991e-08,
      "loss": 0.0009,
      "step": 130870
    },
    {
      "epoch": 23796.363636363636,
      "grad_norm": 0.17357107996940613,
      "learning_rate": 4.868231475969847e-08,
      "loss": 0.0013,
      "step": 130880
    },
    {
      "epoch": 23798.18181818182,
      "grad_norm": 0.17406973242759705,
      "learning_rate": 4.863224695985857e-08,
      "loss": 0.0011,
      "step": 130890
    },
    {
      "epoch": 23800.0,
      "grad_norm": 0.0007938290946185589,
      "learning_rate": 4.8582203603481865e-08,
      "loss": 0.0011,
      "step": 130900
    },
    {
      "epoch": 23801.81818181818,
      "grad_norm": 0.0006706376443617046,
      "learning_rate": 4.853218469327824e-08,
      "loss": 0.0009,
      "step": 130910
    },
    {
      "epoch": 23803.636363636364,
      "grad_norm": 0.16566428542137146,
      "learning_rate": 4.848219023195643e-08,
      "loss": 0.0014,
      "step": 130920
    },
    {
      "epoch": 23805.454545454544,
      "grad_norm": 0.20900064706802368,
      "learning_rate": 4.843222022222393e-08,
      "loss": 0.001,
      "step": 130930
    },
    {
      "epoch": 23807.272727272728,
      "grad_norm": 0.00044648119364865124,
      "learning_rate": 4.838227466678668e-08,
      "loss": 0.0009,
      "step": 130940
    },
    {
      "epoch": 23809.090909090908,
      "grad_norm": 0.0004279610875528306,
      "learning_rate": 4.833235356834958e-08,
      "loss": 0.0012,
      "step": 130950
    },
    {
      "epoch": 23810.909090909092,
      "grad_norm": 0.0005873581394553185,
      "learning_rate": 4.828245692961608e-08,
      "loss": 0.0011,
      "step": 130960
    },
    {
      "epoch": 23812.727272727272,
      "grad_norm": 0.16875791549682617,
      "learning_rate": 4.823258475328818e-08,
      "loss": 0.0012,
      "step": 130970
    },
    {
      "epoch": 23814.545454545456,
      "grad_norm": 0.0004774267436005175,
      "learning_rate": 4.818273704206677e-08,
      "loss": 0.001,
      "step": 130980
    },
    {
      "epoch": 23816.363636363636,
      "grad_norm": 0.0005435717175714672,
      "learning_rate": 4.813291379865125e-08,
      "loss": 0.001,
      "step": 130990
    },
    {
      "epoch": 23818.18181818182,
      "grad_norm": 0.17054900527000427,
      "learning_rate": 4.808311502573975e-08,
      "loss": 0.0012,
      "step": 131000
    },
    {
      "epoch": 23818.18181818182,
      "eval_loss": 5.206511497497559,
      "eval_runtime": 0.9503,
      "eval_samples_per_second": 10.523,
      "eval_steps_per_second": 5.262,
      "step": 131000
    },
    {
      "epoch": 23820.0,
      "grad_norm": 0.233604297041893,
      "learning_rate": 4.803334072602916e-08,
      "loss": 0.001,
      "step": 131010
    },
    {
      "epoch": 23821.81818181818,
      "grad_norm": 0.0008985746535472572,
      "learning_rate": 4.798359090221493e-08,
      "loss": 0.0012,
      "step": 131020
    },
    {
      "epoch": 23823.636363636364,
      "grad_norm": 0.2159937620162964,
      "learning_rate": 4.793386555699108e-08,
      "loss": 0.0009,
      "step": 131030
    },
    {
      "epoch": 23825.454545454544,
      "grad_norm": 0.2071792632341385,
      "learning_rate": 4.7884164693050675e-08,
      "loss": 0.0011,
      "step": 131040
    },
    {
      "epoch": 23827.272727272728,
      "grad_norm": 0.2176297903060913,
      "learning_rate": 4.783448831308506e-08,
      "loss": 0.0012,
      "step": 131050
    },
    {
      "epoch": 23829.090909090908,
      "grad_norm": 0.16119490563869476,
      "learning_rate": 4.7784836419784593e-08,
      "loss": 0.001,
      "step": 131060
    },
    {
      "epoch": 23830.909090909092,
      "grad_norm": 0.0005982887232676148,
      "learning_rate": 4.7735209015837995e-08,
      "loss": 0.0011,
      "step": 131070
    },
    {
      "epoch": 23832.727272727272,
      "grad_norm": 0.00047011300921440125,
      "learning_rate": 4.7685606103932797e-08,
      "loss": 0.0009,
      "step": 131080
    },
    {
      "epoch": 23834.545454545456,
      "grad_norm": 0.0004345678025856614,
      "learning_rate": 4.7636027686755284e-08,
      "loss": 0.001,
      "step": 131090
    },
    {
      "epoch": 23836.363636363636,
      "grad_norm": 0.0005268720560707152,
      "learning_rate": 4.758647376699032e-08,
      "loss": 0.0011,
      "step": 131100
    },
    {
      "epoch": 23838.18181818182,
      "grad_norm": 0.0006669875583611429,
      "learning_rate": 4.7536944347321307e-08,
      "loss": 0.001,
      "step": 131110
    },
    {
      "epoch": 23840.0,
      "grad_norm": 0.0005195948760956526,
      "learning_rate": 4.7487439430430656e-08,
      "loss": 0.0012,
      "step": 131120
    },
    {
      "epoch": 23841.81818181818,
      "grad_norm": 0.2681398391723633,
      "learning_rate": 4.7437959018999274e-08,
      "loss": 0.001,
      "step": 131130
    },
    {
      "epoch": 23843.636363636364,
      "grad_norm": 0.0006341712432913482,
      "learning_rate": 4.7388503115706526e-08,
      "loss": 0.0013,
      "step": 131140
    },
    {
      "epoch": 23845.454545454544,
      "grad_norm": 0.2664955258369446,
      "learning_rate": 4.733907172323093e-08,
      "loss": 0.0009,
      "step": 131150
    },
    {
      "epoch": 23847.272727272728,
      "grad_norm": 0.28756484389305115,
      "learning_rate": 4.728966484424912e-08,
      "loss": 0.0012,
      "step": 131160
    },
    {
      "epoch": 23849.090909090908,
      "grad_norm": 0.0006999888573773205,
      "learning_rate": 4.7240282481436966e-08,
      "loss": 0.0009,
      "step": 131170
    },
    {
      "epoch": 23850.909090909092,
      "grad_norm": 0.16985975205898285,
      "learning_rate": 4.7190924637468596e-08,
      "loss": 0.0012,
      "step": 131180
    },
    {
      "epoch": 23852.727272727272,
      "grad_norm": 0.0016039623878896236,
      "learning_rate": 4.7141591315016885e-08,
      "loss": 0.0009,
      "step": 131190
    },
    {
      "epoch": 23854.545454545456,
      "grad_norm": 0.0006288081058301032,
      "learning_rate": 4.7092282516753565e-08,
      "loss": 0.0015,
      "step": 131200
    },
    {
      "epoch": 23856.363636363636,
      "grad_norm": 0.00039324606768786907,
      "learning_rate": 4.704299824534885e-08,
      "loss": 0.0007,
      "step": 131210
    },
    {
      "epoch": 23858.18181818182,
      "grad_norm": 0.2791120409965515,
      "learning_rate": 4.6993738503471606e-08,
      "loss": 0.0013,
      "step": 131220
    },
    {
      "epoch": 23860.0,
      "grad_norm": 0.20765556395053864,
      "learning_rate": 4.694450329378963e-08,
      "loss": 0.0009,
      "step": 131230
    },
    {
      "epoch": 23861.81818181818,
      "grad_norm": 0.000822062196675688,
      "learning_rate": 4.689529261896907e-08,
      "loss": 0.0012,
      "step": 131240
    },
    {
      "epoch": 23863.636363636364,
      "grad_norm": 0.0003820487472694367,
      "learning_rate": 4.684610648167503e-08,
      "loss": 0.0007,
      "step": 131250
    },
    {
      "epoch": 23865.454545454544,
      "grad_norm": 0.0006197454640641809,
      "learning_rate": 4.6796944884571086e-08,
      "loss": 0.0011,
      "step": 131260
    },
    {
      "epoch": 23867.272727272728,
      "grad_norm": 0.22040317952632904,
      "learning_rate": 4.6747807830319397e-08,
      "loss": 0.0012,
      "step": 131270
    },
    {
      "epoch": 23869.090909090908,
      "grad_norm": 0.0009347236482426524,
      "learning_rate": 4.669869532158116e-08,
      "loss": 0.001,
      "step": 131280
    },
    {
      "epoch": 23870.909090909092,
      "grad_norm": 0.19113166630268097,
      "learning_rate": 4.664960736101597e-08,
      "loss": 0.0012,
      "step": 131290
    },
    {
      "epoch": 23872.727272727272,
      "grad_norm": 0.17357636988162994,
      "learning_rate": 4.660054395128199e-08,
      "loss": 0.0008,
      "step": 131300
    },
    {
      "epoch": 23874.545454545456,
      "grad_norm": 0.00032928603468462825,
      "learning_rate": 4.655150509503641e-08,
      "loss": 0.0014,
      "step": 131310
    },
    {
      "epoch": 23876.363636363636,
      "grad_norm": 0.0008675868157297373,
      "learning_rate": 4.650249079493485e-08,
      "loss": 0.0009,
      "step": 131320
    },
    {
      "epoch": 23878.18181818182,
      "grad_norm": 0.0004274653038010001,
      "learning_rate": 4.6453501053631496e-08,
      "loss": 0.0012,
      "step": 131330
    },
    {
      "epoch": 23880.0,
      "grad_norm": 0.3019486963748932,
      "learning_rate": 4.640453587377957e-08,
      "loss": 0.0012,
      "step": 131340
    },
    {
      "epoch": 23881.81818181818,
      "grad_norm": 0.0009098628652282059,
      "learning_rate": 4.635559525803051e-08,
      "loss": 0.0012,
      "step": 131350
    },
    {
      "epoch": 23883.636363636364,
      "grad_norm": 0.17220620810985565,
      "learning_rate": 4.630667920903486e-08,
      "loss": 0.001,
      "step": 131360
    },
    {
      "epoch": 23885.454545454544,
      "grad_norm": 0.23018844425678253,
      "learning_rate": 4.625778772944156e-08,
      "loss": 0.0009,
      "step": 131370
    },
    {
      "epoch": 23887.272727272728,
      "grad_norm": 0.0012701473897323012,
      "learning_rate": 4.62089208218982e-08,
      "loss": 0.001,
      "step": 131380
    },
    {
      "epoch": 23889.090909090908,
      "grad_norm": 0.39541518688201904,
      "learning_rate": 4.616007848905129e-08,
      "loss": 0.0014,
      "step": 131390
    },
    {
      "epoch": 23890.909090909092,
      "grad_norm": 0.0005278376629576087,
      "learning_rate": 4.611126073354571e-08,
      "loss": 0.0011,
      "step": 131400
    },
    {
      "epoch": 23892.727272727272,
      "grad_norm": 0.16361147165298462,
      "learning_rate": 4.606246755802517e-08,
      "loss": 0.001,
      "step": 131410
    },
    {
      "epoch": 23894.545454545456,
      "grad_norm": 0.00046598861808888614,
      "learning_rate": 4.601369896513208e-08,
      "loss": 0.0013,
      "step": 131420
    },
    {
      "epoch": 23896.363636363636,
      "grad_norm": 0.0005921791889704764,
      "learning_rate": 4.5964954957507405e-08,
      "loss": 0.0009,
      "step": 131430
    },
    {
      "epoch": 23898.18181818182,
      "grad_norm": 0.16748936474323273,
      "learning_rate": 4.591623553779095e-08,
      "loss": 0.0013,
      "step": 131440
    },
    {
      "epoch": 23900.0,
      "grad_norm": 0.17066700756549835,
      "learning_rate": 4.586754070862098e-08,
      "loss": 0.001,
      "step": 131450
    },
    {
      "epoch": 23901.81818181818,
      "grad_norm": 0.0006677696364931762,
      "learning_rate": 4.581887047263444e-08,
      "loss": 0.0012,
      "step": 131460
    },
    {
      "epoch": 23903.636363636364,
      "grad_norm": 0.0011804178357124329,
      "learning_rate": 4.577022483246723e-08,
      "loss": 0.0006,
      "step": 131470
    },
    {
      "epoch": 23905.454545454544,
      "grad_norm": 0.20349884033203125,
      "learning_rate": 4.572160379075363e-08,
      "loss": 0.0014,
      "step": 131480
    },
    {
      "epoch": 23907.272727272728,
      "grad_norm": 0.00047678398550488055,
      "learning_rate": 4.567300735012652e-08,
      "loss": 0.0009,
      "step": 131490
    },
    {
      "epoch": 23909.090909090908,
      "grad_norm": 0.19629748165607452,
      "learning_rate": 4.5624435513217873e-08,
      "loss": 0.0013,
      "step": 131500
    },
    {
      "epoch": 23909.090909090908,
      "eval_loss": 5.146056175231934,
      "eval_runtime": 0.9478,
      "eval_samples_per_second": 10.551,
      "eval_steps_per_second": 5.275,
      "step": 131500
    },
    {
      "epoch": 23910.909090909092,
      "grad_norm": 0.17365962266921997,
      "learning_rate": 4.557588828265796e-08,
      "loss": 0.0009,
      "step": 131510
    },
    {
      "epoch": 23912.727272727272,
      "grad_norm": 0.27326008677482605,
      "learning_rate": 4.552736566107562e-08,
      "loss": 0.0013,
      "step": 131520
    },
    {
      "epoch": 23914.545454545456,
      "grad_norm": 0.0010198478121310472,
      "learning_rate": 4.5478867651098875e-08,
      "loss": 0.0006,
      "step": 131530
    },
    {
      "epoch": 23916.363636363636,
      "grad_norm": 0.0009171797428280115,
      "learning_rate": 4.5430394255353835e-08,
      "loss": 0.0015,
      "step": 131540
    },
    {
      "epoch": 23918.18181818182,
      "grad_norm": 0.0007400233298540115,
      "learning_rate": 4.5381945476465735e-08,
      "loss": 0.001,
      "step": 131550
    },
    {
      "epoch": 23920.0,
      "grad_norm": 0.0006720610545016825,
      "learning_rate": 4.53335213170582e-08,
      "loss": 0.0012,
      "step": 131560
    },
    {
      "epoch": 23921.81818181818,
      "grad_norm": 0.17051726579666138,
      "learning_rate": 4.5285121779753465e-08,
      "loss": 0.0012,
      "step": 131570
    },
    {
      "epoch": 23923.636363636364,
      "grad_norm": 0.19518479704856873,
      "learning_rate": 4.5236746867172825e-08,
      "loss": 0.0009,
      "step": 131580
    },
    {
      "epoch": 23925.454545454544,
      "grad_norm": 0.00040404917672276497,
      "learning_rate": 4.518839658193585e-08,
      "loss": 0.0013,
      "step": 131590
    },
    {
      "epoch": 23927.272727272728,
      "grad_norm": 0.2184116244316101,
      "learning_rate": 4.5140070926660835e-08,
      "loss": 0.0009,
      "step": 131600
    },
    {
      "epoch": 23929.090909090908,
      "grad_norm": 0.27998653054237366,
      "learning_rate": 4.5091769903964963e-08,
      "loss": 0.001,
      "step": 131610
    },
    {
      "epoch": 23930.909090909092,
      "grad_norm": 0.22252672910690308,
      "learning_rate": 4.5043493516463804e-08,
      "loss": 0.0012,
      "step": 131620
    },
    {
      "epoch": 23932.727272727272,
      "grad_norm": 0.00045101280556991696,
      "learning_rate": 4.4995241766772e-08,
      "loss": 0.0012,
      "step": 131630
    },
    {
      "epoch": 23934.545454545456,
      "grad_norm": 0.0012981111649423838,
      "learning_rate": 4.494701465750217e-08,
      "loss": 0.0007,
      "step": 131640
    },
    {
      "epoch": 23936.363636363636,
      "grad_norm": 0.2008032351732254,
      "learning_rate": 4.489881219126629e-08,
      "loss": 0.0012,
      "step": 131650
    },
    {
      "epoch": 23938.18181818182,
      "grad_norm": 0.20765598118305206,
      "learning_rate": 4.485063437067471e-08,
      "loss": 0.0012,
      "step": 131660
    },
    {
      "epoch": 23940.0,
      "grad_norm": 0.0004994099144823849,
      "learning_rate": 4.480248119833641e-08,
      "loss": 0.001,
      "step": 131670
    },
    {
      "epoch": 23941.81818181818,
      "grad_norm": 0.0016608601436018944,
      "learning_rate": 4.475435267685901e-08,
      "loss": 0.0012,
      "step": 131680
    },
    {
      "epoch": 23943.636363636364,
      "grad_norm": 0.20896436274051666,
      "learning_rate": 4.470624880884904e-08,
      "loss": 0.0009,
      "step": 131690
    },
    {
      "epoch": 23945.454545454544,
      "grad_norm": 0.0005378542118705809,
      "learning_rate": 4.465816959691149e-08,
      "loss": 0.0012,
      "step": 131700
    },
    {
      "epoch": 23947.272727272728,
      "grad_norm": 0.26185134053230286,
      "learning_rate": 4.461011504364998e-08,
      "loss": 0.0014,
      "step": 131710
    },
    {
      "epoch": 23949.090909090908,
      "grad_norm": 0.20080359280109406,
      "learning_rate": 4.456208515166682e-08,
      "loss": 0.0008,
      "step": 131720
    },
    {
      "epoch": 23950.909090909092,
      "grad_norm": 0.17100267112255096,
      "learning_rate": 4.4514079923563095e-08,
      "loss": 0.0012,
      "step": 131730
    },
    {
      "epoch": 23952.727272727272,
      "grad_norm": 0.2709279954433441,
      "learning_rate": 4.446609936193857e-08,
      "loss": 0.0012,
      "step": 131740
    },
    {
      "epoch": 23954.545454545456,
      "grad_norm": 0.16967721283435822,
      "learning_rate": 4.441814346939149e-08,
      "loss": 0.0009,
      "step": 131750
    },
    {
      "epoch": 23956.363636363636,
      "grad_norm": 0.0007888731197454035,
      "learning_rate": 4.437021224851889e-08,
      "loss": 0.0012,
      "step": 131760
    },
    {
      "epoch": 23958.18181818182,
      "grad_norm": 0.0020647826604545116,
      "learning_rate": 4.4322305701916476e-08,
      "loss": 0.0009,
      "step": 131770
    },
    {
      "epoch": 23960.0,
      "grad_norm": 0.1784849315881729,
      "learning_rate": 4.427442383217861e-08,
      "loss": 0.0012,
      "step": 131780
    },
    {
      "epoch": 23961.81818181818,
      "grad_norm": 0.0005639687879011035,
      "learning_rate": 4.422656664189817e-08,
      "loss": 0.001,
      "step": 131790
    },
    {
      "epoch": 23963.636363636364,
      "grad_norm": 0.20882536470890045,
      "learning_rate": 4.4178734133667016e-08,
      "loss": 0.001,
      "step": 131800
    },
    {
      "epoch": 23965.454545454544,
      "grad_norm": 0.2614668011665344,
      "learning_rate": 4.413092631007525e-08,
      "loss": 0.0011,
      "step": 131810
    },
    {
      "epoch": 23967.272727272728,
      "grad_norm": 0.0005515963421203196,
      "learning_rate": 4.40831431737122e-08,
      "loss": 0.001,
      "step": 131820
    },
    {
      "epoch": 23969.090909090908,
      "grad_norm": 0.2019442319869995,
      "learning_rate": 4.403538472716511e-08,
      "loss": 0.0012,
      "step": 131830
    },
    {
      "epoch": 23970.909090909092,
      "grad_norm": 0.0008223872282542288,
      "learning_rate": 4.398765097302054e-08,
      "loss": 0.0009,
      "step": 131840
    },
    {
      "epoch": 23972.727272727272,
      "grad_norm": 0.17052116990089417,
      "learning_rate": 4.393994191386352e-08,
      "loss": 0.0012,
      "step": 131850
    },
    {
      "epoch": 23974.545454545456,
      "grad_norm": 0.16586697101593018,
      "learning_rate": 4.38922575522776e-08,
      "loss": 0.001,
      "step": 131860
    },
    {
      "epoch": 23976.363636363636,
      "grad_norm": 0.2324327975511551,
      "learning_rate": 4.3844597890845045e-08,
      "loss": 0.0012,
      "step": 131870
    },
    {
      "epoch": 23978.18181818182,
      "grad_norm": 0.0005118917906656861,
      "learning_rate": 4.3796962932146966e-08,
      "loss": 0.001,
      "step": 131880
    },
    {
      "epoch": 23980.0,
      "grad_norm": 0.0005104960291646421,
      "learning_rate": 4.3749352678762894e-08,
      "loss": 0.0012,
      "step": 131890
    },
    {
      "epoch": 23981.81818181818,
      "grad_norm": 0.20853736996650696,
      "learning_rate": 4.370176713327117e-08,
      "loss": 0.001,
      "step": 131900
    },
    {
      "epoch": 23983.636363636364,
      "grad_norm": 0.284373015165329,
      "learning_rate": 4.365420629824862e-08,
      "loss": 0.0012,
      "step": 131910
    },
    {
      "epoch": 23985.454545454544,
      "grad_norm": 0.0005977429682388902,
      "learning_rate": 4.360667017627101e-08,
      "loss": 0.001,
      "step": 131920
    },
    {
      "epoch": 23987.272727272728,
      "grad_norm": 0.23284639418125153,
      "learning_rate": 4.355915876991262e-08,
      "loss": 0.0011,
      "step": 131930
    },
    {
      "epoch": 23989.090909090908,
      "grad_norm": 0.17353928089141846,
      "learning_rate": 4.351167208174639e-08,
      "loss": 0.0011,
      "step": 131940
    },
    {
      "epoch": 23990.909090909092,
      "grad_norm": 0.21075601875782013,
      "learning_rate": 4.3464210114343815e-08,
      "loss": 0.0012,
      "step": 131950
    },
    {
      "epoch": 23992.727272727272,
      "grad_norm": 0.20898067951202393,
      "learning_rate": 4.3416772870275284e-08,
      "loss": 0.0009,
      "step": 131960
    },
    {
      "epoch": 23994.545454545456,
      "grad_norm": 0.20742349326610565,
      "learning_rate": 4.3369360352109695e-08,
      "loss": 0.0013,
      "step": 131970
    },
    {
      "epoch": 23996.363636363636,
      "grad_norm": 0.17391572892665863,
      "learning_rate": 4.332197256241466e-08,
      "loss": 0.0009,
      "step": 131980
    },
    {
      "epoch": 23998.18181818182,
      "grad_norm": 0.2736206352710724,
      "learning_rate": 4.327460950375622e-08,
      "loss": 0.0013,
      "step": 131990
    },
    {
      "epoch": 24000.0,
      "grad_norm": 0.196128249168396,
      "learning_rate": 4.322727117869951e-08,
      "loss": 0.001,
      "step": 132000
    },
    {
      "epoch": 24000.0,
      "eval_loss": 5.151498794555664,
      "eval_runtime": 0.9483,
      "eval_samples_per_second": 10.546,
      "eval_steps_per_second": 5.273,
      "step": 132000
    },
    {
      "epoch": 24001.81818181818,
      "grad_norm": 0.1652631014585495,
      "learning_rate": 4.317995758980819e-08,
      "loss": 0.0012,
      "step": 132010
    },
    {
      "epoch": 24003.636363636364,
      "grad_norm": 0.0004298352578189224,
      "learning_rate": 4.3132668739644165e-08,
      "loss": 0.0011,
      "step": 132020
    },
    {
      "epoch": 24005.454545454544,
      "grad_norm": 0.001675202394835651,
      "learning_rate": 4.3085404630768483e-08,
      "loss": 0.0009,
      "step": 132030
    },
    {
      "epoch": 24007.272727272728,
      "grad_norm": 0.16764846444129944,
      "learning_rate": 4.303816526574078e-08,
      "loss": 0.0012,
      "step": 132040
    },
    {
      "epoch": 24009.090909090908,
      "grad_norm": 0.1722458451986313,
      "learning_rate": 4.299095064711922e-08,
      "loss": 0.001,
      "step": 132050
    },
    {
      "epoch": 24010.909090909092,
      "grad_norm": 0.16540974378585815,
      "learning_rate": 4.294376077746059e-08,
      "loss": 0.0011,
      "step": 132060
    },
    {
      "epoch": 24012.727272727272,
      "grad_norm": 0.22104555368423462,
      "learning_rate": 4.2896595659320523e-08,
      "loss": 0.0012,
      "step": 132070
    },
    {
      "epoch": 24014.545454545456,
      "grad_norm": 0.21885910630226135,
      "learning_rate": 4.284945529525313e-08,
      "loss": 0.0009,
      "step": 132080
    },
    {
      "epoch": 24016.363636363636,
      "grad_norm": 0.2000233232975006,
      "learning_rate": 4.280233968781139e-08,
      "loss": 0.0011,
      "step": 132090
    },
    {
      "epoch": 24018.18181818182,
      "grad_norm": 0.17775611579418182,
      "learning_rate": 4.2755248839546567e-08,
      "loss": 0.001,
      "step": 132100
    },
    {
      "epoch": 24020.0,
      "grad_norm": 0.14248421788215637,
      "learning_rate": 4.270818275300897e-08,
      "loss": 0.001,
      "step": 132110
    },
    {
      "epoch": 24021.81818181818,
      "grad_norm": 0.2012726366519928,
      "learning_rate": 4.2661141430747504e-08,
      "loss": 0.0012,
      "step": 132120
    },
    {
      "epoch": 24023.636363636364,
      "grad_norm": 0.21767747402191162,
      "learning_rate": 4.2614124875309634e-08,
      "loss": 0.001,
      "step": 132130
    },
    {
      "epoch": 24025.454545454544,
      "grad_norm": 0.21736691892147064,
      "learning_rate": 4.256713308924131e-08,
      "loss": 0.0011,
      "step": 132140
    },
    {
      "epoch": 24027.272727272728,
      "grad_norm": 0.2770722210407257,
      "learning_rate": 4.2520166075087624e-08,
      "loss": 0.0012,
      "step": 132150
    },
    {
      "epoch": 24029.090909090908,
      "grad_norm": 0.20720946788787842,
      "learning_rate": 4.247322383539187e-08,
      "loss": 0.0007,
      "step": 132160
    },
    {
      "epoch": 24030.909090909092,
      "grad_norm": 0.17008472979068756,
      "learning_rate": 4.2426306372696175e-08,
      "loss": 0.0012,
      "step": 132170
    },
    {
      "epoch": 24032.727272727272,
      "grad_norm": 0.00048075366066768765,
      "learning_rate": 4.2379413689541234e-08,
      "loss": 0.0011,
      "step": 132180
    },
    {
      "epoch": 24034.545454545456,
      "grad_norm": 0.2716655135154724,
      "learning_rate": 4.2332545788466565e-08,
      "loss": 0.0012,
      "step": 132190
    },
    {
      "epoch": 24036.363636363636,
      "grad_norm": 0.0014654460828751326,
      "learning_rate": 4.228570267201048e-08,
      "loss": 0.0009,
      "step": 132200
    },
    {
      "epoch": 24038.18181818182,
      "grad_norm": 0.00047337875002995133,
      "learning_rate": 4.223888434270939e-08,
      "loss": 0.0012,
      "step": 132210
    },
    {
      "epoch": 24040.0,
      "grad_norm": 0.20203807950019836,
      "learning_rate": 4.2192090803098824e-08,
      "loss": 0.0012,
      "step": 132220
    },
    {
      "epoch": 24041.81818181818,
      "grad_norm": 0.22011424601078033,
      "learning_rate": 4.214532205571292e-08,
      "loss": 0.001,
      "step": 132230
    },
    {
      "epoch": 24043.636363636364,
      "grad_norm": 0.2044968158006668,
      "learning_rate": 4.209857810308437e-08,
      "loss": 0.0012,
      "step": 132240
    },
    {
      "epoch": 24045.454545454544,
      "grad_norm": 0.20621173083782196,
      "learning_rate": 4.205185894774454e-08,
      "loss": 0.0012,
      "step": 132250
    },
    {
      "epoch": 24047.272727272728,
      "grad_norm": 0.16687749326229095,
      "learning_rate": 4.2005164592223365e-08,
      "loss": 0.0009,
      "step": 132260
    },
    {
      "epoch": 24049.090909090908,
      "grad_norm": 0.27428096532821655,
      "learning_rate": 4.195849503904975e-08,
      "loss": 0.0012,
      "step": 132270
    },
    {
      "epoch": 24050.909090909092,
      "grad_norm": 0.2195054441690445,
      "learning_rate": 4.191185029075089e-08,
      "loss": 0.001,
      "step": 132280
    },
    {
      "epoch": 24052.727272727272,
      "grad_norm": 0.17634190618991852,
      "learning_rate": 4.186523034985279e-08,
      "loss": 0.0012,
      "step": 132290
    },
    {
      "epoch": 24054.545454545456,
      "grad_norm": 0.0009980987524613738,
      "learning_rate": 4.181863521888018e-08,
      "loss": 0.0008,
      "step": 132300
    },
    {
      "epoch": 24056.363636363636,
      "grad_norm": 0.0004335728590376675,
      "learning_rate": 4.177206490035651e-08,
      "loss": 0.0012,
      "step": 132310
    },
    {
      "epoch": 24058.18181818182,
      "grad_norm": 0.209506556391716,
      "learning_rate": 4.1725519396803465e-08,
      "loss": 0.0012,
      "step": 132320
    },
    {
      "epoch": 24060.0,
      "grad_norm": 0.001003649435006082,
      "learning_rate": 4.167899871074193e-08,
      "loss": 0.0012,
      "step": 132330
    },
    {
      "epoch": 24061.81818181818,
      "grad_norm": 0.0005669147940352559,
      "learning_rate": 4.1632502844691e-08,
      "loss": 0.001,
      "step": 132340
    },
    {
      "epoch": 24063.636363636364,
      "grad_norm": 0.16761811077594757,
      "learning_rate": 4.158603180116882e-08,
      "loss": 0.0012,
      "step": 132350
    },
    {
      "epoch": 24065.454545454544,
      "grad_norm": 0.0005214427364990115,
      "learning_rate": 4.153958558269188e-08,
      "loss": 0.001,
      "step": 132360
    },
    {
      "epoch": 24067.272727272728,
      "grad_norm": 0.1726141721010208,
      "learning_rate": 4.1493164191775346e-08,
      "loss": 0.001,
      "step": 132370
    },
    {
      "epoch": 24069.090909090908,
      "grad_norm": 0.0007104586693458259,
      "learning_rate": 4.14467676309333e-08,
      "loss": 0.0011,
      "step": 132380
    },
    {
      "epoch": 24070.909090909092,
      "grad_norm": 0.000547683157492429,
      "learning_rate": 4.140039590267835e-08,
      "loss": 0.0012,
      "step": 132390
    },
    {
      "epoch": 24072.727272727272,
      "grad_norm": 0.0009548129746690392,
      "learning_rate": 4.13540490095215e-08,
      "loss": 0.0009,
      "step": 132400
    },
    {
      "epoch": 24074.545454545456,
      "grad_norm": 0.0021953934337943792,
      "learning_rate": 4.1307726953972723e-08,
      "loss": 0.0009,
      "step": 132410
    },
    {
      "epoch": 24076.363636363636,
      "grad_norm": 0.20717589557170868,
      "learning_rate": 4.126142973854069e-08,
      "loss": 0.0016,
      "step": 132420
    },
    {
      "epoch": 24078.18181818182,
      "grad_norm": 0.19912190735340118,
      "learning_rate": 4.1215157365732444e-08,
      "loss": 0.0009,
      "step": 132430
    },
    {
      "epoch": 24080.0,
      "grad_norm": 0.0005893196794204414,
      "learning_rate": 4.116890983805393e-08,
      "loss": 0.0011,
      "step": 132440
    },
    {
      "epoch": 24081.81818181818,
      "grad_norm": 0.26584282517433167,
      "learning_rate": 4.112268715800943e-08,
      "loss": 0.0011,
      "step": 132450
    },
    {
      "epoch": 24083.636363636364,
      "grad_norm": 0.1724187433719635,
      "learning_rate": 4.107648932810237e-08,
      "loss": 0.0009,
      "step": 132460
    },
    {
      "epoch": 24085.454545454544,
      "grad_norm": 0.0006083417683839798,
      "learning_rate": 4.103031635083437e-08,
      "loss": 0.001,
      "step": 132470
    },
    {
      "epoch": 24087.272727272728,
      "grad_norm": 0.005632573738694191,
      "learning_rate": 4.098416822870593e-08,
      "loss": 0.0013,
      "step": 132480
    },
    {
      "epoch": 24089.090909090908,
      "grad_norm": 0.0005333652370609343,
      "learning_rate": 4.093804496421616e-08,
      "loss": 0.0009,
      "step": 132490
    },
    {
      "epoch": 24090.909090909092,
      "grad_norm": 0.20059485733509064,
      "learning_rate": 4.089194655986306e-08,
      "loss": 0.0012,
      "step": 132500
    },
    {
      "epoch": 24090.909090909092,
      "eval_loss": 5.169578552246094,
      "eval_runtime": 0.951,
      "eval_samples_per_second": 10.515,
      "eval_steps_per_second": 5.258,
      "step": 132500
    },
    {
      "epoch": 24092.727272727272,
      "grad_norm": 0.1808389574289322,
      "learning_rate": 4.084587301814269e-08,
      "loss": 0.0009,
      "step": 132510
    },
    {
      "epoch": 24094.545454545456,
      "grad_norm": 0.21697662770748138,
      "learning_rate": 4.0799824341550324e-08,
      "loss": 0.001,
      "step": 132520
    },
    {
      "epoch": 24096.363636363636,
      "grad_norm": 0.0009859090205281973,
      "learning_rate": 4.0753800532579593e-08,
      "loss": 0.0012,
      "step": 132530
    },
    {
      "epoch": 24098.18181818182,
      "grad_norm": 0.19455334544181824,
      "learning_rate": 4.0707801593723e-08,
      "loss": 0.0012,
      "step": 132540
    },
    {
      "epoch": 24100.0,
      "grad_norm": 0.2748342454433441,
      "learning_rate": 4.066182752747155e-08,
      "loss": 0.001,
      "step": 132550
    },
    {
      "epoch": 24101.81818181818,
      "grad_norm": 0.21842136979103088,
      "learning_rate": 4.0615878336314854e-08,
      "loss": 0.0012,
      "step": 132560
    },
    {
      "epoch": 24103.636363636364,
      "grad_norm": 0.0007679864065721631,
      "learning_rate": 4.056995402274122e-08,
      "loss": 0.0009,
      "step": 132570
    },
    {
      "epoch": 24105.454545454544,
      "grad_norm": 0.14942194521427155,
      "learning_rate": 4.0524054589237964e-08,
      "loss": 0.001,
      "step": 132580
    },
    {
      "epoch": 24107.272727272728,
      "grad_norm": 0.16777050495147705,
      "learning_rate": 4.047818003829029e-08,
      "loss": 0.0013,
      "step": 132590
    },
    {
      "epoch": 24109.090909090908,
      "grad_norm": 0.0006004980532452464,
      "learning_rate": 4.043233037238281e-08,
      "loss": 0.001,
      "step": 132600
    },
    {
      "epoch": 24110.909090909092,
      "grad_norm": 0.2687121033668518,
      "learning_rate": 4.038650559399831e-08,
      "loss": 0.001,
      "step": 132610
    },
    {
      "epoch": 24112.727272727272,
      "grad_norm": 0.21909651160240173,
      "learning_rate": 4.0340705705618486e-08,
      "loss": 0.0014,
      "step": 132620
    },
    {
      "epoch": 24114.545454545456,
      "grad_norm": 0.21766777336597443,
      "learning_rate": 4.029493070972362e-08,
      "loss": 0.0009,
      "step": 132630
    },
    {
      "epoch": 24116.363636363636,
      "grad_norm": 0.21056345105171204,
      "learning_rate": 4.0249180608792453e-08,
      "loss": 0.0012,
      "step": 132640
    },
    {
      "epoch": 24118.18181818182,
      "grad_norm": 0.17091868817806244,
      "learning_rate": 4.020345540530273e-08,
      "loss": 0.0012,
      "step": 132650
    },
    {
      "epoch": 24120.0,
      "grad_norm": 0.2966393232345581,
      "learning_rate": 4.015775510173064e-08,
      "loss": 0.0012,
      "step": 132660
    },
    {
      "epoch": 24121.81818181818,
      "grad_norm": 0.0005313499714247882,
      "learning_rate": 4.011207970055086e-08,
      "loss": 0.001,
      "step": 132670
    },
    {
      "epoch": 24123.636363636364,
      "grad_norm": 0.20569592714309692,
      "learning_rate": 4.006642920423703e-08,
      "loss": 0.0012,
      "step": 132680
    },
    {
      "epoch": 24125.454545454544,
      "grad_norm": 0.0007977043278515339,
      "learning_rate": 4.002080361526156e-08,
      "loss": 0.0011,
      "step": 132690
    },
    {
      "epoch": 24127.272727272728,
      "grad_norm": 0.17002691328525543,
      "learning_rate": 3.99752029360948e-08,
      "loss": 0.0011,
      "step": 132700
    },
    {
      "epoch": 24129.090909090908,
      "grad_norm": 0.20960478484630585,
      "learning_rate": 3.9929627169206615e-08,
      "loss": 0.0012,
      "step": 132710
    },
    {
      "epoch": 24130.909090909092,
      "grad_norm": 0.22014585137367249,
      "learning_rate": 3.9884076317064807e-08,
      "loss": 0.001,
      "step": 132720
    },
    {
      "epoch": 24132.727272727272,
      "grad_norm": 0.0005805272958241403,
      "learning_rate": 3.983855038213646e-08,
      "loss": 0.0009,
      "step": 132730
    },
    {
      "epoch": 24134.545454545456,
      "grad_norm": 0.0005961800925433636,
      "learning_rate": 3.9793049366886766e-08,
      "loss": 0.001,
      "step": 132740
    },
    {
      "epoch": 24136.363636363636,
      "grad_norm": 0.20560060441493988,
      "learning_rate": 3.974757327377981e-08,
      "loss": 0.0012,
      "step": 132750
    },
    {
      "epoch": 24138.18181818182,
      "grad_norm": 0.15958037972450256,
      "learning_rate": 3.97021221052784e-08,
      "loss": 0.0012,
      "step": 132760
    },
    {
      "epoch": 24140.0,
      "grad_norm": 0.0034443545155227184,
      "learning_rate": 3.965669586384407e-08,
      "loss": 0.001,
      "step": 132770
    },
    {
      "epoch": 24141.81818181818,
      "grad_norm": 0.0007901300559751689,
      "learning_rate": 3.96112945519364e-08,
      "loss": 0.001,
      "step": 132780
    },
    {
      "epoch": 24143.636363636364,
      "grad_norm": 0.0006084818160161376,
      "learning_rate": 3.956591817201449e-08,
      "loss": 0.0012,
      "step": 132790
    },
    {
      "epoch": 24145.454545454544,
      "grad_norm": 0.0004500492650549859,
      "learning_rate": 3.9520566726535365e-08,
      "loss": 0.001,
      "step": 132800
    },
    {
      "epoch": 24147.272727272728,
      "grad_norm": 0.00038886128459125757,
      "learning_rate": 3.9475240217955174e-08,
      "loss": 0.001,
      "step": 132810
    },
    {
      "epoch": 24149.090909090908,
      "grad_norm": 0.19876430928707123,
      "learning_rate": 3.942993864872851e-08,
      "loss": 0.0012,
      "step": 132820
    },
    {
      "epoch": 24150.909090909092,
      "grad_norm": 0.002371063455939293,
      "learning_rate": 3.9384662021308526e-08,
      "loss": 0.001,
      "step": 132830
    },
    {
      "epoch": 24152.727272727272,
      "grad_norm": 0.27702364325523376,
      "learning_rate": 3.933941033814736e-08,
      "loss": 0.0009,
      "step": 132840
    },
    {
      "epoch": 24154.545454545456,
      "grad_norm": 0.27820485830307007,
      "learning_rate": 3.9294183601695396e-08,
      "loss": 0.0013,
      "step": 132850
    },
    {
      "epoch": 24156.363636363636,
      "grad_norm": 0.0006096852011978626,
      "learning_rate": 3.924898181440184e-08,
      "loss": 0.001,
      "step": 132860
    },
    {
      "epoch": 24158.18181818182,
      "grad_norm": 0.0005518551333807409,
      "learning_rate": 3.920380497871473e-08,
      "loss": 0.0009,
      "step": 132870
    },
    {
      "epoch": 24160.0,
      "grad_norm": 0.20795488357543945,
      "learning_rate": 3.9158653097080496e-08,
      "loss": 0.0012,
      "step": 132880
    },
    {
      "epoch": 24161.81818181818,
      "grad_norm": 0.0009103387128561735,
      "learning_rate": 3.911352617194419e-08,
      "loss": 0.001,
      "step": 132890
    },
    {
      "epoch": 24163.636363636364,
      "grad_norm": 0.21543101966381073,
      "learning_rate": 3.9068424205749794e-08,
      "loss": 0.001,
      "step": 132900
    },
    {
      "epoch": 24165.454545454544,
      "grad_norm": 0.17193779349327087,
      "learning_rate": 3.902334720093958e-08,
      "loss": 0.001,
      "step": 132910
    },
    {
      "epoch": 24167.272727272728,
      "grad_norm": 0.16966010630130768,
      "learning_rate": 3.897829515995493e-08,
      "loss": 0.0015,
      "step": 132920
    },
    {
      "epoch": 24169.090909090908,
      "grad_norm": 0.16210438311100006,
      "learning_rate": 3.893326808523539e-08,
      "loss": 0.0007,
      "step": 132930
    },
    {
      "epoch": 24170.909090909092,
      "grad_norm": 0.2075246125459671,
      "learning_rate": 3.8888265979219346e-08,
      "loss": 0.0012,
      "step": 132940
    },
    {
      "epoch": 24172.727272727272,
      "grad_norm": 0.14824233949184418,
      "learning_rate": 3.884328884434401e-08,
      "loss": 0.0013,
      "step": 132950
    },
    {
      "epoch": 24174.545454545456,
      "grad_norm": 0.000470842671347782,
      "learning_rate": 3.8798336683045055e-08,
      "loss": 0.0008,
      "step": 132960
    },
    {
      "epoch": 24176.363636363636,
      "grad_norm": 0.0005681447801180184,
      "learning_rate": 3.8753409497756626e-08,
      "loss": 0.0011,
      "step": 132970
    },
    {
      "epoch": 24178.18181818182,
      "grad_norm": 0.0003381340065971017,
      "learning_rate": 3.870850729091196e-08,
      "loss": 0.001,
      "step": 132980
    },
    {
      "epoch": 24180.0,
      "grad_norm": 0.0047650812193751335,
      "learning_rate": 3.8663630064942555e-08,
      "loss": 0.0012,
      "step": 132990
    },
    {
      "epoch": 24181.81818181818,
      "grad_norm": 0.20422184467315674,
      "learning_rate": 3.861877782227885e-08,
      "loss": 0.001,
      "step": 133000
    },
    {
      "epoch": 24181.81818181818,
      "eval_loss": 5.172702789306641,
      "eval_runtime": 0.9474,
      "eval_samples_per_second": 10.556,
      "eval_steps_per_second": 5.278,
      "step": 133000
    },
    {
      "epoch": 24183.636363636364,
      "grad_norm": 0.27186331152915955,
      "learning_rate": 3.857395056534962e-08,
      "loss": 0.001,
      "step": 133010
    },
    {
      "epoch": 24185.454545454544,
      "grad_norm": 0.00035360216861590743,
      "learning_rate": 3.852914829658249e-08,
      "loss": 0.001,
      "step": 133020
    },
    {
      "epoch": 24187.272727272728,
      "grad_norm": 0.0004982493701390922,
      "learning_rate": 3.8484371018403794e-08,
      "loss": 0.0012,
      "step": 133030
    },
    {
      "epoch": 24189.090909090908,
      "grad_norm": 0.1662888377904892,
      "learning_rate": 3.843961873323831e-08,
      "loss": 0.0011,
      "step": 133040
    },
    {
      "epoch": 24190.909090909092,
      "grad_norm": 0.2030298262834549,
      "learning_rate": 3.839489144350955e-08,
      "loss": 0.0012,
      "step": 133050
    },
    {
      "epoch": 24192.727272727272,
      "grad_norm": 0.17305661737918854,
      "learning_rate": 3.835018915163979e-08,
      "loss": 0.001,
      "step": 133060
    },
    {
      "epoch": 24194.545454545456,
      "grad_norm": 0.0005233313422650099,
      "learning_rate": 3.8305511860049764e-08,
      "loss": 0.0012,
      "step": 133070
    },
    {
      "epoch": 24196.363636363636,
      "grad_norm": 0.0009674141183495522,
      "learning_rate": 3.8260859571158875e-08,
      "loss": 0.0007,
      "step": 133080
    },
    {
      "epoch": 24198.18181818182,
      "grad_norm": 0.00043695225031115115,
      "learning_rate": 3.821623228738535e-08,
      "loss": 0.0012,
      "step": 133090
    },
    {
      "epoch": 24200.0,
      "grad_norm": 0.17366226017475128,
      "learning_rate": 3.8171630011145875e-08,
      "loss": 0.0011,
      "step": 133100
    },
    {
      "epoch": 24201.81818181818,
      "grad_norm": 0.0010441627819091082,
      "learning_rate": 3.812705274485595e-08,
      "loss": 0.0007,
      "step": 133110
    },
    {
      "epoch": 24203.636363636364,
      "grad_norm": 0.1644192487001419,
      "learning_rate": 3.808250049092954e-08,
      "loss": 0.0015,
      "step": 133120
    },
    {
      "epoch": 24205.454545454544,
      "grad_norm": 0.20919173955917358,
      "learning_rate": 3.803797325177927e-08,
      "loss": 0.0011,
      "step": 133130
    },
    {
      "epoch": 24207.272727272728,
      "grad_norm": 0.0007376829744316638,
      "learning_rate": 3.799347102981665e-08,
      "loss": 0.0007,
      "step": 133140
    },
    {
      "epoch": 24209.090909090908,
      "grad_norm": 0.26986217498779297,
      "learning_rate": 3.7948993827451536e-08,
      "loss": 0.0015,
      "step": 133150
    },
    {
      "epoch": 24210.909090909092,
      "grad_norm": 0.0003939308226108551,
      "learning_rate": 3.7904541647092504e-08,
      "loss": 0.0009,
      "step": 133160
    },
    {
      "epoch": 24212.727272727272,
      "grad_norm": 0.0009169341064989567,
      "learning_rate": 3.786011449114701e-08,
      "loss": 0.0011,
      "step": 133170
    },
    {
      "epoch": 24214.545454545456,
      "grad_norm": 0.000744905206374824,
      "learning_rate": 3.78157123620208e-08,
      "loss": 0.001,
      "step": 133180
    },
    {
      "epoch": 24216.363636363636,
      "grad_norm": 0.16626262664794922,
      "learning_rate": 3.7771335262118566e-08,
      "loss": 0.001,
      "step": 133190
    },
    {
      "epoch": 24218.18181818182,
      "grad_norm": 0.27105996012687683,
      "learning_rate": 3.7726983193843485e-08,
      "loss": 0.0014,
      "step": 133200
    },
    {
      "epoch": 24220.0,
      "grad_norm": 0.0005764086963608861,
      "learning_rate": 3.7682656159597257e-08,
      "loss": 0.0008,
      "step": 133210
    },
    {
      "epoch": 24221.81818181818,
      "grad_norm": 0.16393394768238068,
      "learning_rate": 3.763835416178057e-08,
      "loss": 0.001,
      "step": 133220
    },
    {
      "epoch": 24223.636363636364,
      "grad_norm": 0.20775553584098816,
      "learning_rate": 3.7594077202792566e-08,
      "loss": 0.001,
      "step": 133230
    },
    {
      "epoch": 24225.454545454544,
      "grad_norm": 0.22016802430152893,
      "learning_rate": 3.754982528503081e-08,
      "loss": 0.0012,
      "step": 133240
    },
    {
      "epoch": 24227.272727272728,
      "grad_norm": 0.0005506002344191074,
      "learning_rate": 3.750559841089196e-08,
      "loss": 0.0009,
      "step": 133250
    },
    {
      "epoch": 24229.090909090908,
      "grad_norm": 0.29174816608428955,
      "learning_rate": 3.746139658277103e-08,
      "loss": 0.0014,
      "step": 133260
    },
    {
      "epoch": 24230.909090909092,
      "grad_norm": 0.171062171459198,
      "learning_rate": 3.741721980306162e-08,
      "loss": 0.001,
      "step": 133270
    },
    {
      "epoch": 24232.727272727272,
      "grad_norm": 0.0010616062209010124,
      "learning_rate": 3.737306807415625e-08,
      "loss": 0.0009,
      "step": 133280
    },
    {
      "epoch": 24234.545454545456,
      "grad_norm": 0.23658870160579681,
      "learning_rate": 3.732894139844578e-08,
      "loss": 0.0013,
      "step": 133290
    },
    {
      "epoch": 24236.363636363636,
      "grad_norm": 0.20409241318702698,
      "learning_rate": 3.728483977831998e-08,
      "loss": 0.0009,
      "step": 133300
    },
    {
      "epoch": 24238.18181818182,
      "grad_norm": 0.26399773359298706,
      "learning_rate": 3.724076321616709e-08,
      "loss": 0.0013,
      "step": 133310
    },
    {
      "epoch": 24240.0,
      "grad_norm": 0.0007068425184115767,
      "learning_rate": 3.719671171437394e-08,
      "loss": 0.001,
      "step": 133320
    },
    {
      "epoch": 24241.81818181818,
      "grad_norm": 0.017501726746559143,
      "learning_rate": 3.715268527532628e-08,
      "loss": 0.0012,
      "step": 133330
    },
    {
      "epoch": 24243.636363636364,
      "grad_norm": 0.00045970920473337173,
      "learning_rate": 3.7108683901408254e-08,
      "loss": 0.0007,
      "step": 133340
    },
    {
      "epoch": 24245.454545454544,
      "grad_norm": 0.16789939999580383,
      "learning_rate": 3.706470759500263e-08,
      "loss": 0.0014,
      "step": 133350
    },
    {
      "epoch": 24247.272727272728,
      "grad_norm": 0.0009106653742492199,
      "learning_rate": 3.702075635849106e-08,
      "loss": 0.0009,
      "step": 133360
    },
    {
      "epoch": 24249.090909090908,
      "grad_norm": 0.17147357761859894,
      "learning_rate": 3.697683019425352e-08,
      "loss": 0.0013,
      "step": 133370
    },
    {
      "epoch": 24250.909090909092,
      "grad_norm": 0.20553196966648102,
      "learning_rate": 3.693292910466905e-08,
      "loss": 0.0011,
      "step": 133380
    },
    {
      "epoch": 24252.727272727272,
      "grad_norm": 0.17461958527565002,
      "learning_rate": 3.6889053092114873e-08,
      "loss": 0.0012,
      "step": 133390
    },
    {
      "epoch": 24254.545454545456,
      "grad_norm": 0.0008056199294514954,
      "learning_rate": 3.684520215896703e-08,
      "loss": 0.0012,
      "step": 133400
    },
    {
      "epoch": 24256.363636363636,
      "grad_norm": 0.0004990961751900613,
      "learning_rate": 3.6801376307600386e-08,
      "loss": 0.0009,
      "step": 133410
    },
    {
      "epoch": 24258.18181818182,
      "grad_norm": 0.2167777419090271,
      "learning_rate": 3.675757554038822e-08,
      "loss": 0.0013,
      "step": 133420
    },
    {
      "epoch": 24260.0,
      "grad_norm": 0.0004863979993388057,
      "learning_rate": 3.671379985970247e-08,
      "loss": 0.001,
      "step": 133430
    },
    {
      "epoch": 24261.81818181818,
      "grad_norm": 0.0007044468075037003,
      "learning_rate": 3.667004926791395e-08,
      "loss": 0.0011,
      "step": 133440
    },
    {
      "epoch": 24263.636363636364,
      "grad_norm": 0.0004568247532006353,
      "learning_rate": 3.662632376739177e-08,
      "loss": 0.001,
      "step": 133450
    },
    {
      "epoch": 24265.454545454544,
      "grad_norm": 0.20238876342773438,
      "learning_rate": 3.6582623360503826e-08,
      "loss": 0.001,
      "step": 133460
    },
    {
      "epoch": 24267.272727272728,
      "grad_norm": 0.16452175378799438,
      "learning_rate": 3.653894804961688e-08,
      "loss": 0.0012,
      "step": 133470
    },
    {
      "epoch": 24269.090909090908,
      "grad_norm": 0.17091065645217896,
      "learning_rate": 3.649529783709587e-08,
      "loss": 0.0012,
      "step": 133480
    },
    {
      "epoch": 24270.909090909092,
      "grad_norm": 0.26951923966407776,
      "learning_rate": 3.645167272530497e-08,
      "loss": 0.001,
      "step": 133490
    },
    {
      "epoch": 24272.727272727272,
      "grad_norm": 0.0005746828974224627,
      "learning_rate": 3.6408072716606345e-08,
      "loss": 0.0012,
      "step": 133500
    },
    {
      "epoch": 24272.727272727272,
      "eval_loss": 5.285261631011963,
      "eval_runtime": 0.9506,
      "eval_samples_per_second": 10.52,
      "eval_steps_per_second": 5.26,
      "step": 133500
    },
    {
      "epoch": 24274.545454545456,
      "grad_norm": 0.1767701506614685,
      "learning_rate": 3.636449781336121e-08,
      "loss": 0.0009,
      "step": 133510
    },
    {
      "epoch": 24276.363636363636,
      "grad_norm": 0.00040974776493385434,
      "learning_rate": 3.632094801792945e-08,
      "loss": 0.001,
      "step": 133520
    },
    {
      "epoch": 24278.18181818182,
      "grad_norm": 0.2067088484764099,
      "learning_rate": 3.6277423332669365e-08,
      "loss": 0.0012,
      "step": 133530
    },
    {
      "epoch": 24280.0,
      "grad_norm": 0.20372599363327026,
      "learning_rate": 3.6233923759938e-08,
      "loss": 0.001,
      "step": 133540
    },
    {
      "epoch": 24281.81818181818,
      "grad_norm": 0.00046249301522038877,
      "learning_rate": 3.619044930209109e-08,
      "loss": 0.0012,
      "step": 133550
    },
    {
      "epoch": 24283.636363636364,
      "grad_norm": 0.00043294287752360106,
      "learning_rate": 3.614699996148285e-08,
      "loss": 0.0009,
      "step": 133560
    },
    {
      "epoch": 24285.454545454544,
      "grad_norm": 0.16953998804092407,
      "learning_rate": 3.610357574046652e-08,
      "loss": 0.0014,
      "step": 133570
    },
    {
      "epoch": 24287.272727272728,
      "grad_norm": 0.21820876002311707,
      "learning_rate": 3.606017664139332e-08,
      "loss": 0.0009,
      "step": 133580
    },
    {
      "epoch": 24289.090909090908,
      "grad_norm": 0.17177535593509674,
      "learning_rate": 3.6016802666613666e-08,
      "loss": 0.001,
      "step": 133590
    },
    {
      "epoch": 24290.909090909092,
      "grad_norm": 0.0007887177634984255,
      "learning_rate": 3.597345381847655e-08,
      "loss": 0.0012,
      "step": 133600
    },
    {
      "epoch": 24292.727272727272,
      "grad_norm": 0.2160189151763916,
      "learning_rate": 3.5930130099329436e-08,
      "loss": 0.0012,
      "step": 133610
    },
    {
      "epoch": 24294.545454545456,
      "grad_norm": 0.0005182288587093353,
      "learning_rate": 3.588683151151833e-08,
      "loss": 0.0009,
      "step": 133620
    },
    {
      "epoch": 24296.363636363636,
      "grad_norm": 0.0006159533513709903,
      "learning_rate": 3.5843558057388254e-08,
      "loss": 0.0013,
      "step": 133630
    },
    {
      "epoch": 24298.18181818182,
      "grad_norm": 0.0004939453792758286,
      "learning_rate": 3.580030973928255e-08,
      "loss": 0.0009,
      "step": 133640
    },
    {
      "epoch": 24300.0,
      "grad_norm": 0.15627403557300568,
      "learning_rate": 3.5757086559543236e-08,
      "loss": 0.0011,
      "step": 133650
    },
    {
      "epoch": 24301.81818181818,
      "grad_norm": 0.0008565871976315975,
      "learning_rate": 3.571388852051116e-08,
      "loss": 0.001,
      "step": 133660
    },
    {
      "epoch": 24303.636363636364,
      "grad_norm": 0.1746290773153305,
      "learning_rate": 3.5670715624525506e-08,
      "loss": 0.0012,
      "step": 133670
    },
    {
      "epoch": 24305.454545454544,
      "grad_norm": 0.1686352789402008,
      "learning_rate": 3.5627567873924514e-08,
      "loss": 0.0012,
      "step": 133680
    },
    {
      "epoch": 24307.272727272728,
      "grad_norm": 0.0004020931664854288,
      "learning_rate": 3.558444527104454e-08,
      "loss": 0.0008,
      "step": 133690
    },
    {
      "epoch": 24309.090909090908,
      "grad_norm": 0.2171611189842224,
      "learning_rate": 3.554134781822093e-08,
      "loss": 0.0012,
      "step": 133700
    },
    {
      "epoch": 24310.909090909092,
      "grad_norm": 0.0005516016390174627,
      "learning_rate": 3.5498275517787776e-08,
      "loss": 0.0012,
      "step": 133710
    },
    {
      "epoch": 24312.727272727272,
      "grad_norm": 0.0004620487743522972,
      "learning_rate": 3.545522837207748e-08,
      "loss": 0.0009,
      "step": 133720
    },
    {
      "epoch": 24314.545454545456,
      "grad_norm": 0.0006408377666957676,
      "learning_rate": 3.541220638342118e-08,
      "loss": 0.0017,
      "step": 133730
    },
    {
      "epoch": 24316.363636363636,
      "grad_norm": 0.0006053969846107066,
      "learning_rate": 3.536920955414885e-08,
      "loss": 0.0007,
      "step": 133740
    },
    {
      "epoch": 24318.18181818182,
      "grad_norm": 0.2102176696062088,
      "learning_rate": 3.5326237886588725e-08,
      "loss": 0.0012,
      "step": 133750
    },
    {
      "epoch": 24320.0,
      "grad_norm": 0.0008451628964394331,
      "learning_rate": 3.528329138306824e-08,
      "loss": 0.0012,
      "step": 133760
    },
    {
      "epoch": 24321.81818181818,
      "grad_norm": 0.0008115812670439482,
      "learning_rate": 3.5240370045912735e-08,
      "loss": 0.0012,
      "step": 133770
    },
    {
      "epoch": 24323.636363636364,
      "grad_norm": 0.000528851873241365,
      "learning_rate": 3.519747387744681e-08,
      "loss": 0.0009,
      "step": 133780
    },
    {
      "epoch": 24325.454545454544,
      "grad_norm": 0.19887298345565796,
      "learning_rate": 3.5154602879993446e-08,
      "loss": 0.0012,
      "step": 133790
    },
    {
      "epoch": 24327.272727272728,
      "grad_norm": 0.0005460816319100559,
      "learning_rate": 3.5111757055874326e-08,
      "loss": 0.0009,
      "step": 133800
    },
    {
      "epoch": 24329.090909090908,
      "grad_norm": 0.2040501832962036,
      "learning_rate": 3.50689364074096e-08,
      "loss": 0.0013,
      "step": 133810
    },
    {
      "epoch": 24330.909090909092,
      "grad_norm": 0.17128689587116241,
      "learning_rate": 3.50261409369183e-08,
      "loss": 0.001,
      "step": 133820
    },
    {
      "epoch": 24332.727272727272,
      "grad_norm": 0.1517789661884308,
      "learning_rate": 3.498337064671802e-08,
      "loss": 0.0012,
      "step": 133830
    },
    {
      "epoch": 24334.545454545456,
      "grad_norm": 0.18457652628421783,
      "learning_rate": 3.49406255391248e-08,
      "loss": 0.0013,
      "step": 133840
    },
    {
      "epoch": 24336.363636363636,
      "grad_norm": 0.0008165542967617512,
      "learning_rate": 3.4897905616453505e-08,
      "loss": 0.0007,
      "step": 133850
    },
    {
      "epoch": 24338.18181818182,
      "grad_norm": 0.20605339109897614,
      "learning_rate": 3.4855210881017674e-08,
      "loss": 0.0014,
      "step": 133860
    },
    {
      "epoch": 24340.0,
      "grad_norm": 0.28863027691841125,
      "learning_rate": 3.481254133512945e-08,
      "loss": 0.001,
      "step": 133870
    },
    {
      "epoch": 24341.81818181818,
      "grad_norm": 0.17362481355667114,
      "learning_rate": 3.476989698109939e-08,
      "loss": 0.001,
      "step": 133880
    },
    {
      "epoch": 24343.636363636364,
      "grad_norm": 0.00035863780067302287,
      "learning_rate": 3.4727277821236965e-08,
      "loss": 0.0009,
      "step": 133890
    },
    {
      "epoch": 24345.454545454544,
      "grad_norm": 0.0003422565932851285,
      "learning_rate": 3.468468385785023e-08,
      "loss": 0.0012,
      "step": 133900
    },
    {
      "epoch": 24347.272727272728,
      "grad_norm": 0.003773416392505169,
      "learning_rate": 3.464211509324583e-08,
      "loss": 0.0012,
      "step": 133910
    },
    {
      "epoch": 24349.090909090908,
      "grad_norm": 0.0021833430510014296,
      "learning_rate": 3.4599571529728864e-08,
      "loss": 0.001,
      "step": 133920
    },
    {
      "epoch": 24350.909090909092,
      "grad_norm": 0.21698860824108124,
      "learning_rate": 3.455705316960344e-08,
      "loss": 0.0012,
      "step": 133930
    },
    {
      "epoch": 24352.727272727272,
      "grad_norm": 0.0006618326297029853,
      "learning_rate": 3.4514560015172e-08,
      "loss": 0.0007,
      "step": 133940
    },
    {
      "epoch": 24354.545454545456,
      "grad_norm": 0.0007592897745780647,
      "learning_rate": 3.447209206873591e-08,
      "loss": 0.0013,
      "step": 133950
    },
    {
      "epoch": 24356.363636363636,
      "grad_norm": 0.22179201245307922,
      "learning_rate": 3.4429649332594735e-08,
      "loss": 0.0013,
      "step": 133960
    },
    {
      "epoch": 24358.18181818182,
      "grad_norm": 0.2077471911907196,
      "learning_rate": 3.438723180904696e-08,
      "loss": 0.0009,
      "step": 133970
    },
    {
      "epoch": 24360.0,
      "grad_norm": 0.22160732746124268,
      "learning_rate": 3.4344839500389854e-08,
      "loss": 0.001,
      "step": 133980
    },
    {
      "epoch": 24361.81818181818,
      "grad_norm": 0.0005672067636623979,
      "learning_rate": 3.4302472408919035e-08,
      "loss": 0.001,
      "step": 133990
    },
    {
      "epoch": 24363.636363636364,
      "grad_norm": 0.0009718927321955562,
      "learning_rate": 3.426013053692878e-08,
      "loss": 0.0013,
      "step": 134000
    },
    {
      "epoch": 24363.636363636364,
      "eval_loss": 5.220911979675293,
      "eval_runtime": 0.95,
      "eval_samples_per_second": 10.526,
      "eval_steps_per_second": 5.263,
      "step": 134000
    },
    {
      "epoch": 24365.454545454544,
      "grad_norm": 0.1610041856765747,
      "learning_rate": 3.4217813886712244e-08,
      "loss": 0.0009,
      "step": 134010
    },
    {
      "epoch": 24367.272727272728,
      "grad_norm": 0.20130498707294464,
      "learning_rate": 3.417552246056099e-08,
      "loss": 0.0013,
      "step": 134020
    },
    {
      "epoch": 24369.090909090908,
      "grad_norm": 0.0005694438004866242,
      "learning_rate": 3.413325626076524e-08,
      "loss": 0.0009,
      "step": 134030
    },
    {
      "epoch": 24370.909090909092,
      "grad_norm": 0.0010222253622487187,
      "learning_rate": 3.409101528961378e-08,
      "loss": 0.0012,
      "step": 134040
    },
    {
      "epoch": 24372.727272727272,
      "grad_norm": 0.22306393086910248,
      "learning_rate": 3.4048799549394266e-08,
      "loss": 0.0009,
      "step": 134050
    },
    {
      "epoch": 24374.545454545456,
      "grad_norm": 0.00039957708213478327,
      "learning_rate": 3.400660904239305e-08,
      "loss": 0.001,
      "step": 134060
    },
    {
      "epoch": 24376.363636363636,
      "grad_norm": 0.0009457073756493628,
      "learning_rate": 3.3964443770894523e-08,
      "loss": 0.001,
      "step": 134070
    },
    {
      "epoch": 24378.18181818182,
      "grad_norm": 0.006076953373849392,
      "learning_rate": 3.3922303737182354e-08,
      "loss": 0.0015,
      "step": 134080
    },
    {
      "epoch": 24380.0,
      "grad_norm": 0.00044101744424551725,
      "learning_rate": 3.388018894353861e-08,
      "loss": 0.0009,
      "step": 134090
    },
    {
      "epoch": 24381.81818181818,
      "grad_norm": 0.00044362034532241523,
      "learning_rate": 3.3838099392243915e-08,
      "loss": 0.0013,
      "step": 134100
    },
    {
      "epoch": 24383.636363636364,
      "grad_norm": 0.0005425876588560641,
      "learning_rate": 3.379603508557766e-08,
      "loss": 0.0011,
      "step": 134110
    },
    {
      "epoch": 24385.454545454544,
      "grad_norm": 0.20775607228279114,
      "learning_rate": 3.3753996025817635e-08,
      "loss": 0.0013,
      "step": 134120
    },
    {
      "epoch": 24387.272727272728,
      "grad_norm": 0.0006114186835475266,
      "learning_rate": 3.3711982215240686e-08,
      "loss": 0.0009,
      "step": 134130
    },
    {
      "epoch": 24389.090909090908,
      "grad_norm": 0.2768057584762573,
      "learning_rate": 3.366999365612189e-08,
      "loss": 0.001,
      "step": 134140
    },
    {
      "epoch": 24390.909090909092,
      "grad_norm": 0.0008074636571109295,
      "learning_rate": 3.362803035073503e-08,
      "loss": 0.001,
      "step": 134150
    },
    {
      "epoch": 24392.727272727272,
      "grad_norm": 0.17209941148757935,
      "learning_rate": 3.3586092301352674e-08,
      "loss": 0.001,
      "step": 134160
    },
    {
      "epoch": 24394.545454545456,
      "grad_norm": 0.22126349806785583,
      "learning_rate": 3.3544179510246076e-08,
      "loss": 0.001,
      "step": 134170
    },
    {
      "epoch": 24396.363636363636,
      "grad_norm": 0.17041248083114624,
      "learning_rate": 3.3502291979684846e-08,
      "loss": 0.0013,
      "step": 134180
    },
    {
      "epoch": 24398.18181818182,
      "grad_norm": 0.0006118929595686495,
      "learning_rate": 3.346042971193741e-08,
      "loss": 0.0007,
      "step": 134190
    },
    {
      "epoch": 24400.0,
      "grad_norm": 0.2191496193408966,
      "learning_rate": 3.341859270927067e-08,
      "loss": 0.0012,
      "step": 134200
    },
    {
      "epoch": 24401.81818181818,
      "grad_norm": 0.2198682725429535,
      "learning_rate": 3.337678097395047e-08,
      "loss": 0.0012,
      "step": 134210
    },
    {
      "epoch": 24403.636363636364,
      "grad_norm": 0.22009596228599548,
      "learning_rate": 3.333499450824101e-08,
      "loss": 0.001,
      "step": 134220
    },
    {
      "epoch": 24405.454545454544,
      "grad_norm": 0.16936679184436798,
      "learning_rate": 3.329323331440509e-08,
      "loss": 0.001,
      "step": 134230
    },
    {
      "epoch": 24407.272727272728,
      "grad_norm": 0.21910466253757477,
      "learning_rate": 3.32514973947044e-08,
      "loss": 0.0011,
      "step": 134240
    },
    {
      "epoch": 24409.090909090908,
      "grad_norm": 0.0010918438201770186,
      "learning_rate": 3.3209786751399184e-08,
      "loss": 0.001,
      "step": 134250
    },
    {
      "epoch": 24410.909090909092,
      "grad_norm": 0.17719432711601257,
      "learning_rate": 3.316810138674803e-08,
      "loss": 0.0012,
      "step": 134260
    },
    {
      "epoch": 24412.727272727272,
      "grad_norm": 0.16654528677463531,
      "learning_rate": 3.31264413030084e-08,
      "loss": 0.0012,
      "step": 134270
    },
    {
      "epoch": 24414.545454545456,
      "grad_norm": 0.0022013194393366575,
      "learning_rate": 3.308480650243661e-08,
      "loss": 0.0009,
      "step": 134280
    },
    {
      "epoch": 24416.363636363636,
      "grad_norm": 0.0016568807186558843,
      "learning_rate": 3.3043196987287134e-08,
      "loss": 0.001,
      "step": 134290
    },
    {
      "epoch": 24418.18181818182,
      "grad_norm": 0.0014462131075561047,
      "learning_rate": 3.300161275981339e-08,
      "loss": 0.001,
      "step": 134300
    },
    {
      "epoch": 24420.0,
      "grad_norm": 0.2189093679189682,
      "learning_rate": 3.2960053822267245e-08,
      "loss": 0.0012,
      "step": 134310
    },
    {
      "epoch": 24421.81818181818,
      "grad_norm": 0.1692674607038498,
      "learning_rate": 3.291852017689945e-08,
      "loss": 0.001,
      "step": 134320
    },
    {
      "epoch": 24423.636363636364,
      "grad_norm": 0.20932802557945251,
      "learning_rate": 3.287701182595909e-08,
      "loss": 0.001,
      "step": 134330
    },
    {
      "epoch": 24425.454545454544,
      "grad_norm": 0.21587581932544708,
      "learning_rate": 3.283552877169399e-08,
      "loss": 0.0015,
      "step": 134340
    },
    {
      "epoch": 24427.272727272728,
      "grad_norm": 0.0008429111912846565,
      "learning_rate": 3.2794071016350735e-08,
      "loss": 0.0008,
      "step": 134350
    },
    {
      "epoch": 24429.090909090908,
      "grad_norm": 0.0014943800633773208,
      "learning_rate": 3.2752638562174416e-08,
      "loss": 0.0012,
      "step": 134360
    },
    {
      "epoch": 24430.909090909092,
      "grad_norm": 0.0005368071724660695,
      "learning_rate": 3.271123141140886e-08,
      "loss": 0.001,
      "step": 134370
    },
    {
      "epoch": 24432.727272727272,
      "grad_norm": 0.17157112061977386,
      "learning_rate": 3.266984956629626e-08,
      "loss": 0.0013,
      "step": 134380
    },
    {
      "epoch": 24434.545454545456,
      "grad_norm": 0.1615929752588272,
      "learning_rate": 3.262849302907766e-08,
      "loss": 0.001,
      "step": 134390
    },
    {
      "epoch": 24436.363636363636,
      "grad_norm": 0.045205019414424896,
      "learning_rate": 3.258716180199278e-08,
      "loss": 0.001,
      "step": 134400
    },
    {
      "epoch": 24438.18181818182,
      "grad_norm": 0.22015626728534698,
      "learning_rate": 3.254585588727987e-08,
      "loss": 0.001,
      "step": 134410
    },
    {
      "epoch": 24440.0,
      "grad_norm": 0.216851145029068,
      "learning_rate": 3.250457528717571e-08,
      "loss": 0.0012,
      "step": 134420
    },
    {
      "epoch": 24441.81818181818,
      "grad_norm": 0.20438800752162933,
      "learning_rate": 3.246332000391583e-08,
      "loss": 0.0012,
      "step": 134430
    },
    {
      "epoch": 24443.636363636364,
      "grad_norm": 0.2862594425678253,
      "learning_rate": 3.242209003973462e-08,
      "loss": 0.0012,
      "step": 134440
    },
    {
      "epoch": 24445.454545454544,
      "grad_norm": 0.21986953914165497,
      "learning_rate": 3.238088539686451e-08,
      "loss": 0.001,
      "step": 134450
    },
    {
      "epoch": 24447.272727272728,
      "grad_norm": 0.2153240442276001,
      "learning_rate": 3.233970607753717e-08,
      "loss": 0.001,
      "step": 134460
    },
    {
      "epoch": 24449.090909090908,
      "grad_norm": 0.2694622874259949,
      "learning_rate": 3.229855208398241e-08,
      "loss": 0.0012,
      "step": 134470
    },
    {
      "epoch": 24450.909090909092,
      "grad_norm": 0.01247071847319603,
      "learning_rate": 3.2257423418429076e-08,
      "loss": 0.0009,
      "step": 134480
    },
    {
      "epoch": 24452.727272727272,
      "grad_norm": 0.0010490150889381766,
      "learning_rate": 3.221632008310443e-08,
      "loss": 0.0011,
      "step": 134490
    },
    {
      "epoch": 24454.545454545456,
      "grad_norm": 0.00046079058665782213,
      "learning_rate": 3.217524208023431e-08,
      "loss": 0.0007,
      "step": 134500
    },
    {
      "epoch": 24454.545454545456,
      "eval_loss": 5.281432151794434,
      "eval_runtime": 0.9476,
      "eval_samples_per_second": 10.553,
      "eval_steps_per_second": 5.276,
      "step": 134500
    },
    {
      "epoch": 24456.363636363636,
      "grad_norm": 0.16558995842933655,
      "learning_rate": 3.213418941204332e-08,
      "loss": 0.0011,
      "step": 134510
    },
    {
      "epoch": 24458.18181818182,
      "grad_norm": 0.006953134201467037,
      "learning_rate": 3.209316208075463e-08,
      "loss": 0.0015,
      "step": 134520
    },
    {
      "epoch": 24460.0,
      "grad_norm": 0.21898941695690155,
      "learning_rate": 3.2052160088589964e-08,
      "loss": 0.0009,
      "step": 134530
    },
    {
      "epoch": 24461.81818181818,
      "grad_norm": 0.0005213021067902446,
      "learning_rate": 3.201118343776982e-08,
      "loss": 0.0011,
      "step": 134540
    },
    {
      "epoch": 24463.636363636364,
      "grad_norm": 0.26445016264915466,
      "learning_rate": 3.1970232130513367e-08,
      "loss": 0.0011,
      "step": 134550
    },
    {
      "epoch": 24465.454545454544,
      "grad_norm": 0.2681538164615631,
      "learning_rate": 3.192930616903816e-08,
      "loss": 0.0009,
      "step": 134560
    },
    {
      "epoch": 24467.272727272728,
      "grad_norm": 0.0005179460858926177,
      "learning_rate": 3.1888405555560494e-08,
      "loss": 0.0008,
      "step": 134570
    },
    {
      "epoch": 24469.090909090908,
      "grad_norm": 0.001139571308158338,
      "learning_rate": 3.184753029229531e-08,
      "loss": 0.0012,
      "step": 134580
    },
    {
      "epoch": 24470.909090909092,
      "grad_norm": 0.17729783058166504,
      "learning_rate": 3.1806680381456284e-08,
      "loss": 0.0012,
      "step": 134590
    },
    {
      "epoch": 24472.727272727272,
      "grad_norm": 0.20212525129318237,
      "learning_rate": 3.1765855825255536e-08,
      "loss": 0.001,
      "step": 134600
    },
    {
      "epoch": 24474.545454545456,
      "grad_norm": 0.2777429521083832,
      "learning_rate": 3.172505662590386e-08,
      "loss": 0.0014,
      "step": 134610
    },
    {
      "epoch": 24476.363636363636,
      "grad_norm": 0.00044433088623918593,
      "learning_rate": 3.1684282785610695e-08,
      "loss": 0.0007,
      "step": 134620
    },
    {
      "epoch": 24478.18181818182,
      "grad_norm": 0.17135538160800934,
      "learning_rate": 3.164353430658428e-08,
      "loss": 0.0012,
      "step": 134630
    },
    {
      "epoch": 24480.0,
      "grad_norm": 0.22089862823486328,
      "learning_rate": 3.160281119103109e-08,
      "loss": 0.001,
      "step": 134640
    },
    {
      "epoch": 24481.81818181818,
      "grad_norm": 0.2204224020242691,
      "learning_rate": 3.156211344115661e-08,
      "loss": 0.0012,
      "step": 134650
    },
    {
      "epoch": 24483.636363636364,
      "grad_norm": 0.22075814008712769,
      "learning_rate": 3.1521441059164656e-08,
      "loss": 0.0012,
      "step": 134660
    },
    {
      "epoch": 24485.454545454544,
      "grad_norm": 0.1742621809244156,
      "learning_rate": 3.148079404725801e-08,
      "loss": 0.0009,
      "step": 134670
    },
    {
      "epoch": 24487.272727272728,
      "grad_norm": 0.0006620201165787876,
      "learning_rate": 3.144017240763775e-08,
      "loss": 0.001,
      "step": 134680
    },
    {
      "epoch": 24489.090909090908,
      "grad_norm": 0.21941950917243958,
      "learning_rate": 3.1399576142503606e-08,
      "loss": 0.001,
      "step": 134690
    },
    {
      "epoch": 24490.909090909092,
      "grad_norm": 0.2083195447921753,
      "learning_rate": 3.135900525405427e-08,
      "loss": 0.0012,
      "step": 134700
    },
    {
      "epoch": 24492.727272727272,
      "grad_norm": 0.26667463779449463,
      "learning_rate": 3.131845974448671e-08,
      "loss": 0.0011,
      "step": 134710
    },
    {
      "epoch": 24494.545454545456,
      "grad_norm": 0.22192727029323578,
      "learning_rate": 3.1277939615996566e-08,
      "loss": 0.0013,
      "step": 134720
    },
    {
      "epoch": 24496.363636363636,
      "grad_norm": 0.22149799764156342,
      "learning_rate": 3.1237444870778285e-08,
      "loss": 0.001,
      "step": 134730
    },
    {
      "epoch": 24498.18181818182,
      "grad_norm": 0.29408758878707886,
      "learning_rate": 3.119697551102468e-08,
      "loss": 0.0011,
      "step": 134740
    },
    {
      "epoch": 24500.0,
      "grad_norm": 0.22139935195446014,
      "learning_rate": 3.115653153892761e-08,
      "loss": 0.0009,
      "step": 134750
    },
    {
      "epoch": 24501.81818181818,
      "grad_norm": 0.17011518776416779,
      "learning_rate": 3.111611295667704e-08,
      "loss": 0.0012,
      "step": 134760
    },
    {
      "epoch": 24503.636363636364,
      "grad_norm": 0.0010027885437011719,
      "learning_rate": 3.1075719766461836e-08,
      "loss": 0.0009,
      "step": 134770
    },
    {
      "epoch": 24505.454545454544,
      "grad_norm": 0.21865412592887878,
      "learning_rate": 3.103535197046958e-08,
      "loss": 0.0011,
      "step": 134780
    },
    {
      "epoch": 24507.272727272728,
      "grad_norm": 0.0010785901686176658,
      "learning_rate": 3.09950095708863e-08,
      "loss": 0.0011,
      "step": 134790
    },
    {
      "epoch": 24509.090909090908,
      "grad_norm": 0.1735893189907074,
      "learning_rate": 3.095469256989658e-08,
      "loss": 0.0011,
      "step": 134800
    },
    {
      "epoch": 24510.909090909092,
      "grad_norm": 0.0004925144021399319,
      "learning_rate": 3.091440096968401e-08,
      "loss": 0.0008,
      "step": 134810
    },
    {
      "epoch": 24512.727272727272,
      "grad_norm": 0.0005887594888918102,
      "learning_rate": 3.087413477243034e-08,
      "loss": 0.0008,
      "step": 134820
    },
    {
      "epoch": 24514.545454545456,
      "grad_norm": 0.0003334370267111808,
      "learning_rate": 3.083389398031616e-08,
      "loss": 0.0015,
      "step": 134830
    },
    {
      "epoch": 24516.363636363636,
      "grad_norm": 0.1635536104440689,
      "learning_rate": 3.079367859552084e-08,
      "loss": 0.0011,
      "step": 134840
    },
    {
      "epoch": 24518.18181818182,
      "grad_norm": 0.04175392538309097,
      "learning_rate": 3.0753488620222036e-08,
      "loss": 0.0013,
      "step": 134850
    },
    {
      "epoch": 24520.0,
      "grad_norm": 0.0004957648343406618,
      "learning_rate": 3.071332405659632e-08,
      "loss": 0.0009,
      "step": 134860
    },
    {
      "epoch": 24521.81818181818,
      "grad_norm": 0.21883267164230347,
      "learning_rate": 3.067318490681875e-08,
      "loss": 0.0009,
      "step": 134870
    },
    {
      "epoch": 24523.636363636364,
      "grad_norm": 0.000502389098983258,
      "learning_rate": 3.0633071173062966e-08,
      "loss": 0.0012,
      "step": 134880
    },
    {
      "epoch": 24525.454545454544,
      "grad_norm": 0.0005631967214867473,
      "learning_rate": 3.059298285750139e-08,
      "loss": 0.0016,
      "step": 134890
    },
    {
      "epoch": 24527.272727272728,
      "grad_norm": 0.21527491509914398,
      "learning_rate": 3.055291996230491e-08,
      "loss": 0.001,
      "step": 134900
    },
    {
      "epoch": 24529.090909090908,
      "grad_norm": 0.1604662537574768,
      "learning_rate": 3.0512882489643064e-08,
      "loss": 0.0009,
      "step": 134910
    },
    {
      "epoch": 24530.909090909092,
      "grad_norm": 0.0011641501914709806,
      "learning_rate": 3.0472870441684175e-08,
      "loss": 0.0012,
      "step": 134920
    },
    {
      "epoch": 24532.727272727272,
      "grad_norm": 0.163065105676651,
      "learning_rate": 3.043288382059489e-08,
      "loss": 0.0012,
      "step": 134930
    },
    {
      "epoch": 24534.545454545456,
      "grad_norm": 0.0017899805679917336,
      "learning_rate": 3.039292262854087e-08,
      "loss": 0.0009,
      "step": 134940
    },
    {
      "epoch": 24536.363636363636,
      "grad_norm": 0.2669828236103058,
      "learning_rate": 3.0352986867686e-08,
      "loss": 0.0016,
      "step": 134950
    },
    {
      "epoch": 24538.18181818182,
      "grad_norm": 0.2856985926628113,
      "learning_rate": 3.031307654019299e-08,
      "loss": 0.001,
      "step": 134960
    },
    {
      "epoch": 24540.0,
      "grad_norm": 0.217527374625206,
      "learning_rate": 3.0273191648223284e-08,
      "loss": 0.0009,
      "step": 134970
    },
    {
      "epoch": 24541.81818181818,
      "grad_norm": 0.000716790440492332,
      "learning_rate": 3.023333219393675e-08,
      "loss": 0.0009,
      "step": 134980
    },
    {
      "epoch": 24543.636363636364,
      "grad_norm": 0.18501846492290497,
      "learning_rate": 3.019349817949179e-08,
      "loss": 0.001,
      "step": 134990
    },
    {
      "epoch": 24545.454545454544,
      "grad_norm": 0.17107927799224854,
      "learning_rate": 3.015368960704584e-08,
      "loss": 0.0012,
      "step": 135000
    },
    {
      "epoch": 24545.454545454544,
      "eval_loss": 5.1504106521606445,
      "eval_runtime": 0.9503,
      "eval_samples_per_second": 10.523,
      "eval_steps_per_second": 5.262,
      "step": 135000
    },
    {
      "epoch": 24547.272727272728,
      "grad_norm": 0.0006001651636324823,
      "learning_rate": 3.011390647875456e-08,
      "loss": 0.001,
      "step": 135010
    },
    {
      "epoch": 24549.090909090908,
      "grad_norm": 0.000517002772539854,
      "learning_rate": 3.007414879677228e-08,
      "loss": 0.0012,
      "step": 135020
    },
    {
      "epoch": 24550.909090909092,
      "grad_norm": 0.0009006819454953074,
      "learning_rate": 3.003441656325228e-08,
      "loss": 0.0012,
      "step": 135030
    },
    {
      "epoch": 24552.727272727272,
      "grad_norm": 0.0012880763970315456,
      "learning_rate": 2.999470978034602e-08,
      "loss": 0.0012,
      "step": 135040
    },
    {
      "epoch": 24554.545454545456,
      "grad_norm": 0.1773548126220703,
      "learning_rate": 2.9955028450203975e-08,
      "loss": 0.0009,
      "step": 135050
    },
    {
      "epoch": 24556.363636363636,
      "grad_norm": 0.2730177640914917,
      "learning_rate": 2.99153725749749e-08,
      "loss": 0.0012,
      "step": 135060
    },
    {
      "epoch": 24558.18181818182,
      "grad_norm": 0.000800476293079555,
      "learning_rate": 2.9875742156806385e-08,
      "loss": 0.0009,
      "step": 135070
    },
    {
      "epoch": 24560.0,
      "grad_norm": 0.1635289192199707,
      "learning_rate": 2.983613719784456e-08,
      "loss": 0.0012,
      "step": 135080
    },
    {
      "epoch": 24561.81818181818,
      "grad_norm": 0.16824832558631897,
      "learning_rate": 2.9796557700234315e-08,
      "loss": 0.0011,
      "step": 135090
    },
    {
      "epoch": 24563.636363636364,
      "grad_norm": 0.20835839211940765,
      "learning_rate": 2.975700366611883e-08,
      "loss": 0.0013,
      "step": 135100
    },
    {
      "epoch": 24565.454545454544,
      "grad_norm": 0.1703246831893921,
      "learning_rate": 2.9717475097640333e-08,
      "loss": 0.0011,
      "step": 135110
    },
    {
      "epoch": 24567.272727272728,
      "grad_norm": 0.0010199855314567685,
      "learning_rate": 2.9677971996939277e-08,
      "loss": 0.0007,
      "step": 135120
    },
    {
      "epoch": 24569.090909090908,
      "grad_norm": 0.00045532031799666584,
      "learning_rate": 2.963849436615512e-08,
      "loss": 0.0012,
      "step": 135130
    },
    {
      "epoch": 24570.909090909092,
      "grad_norm": 0.002323269611224532,
      "learning_rate": 2.959904220742565e-08,
      "loss": 0.0009,
      "step": 135140
    },
    {
      "epoch": 24572.727272727272,
      "grad_norm": 0.0005389385623857379,
      "learning_rate": 2.955961552288727e-08,
      "loss": 0.0015,
      "step": 135150
    },
    {
      "epoch": 24574.545454545456,
      "grad_norm": 0.0005051138577982783,
      "learning_rate": 2.952021431467522e-08,
      "loss": 0.0007,
      "step": 135160
    },
    {
      "epoch": 24576.363636363636,
      "grad_norm": 0.22235573828220367,
      "learning_rate": 2.9480838584923286e-08,
      "loss": 0.0012,
      "step": 135170
    },
    {
      "epoch": 24578.18181818182,
      "grad_norm": 0.03957285359501839,
      "learning_rate": 2.9441488335763654e-08,
      "loss": 0.0013,
      "step": 135180
    },
    {
      "epoch": 24580.0,
      "grad_norm": 0.0007448234828189015,
      "learning_rate": 2.9402163569327455e-08,
      "loss": 0.0009,
      "step": 135190
    },
    {
      "epoch": 24581.81818181818,
      "grad_norm": 0.21808551251888275,
      "learning_rate": 2.936286428774426e-08,
      "loss": 0.001,
      "step": 135200
    },
    {
      "epoch": 24583.636363636364,
      "grad_norm": 0.21813592314720154,
      "learning_rate": 2.93235904931422e-08,
      "loss": 0.0009,
      "step": 135210
    },
    {
      "epoch": 24585.454545454544,
      "grad_norm": 0.00046362390276044607,
      "learning_rate": 2.9284342187648247e-08,
      "loss": 0.0013,
      "step": 135220
    },
    {
      "epoch": 24587.272727272728,
      "grad_norm": 0.0005714308936148882,
      "learning_rate": 2.924511937338775e-08,
      "loss": 0.0009,
      "step": 135230
    },
    {
      "epoch": 24589.090909090908,
      "grad_norm": 0.21887731552124023,
      "learning_rate": 2.9205922052484956e-08,
      "loss": 0.0013,
      "step": 135240
    },
    {
      "epoch": 24590.909090909092,
      "grad_norm": 0.000758395588491112,
      "learning_rate": 2.9166750227062387e-08,
      "loss": 0.0012,
      "step": 135250
    },
    {
      "epoch": 24592.727272727272,
      "grad_norm": 0.23086291551589966,
      "learning_rate": 2.912760389924135e-08,
      "loss": 0.0012,
      "step": 135260
    },
    {
      "epoch": 24594.545454545456,
      "grad_norm": 0.0007571114692837,
      "learning_rate": 2.9088483071141977e-08,
      "loss": 0.0009,
      "step": 135270
    },
    {
      "epoch": 24596.363636363636,
      "grad_norm": 0.27428150177001953,
      "learning_rate": 2.904938774488269e-08,
      "loss": 0.0015,
      "step": 135280
    },
    {
      "epoch": 24598.18181818182,
      "grad_norm": 0.14615152776241302,
      "learning_rate": 2.9010317922580575e-08,
      "loss": 0.001,
      "step": 135290
    },
    {
      "epoch": 24600.0,
      "grad_norm": 0.20798450708389282,
      "learning_rate": 2.8971273606351655e-08,
      "loss": 0.001,
      "step": 135300
    },
    {
      "epoch": 24601.81818181818,
      "grad_norm": 0.2066114991903305,
      "learning_rate": 2.893225479831024e-08,
      "loss": 0.001,
      "step": 135310
    },
    {
      "epoch": 24603.636363636364,
      "grad_norm": 0.20858289301395416,
      "learning_rate": 2.8893261500569198e-08,
      "loss": 0.0013,
      "step": 135320
    },
    {
      "epoch": 24605.454545454544,
      "grad_norm": 0.0004688637563958764,
      "learning_rate": 2.8854293715240453e-08,
      "loss": 0.0007,
      "step": 135330
    },
    {
      "epoch": 24607.272727272728,
      "grad_norm": 0.17076705396175385,
      "learning_rate": 2.8815351444434088e-08,
      "loss": 0.0012,
      "step": 135340
    },
    {
      "epoch": 24609.090909090908,
      "grad_norm": 0.1660512536764145,
      "learning_rate": 2.8776434690259145e-08,
      "loss": 0.001,
      "step": 135350
    },
    {
      "epoch": 24610.909090909092,
      "grad_norm": 0.15113084018230438,
      "learning_rate": 2.873754345482299e-08,
      "loss": 0.0012,
      "step": 135360
    },
    {
      "epoch": 24612.727272727272,
      "grad_norm": 0.2010801136493683,
      "learning_rate": 2.8698677740231768e-08,
      "loss": 0.001,
      "step": 135370
    },
    {
      "epoch": 24614.545454545456,
      "grad_norm": 0.282659649848938,
      "learning_rate": 2.8659837548590306e-08,
      "loss": 0.0012,
      "step": 135380
    },
    {
      "epoch": 24616.363636363636,
      "grad_norm": 0.0013209511525928974,
      "learning_rate": 2.8621022882001855e-08,
      "loss": 0.0009,
      "step": 135390
    },
    {
      "epoch": 24618.18181818182,
      "grad_norm": 0.000678352895192802,
      "learning_rate": 2.8582233742568406e-08,
      "loss": 0.001,
      "step": 135400
    },
    {
      "epoch": 24620.0,
      "grad_norm": 0.000697568990290165,
      "learning_rate": 2.8543470132390724e-08,
      "loss": 0.0012,
      "step": 135410
    },
    {
      "epoch": 24621.81818181818,
      "grad_norm": 0.0022705220617353916,
      "learning_rate": 2.8504732053567737e-08,
      "loss": 0.001,
      "step": 135420
    },
    {
      "epoch": 24623.636363636364,
      "grad_norm": 0.0005313621368259192,
      "learning_rate": 2.8466019508197604e-08,
      "loss": 0.0012,
      "step": 135430
    },
    {
      "epoch": 24625.454545454544,
      "grad_norm": 0.2604278028011322,
      "learning_rate": 2.8427332498376478e-08,
      "loss": 0.0011,
      "step": 135440
    },
    {
      "epoch": 24627.272727272728,
      "grad_norm": 0.0006243163370527327,
      "learning_rate": 2.8388671026199518e-08,
      "loss": 0.0008,
      "step": 135450
    },
    {
      "epoch": 24629.090909090908,
      "grad_norm": 0.000501592643558979,
      "learning_rate": 2.8350035093760493e-08,
      "loss": 0.0012,
      "step": 135460
    },
    {
      "epoch": 24630.909090909092,
      "grad_norm": 0.00041849256376735866,
      "learning_rate": 2.831142470315162e-08,
      "loss": 0.001,
      "step": 135470
    },
    {
      "epoch": 24632.727272727272,
      "grad_norm": 0.0005099583650007844,
      "learning_rate": 2.8272839856463782e-08,
      "loss": 0.001,
      "step": 135480
    },
    {
      "epoch": 24634.545454545456,
      "grad_norm": 0.0006273804465308785,
      "learning_rate": 2.8234280555786637e-08,
      "loss": 0.0013,
      "step": 135490
    },
    {
      "epoch": 24636.363636363636,
      "grad_norm": 0.17605769634246826,
      "learning_rate": 2.8195746803208242e-08,
      "loss": 0.0012,
      "step": 135500
    },
    {
      "epoch": 24636.363636363636,
      "eval_loss": 5.250876426696777,
      "eval_runtime": 0.9498,
      "eval_samples_per_second": 10.528,
      "eval_steps_per_second": 5.264,
      "step": 135500
    },
    {
      "epoch": 24638.18181818182,
      "grad_norm": 0.2188654989004135,
      "learning_rate": 2.815723860081537e-08,
      "loss": 0.001,
      "step": 135510
    },
    {
      "epoch": 24640.0,
      "grad_norm": 0.00041011261055245996,
      "learning_rate": 2.8118755950693408e-08,
      "loss": 0.0011,
      "step": 135520
    },
    {
      "epoch": 24641.81818181818,
      "grad_norm": 0.004998112563043833,
      "learning_rate": 2.8080298854926298e-08,
      "loss": 0.0012,
      "step": 135530
    },
    {
      "epoch": 24643.636363636364,
      "grad_norm": 0.00038996897637844086,
      "learning_rate": 2.8041867315596768e-08,
      "loss": 0.0007,
      "step": 135540
    },
    {
      "epoch": 24645.454545454544,
      "grad_norm": 0.0004109964065719396,
      "learning_rate": 2.8003461334786038e-08,
      "loss": 0.0012,
      "step": 135550
    },
    {
      "epoch": 24647.272727272728,
      "grad_norm": 0.201161190867424,
      "learning_rate": 2.796508091457378e-08,
      "loss": 0.0011,
      "step": 135560
    },
    {
      "epoch": 24649.090909090908,
      "grad_norm": 0.0010128655703738332,
      "learning_rate": 2.7926726057038664e-08,
      "loss": 0.001,
      "step": 135570
    },
    {
      "epoch": 24650.909090909092,
      "grad_norm": 0.21116496622562408,
      "learning_rate": 2.7888396764257693e-08,
      "loss": 0.0009,
      "step": 135580
    },
    {
      "epoch": 24652.727272727272,
      "grad_norm": 0.011732405982911587,
      "learning_rate": 2.7850093038306487e-08,
      "loss": 0.0012,
      "step": 135590
    },
    {
      "epoch": 24654.545454545456,
      "grad_norm": 0.04382731020450592,
      "learning_rate": 2.78118148812595e-08,
      "loss": 0.0014,
      "step": 135600
    },
    {
      "epoch": 24656.363636363636,
      "grad_norm": 0.0005538109107874334,
      "learning_rate": 2.7773562295189457e-08,
      "loss": 0.0006,
      "step": 135610
    },
    {
      "epoch": 24658.18181818182,
      "grad_norm": 0.0006827610777691007,
      "learning_rate": 2.7735335282168205e-08,
      "loss": 0.0012,
      "step": 135620
    },
    {
      "epoch": 24660.0,
      "grad_norm": 0.1973685622215271,
      "learning_rate": 2.769713384426553e-08,
      "loss": 0.0012,
      "step": 135630
    },
    {
      "epoch": 24661.81818181818,
      "grad_norm": 0.27198028564453125,
      "learning_rate": 2.765895798355039e-08,
      "loss": 0.0012,
      "step": 135640
    },
    {
      "epoch": 24663.636363636364,
      "grad_norm": 0.0005487827002070844,
      "learning_rate": 2.762080770209019e-08,
      "loss": 0.0009,
      "step": 135650
    },
    {
      "epoch": 24665.454545454544,
      "grad_norm": 0.0005779674975201488,
      "learning_rate": 2.7582683001950936e-08,
      "loss": 0.0008,
      "step": 135660
    },
    {
      "epoch": 24667.272727272728,
      "grad_norm": 0.17686018347740173,
      "learning_rate": 2.754458388519709e-08,
      "loss": 0.0015,
      "step": 135670
    },
    {
      "epoch": 24669.090909090908,
      "grad_norm": 0.0006474938127212226,
      "learning_rate": 2.7506510353892065e-08,
      "loss": 0.001,
      "step": 135680
    },
    {
      "epoch": 24670.909090909092,
      "grad_norm": 0.00031412867247126997,
      "learning_rate": 2.746846241009765e-08,
      "loss": 0.0012,
      "step": 135690
    },
    {
      "epoch": 24672.727272727272,
      "grad_norm": 0.18813952803611755,
      "learning_rate": 2.743044005587425e-08,
      "loss": 0.0011,
      "step": 135700
    },
    {
      "epoch": 24674.545454545456,
      "grad_norm": 0.00037329288898035884,
      "learning_rate": 2.7392443293280887e-08,
      "loss": 0.0008,
      "step": 135710
    },
    {
      "epoch": 24676.363636363636,
      "grad_norm": 0.19491124153137207,
      "learning_rate": 2.7354472124375307e-08,
      "loss": 0.0012,
      "step": 135720
    },
    {
      "epoch": 24678.18181818182,
      "grad_norm": 0.1766648143529892,
      "learning_rate": 2.7316526551213914e-08,
      "loss": 0.001,
      "step": 135730
    },
    {
      "epoch": 24680.0,
      "grad_norm": 0.20692719519138336,
      "learning_rate": 2.7278606575851513e-08,
      "loss": 0.001,
      "step": 135740
    },
    {
      "epoch": 24681.81818181818,
      "grad_norm": 0.17787615954875946,
      "learning_rate": 2.7240712200341576e-08,
      "loss": 0.0012,
      "step": 135750
    },
    {
      "epoch": 24683.636363636364,
      "grad_norm": 0.0005299305776134133,
      "learning_rate": 2.7202843426736398e-08,
      "loss": 0.001,
      "step": 135760
    },
    {
      "epoch": 24685.454545454544,
      "grad_norm": 0.0005846399581059813,
      "learning_rate": 2.7165000257086625e-08,
      "loss": 0.0011,
      "step": 135770
    },
    {
      "epoch": 24687.272727272728,
      "grad_norm": 0.0009855168173089623,
      "learning_rate": 2.7127182693441608e-08,
      "loss": 0.001,
      "step": 135780
    },
    {
      "epoch": 24689.090909090908,
      "grad_norm": 0.00038721461896784604,
      "learning_rate": 2.7089390737849382e-08,
      "loss": 0.001,
      "step": 135790
    },
    {
      "epoch": 24690.909090909092,
      "grad_norm": 0.1786416620016098,
      "learning_rate": 2.7051624392356476e-08,
      "loss": 0.001,
      "step": 135800
    },
    {
      "epoch": 24692.727272727272,
      "grad_norm": 0.19975467026233673,
      "learning_rate": 2.7013883659008308e-08,
      "loss": 0.0011,
      "step": 135810
    },
    {
      "epoch": 24694.545454545456,
      "grad_norm": 0.20159274339675903,
      "learning_rate": 2.6976168539848353e-08,
      "loss": 0.0012,
      "step": 135820
    },
    {
      "epoch": 24696.363636363636,
      "grad_norm": 0.0008018443477340043,
      "learning_rate": 2.693847903691926e-08,
      "loss": 0.0008,
      "step": 135830
    },
    {
      "epoch": 24698.18181818182,
      "grad_norm": 0.000793654122389853,
      "learning_rate": 2.6900815152262058e-08,
      "loss": 0.0012,
      "step": 135840
    },
    {
      "epoch": 24700.0,
      "grad_norm": 0.21023502945899963,
      "learning_rate": 2.6863176887916396e-08,
      "loss": 0.0012,
      "step": 135850
    },
    {
      "epoch": 24701.81818181818,
      "grad_norm": 0.00047886240645311773,
      "learning_rate": 2.6825564245920473e-08,
      "loss": 0.001,
      "step": 135860
    },
    {
      "epoch": 24703.636363636364,
      "grad_norm": 0.000630005553830415,
      "learning_rate": 2.6787977228311333e-08,
      "loss": 0.0012,
      "step": 135870
    },
    {
      "epoch": 24705.454545454544,
      "grad_norm": 0.000701547134667635,
      "learning_rate": 2.675041583712434e-08,
      "loss": 0.001,
      "step": 135880
    },
    {
      "epoch": 24707.272727272728,
      "grad_norm": 0.0005647437646985054,
      "learning_rate": 2.6712880074393594e-08,
      "loss": 0.0009,
      "step": 135890
    },
    {
      "epoch": 24709.090909090908,
      "grad_norm": 0.16756054759025574,
      "learning_rate": 2.667536994215186e-08,
      "loss": 0.0012,
      "step": 135900
    },
    {
      "epoch": 24710.909090909092,
      "grad_norm": 0.0003744893765542656,
      "learning_rate": 2.6637885442430398e-08,
      "loss": 0.001,
      "step": 135910
    },
    {
      "epoch": 24712.727272727272,
      "grad_norm": 0.0007280472200363874,
      "learning_rate": 2.6600426577259306e-08,
      "loss": 0.0012,
      "step": 135920
    },
    {
      "epoch": 24714.545454545456,
      "grad_norm": 0.0004216834204271436,
      "learning_rate": 2.6562993348667016e-08,
      "loss": 0.0009,
      "step": 135930
    },
    {
      "epoch": 24716.363636363636,
      "grad_norm": 0.0005896593793295324,
      "learning_rate": 2.652558575868069e-08,
      "loss": 0.0013,
      "step": 135940
    },
    {
      "epoch": 24718.18181818182,
      "grad_norm": 0.20987391471862793,
      "learning_rate": 2.6488203809326204e-08,
      "loss": 0.001,
      "step": 135950
    },
    {
      "epoch": 24720.0,
      "grad_norm": 0.2741178870201111,
      "learning_rate": 2.6450847502627882e-08,
      "loss": 0.001,
      "step": 135960
    },
    {
      "epoch": 24721.81818181818,
      "grad_norm": 0.0009656292968429625,
      "learning_rate": 2.6413516840608717e-08,
      "loss": 0.0012,
      "step": 135970
    },
    {
      "epoch": 24723.636363636364,
      "grad_norm": 0.0005814240430481732,
      "learning_rate": 2.6376211825290318e-08,
      "loss": 0.001,
      "step": 135980
    },
    {
      "epoch": 24725.454545454544,
      "grad_norm": 0.1804651916027069,
      "learning_rate": 2.6338932458692842e-08,
      "loss": 0.0009,
      "step": 135990
    },
    {
      "epoch": 24727.272727272728,
      "grad_norm": 0.2728267312049866,
      "learning_rate": 2.6301678742835397e-08,
      "loss": 0.0013,
      "step": 136000
    },
    {
      "epoch": 24727.272727272728,
      "eval_loss": 5.208151817321777,
      "eval_runtime": 0.9517,
      "eval_samples_per_second": 10.507,
      "eval_steps_per_second": 5.254,
      "step": 136000
    },
    {
      "epoch": 24729.090909090908,
      "grad_norm": 0.20538341999053955,
      "learning_rate": 2.6264450679735097e-08,
      "loss": 0.001,
      "step": 136010
    },
    {
      "epoch": 24730.909090909092,
      "grad_norm": 0.0008268765523098409,
      "learning_rate": 2.6227248271408153e-08,
      "loss": 0.001,
      "step": 136020
    },
    {
      "epoch": 24732.727272727272,
      "grad_norm": 0.17942562699317932,
      "learning_rate": 2.619007151986924e-08,
      "loss": 0.0012,
      "step": 136030
    },
    {
      "epoch": 24734.545454545456,
      "grad_norm": 0.00032324279891327024,
      "learning_rate": 2.615292042713163e-08,
      "loss": 0.001,
      "step": 136040
    },
    {
      "epoch": 24736.363636363636,
      "grad_norm": 0.21677841246128082,
      "learning_rate": 2.6115794995207217e-08,
      "loss": 0.0009,
      "step": 136050
    },
    {
      "epoch": 24738.18181818182,
      "grad_norm": 0.0007747222553007305,
      "learning_rate": 2.6078695226106396e-08,
      "loss": 0.001,
      "step": 136060
    },
    {
      "epoch": 24740.0,
      "grad_norm": 0.22067709267139435,
      "learning_rate": 2.6041621121838388e-08,
      "loss": 0.0012,
      "step": 136070
    },
    {
      "epoch": 24741.81818181818,
      "grad_norm": 0.2817450165748596,
      "learning_rate": 2.600457268441092e-08,
      "loss": 0.0012,
      "step": 136080
    },
    {
      "epoch": 24743.636363636364,
      "grad_norm": 0.1982109099626541,
      "learning_rate": 2.5967549915830167e-08,
      "loss": 0.0012,
      "step": 136090
    },
    {
      "epoch": 24745.454545454544,
      "grad_norm": 0.29189005494117737,
      "learning_rate": 2.593055281810125e-08,
      "loss": 0.0009,
      "step": 136100
    },
    {
      "epoch": 24747.272727272728,
      "grad_norm": 0.15724444389343262,
      "learning_rate": 2.589358139322767e-08,
      "loss": 0.0012,
      "step": 136110
    },
    {
      "epoch": 24749.090909090908,
      "grad_norm": 0.4403190314769745,
      "learning_rate": 2.5856635643211555e-08,
      "loss": 0.001,
      "step": 136120
    },
    {
      "epoch": 24750.909090909092,
      "grad_norm": 0.16623251140117645,
      "learning_rate": 2.5819715570053634e-08,
      "loss": 0.0011,
      "step": 136130
    },
    {
      "epoch": 24752.727272727272,
      "grad_norm": 0.000642197672277689,
      "learning_rate": 2.5782821175753423e-08,
      "loss": 0.0011,
      "step": 136140
    },
    {
      "epoch": 24754.545454545456,
      "grad_norm": 0.0015517406864091754,
      "learning_rate": 2.5745952462308762e-08,
      "loss": 0.0012,
      "step": 136150
    },
    {
      "epoch": 24756.363636363636,
      "grad_norm": 0.001143460045568645,
      "learning_rate": 2.5709109431716335e-08,
      "loss": 0.001,
      "step": 136160
    },
    {
      "epoch": 24758.18181818182,
      "grad_norm": 0.0013410866959020495,
      "learning_rate": 2.567229208597127e-08,
      "loss": 0.001,
      "step": 136170
    },
    {
      "epoch": 24760.0,
      "grad_norm": 0.22125574946403503,
      "learning_rate": 2.5635500427067413e-08,
      "loss": 0.0012,
      "step": 136180
    },
    {
      "epoch": 24761.81818181818,
      "grad_norm": 0.0004980124649591744,
      "learning_rate": 2.5598734456997285e-08,
      "loss": 0.0012,
      "step": 136190
    },
    {
      "epoch": 24763.636363636364,
      "grad_norm": 0.0004899665364064276,
      "learning_rate": 2.5561994177751732e-08,
      "loss": 0.0009,
      "step": 136200
    },
    {
      "epoch": 24765.454545454544,
      "grad_norm": 0.0008833542815409601,
      "learning_rate": 2.5525279591320505e-08,
      "loss": 0.0013,
      "step": 136210
    },
    {
      "epoch": 24767.272727272728,
      "grad_norm": 0.1697399616241455,
      "learning_rate": 2.5488590699691893e-08,
      "loss": 0.0009,
      "step": 136220
    },
    {
      "epoch": 24769.090909090908,
      "grad_norm": 0.1712435930967331,
      "learning_rate": 2.5451927504852755e-08,
      "loss": 0.0011,
      "step": 136230
    },
    {
      "epoch": 24770.909090909092,
      "grad_norm": 0.2203368991613388,
      "learning_rate": 2.541529000878845e-08,
      "loss": 0.0009,
      "step": 136240
    },
    {
      "epoch": 24772.727272727272,
      "grad_norm": 0.0006124554784037173,
      "learning_rate": 2.5378678213483052e-08,
      "loss": 0.001,
      "step": 136250
    },
    {
      "epoch": 24774.545454545456,
      "grad_norm": 0.0003869327192660421,
      "learning_rate": 2.534209212091937e-08,
      "loss": 0.001,
      "step": 136260
    },
    {
      "epoch": 24776.363636363636,
      "grad_norm": 0.1489102840423584,
      "learning_rate": 2.5305531733078645e-08,
      "loss": 0.0013,
      "step": 136270
    },
    {
      "epoch": 24778.18181818182,
      "grad_norm": 0.16668681800365448,
      "learning_rate": 2.5268997051940688e-08,
      "loss": 0.001,
      "step": 136280
    },
    {
      "epoch": 24780.0,
      "grad_norm": 0.2311377227306366,
      "learning_rate": 2.5232488079484027e-08,
      "loss": 0.0011,
      "step": 136290
    },
    {
      "epoch": 24781.81818181818,
      "grad_norm": 0.0007137631182558835,
      "learning_rate": 2.5196004817685966e-08,
      "loss": 0.001,
      "step": 136300
    },
    {
      "epoch": 24783.636363636364,
      "grad_norm": 0.0004970072768628597,
      "learning_rate": 2.515954726852204e-08,
      "loss": 0.0012,
      "step": 136310
    },
    {
      "epoch": 24785.454545454544,
      "grad_norm": 0.0007075174362398684,
      "learning_rate": 2.5123115433966613e-08,
      "loss": 0.001,
      "step": 136320
    },
    {
      "epoch": 24787.272727272728,
      "grad_norm": 0.2068101167678833,
      "learning_rate": 2.5086709315992604e-08,
      "loss": 0.0011,
      "step": 136330
    },
    {
      "epoch": 24789.090909090908,
      "grad_norm": 0.005600308999419212,
      "learning_rate": 2.5050328916571607e-08,
      "loss": 0.0012,
      "step": 136340
    },
    {
      "epoch": 24790.909090909092,
      "grad_norm": 0.2083750069141388,
      "learning_rate": 2.501397423767382e-08,
      "loss": 0.0012,
      "step": 136350
    },
    {
      "epoch": 24792.727272727272,
      "grad_norm": 0.0005303487996570766,
      "learning_rate": 2.497764528126778e-08,
      "loss": 0.0012,
      "step": 136360
    },
    {
      "epoch": 24794.545454545456,
      "grad_norm": 0.00043697308865375817,
      "learning_rate": 2.494134204932108e-08,
      "loss": 0.0008,
      "step": 136370
    },
    {
      "epoch": 24796.363636363636,
      "grad_norm": 0.23704373836517334,
      "learning_rate": 2.4905064543799703e-08,
      "loss": 0.0013,
      "step": 136380
    },
    {
      "epoch": 24798.18181818182,
      "grad_norm": 0.0019600605592131615,
      "learning_rate": 2.4868812766668078e-08,
      "loss": 0.0008,
      "step": 136390
    },
    {
      "epoch": 24800.0,
      "grad_norm": 0.000516226573381573,
      "learning_rate": 2.4832586719889416e-08,
      "loss": 0.0012,
      "step": 136400
    },
    {
      "epoch": 24801.81818181818,
      "grad_norm": 0.17233166098594666,
      "learning_rate": 2.479638640542564e-08,
      "loss": 0.001,
      "step": 136410
    },
    {
      "epoch": 24803.636363636364,
      "grad_norm": 0.17525942623615265,
      "learning_rate": 2.4760211825237022e-08,
      "loss": 0.0011,
      "step": 136420
    },
    {
      "epoch": 24805.454545454544,
      "grad_norm": 0.16631226241588593,
      "learning_rate": 2.4724062981282657e-08,
      "loss": 0.0009,
      "step": 136430
    },
    {
      "epoch": 24807.272727272728,
      "grad_norm": 0.0006513621774502099,
      "learning_rate": 2.4687939875519982e-08,
      "loss": 0.0012,
      "step": 136440
    },
    {
      "epoch": 24809.090909090908,
      "grad_norm": 0.4133961498737335,
      "learning_rate": 2.4651842509905484e-08,
      "loss": 0.0013,
      "step": 136450
    },
    {
      "epoch": 24810.909090909092,
      "grad_norm": 0.0004570269084069878,
      "learning_rate": 2.461577088639377e-08,
      "loss": 0.0009,
      "step": 136460
    },
    {
      "epoch": 24812.727272727272,
      "grad_norm": 0.011716462671756744,
      "learning_rate": 2.4579725006938335e-08,
      "loss": 0.001,
      "step": 136470
    },
    {
      "epoch": 24814.545454545456,
      "grad_norm": 0.20997478067874908,
      "learning_rate": 2.4543704873491222e-08,
      "loss": 0.0012,
      "step": 136480
    },
    {
      "epoch": 24816.363636363636,
      "grad_norm": 0.0006638672784902155,
      "learning_rate": 2.4507710488003154e-08,
      "loss": 0.0007,
      "step": 136490
    },
    {
      "epoch": 24818.18181818182,
      "grad_norm": 0.0005556553951464593,
      "learning_rate": 2.4471741852423233e-08,
      "loss": 0.0012,
      "step": 136500
    },
    {
      "epoch": 24818.18181818182,
      "eval_loss": 5.295879364013672,
      "eval_runtime": 0.9581,
      "eval_samples_per_second": 10.437,
      "eval_steps_per_second": 5.219,
      "step": 136500
    },
    {
      "epoch": 24820.0,
      "grad_norm": 0.0009949576342478395,
      "learning_rate": 2.4435798968699406e-08,
      "loss": 0.0012,
      "step": 136510
    },
    {
      "epoch": 24821.81818181818,
      "grad_norm": 0.17706555128097534,
      "learning_rate": 2.4399881838778057e-08,
      "loss": 0.0009,
      "step": 136520
    },
    {
      "epoch": 24823.636363636364,
      "grad_norm": 0.0005591806839220226,
      "learning_rate": 2.4363990464604357e-08,
      "loss": 0.0014,
      "step": 136530
    },
    {
      "epoch": 24825.454545454544,
      "grad_norm": 0.2119990885257721,
      "learning_rate": 2.4328124848121912e-08,
      "loss": 0.0007,
      "step": 136540
    },
    {
      "epoch": 24827.272727272728,
      "grad_norm": 0.2965089976787567,
      "learning_rate": 2.4292284991272948e-08,
      "loss": 0.0016,
      "step": 136550
    },
    {
      "epoch": 24829.090909090908,
      "grad_norm": 0.00040732818888500333,
      "learning_rate": 2.425647089599836e-08,
      "loss": 0.0006,
      "step": 136560
    },
    {
      "epoch": 24830.909090909092,
      "grad_norm": 0.2230781465768814,
      "learning_rate": 2.4220682564237872e-08,
      "loss": 0.0012,
      "step": 136570
    },
    {
      "epoch": 24832.727272727272,
      "grad_norm": 0.0012411888455972075,
      "learning_rate": 2.4184919997929155e-08,
      "loss": 0.001,
      "step": 136580
    },
    {
      "epoch": 24834.545454545456,
      "grad_norm": 0.0005626616184599698,
      "learning_rate": 2.4149183199009215e-08,
      "loss": 0.0011,
      "step": 136590
    },
    {
      "epoch": 24836.363636363636,
      "grad_norm": 0.0014107010792940855,
      "learning_rate": 2.4113472169413174e-08,
      "loss": 0.0011,
      "step": 136600
    },
    {
      "epoch": 24838.18181818182,
      "grad_norm": 0.0003467990318313241,
      "learning_rate": 2.4077786911075094e-08,
      "loss": 0.001,
      "step": 136610
    },
    {
      "epoch": 24840.0,
      "grad_norm": 0.0005972344079054892,
      "learning_rate": 2.404212742592743e-08,
      "loss": 0.0012,
      "step": 136620
    },
    {
      "epoch": 24841.81818181818,
      "grad_norm": 0.00057400664081797,
      "learning_rate": 2.400649371590113e-08,
      "loss": 0.001,
      "step": 136630
    },
    {
      "epoch": 24843.636363636364,
      "grad_norm": 0.1797594279050827,
      "learning_rate": 2.3970885782926165e-08,
      "loss": 0.0011,
      "step": 136640
    },
    {
      "epoch": 24845.454545454544,
      "grad_norm": 0.0005127332406118512,
      "learning_rate": 2.3935303628930705e-08,
      "loss": 0.0009,
      "step": 136650
    },
    {
      "epoch": 24847.272727272728,
      "grad_norm": 0.0013810822274535894,
      "learning_rate": 2.3899747255841594e-08,
      "loss": 0.001,
      "step": 136660
    },
    {
      "epoch": 24849.090909090908,
      "grad_norm": 0.0014953888021409512,
      "learning_rate": 2.386421666558458e-08,
      "loss": 0.0012,
      "step": 136670
    },
    {
      "epoch": 24850.909090909092,
      "grad_norm": 0.26988035440444946,
      "learning_rate": 2.3828711860083672e-08,
      "loss": 0.0012,
      "step": 136680
    },
    {
      "epoch": 24852.727272727272,
      "grad_norm": 0.2015899270772934,
      "learning_rate": 2.3793232841261557e-08,
      "loss": 0.0012,
      "step": 136690
    },
    {
      "epoch": 24854.545454545456,
      "grad_norm": 0.0005825873231515288,
      "learning_rate": 2.3757779611039697e-08,
      "loss": 0.0007,
      "step": 136700
    },
    {
      "epoch": 24856.363636363636,
      "grad_norm": 0.0003583953366614878,
      "learning_rate": 2.3722352171337833e-08,
      "loss": 0.001,
      "step": 136710
    },
    {
      "epoch": 24858.18181818182,
      "grad_norm": 0.0005960310809314251,
      "learning_rate": 2.3686950524074766e-08,
      "loss": 0.0012,
      "step": 136720
    },
    {
      "epoch": 24860.0,
      "grad_norm": 0.2646278440952301,
      "learning_rate": 2.365157467116752e-08,
      "loss": 0.0012,
      "step": 136730
    },
    {
      "epoch": 24861.81818181818,
      "grad_norm": 0.2194642424583435,
      "learning_rate": 2.3616224614531777e-08,
      "loss": 0.0011,
      "step": 136740
    },
    {
      "epoch": 24863.636363636364,
      "grad_norm": 0.22380393743515015,
      "learning_rate": 2.35809003560819e-08,
      "loss": 0.0012,
      "step": 136750
    },
    {
      "epoch": 24865.454545454544,
      "grad_norm": 0.21666623651981354,
      "learning_rate": 2.3545601897731082e-08,
      "loss": 0.001,
      "step": 136760
    },
    {
      "epoch": 24867.272727272728,
      "grad_norm": 0.0014105973532423377,
      "learning_rate": 2.3510329241390625e-08,
      "loss": 0.0011,
      "step": 136770
    },
    {
      "epoch": 24869.090909090908,
      "grad_norm": 0.0006376969395205379,
      "learning_rate": 2.347508238897078e-08,
      "loss": 0.001,
      "step": 136780
    },
    {
      "epoch": 24870.909090909092,
      "grad_norm": 0.000569458119571209,
      "learning_rate": 2.3439861342380242e-08,
      "loss": 0.001,
      "step": 136790
    },
    {
      "epoch": 24872.727272727272,
      "grad_norm": 0.0007871285197325051,
      "learning_rate": 2.3404666103526537e-08,
      "loss": 0.0012,
      "step": 136800
    },
    {
      "epoch": 24874.545454545456,
      "grad_norm": 0.16673849523067474,
      "learning_rate": 2.336949667431548e-08,
      "loss": 0.0012,
      "step": 136810
    },
    {
      "epoch": 24876.363636363636,
      "grad_norm": 0.2821336090564728,
      "learning_rate": 2.3334353056651713e-08,
      "loss": 0.0012,
      "step": 136820
    },
    {
      "epoch": 24878.18181818182,
      "grad_norm": 0.27246689796447754,
      "learning_rate": 2.329923525243843e-08,
      "loss": 0.0012,
      "step": 136830
    },
    {
      "epoch": 24880.0,
      "grad_norm": 0.1788221001625061,
      "learning_rate": 2.3264143263577395e-08,
      "loss": 0.0009,
      "step": 136840
    },
    {
      "epoch": 24881.81818181818,
      "grad_norm": 0.21717149019241333,
      "learning_rate": 2.3229077091968863e-08,
      "loss": 0.0012,
      "step": 136850
    },
    {
      "epoch": 24883.636363636364,
      "grad_norm": 0.0003951861581299454,
      "learning_rate": 2.3194036739512035e-08,
      "loss": 0.0009,
      "step": 136860
    },
    {
      "epoch": 24885.454545454544,
      "grad_norm": 0.001163923880085349,
      "learning_rate": 2.3159022208104394e-08,
      "loss": 0.0012,
      "step": 136870
    },
    {
      "epoch": 24887.272727272728,
      "grad_norm": 0.20514291524887085,
      "learning_rate": 2.3124033499642037e-08,
      "loss": 0.001,
      "step": 136880
    },
    {
      "epoch": 24889.090909090908,
      "grad_norm": 0.0004636066732928157,
      "learning_rate": 2.3089070616019833e-08,
      "loss": 0.001,
      "step": 136890
    },
    {
      "epoch": 24890.909090909092,
      "grad_norm": 0.2075740545988083,
      "learning_rate": 2.3054133559131162e-08,
      "loss": 0.0012,
      "step": 136900
    },
    {
      "epoch": 24892.727272727272,
      "grad_norm": 0.0005058823735453188,
      "learning_rate": 2.301922233086806e-08,
      "loss": 0.001,
      "step": 136910
    },
    {
      "epoch": 24894.545454545456,
      "grad_norm": 0.0007103236857801676,
      "learning_rate": 2.2984336933121075e-08,
      "loss": 0.0009,
      "step": 136920
    },
    {
      "epoch": 24896.363636363636,
      "grad_norm": 0.2745504081249237,
      "learning_rate": 2.2949477367779303e-08,
      "loss": 0.0014,
      "step": 136930
    },
    {
      "epoch": 24898.18181818182,
      "grad_norm": 0.1624695509672165,
      "learning_rate": 2.2914643636730736e-08,
      "loss": 0.0009,
      "step": 136940
    },
    {
      "epoch": 24900.0,
      "grad_norm": 0.0006146421073935926,
      "learning_rate": 2.2879835741861585e-08,
      "loss": 0.0011,
      "step": 136950
    },
    {
      "epoch": 24901.81818181818,
      "grad_norm": 0.0004095322801731527,
      "learning_rate": 2.2845053685056902e-08,
      "loss": 0.001,
      "step": 136960
    },
    {
      "epoch": 24903.636363636364,
      "grad_norm": 0.22169318795204163,
      "learning_rate": 2.2810297468200346e-08,
      "loss": 0.0012,
      "step": 136970
    },
    {
      "epoch": 24905.454545454544,
      "grad_norm": 0.17563146352767944,
      "learning_rate": 2.277556709317402e-08,
      "loss": 0.0013,
      "step": 136980
    },
    {
      "epoch": 24907.272727272728,
      "grad_norm": 0.22059692442417145,
      "learning_rate": 2.274086256185881e-08,
      "loss": 0.0007,
      "step": 136990
    },
    {
      "epoch": 24909.090909090908,
      "grad_norm": 0.0008576993132010102,
      "learning_rate": 2.2706183876134045e-08,
      "loss": 0.0011,
      "step": 137000
    },
    {
      "epoch": 24909.090909090908,
      "eval_loss": 5.153801441192627,
      "eval_runtime": 0.9505,
      "eval_samples_per_second": 10.521,
      "eval_steps_per_second": 5.26,
      "step": 137000
    },
    {
      "epoch": 24910.909090909092,
      "grad_norm": 0.14942027628421783,
      "learning_rate": 2.2671531037877724e-08,
      "loss": 0.001,
      "step": 137010
    },
    {
      "epoch": 24912.727272727272,
      "grad_norm": 0.2228366732597351,
      "learning_rate": 2.2636904048966454e-08,
      "loss": 0.0011,
      "step": 137020
    },
    {
      "epoch": 24914.545454545456,
      "grad_norm": 0.17140266299247742,
      "learning_rate": 2.2602302911275516e-08,
      "loss": 0.0013,
      "step": 137030
    },
    {
      "epoch": 24916.363636363636,
      "grad_norm": 0.17444324493408203,
      "learning_rate": 2.2567727626678523e-08,
      "loss": 0.0009,
      "step": 137040
    },
    {
      "epoch": 24918.18181818182,
      "grad_norm": 0.0011490607867017388,
      "learning_rate": 2.2533178197048032e-08,
      "loss": 0.0011,
      "step": 137050
    },
    {
      "epoch": 24920.0,
      "grad_norm": 0.000695899419952184,
      "learning_rate": 2.249865462425504e-08,
      "loss": 0.0012,
      "step": 137060
    },
    {
      "epoch": 24921.81818181818,
      "grad_norm": 0.17030081152915955,
      "learning_rate": 2.2464156910168952e-08,
      "loss": 0.001,
      "step": 137070
    },
    {
      "epoch": 24923.636363636364,
      "grad_norm": 0.0004557328356895596,
      "learning_rate": 2.242968505665821e-08,
      "loss": 0.0012,
      "step": 137080
    },
    {
      "epoch": 24925.454545454544,
      "grad_norm": 0.17461247742176056,
      "learning_rate": 2.2395239065589434e-08,
      "loss": 0.0013,
      "step": 137090
    },
    {
      "epoch": 24927.272727272728,
      "grad_norm": 0.0003112982085440308,
      "learning_rate": 2.2360818938828187e-08,
      "loss": 0.0008,
      "step": 137100
    },
    {
      "epoch": 24929.090909090908,
      "grad_norm": 0.2871648371219635,
      "learning_rate": 2.232642467823831e-08,
      "loss": 0.0012,
      "step": 137110
    },
    {
      "epoch": 24930.909090909092,
      "grad_norm": 0.00045350525761023164,
      "learning_rate": 2.2292056285682425e-08,
      "loss": 0.0009,
      "step": 137120
    },
    {
      "epoch": 24932.727272727272,
      "grad_norm": 0.20813412964344025,
      "learning_rate": 2.2257713763021825e-08,
      "loss": 0.0012,
      "step": 137130
    },
    {
      "epoch": 24934.545454545456,
      "grad_norm": 0.018007036298513412,
      "learning_rate": 2.2223397112116183e-08,
      "loss": 0.0011,
      "step": 137140
    },
    {
      "epoch": 24936.363636363636,
      "grad_norm": 0.00041975107160396874,
      "learning_rate": 2.2189106334823904e-08,
      "loss": 0.0008,
      "step": 137150
    },
    {
      "epoch": 24938.18181818182,
      "grad_norm": 0.26942506432533264,
      "learning_rate": 2.215484143300206e-08,
      "loss": 0.0014,
      "step": 137160
    },
    {
      "epoch": 24940.0,
      "grad_norm": 0.0006321207038126886,
      "learning_rate": 2.2120602408506162e-08,
      "loss": 0.0009,
      "step": 137170
    },
    {
      "epoch": 24941.81818181818,
      "grad_norm": 0.16823571920394897,
      "learning_rate": 2.2086389263190453e-08,
      "loss": 0.001,
      "step": 137180
    },
    {
      "epoch": 24943.636363636364,
      "grad_norm": 0.22075389325618744,
      "learning_rate": 2.205220199890767e-08,
      "loss": 0.0012,
      "step": 137190
    },
    {
      "epoch": 24945.454545454544,
      "grad_norm": 0.0006579473847523332,
      "learning_rate": 2.201804061750917e-08,
      "loss": 0.001,
      "step": 137200
    },
    {
      "epoch": 24947.272727272728,
      "grad_norm": 0.27993154525756836,
      "learning_rate": 2.1983905120845082e-08,
      "loss": 0.0012,
      "step": 137210
    },
    {
      "epoch": 24949.090909090908,
      "grad_norm": 0.0009042127639986575,
      "learning_rate": 2.194979551076387e-08,
      "loss": 0.0009,
      "step": 137220
    },
    {
      "epoch": 24950.909090909092,
      "grad_norm": 0.17170459032058716,
      "learning_rate": 2.191571178911267e-08,
      "loss": 0.001,
      "step": 137230
    },
    {
      "epoch": 24952.727272727272,
      "grad_norm": 0.0007986440905369818,
      "learning_rate": 2.188165395773739e-08,
      "loss": 0.0012,
      "step": 137240
    },
    {
      "epoch": 24954.545454545456,
      "grad_norm": 0.20975147187709808,
      "learning_rate": 2.1847622018482282e-08,
      "loss": 0.0012,
      "step": 137250
    },
    {
      "epoch": 24956.363636363636,
      "grad_norm": 0.20366573333740234,
      "learning_rate": 2.1813615973190368e-08,
      "loss": 0.0011,
      "step": 137260
    },
    {
      "epoch": 24958.18181818182,
      "grad_norm": 0.21419604122638702,
      "learning_rate": 2.177963582370329e-08,
      "loss": 0.0015,
      "step": 137270
    },
    {
      "epoch": 24960.0,
      "grad_norm": 0.22515250742435455,
      "learning_rate": 2.174568157186102e-08,
      "loss": 0.0007,
      "step": 137280
    },
    {
      "epoch": 24961.81818181818,
      "grad_norm": 0.17332668602466583,
      "learning_rate": 2.1711753219502583e-08,
      "loss": 0.001,
      "step": 137290
    },
    {
      "epoch": 24963.636363636364,
      "grad_norm": 0.0005345639656297863,
      "learning_rate": 2.1677850768465178e-08,
      "loss": 0.001,
      "step": 137300
    },
    {
      "epoch": 24965.454545454544,
      "grad_norm": 0.17211775481700897,
      "learning_rate": 2.1643974220584727e-08,
      "loss": 0.001,
      "step": 137310
    },
    {
      "epoch": 24967.272727272728,
      "grad_norm": 0.001562907942570746,
      "learning_rate": 2.161012357769598e-08,
      "loss": 0.001,
      "step": 137320
    },
    {
      "epoch": 24969.090909090908,
      "grad_norm": 0.21896639466285706,
      "learning_rate": 2.1576298841631924e-08,
      "loss": 0.0012,
      "step": 137330
    },
    {
      "epoch": 24970.909090909092,
      "grad_norm": 0.23274467885494232,
      "learning_rate": 2.1542500014224306e-08,
      "loss": 0.0012,
      "step": 137340
    },
    {
      "epoch": 24972.727272727272,
      "grad_norm": 0.27489885687828064,
      "learning_rate": 2.1508727097303558e-08,
      "loss": 0.0012,
      "step": 137350
    },
    {
      "epoch": 24974.545454545456,
      "grad_norm": 0.0005552180809900165,
      "learning_rate": 2.1474980092698547e-08,
      "loss": 0.0009,
      "step": 137360
    },
    {
      "epoch": 24976.363636363636,
      "grad_norm": 0.2791484296321869,
      "learning_rate": 2.1441259002236977e-08,
      "loss": 0.0013,
      "step": 137370
    },
    {
      "epoch": 24978.18181818182,
      "grad_norm": 0.20893855392932892,
      "learning_rate": 2.1407563827744778e-08,
      "loss": 0.001,
      "step": 137380
    },
    {
      "epoch": 24980.0,
      "grad_norm": 0.1633671373128891,
      "learning_rate": 2.137389457104677e-08,
      "loss": 0.001,
      "step": 137390
    },
    {
      "epoch": 24981.81818181818,
      "grad_norm": 0.00043819446000270545,
      "learning_rate": 2.1340251233966377e-08,
      "loss": 0.001,
      "step": 137400
    },
    {
      "epoch": 24983.636363636364,
      "grad_norm": 0.0018468216294422746,
      "learning_rate": 2.1306633818325425e-08,
      "loss": 0.0012,
      "step": 137410
    },
    {
      "epoch": 24985.454545454544,
      "grad_norm": 0.17613333463668823,
      "learning_rate": 2.12730423259444e-08,
      "loss": 0.001,
      "step": 137420
    },
    {
      "epoch": 24987.272727272728,
      "grad_norm": 0.0008121557184495032,
      "learning_rate": 2.1239476758642517e-08,
      "loss": 0.0009,
      "step": 137430
    },
    {
      "epoch": 24989.090909090908,
      "grad_norm": 0.17211879789829254,
      "learning_rate": 2.1205937118237483e-08,
      "loss": 0.0012,
      "step": 137440
    },
    {
      "epoch": 24990.909090909092,
      "grad_norm": 0.0005639657028950751,
      "learning_rate": 2.1172423406545515e-08,
      "loss": 0.001,
      "step": 137450
    },
    {
      "epoch": 24992.727272727272,
      "grad_norm": 0.20807379484176636,
      "learning_rate": 2.113893562538166e-08,
      "loss": 0.001,
      "step": 137460
    },
    {
      "epoch": 24994.545454545456,
      "grad_norm": 0.16777396202087402,
      "learning_rate": 2.11054737765593e-08,
      "loss": 0.0012,
      "step": 137470
    },
    {
      "epoch": 24996.363636363636,
      "grad_norm": 0.0007450815755873919,
      "learning_rate": 2.10720378618906e-08,
      "loss": 0.0012,
      "step": 137480
    },
    {
      "epoch": 24998.18181818182,
      "grad_norm": 0.2209848165512085,
      "learning_rate": 2.1038627883186276e-08,
      "loss": 0.0011,
      "step": 137490
    },
    {
      "epoch": 25000.0,
      "grad_norm": 0.0006959449965506792,
      "learning_rate": 2.100524384225555e-08,
      "loss": 0.0011,
      "step": 137500
    },
    {
      "epoch": 25000.0,
      "eval_loss": 5.249763488769531,
      "eval_runtime": 0.9474,
      "eval_samples_per_second": 10.555,
      "eval_steps_per_second": 5.277,
      "step": 137500
    },
    {
      "epoch": 25001.81818181818,
      "grad_norm": 0.1748778074979782,
      "learning_rate": 2.097188574090636e-08,
      "loss": 0.0012,
      "step": 137510
    },
    {
      "epoch": 25003.636363636364,
      "grad_norm": 0.0005043148412369192,
      "learning_rate": 2.0938553580945206e-08,
      "loss": 0.0008,
      "step": 137520
    },
    {
      "epoch": 25005.454545454544,
      "grad_norm": 0.0006690812297165394,
      "learning_rate": 2.090524736417709e-08,
      "loss": 0.001,
      "step": 137530
    },
    {
      "epoch": 25007.272727272728,
      "grad_norm": 0.0004790551611222327,
      "learning_rate": 2.087196709240574e-08,
      "loss": 0.0013,
      "step": 137540
    },
    {
      "epoch": 25009.090909090908,
      "grad_norm": 0.3389468491077423,
      "learning_rate": 2.0838712767433376e-08,
      "loss": 0.0012,
      "step": 137550
    },
    {
      "epoch": 25010.909090909092,
      "grad_norm": 0.0006505658966489136,
      "learning_rate": 2.0805484391061002e-08,
      "loss": 0.0009,
      "step": 137560
    },
    {
      "epoch": 25012.727272727272,
      "grad_norm": 0.1750705987215042,
      "learning_rate": 2.0772281965087846e-08,
      "loss": 0.0013,
      "step": 137570
    },
    {
      "epoch": 25014.545454545456,
      "grad_norm": 0.3026248514652252,
      "learning_rate": 2.0739105491312026e-08,
      "loss": 0.0009,
      "step": 137580
    },
    {
      "epoch": 25016.363636363636,
      "grad_norm": 0.20716503262519836,
      "learning_rate": 2.070595497153038e-08,
      "loss": 0.0011,
      "step": 137590
    },
    {
      "epoch": 25018.18181818182,
      "grad_norm": 0.0009393322397954762,
      "learning_rate": 2.0672830407537923e-08,
      "loss": 0.0009,
      "step": 137600
    },
    {
      "epoch": 25020.0,
      "grad_norm": 0.0484558641910553,
      "learning_rate": 2.06397318011286e-08,
      "loss": 0.0012,
      "step": 137610
    },
    {
      "epoch": 25021.81818181818,
      "grad_norm": 0.0004398133896756917,
      "learning_rate": 2.060665915409482e-08,
      "loss": 0.0013,
      "step": 137620
    },
    {
      "epoch": 25023.636363636364,
      "grad_norm": 0.0008114748634397984,
      "learning_rate": 2.0573612468227586e-08,
      "loss": 0.001,
      "step": 137630
    },
    {
      "epoch": 25025.454545454544,
      "grad_norm": 0.000514874467626214,
      "learning_rate": 2.0540591745316526e-08,
      "loss": 0.001,
      "step": 137640
    },
    {
      "epoch": 25027.272727272728,
      "grad_norm": 0.2709388732910156,
      "learning_rate": 2.0507596987149876e-08,
      "loss": 0.0013,
      "step": 137650
    },
    {
      "epoch": 25029.090909090908,
      "grad_norm": 0.2217109650373459,
      "learning_rate": 2.0474628195514322e-08,
      "loss": 0.0008,
      "step": 137660
    },
    {
      "epoch": 25030.909090909092,
      "grad_norm": 0.2793220579624176,
      "learning_rate": 2.0441685372195483e-08,
      "loss": 0.0012,
      "step": 137670
    },
    {
      "epoch": 25032.727272727272,
      "grad_norm": 0.267754465341568,
      "learning_rate": 2.0408768518977216e-08,
      "loss": 0.0013,
      "step": 137680
    },
    {
      "epoch": 25034.545454545456,
      "grad_norm": 0.0005349754355847836,
      "learning_rate": 2.0375877637642036e-08,
      "loss": 0.0012,
      "step": 137690
    },
    {
      "epoch": 25036.363636363636,
      "grad_norm": 0.2182653397321701,
      "learning_rate": 2.0343012729971243e-08,
      "loss": 0.0007,
      "step": 137700
    },
    {
      "epoch": 25038.18181818182,
      "grad_norm": 0.1732705980539322,
      "learning_rate": 2.031017379774458e-08,
      "loss": 0.0014,
      "step": 137710
    },
    {
      "epoch": 25040.0,
      "grad_norm": 0.17771469056606293,
      "learning_rate": 2.02773608427404e-08,
      "loss": 0.0008,
      "step": 137720
    },
    {
      "epoch": 25041.81818181818,
      "grad_norm": 0.0005648430087603629,
      "learning_rate": 2.024457386673567e-08,
      "loss": 0.0009,
      "step": 137730
    },
    {
      "epoch": 25043.636363636364,
      "grad_norm": 0.22045506536960602,
      "learning_rate": 2.021181287150592e-08,
      "loss": 0.0012,
      "step": 137740
    },
    {
      "epoch": 25045.454545454544,
      "grad_norm": 0.0014176391996443272,
      "learning_rate": 2.0179077858825445e-08,
      "loss": 0.0011,
      "step": 137750
    },
    {
      "epoch": 25047.272727272728,
      "grad_norm": 0.21025219559669495,
      "learning_rate": 2.0146368830466666e-08,
      "loss": 0.0012,
      "step": 137760
    },
    {
      "epoch": 25049.090909090908,
      "grad_norm": 0.0006303380359895527,
      "learning_rate": 2.0113685788201162e-08,
      "loss": 0.0011,
      "step": 137770
    },
    {
      "epoch": 25050.909090909092,
      "grad_norm": 0.17092262208461761,
      "learning_rate": 2.0081028733798855e-08,
      "loss": 0.0012,
      "step": 137780
    },
    {
      "epoch": 25052.727272727272,
      "grad_norm": 0.2751149535179138,
      "learning_rate": 2.0048397669028162e-08,
      "loss": 0.0011,
      "step": 137790
    },
    {
      "epoch": 25054.545454545456,
      "grad_norm": 0.22128817439079285,
      "learning_rate": 2.0015792595656222e-08,
      "loss": 0.0011,
      "step": 137800
    },
    {
      "epoch": 25056.363636363636,
      "grad_norm": 0.00044991422328166664,
      "learning_rate": 1.9983213515448794e-08,
      "loss": 0.0008,
      "step": 137810
    },
    {
      "epoch": 25058.18181818182,
      "grad_norm": 0.0007799640879966319,
      "learning_rate": 1.995066043017013e-08,
      "loss": 0.001,
      "step": 137820
    },
    {
      "epoch": 25060.0,
      "grad_norm": 0.000532346370164305,
      "learning_rate": 1.9918133341583155e-08,
      "loss": 0.0012,
      "step": 137830
    },
    {
      "epoch": 25061.81818181818,
      "grad_norm": 0.0005175699479877949,
      "learning_rate": 1.988563225144918e-08,
      "loss": 0.001,
      "step": 137840
    },
    {
      "epoch": 25063.636363636364,
      "grad_norm": 0.27430295944213867,
      "learning_rate": 1.9853157161528468e-08,
      "loss": 0.0012,
      "step": 137850
    },
    {
      "epoch": 25065.454545454544,
      "grad_norm": 0.1727685034275055,
      "learning_rate": 1.9820708073579663e-08,
      "loss": 0.001,
      "step": 137860
    },
    {
      "epoch": 25067.272727272728,
      "grad_norm": 0.0005503862048499286,
      "learning_rate": 1.9788284989359916e-08,
      "loss": 0.0009,
      "step": 137870
    },
    {
      "epoch": 25069.090909090908,
      "grad_norm": 0.2856237590312958,
      "learning_rate": 1.9755887910625103e-08,
      "loss": 0.0014,
      "step": 137880
    },
    {
      "epoch": 25070.909090909092,
      "grad_norm": 0.0005473207565955818,
      "learning_rate": 1.9723516839129762e-08,
      "loss": 0.0009,
      "step": 137890
    },
    {
      "epoch": 25072.727272727272,
      "grad_norm": 0.00046235640184022486,
      "learning_rate": 1.969117177662688e-08,
      "loss": 0.0011,
      "step": 137900
    },
    {
      "epoch": 25074.545454545456,
      "grad_norm": 0.16422095894813538,
      "learning_rate": 1.9658852724868002e-08,
      "loss": 0.0014,
      "step": 137910
    },
    {
      "epoch": 25076.363636363636,
      "grad_norm": 0.22342060506343842,
      "learning_rate": 1.962655968560334e-08,
      "loss": 0.0009,
      "step": 137920
    },
    {
      "epoch": 25078.18181818182,
      "grad_norm": 0.1749797910451889,
      "learning_rate": 1.959429266058177e-08,
      "loss": 0.0012,
      "step": 137930
    },
    {
      "epoch": 25080.0,
      "grad_norm": 0.0010199099779129028,
      "learning_rate": 1.9562051651550782e-08,
      "loss": 0.0011,
      "step": 137940
    },
    {
      "epoch": 25081.81818181818,
      "grad_norm": 0.000716080772690475,
      "learning_rate": 1.9529836660256095e-08,
      "loss": 0.001,
      "step": 137950
    },
    {
      "epoch": 25083.636363636364,
      "grad_norm": 0.0005556090618483722,
      "learning_rate": 1.9497647688442476e-08,
      "loss": 0.0009,
      "step": 137960
    },
    {
      "epoch": 25085.454545454544,
      "grad_norm": 0.2733912765979767,
      "learning_rate": 1.946548473785309e-08,
      "loss": 0.0015,
      "step": 137970
    },
    {
      "epoch": 25087.272727272728,
      "grad_norm": 0.19551199674606323,
      "learning_rate": 1.943334781022965e-08,
      "loss": 0.0009,
      "step": 137980
    },
    {
      "epoch": 25089.090909090908,
      "grad_norm": 0.00047273209202103317,
      "learning_rate": 1.9401236907312436e-08,
      "loss": 0.001,
      "step": 137990
    },
    {
      "epoch": 25090.909090909092,
      "grad_norm": 0.16441623866558075,
      "learning_rate": 1.936915203084055e-08,
      "loss": 0.001,
      "step": 138000
    },
    {
      "epoch": 25090.909090909092,
      "eval_loss": 5.144680023193359,
      "eval_runtime": 0.951,
      "eval_samples_per_second": 10.515,
      "eval_steps_per_second": 5.257,
      "step": 138000
    },
    {
      "epoch": 25092.727272727272,
      "grad_norm": 0.0005098000983707607,
      "learning_rate": 1.93370931825515e-08,
      "loss": 0.001,
      "step": 138010
    },
    {
      "epoch": 25094.545454545456,
      "grad_norm": 0.2713282108306885,
      "learning_rate": 1.930506036418128e-08,
      "loss": 0.0014,
      "step": 138020
    },
    {
      "epoch": 25096.363636363636,
      "grad_norm": 0.0006527486839331686,
      "learning_rate": 1.9273053577464616e-08,
      "loss": 0.0008,
      "step": 138030
    },
    {
      "epoch": 25098.18181818182,
      "grad_norm": 0.0006793627981096506,
      "learning_rate": 1.924107282413484e-08,
      "loss": 0.001,
      "step": 138040
    },
    {
      "epoch": 25100.0,
      "grad_norm": 0.0008561204886063933,
      "learning_rate": 1.9209118105924072e-08,
      "loss": 0.0012,
      "step": 138050
    },
    {
      "epoch": 25101.81818181818,
      "grad_norm": 0.16942983865737915,
      "learning_rate": 1.917718942456237e-08,
      "loss": 0.0013,
      "step": 138060
    },
    {
      "epoch": 25103.636363636364,
      "grad_norm": 0.001409144140779972,
      "learning_rate": 1.9145286781779123e-08,
      "loss": 0.0009,
      "step": 138070
    },
    {
      "epoch": 25105.454545454544,
      "grad_norm": 0.0010424641659483314,
      "learning_rate": 1.9113410179301902e-08,
      "loss": 0.0012,
      "step": 138080
    },
    {
      "epoch": 25107.272727272728,
      "grad_norm": 0.0014021380338817835,
      "learning_rate": 1.9081559618856936e-08,
      "loss": 0.0009,
      "step": 138090
    },
    {
      "epoch": 25109.090909090908,
      "grad_norm": 0.22040025889873505,
      "learning_rate": 1.9049735102169117e-08,
      "loss": 0.0013,
      "step": 138100
    },
    {
      "epoch": 25110.909090909092,
      "grad_norm": 0.2311716079711914,
      "learning_rate": 1.9017936630961795e-08,
      "loss": 0.0011,
      "step": 138110
    },
    {
      "epoch": 25112.727272727272,
      "grad_norm": 0.14883926510810852,
      "learning_rate": 1.8986164206957034e-08,
      "loss": 0.0009,
      "step": 138120
    },
    {
      "epoch": 25114.545454545456,
      "grad_norm": 0.20309999585151672,
      "learning_rate": 1.895441783187557e-08,
      "loss": 0.0013,
      "step": 138130
    },
    {
      "epoch": 25116.363636363636,
      "grad_norm": 0.0007612720364704728,
      "learning_rate": 1.8922697507436358e-08,
      "loss": 0.0012,
      "step": 138140
    },
    {
      "epoch": 25118.18181818182,
      "grad_norm": 0.00044995712232775986,
      "learning_rate": 1.8891003235357307e-08,
      "loss": 0.0008,
      "step": 138150
    },
    {
      "epoch": 25120.0,
      "grad_norm": 0.23259881138801575,
      "learning_rate": 1.8859335017354873e-08,
      "loss": 0.0012,
      "step": 138160
    },
    {
      "epoch": 25121.81818181818,
      "grad_norm": 0.1844092756509781,
      "learning_rate": 1.8827692855143962e-08,
      "loss": 0.0012,
      "step": 138170
    },
    {
      "epoch": 25123.636363636364,
      "grad_norm": 0.00040686075226403773,
      "learning_rate": 1.8796076750438094e-08,
      "loss": 0.0012,
      "step": 138180
    },
    {
      "epoch": 25125.454545454544,
      "grad_norm": 0.20295338332653046,
      "learning_rate": 1.8764486704949402e-08,
      "loss": 0.0012,
      "step": 138190
    },
    {
      "epoch": 25127.272727272728,
      "grad_norm": 0.0005321991047821939,
      "learning_rate": 1.873292272038868e-08,
      "loss": 0.0009,
      "step": 138200
    },
    {
      "epoch": 25129.090909090908,
      "grad_norm": 0.0004069607239216566,
      "learning_rate": 1.8701384798465282e-08,
      "loss": 0.001,
      "step": 138210
    },
    {
      "epoch": 25130.909090909092,
      "grad_norm": 0.16718903183937073,
      "learning_rate": 1.8669872940886954e-08,
      "loss": 0.0011,
      "step": 138220
    },
    {
      "epoch": 25132.727272727272,
      "grad_norm": 0.16994860768318176,
      "learning_rate": 1.8638387149360336e-08,
      "loss": 0.0011,
      "step": 138230
    },
    {
      "epoch": 25134.545454545456,
      "grad_norm": 0.20739613473415375,
      "learning_rate": 1.860692742559061e-08,
      "loss": 0.0009,
      "step": 138240
    },
    {
      "epoch": 25136.363636363636,
      "grad_norm": 0.17414166033267975,
      "learning_rate": 1.8575493771281203e-08,
      "loss": 0.0012,
      "step": 138250
    },
    {
      "epoch": 25138.18181818182,
      "grad_norm": 0.0005125714815221727,
      "learning_rate": 1.8544086188134522e-08,
      "loss": 0.0009,
      "step": 138260
    },
    {
      "epoch": 25140.0,
      "grad_norm": 0.16922630369663239,
      "learning_rate": 1.8512704677851487e-08,
      "loss": 0.0012,
      "step": 138270
    },
    {
      "epoch": 25141.81818181818,
      "grad_norm": 0.26886340975761414,
      "learning_rate": 1.8481349242131406e-08,
      "loss": 0.0012,
      "step": 138280
    },
    {
      "epoch": 25143.636363636364,
      "grad_norm": 0.2188578099012375,
      "learning_rate": 1.8450019882672364e-08,
      "loss": 0.0009,
      "step": 138290
    },
    {
      "epoch": 25145.454545454544,
      "grad_norm": 0.0009307305444963276,
      "learning_rate": 1.8418716601170947e-08,
      "loss": 0.0009,
      "step": 138300
    },
    {
      "epoch": 25147.272727272728,
      "grad_norm": 0.2669498324394226,
      "learning_rate": 1.8387439399322356e-08,
      "loss": 0.0016,
      "step": 138310
    },
    {
      "epoch": 25149.090909090908,
      "grad_norm": 0.2759850323200226,
      "learning_rate": 1.8356188278820518e-08,
      "loss": 0.001,
      "step": 138320
    },
    {
      "epoch": 25150.909090909092,
      "grad_norm": 0.17037849128246307,
      "learning_rate": 1.8324963241357626e-08,
      "loss": 0.0007,
      "step": 138330
    },
    {
      "epoch": 25152.727272727272,
      "grad_norm": 0.20604699850082397,
      "learning_rate": 1.829376428862467e-08,
      "loss": 0.0009,
      "step": 138340
    },
    {
      "epoch": 25154.545454545456,
      "grad_norm": 0.16328586637973785,
      "learning_rate": 1.8262591422311346e-08,
      "loss": 0.0015,
      "step": 138350
    },
    {
      "epoch": 25156.363636363636,
      "grad_norm": 0.0004901894717477262,
      "learning_rate": 1.8231444644105752e-08,
      "loss": 0.0009,
      "step": 138360
    },
    {
      "epoch": 25158.18181818182,
      "grad_norm": 0.0004975572228431702,
      "learning_rate": 1.820032395569454e-08,
      "loss": 0.001,
      "step": 138370
    },
    {
      "epoch": 25160.0,
      "grad_norm": 0.20822298526763916,
      "learning_rate": 1.8169229358763027e-08,
      "loss": 0.0012,
      "step": 138380
    },
    {
      "epoch": 25161.81818181818,
      "grad_norm": 0.0005535583477467299,
      "learning_rate": 1.8138160854995144e-08,
      "loss": 0.001,
      "step": 138390
    },
    {
      "epoch": 25163.636363636364,
      "grad_norm": 0.0006515755085274577,
      "learning_rate": 1.810711844607349e-08,
      "loss": 0.001,
      "step": 138400
    },
    {
      "epoch": 25165.454545454544,
      "grad_norm": 0.0017387071857228875,
      "learning_rate": 1.8076102133678884e-08,
      "loss": 0.001,
      "step": 138410
    },
    {
      "epoch": 25167.272727272728,
      "grad_norm": 0.003056703833863139,
      "learning_rate": 1.8045111919491206e-08,
      "loss": 0.001,
      "step": 138420
    },
    {
      "epoch": 25169.090909090908,
      "grad_norm": 0.16472892463207245,
      "learning_rate": 1.8014147805188784e-08,
      "loss": 0.0012,
      "step": 138430
    },
    {
      "epoch": 25170.909090909092,
      "grad_norm": 0.17897789180278778,
      "learning_rate": 1.7983209792448162e-08,
      "loss": 0.001,
      "step": 138440
    },
    {
      "epoch": 25172.727272727272,
      "grad_norm": 0.20561395585536957,
      "learning_rate": 1.7952297882945e-08,
      "loss": 0.001,
      "step": 138450
    },
    {
      "epoch": 25174.545454545456,
      "grad_norm": 0.19693967700004578,
      "learning_rate": 1.7921412078353126e-08,
      "loss": 0.0014,
      "step": 138460
    },
    {
      "epoch": 25176.363636363636,
      "grad_norm": 0.0004993270267732441,
      "learning_rate": 1.7890552380345313e-08,
      "loss": 0.001,
      "step": 138470
    },
    {
      "epoch": 25178.18181818182,
      "grad_norm": 0.0004912782460451126,
      "learning_rate": 1.7859718790592725e-08,
      "loss": 0.0009,
      "step": 138480
    },
    {
      "epoch": 25180.0,
      "grad_norm": 0.0010545873083174229,
      "learning_rate": 1.7828911310764972e-08,
      "loss": 0.0012,
      "step": 138490
    },
    {
      "epoch": 25181.81818181818,
      "grad_norm": 0.0004083164385519922,
      "learning_rate": 1.7798129942530548e-08,
      "loss": 0.0009,
      "step": 138500
    },
    {
      "epoch": 25181.81818181818,
      "eval_loss": 5.17501163482666,
      "eval_runtime": 0.9503,
      "eval_samples_per_second": 10.523,
      "eval_steps_per_second": 5.262,
      "step": 138500
    },
    {
      "epoch": 25183.636363636364,
      "grad_norm": 0.16889269649982452,
      "learning_rate": 1.7767374687556403e-08,
      "loss": 0.0012,
      "step": 138510
    },
    {
      "epoch": 25185.454545454544,
      "grad_norm": 0.0010882901260629296,
      "learning_rate": 1.7736645547507924e-08,
      "loss": 0.0009,
      "step": 138520
    },
    {
      "epoch": 25187.272727272728,
      "grad_norm": 0.23666852712631226,
      "learning_rate": 1.770594252404939e-08,
      "loss": 0.0013,
      "step": 138530
    },
    {
      "epoch": 25189.090909090908,
      "grad_norm": 0.2706700563430786,
      "learning_rate": 1.767526561884336e-08,
      "loss": 0.001,
      "step": 138540
    },
    {
      "epoch": 25190.909090909092,
      "grad_norm": 0.0007686084718443453,
      "learning_rate": 1.764461483355123e-08,
      "loss": 0.0012,
      "step": 138550
    },
    {
      "epoch": 25192.727272727272,
      "grad_norm": 0.0005353529704734683,
      "learning_rate": 1.7613990169832893e-08,
      "loss": 0.0009,
      "step": 138560
    },
    {
      "epoch": 25194.545454545456,
      "grad_norm": 0.2349957525730133,
      "learning_rate": 1.7583391629346577e-08,
      "loss": 0.0012,
      "step": 138570
    },
    {
      "epoch": 25196.363636363636,
      "grad_norm": 0.20598983764648438,
      "learning_rate": 1.755281921374957e-08,
      "loss": 0.0013,
      "step": 138580
    },
    {
      "epoch": 25198.18181818182,
      "grad_norm": 0.21971818804740906,
      "learning_rate": 1.7522272924697435e-08,
      "loss": 0.0009,
      "step": 138590
    },
    {
      "epoch": 25200.0,
      "grad_norm": 0.0005365617689676583,
      "learning_rate": 1.7491752763844292e-08,
      "loss": 0.001,
      "step": 138600
    },
    {
      "epoch": 25201.81818181818,
      "grad_norm": 0.0007873534341342747,
      "learning_rate": 1.7461258732842986e-08,
      "loss": 0.0012,
      "step": 138610
    },
    {
      "epoch": 25203.636363636364,
      "grad_norm": 0.22178702056407928,
      "learning_rate": 1.7430790833344976e-08,
      "loss": 0.001,
      "step": 138620
    },
    {
      "epoch": 25205.454545454544,
      "grad_norm": 0.21968033909797668,
      "learning_rate": 1.740034906700011e-08,
      "loss": 0.0009,
      "step": 138630
    },
    {
      "epoch": 25207.272727272728,
      "grad_norm": 0.18018974363803864,
      "learning_rate": 1.7369933435457006e-08,
      "loss": 0.0012,
      "step": 138640
    },
    {
      "epoch": 25209.090909090908,
      "grad_norm": 0.2102426141500473,
      "learning_rate": 1.733954394036269e-08,
      "loss": 0.0012,
      "step": 138650
    },
    {
      "epoch": 25210.909090909092,
      "grad_norm": 0.00047566028661094606,
      "learning_rate": 1.730918058336306e-08,
      "loss": 0.001,
      "step": 138660
    },
    {
      "epoch": 25212.727272727272,
      "grad_norm": 0.1710377335548401,
      "learning_rate": 1.727884336610236e-08,
      "loss": 0.0012,
      "step": 138670
    },
    {
      "epoch": 25214.545454545456,
      "grad_norm": 0.2073224037885666,
      "learning_rate": 1.7248532290223327e-08,
      "loss": 0.0009,
      "step": 138680
    },
    {
      "epoch": 25216.363636363636,
      "grad_norm": 0.1750517636537552,
      "learning_rate": 1.7218247357367656e-08,
      "loss": 0.0012,
      "step": 138690
    },
    {
      "epoch": 25218.18181818182,
      "grad_norm": 0.0009935371344909072,
      "learning_rate": 1.7187988569175304e-08,
      "loss": 0.001,
      "step": 138700
    },
    {
      "epoch": 25220.0,
      "grad_norm": 0.2235664427280426,
      "learning_rate": 1.71577559272848e-08,
      "loss": 0.0012,
      "step": 138710
    },
    {
      "epoch": 25221.81818181818,
      "grad_norm": 0.16783902049064636,
      "learning_rate": 1.7127549433333554e-08,
      "loss": 0.0012,
      "step": 138720
    },
    {
      "epoch": 25223.636363636364,
      "grad_norm": 0.0006693427567370236,
      "learning_rate": 1.709736908895726e-08,
      "loss": 0.001,
      "step": 138730
    },
    {
      "epoch": 25225.454545454544,
      "grad_norm": 0.17100238800048828,
      "learning_rate": 1.7067214895790382e-08,
      "loss": 0.0012,
      "step": 138740
    },
    {
      "epoch": 25227.272727272728,
      "grad_norm": 0.1666748821735382,
      "learning_rate": 1.7037086855465898e-08,
      "loss": 0.0011,
      "step": 138750
    },
    {
      "epoch": 25229.090909090908,
      "grad_norm": 0.22089119255542755,
      "learning_rate": 1.7006984969615224e-08,
      "loss": 0.001,
      "step": 138760
    },
    {
      "epoch": 25230.909090909092,
      "grad_norm": 0.0006894555990584195,
      "learning_rate": 1.697690923986872e-08,
      "loss": 0.001,
      "step": 138770
    },
    {
      "epoch": 25232.727272727272,
      "grad_norm": 0.16687273979187012,
      "learning_rate": 1.6946859667854973e-08,
      "loss": 0.0012,
      "step": 138780
    },
    {
      "epoch": 25234.545454545456,
      "grad_norm": 0.0005431321333162487,
      "learning_rate": 1.6916836255201295e-08,
      "loss": 0.001,
      "step": 138790
    },
    {
      "epoch": 25236.363636363636,
      "grad_norm": 0.0008500593830831349,
      "learning_rate": 1.688683900353366e-08,
      "loss": 0.001,
      "step": 138800
    },
    {
      "epoch": 25238.18181818182,
      "grad_norm": 0.22165317833423615,
      "learning_rate": 1.685686791447649e-08,
      "loss": 0.0012,
      "step": 138810
    },
    {
      "epoch": 25240.0,
      "grad_norm": 0.0005799502832815051,
      "learning_rate": 1.682692298965277e-08,
      "loss": 0.001,
      "step": 138820
    },
    {
      "epoch": 25241.81818181818,
      "grad_norm": 0.21865016222000122,
      "learning_rate": 1.6797004230684307e-08,
      "loss": 0.0009,
      "step": 138830
    },
    {
      "epoch": 25243.636363636364,
      "grad_norm": 0.16772007942199707,
      "learning_rate": 1.6767111639191202e-08,
      "loss": 0.0012,
      "step": 138840
    },
    {
      "epoch": 25245.454545454544,
      "grad_norm": 0.0008429918671026826,
      "learning_rate": 1.6737245216792374e-08,
      "loss": 0.0009,
      "step": 138850
    },
    {
      "epoch": 25247.272727272728,
      "grad_norm": 0.26579394936561584,
      "learning_rate": 1.6707404965105087e-08,
      "loss": 0.0015,
      "step": 138860
    },
    {
      "epoch": 25249.090909090908,
      "grad_norm": 0.0006592468125745654,
      "learning_rate": 1.6677590885745384e-08,
      "loss": 0.0009,
      "step": 138870
    },
    {
      "epoch": 25250.909090909092,
      "grad_norm": 0.005160746164619923,
      "learning_rate": 1.6647802980327863e-08,
      "loss": 0.0012,
      "step": 138880
    },
    {
      "epoch": 25252.727272727272,
      "grad_norm": 0.00064267055131495,
      "learning_rate": 1.661804125046562e-08,
      "loss": 0.0012,
      "step": 138890
    },
    {
      "epoch": 25254.545454545456,
      "grad_norm": 0.20574943721294403,
      "learning_rate": 1.658830569777031e-08,
      "loss": 0.0013,
      "step": 138900
    },
    {
      "epoch": 25256.363636363636,
      "grad_norm": 0.1679590344429016,
      "learning_rate": 1.655859632385237e-08,
      "loss": 0.0008,
      "step": 138910
    },
    {
      "epoch": 25258.18181818182,
      "grad_norm": 0.17506630718708038,
      "learning_rate": 1.6528913130320566e-08,
      "loss": 0.0015,
      "step": 138920
    },
    {
      "epoch": 25260.0,
      "grad_norm": 0.0015414098743349314,
      "learning_rate": 1.64992561187825e-08,
      "loss": 0.0007,
      "step": 138930
    },
    {
      "epoch": 25261.81818181818,
      "grad_norm": 0.17481504380702972,
      "learning_rate": 1.6469625290844112e-08,
      "loss": 0.001,
      "step": 138940
    },
    {
      "epoch": 25263.636363636364,
      "grad_norm": 0.20605437457561493,
      "learning_rate": 1.6440020648110065e-08,
      "loss": 0.0013,
      "step": 138950
    },
    {
      "epoch": 25265.454545454544,
      "grad_norm": 0.20996959507465363,
      "learning_rate": 1.641044219218357e-08,
      "loss": 0.001,
      "step": 138960
    },
    {
      "epoch": 25267.272727272728,
      "grad_norm": 0.0007188565214164555,
      "learning_rate": 1.638088992466652e-08,
      "loss": 0.0009,
      "step": 138970
    },
    {
      "epoch": 25269.090909090908,
      "grad_norm": 0.17392811179161072,
      "learning_rate": 1.6351363847159127e-08,
      "loss": 0.0012,
      "step": 138980
    },
    {
      "epoch": 25270.909090909092,
      "grad_norm": 0.00044662386062555015,
      "learning_rate": 1.632186396126045e-08,
      "loss": 0.0012,
      "step": 138990
    },
    {
      "epoch": 25272.727272727272,
      "grad_norm": 0.0005284656654112041,
      "learning_rate": 1.62923902685681e-08,
      "loss": 0.001,
      "step": 139000
    },
    {
      "epoch": 25272.727272727272,
      "eval_loss": 5.210816860198975,
      "eval_runtime": 0.9488,
      "eval_samples_per_second": 10.539,
      "eval_steps_per_second": 5.27,
      "step": 139000
    },
    {
      "epoch": 25274.545454545456,
      "grad_norm": 0.22439096868038177,
      "learning_rate": 1.6262942770678022e-08,
      "loss": 0.0012,
      "step": 139010
    },
    {
      "epoch": 25276.363636363636,
      "grad_norm": 0.0004436535236891359,
      "learning_rate": 1.6233521469185054e-08,
      "loss": 0.0007,
      "step": 139020
    },
    {
      "epoch": 25278.18181818182,
      "grad_norm": 0.0006204242818057537,
      "learning_rate": 1.6204126365682412e-08,
      "loss": 0.0012,
      "step": 139030
    },
    {
      "epoch": 25280.0,
      "grad_norm": 0.005739851389080286,
      "learning_rate": 1.6174757461762056e-08,
      "loss": 0.0012,
      "step": 139040
    },
    {
      "epoch": 25281.81818181818,
      "grad_norm": 0.0007262491853907704,
      "learning_rate": 1.614541475901443e-08,
      "loss": 0.0012,
      "step": 139050
    },
    {
      "epoch": 25283.636363636364,
      "grad_norm": 0.0006527421064674854,
      "learning_rate": 1.611609825902843e-08,
      "loss": 0.0009,
      "step": 139060
    },
    {
      "epoch": 25285.454545454544,
      "grad_norm": 0.0005691460100933909,
      "learning_rate": 1.608680796339179e-08,
      "loss": 0.0012,
      "step": 139070
    },
    {
      "epoch": 25287.272727272728,
      "grad_norm": 0.0004324197943788022,
      "learning_rate": 1.6057543873690683e-08,
      "loss": 0.0011,
      "step": 139080
    },
    {
      "epoch": 25289.090909090908,
      "grad_norm": 0.18115030229091644,
      "learning_rate": 1.602830599150984e-08,
      "loss": 0.0013,
      "step": 139090
    },
    {
      "epoch": 25290.909090909092,
      "grad_norm": 0.21836917102336884,
      "learning_rate": 1.599909431843266e-08,
      "loss": 0.0009,
      "step": 139100
    },
    {
      "epoch": 25292.727272727272,
      "grad_norm": 0.21871337294578552,
      "learning_rate": 1.5969908856041047e-08,
      "loss": 0.0013,
      "step": 139110
    },
    {
      "epoch": 25294.545454545456,
      "grad_norm": 0.000699733616784215,
      "learning_rate": 1.5940749605915572e-08,
      "loss": 0.0007,
      "step": 139120
    },
    {
      "epoch": 25296.363636363636,
      "grad_norm": 0.15245483815670013,
      "learning_rate": 1.5911616569635246e-08,
      "loss": 0.0015,
      "step": 139130
    },
    {
      "epoch": 25298.18181818182,
      "grad_norm": 0.0008396885241381824,
      "learning_rate": 1.5882509748777807e-08,
      "loss": 0.0009,
      "step": 139140
    },
    {
      "epoch": 25300.0,
      "grad_norm": 0.0005337570910342038,
      "learning_rate": 1.5853429144919493e-08,
      "loss": 0.0012,
      "step": 139150
    },
    {
      "epoch": 25301.81818181818,
      "grad_norm": 0.0003549320681486279,
      "learning_rate": 1.582437475963516e-08,
      "loss": 0.0012,
      "step": 139160
    },
    {
      "epoch": 25303.636363636364,
      "grad_norm": 0.0004812846891582012,
      "learning_rate": 1.579534659449816e-08,
      "loss": 0.0012,
      "step": 139170
    },
    {
      "epoch": 25305.454545454544,
      "grad_norm": 0.0003714262566063553,
      "learning_rate": 1.576634465108062e-08,
      "loss": 0.0007,
      "step": 139180
    },
    {
      "epoch": 25307.272727272728,
      "grad_norm": 0.2714674472808838,
      "learning_rate": 1.573736893095301e-08,
      "loss": 0.0014,
      "step": 139190
    },
    {
      "epoch": 25309.090909090908,
      "grad_norm": 0.17161238193511963,
      "learning_rate": 1.570841943568446e-08,
      "loss": 0.001,
      "step": 139200
    },
    {
      "epoch": 25310.909090909092,
      "grad_norm": 0.005836762022227049,
      "learning_rate": 1.5679496166842776e-08,
      "loss": 0.001,
      "step": 139210
    },
    {
      "epoch": 25312.727272727272,
      "grad_norm": 0.16424597799777985,
      "learning_rate": 1.565059912599426e-08,
      "loss": 0.0012,
      "step": 139220
    },
    {
      "epoch": 25314.545454545456,
      "grad_norm": 0.0005349699640646577,
      "learning_rate": 1.562172831470382e-08,
      "loss": 0.001,
      "step": 139230
    },
    {
      "epoch": 25316.363636363636,
      "grad_norm": 0.001250426983460784,
      "learning_rate": 1.5592883734534933e-08,
      "loss": 0.0012,
      "step": 139240
    },
    {
      "epoch": 25318.18181818182,
      "grad_norm": 0.20827046036720276,
      "learning_rate": 1.556406538704963e-08,
      "loss": 0.001,
      "step": 139250
    },
    {
      "epoch": 25320.0,
      "grad_norm": 0.19849632680416107,
      "learning_rate": 1.5535273273808546e-08,
      "loss": 0.001,
      "step": 139260
    },
    {
      "epoch": 25321.81818181818,
      "grad_norm": 0.20577943325042725,
      "learning_rate": 1.5506507396370937e-08,
      "loss": 0.001,
      "step": 139270
    },
    {
      "epoch": 25323.636363636364,
      "grad_norm": 0.0008814194006845355,
      "learning_rate": 1.5477767756294503e-08,
      "loss": 0.0012,
      "step": 139280
    },
    {
      "epoch": 25325.454545454544,
      "grad_norm": 0.001034242333844304,
      "learning_rate": 1.5449054355135714e-08,
      "loss": 0.0007,
      "step": 139290
    },
    {
      "epoch": 25327.272727272728,
      "grad_norm": 0.1756618469953537,
      "learning_rate": 1.5420367194449446e-08,
      "loss": 0.0013,
      "step": 139300
    },
    {
      "epoch": 25329.090909090908,
      "grad_norm": 0.41925808787345886,
      "learning_rate": 1.539170627578934e-08,
      "loss": 0.0013,
      "step": 139310
    },
    {
      "epoch": 25330.909090909092,
      "grad_norm": 0.00048562389565631747,
      "learning_rate": 1.5363071600707434e-08,
      "loss": 0.001,
      "step": 139320
    },
    {
      "epoch": 25332.727272727272,
      "grad_norm": 0.16806845366954803,
      "learning_rate": 1.533446317075432e-08,
      "loss": 0.001,
      "step": 139330
    },
    {
      "epoch": 25334.545454545456,
      "grad_norm": 0.0007341318414546549,
      "learning_rate": 1.530588098747948e-08,
      "loss": 0.0009,
      "step": 139340
    },
    {
      "epoch": 25336.363636363636,
      "grad_norm": 0.0004924219101667404,
      "learning_rate": 1.5277325052430568e-08,
      "loss": 0.0013,
      "step": 139350
    },
    {
      "epoch": 25338.18181818182,
      "grad_norm": 0.27047327160835266,
      "learning_rate": 1.5248795367154065e-08,
      "loss": 0.0012,
      "step": 139360
    },
    {
      "epoch": 25340.0,
      "grad_norm": 0.00042645164649002254,
      "learning_rate": 1.5220291933195074e-08,
      "loss": 0.0009,
      "step": 139370
    },
    {
      "epoch": 25341.81818181818,
      "grad_norm": 0.22154225409030914,
      "learning_rate": 1.519181475209702e-08,
      "loss": 0.0009,
      "step": 139380
    },
    {
      "epoch": 25343.636363636364,
      "grad_norm": 0.0007047594990581274,
      "learning_rate": 1.516336382540212e-08,
      "loss": 0.0013,
      "step": 139390
    },
    {
      "epoch": 25345.454545454544,
      "grad_norm": 0.0005675521097145975,
      "learning_rate": 1.5134939154651193e-08,
      "loss": 0.001,
      "step": 139400
    },
    {
      "epoch": 25347.272727272728,
      "grad_norm": 0.00037351640639826655,
      "learning_rate": 1.51065407413834e-08,
      "loss": 0.001,
      "step": 139410
    },
    {
      "epoch": 25349.090909090908,
      "grad_norm": 0.00047927958075888455,
      "learning_rate": 1.507816858713684e-08,
      "loss": 0.0012,
      "step": 139420
    },
    {
      "epoch": 25350.909090909092,
      "grad_norm": 0.0012438517296686769,
      "learning_rate": 1.504982269344779e-08,
      "loss": 0.001,
      "step": 139430
    },
    {
      "epoch": 25352.727272727272,
      "grad_norm": 0.0004071891016792506,
      "learning_rate": 1.502150306185135e-08,
      "loss": 0.0011,
      "step": 139440
    },
    {
      "epoch": 25354.545454545456,
      "grad_norm": 0.00047070818254724145,
      "learning_rate": 1.4993209693881183e-08,
      "loss": 0.0012,
      "step": 139450
    },
    {
      "epoch": 25356.363636363636,
      "grad_norm": 0.167121022939682,
      "learning_rate": 1.4964942591069563e-08,
      "loss": 0.0011,
      "step": 139460
    },
    {
      "epoch": 25358.18181818182,
      "grad_norm": 0.20950059592723846,
      "learning_rate": 1.49367017549471e-08,
      "loss": 0.0012,
      "step": 139470
    },
    {
      "epoch": 25360.0,
      "grad_norm": 0.006510721053928137,
      "learning_rate": 1.4908487187043294e-08,
      "loss": 0.0011,
      "step": 139480
    },
    {
      "epoch": 25361.81818181818,
      "grad_norm": 0.22222672402858734,
      "learning_rate": 1.4880298888885977e-08,
      "loss": 0.001,
      "step": 139490
    },
    {
      "epoch": 25363.636363636364,
      "grad_norm": 0.0009940349264070392,
      "learning_rate": 1.4852136862001763e-08,
      "loss": 0.0013,
      "step": 139500
    },
    {
      "epoch": 25363.636363636364,
      "eval_loss": 5.20344352722168,
      "eval_runtime": 0.9503,
      "eval_samples_per_second": 10.522,
      "eval_steps_per_second": 5.261,
      "step": 139500
    },
    {
      "epoch": 25365.454545454544,
      "grad_norm": 0.0004531194572336972,
      "learning_rate": 1.4824001107915706e-08,
      "loss": 0.001,
      "step": 139510
    },
    {
      "epoch": 25367.272727272728,
      "grad_norm": 0.0004145752754993737,
      "learning_rate": 1.479589162815148e-08,
      "loss": 0.0009,
      "step": 139520
    },
    {
      "epoch": 25369.090909090908,
      "grad_norm": 0.22499564290046692,
      "learning_rate": 1.476780842423131e-08,
      "loss": 0.0012,
      "step": 139530
    },
    {
      "epoch": 25370.909090909092,
      "grad_norm": 0.04095916450023651,
      "learning_rate": 1.4739751497676035e-08,
      "loss": 0.0012,
      "step": 139540
    },
    {
      "epoch": 25372.727272727272,
      "grad_norm": 0.0004330190713517368,
      "learning_rate": 1.471172085000505e-08,
      "loss": 0.0007,
      "step": 139550
    },
    {
      "epoch": 25374.545454545456,
      "grad_norm": 0.2231915444135666,
      "learning_rate": 1.4683716482736363e-08,
      "loss": 0.0017,
      "step": 139560
    },
    {
      "epoch": 25376.363636363636,
      "grad_norm": 0.17500139772891998,
      "learning_rate": 1.4655738397386486e-08,
      "loss": 0.0008,
      "step": 139570
    },
    {
      "epoch": 25378.18181818182,
      "grad_norm": 0.0005562168080359697,
      "learning_rate": 1.4627786595470538e-08,
      "loss": 0.001,
      "step": 139580
    },
    {
      "epoch": 25380.0,
      "grad_norm": 0.0006010605720803142,
      "learning_rate": 1.4599861078502308e-08,
      "loss": 0.0012,
      "step": 139590
    },
    {
      "epoch": 25381.81818181818,
      "grad_norm": 0.0017611718503758311,
      "learning_rate": 1.4571961847993974e-08,
      "loss": 0.0012,
      "step": 139600
    },
    {
      "epoch": 25383.636363636364,
      "grad_norm": 0.21346153318881989,
      "learning_rate": 1.4544088905456553e-08,
      "loss": 0.001,
      "step": 139610
    },
    {
      "epoch": 25385.454545454544,
      "grad_norm": 0.017900288105010986,
      "learning_rate": 1.4516242252399225e-08,
      "loss": 0.0013,
      "step": 139620
    },
    {
      "epoch": 25387.272727272728,
      "grad_norm": 0.16776308417320251,
      "learning_rate": 1.4488421890330226e-08,
      "loss": 0.0007,
      "step": 139630
    },
    {
      "epoch": 25389.090909090908,
      "grad_norm": 0.00046321123954840004,
      "learning_rate": 1.4460627820756078e-08,
      "loss": 0.001,
      "step": 139640
    },
    {
      "epoch": 25390.909090909092,
      "grad_norm": 0.011064188554883003,
      "learning_rate": 1.4432860045182016e-08,
      "loss": 0.0012,
      "step": 139650
    },
    {
      "epoch": 25392.727272727272,
      "grad_norm": 0.0005996295367367566,
      "learning_rate": 1.4405118565111618e-08,
      "loss": 0.001,
      "step": 139660
    },
    {
      "epoch": 25394.545454545456,
      "grad_norm": 0.0005592340021394193,
      "learning_rate": 1.4377403382047348e-08,
      "loss": 0.0013,
      "step": 139670
    },
    {
      "epoch": 25396.363636363636,
      "grad_norm": 0.0009367157472297549,
      "learning_rate": 1.4349714497490006e-08,
      "loss": 0.0009,
      "step": 139680
    },
    {
      "epoch": 25398.18181818182,
      "grad_norm": 0.2073415070772171,
      "learning_rate": 1.432205191293917e-08,
      "loss": 0.0013,
      "step": 139690
    },
    {
      "epoch": 25400.0,
      "grad_norm": 0.005681928712874651,
      "learning_rate": 1.4294415629892753e-08,
      "loss": 0.0012,
      "step": 139700
    },
    {
      "epoch": 25401.81818181818,
      "grad_norm": 0.2197776883840561,
      "learning_rate": 1.4266805649847392e-08,
      "loss": 0.001,
      "step": 139710
    },
    {
      "epoch": 25403.636363636364,
      "grad_norm": 0.25732213258743286,
      "learning_rate": 1.4239221974298443e-08,
      "loss": 0.0011,
      "step": 139720
    },
    {
      "epoch": 25405.454545454544,
      "grad_norm": 0.20856814086437225,
      "learning_rate": 1.4211664604739549e-08,
      "loss": 0.0012,
      "step": 139730
    },
    {
      "epoch": 25407.272727272728,
      "grad_norm": 0.20972363650798798,
      "learning_rate": 1.4184133542663012e-08,
      "loss": 0.001,
      "step": 139740
    },
    {
      "epoch": 25409.090909090908,
      "grad_norm": 0.14345374703407288,
      "learning_rate": 1.4156628789559922e-08,
      "loss": 0.0011,
      "step": 139750
    },
    {
      "epoch": 25410.909090909092,
      "grad_norm": 0.17159949243068695,
      "learning_rate": 1.4129150346919582e-08,
      "loss": 0.0011,
      "step": 139760
    },
    {
      "epoch": 25412.727272727272,
      "grad_norm": 0.0006402452709153295,
      "learning_rate": 1.410169821623025e-08,
      "loss": 0.0011,
      "step": 139770
    },
    {
      "epoch": 25414.545454545456,
      "grad_norm": 0.26331502199172974,
      "learning_rate": 1.4074272398978404e-08,
      "loss": 0.0012,
      "step": 139780
    },
    {
      "epoch": 25416.363636363636,
      "grad_norm": 0.0011368381092324853,
      "learning_rate": 1.4046872896649297e-08,
      "loss": 0.0008,
      "step": 139790
    },
    {
      "epoch": 25418.18181818182,
      "grad_norm": 0.0004562495741993189,
      "learning_rate": 1.4019499710726911e-08,
      "loss": 0.0012,
      "step": 139800
    },
    {
      "epoch": 25420.0,
      "grad_norm": 0.0005860542296431959,
      "learning_rate": 1.3992152842693395e-08,
      "loss": 0.0012,
      "step": 139810
    },
    {
      "epoch": 25421.81818181818,
      "grad_norm": 0.16860903799533844,
      "learning_rate": 1.3964832294029782e-08,
      "loss": 0.0012,
      "step": 139820
    },
    {
      "epoch": 25423.636363636364,
      "grad_norm": 0.1692446619272232,
      "learning_rate": 1.3937538066215671e-08,
      "loss": 0.0009,
      "step": 139830
    },
    {
      "epoch": 25425.454545454544,
      "grad_norm": 0.16578182578086853,
      "learning_rate": 1.3910270160729043e-08,
      "loss": 0.0013,
      "step": 139840
    },
    {
      "epoch": 25427.272727272728,
      "grad_norm": 0.00042582093738019466,
      "learning_rate": 1.3883028579046607e-08,
      "loss": 0.0009,
      "step": 139850
    },
    {
      "epoch": 25429.090909090908,
      "grad_norm": 0.2208302766084671,
      "learning_rate": 1.3855813322643628e-08,
      "loss": 0.0013,
      "step": 139860
    },
    {
      "epoch": 25430.909090909092,
      "grad_norm": 0.1654166728258133,
      "learning_rate": 1.3828624392993926e-08,
      "loss": 0.0012,
      "step": 139870
    },
    {
      "epoch": 25432.727272727272,
      "grad_norm": 0.00035321005270816386,
      "learning_rate": 1.3801461791569879e-08,
      "loss": 0.0012,
      "step": 139880
    },
    {
      "epoch": 25434.545454545456,
      "grad_norm": 0.0005478589446283877,
      "learning_rate": 1.3774325519842423e-08,
      "loss": 0.0008,
      "step": 139890
    },
    {
      "epoch": 25436.363636363636,
      "grad_norm": 0.2060929387807846,
      "learning_rate": 1.3747215579281157e-08,
      "loss": 0.001,
      "step": 139900
    },
    {
      "epoch": 25438.18181818182,
      "grad_norm": 0.21272309124469757,
      "learning_rate": 1.3720131971354187e-08,
      "loss": 0.0012,
      "step": 139910
    },
    {
      "epoch": 25440.0,
      "grad_norm": 0.16414467990398407,
      "learning_rate": 1.3693074697528228e-08,
      "loss": 0.001,
      "step": 139920
    },
    {
      "epoch": 25441.81818181818,
      "grad_norm": 0.20551662147045135,
      "learning_rate": 1.3666043759268497e-08,
      "loss": 0.0011,
      "step": 139930
    },
    {
      "epoch": 25443.636363636364,
      "grad_norm": 0.0005494357319548726,
      "learning_rate": 1.363903915803888e-08,
      "loss": 0.0007,
      "step": 139940
    },
    {
      "epoch": 25445.454545454544,
      "grad_norm": 0.21790382266044617,
      "learning_rate": 1.3612060895301757e-08,
      "loss": 0.0013,
      "step": 139950
    },
    {
      "epoch": 25447.272727272728,
      "grad_norm": 0.2526780068874359,
      "learning_rate": 1.3585108972518078e-08,
      "loss": 0.0013,
      "step": 139960
    },
    {
      "epoch": 25449.090909090908,
      "grad_norm": 0.0012016972759738564,
      "learning_rate": 1.3558183391147449e-08,
      "loss": 0.0008,
      "step": 139970
    },
    {
      "epoch": 25450.909090909092,
      "grad_norm": 0.18872350454330444,
      "learning_rate": 1.3531284152647981e-08,
      "loss": 0.001,
      "step": 139980
    },
    {
      "epoch": 25452.727272727272,
      "grad_norm": 0.0006368804024532437,
      "learning_rate": 1.3504411258476511e-08,
      "loss": 0.0013,
      "step": 139990
    },
    {
      "epoch": 25454.545454545456,
      "grad_norm": 0.0004344256885815412,
      "learning_rate": 1.3477564710088096e-08,
      "loss": 0.0009,
      "step": 140000
    },
    {
      "epoch": 25454.545454545456,
      "eval_loss": 5.187554836273193,
      "eval_runtime": 0.9473,
      "eval_samples_per_second": 10.557,
      "eval_steps_per_second": 5.278,
      "step": 140000
    },
    {
      "epoch": 25456.363636363636,
      "grad_norm": 0.20365628600120544,
      "learning_rate": 1.3450744508936684e-08,
      "loss": 0.0015,
      "step": 140010
    },
    {
      "epoch": 25458.18181818182,
      "grad_norm": 0.16588141024112701,
      "learning_rate": 1.3423950656474781e-08,
      "loss": 0.001,
      "step": 140020
    },
    {
      "epoch": 25460.0,
      "grad_norm": 0.0005598374991677701,
      "learning_rate": 1.3397183154153336e-08,
      "loss": 0.0011,
      "step": 140030
    },
    {
      "epoch": 25461.81818181818,
      "grad_norm": 0.0004927747650071979,
      "learning_rate": 1.337044200342191e-08,
      "loss": 0.0012,
      "step": 140040
    },
    {
      "epoch": 25463.636363636364,
      "grad_norm": 0.0006099067977629602,
      "learning_rate": 1.3343727205728627e-08,
      "loss": 0.001,
      "step": 140050
    },
    {
      "epoch": 25465.454545454544,
      "grad_norm": 0.0006202877848409116,
      "learning_rate": 1.3317038762520217e-08,
      "loss": 0.001,
      "step": 140060
    },
    {
      "epoch": 25467.272727272728,
      "grad_norm": 0.2222357839345932,
      "learning_rate": 1.329037667524202e-08,
      "loss": 0.0011,
      "step": 140070
    },
    {
      "epoch": 25469.090909090908,
      "grad_norm": 0.00047161828842945397,
      "learning_rate": 1.3263740945337831e-08,
      "loss": 0.001,
      "step": 140080
    },
    {
      "epoch": 25470.909090909092,
      "grad_norm": 0.0005077791283838451,
      "learning_rate": 1.3237131574250048e-08,
      "loss": 0.001,
      "step": 140090
    },
    {
      "epoch": 25472.727272727272,
      "grad_norm": 0.2705513834953308,
      "learning_rate": 1.3210548563419855e-08,
      "loss": 0.0014,
      "step": 140100
    },
    {
      "epoch": 25474.545454545456,
      "grad_norm": 0.18640780448913574,
      "learning_rate": 1.3183991914286708e-08,
      "loss": 0.0011,
      "step": 140110
    },
    {
      "epoch": 25476.363636363636,
      "grad_norm": 0.17109046876430511,
      "learning_rate": 1.3157461628288791e-08,
      "loss": 0.0013,
      "step": 140120
    },
    {
      "epoch": 25478.18181818182,
      "grad_norm": 0.17585690319538116,
      "learning_rate": 1.3130957706862788e-08,
      "loss": 0.0011,
      "step": 140130
    },
    {
      "epoch": 25480.0,
      "grad_norm": 0.0005871006287634373,
      "learning_rate": 1.3104480151444108e-08,
      "loss": 0.001,
      "step": 140140
    },
    {
      "epoch": 25481.81818181818,
      "grad_norm": 0.17362673580646515,
      "learning_rate": 1.3078028963466492e-08,
      "loss": 0.001,
      "step": 140150
    },
    {
      "epoch": 25483.636363636364,
      "grad_norm": 0.27722442150115967,
      "learning_rate": 1.3051604144362406e-08,
      "loss": 0.001,
      "step": 140160
    },
    {
      "epoch": 25485.454545454544,
      "grad_norm": 0.0011326678795740008,
      "learning_rate": 1.302520569556287e-08,
      "loss": 0.001,
      "step": 140170
    },
    {
      "epoch": 25487.272727272728,
      "grad_norm": 0.0004749833606183529,
      "learning_rate": 1.2998833618497629e-08,
      "loss": 0.001,
      "step": 140180
    },
    {
      "epoch": 25489.090909090908,
      "grad_norm": 0.15367282927036285,
      "learning_rate": 1.2972487914594598e-08,
      "loss": 0.0013,
      "step": 140190
    },
    {
      "epoch": 25490.909090909092,
      "grad_norm": 0.0005672686384059489,
      "learning_rate": 1.2946168585280637e-08,
      "loss": 0.0009,
      "step": 140200
    },
    {
      "epoch": 25492.727272727272,
      "grad_norm": 0.005961843300610781,
      "learning_rate": 1.2919875631981103e-08,
      "loss": 0.0014,
      "step": 140210
    },
    {
      "epoch": 25494.545454545456,
      "grad_norm": 0.2650402784347534,
      "learning_rate": 1.2893609056119747e-08,
      "loss": 0.001,
      "step": 140220
    },
    {
      "epoch": 25496.363636363636,
      "grad_norm": 0.1699354499578476,
      "learning_rate": 1.2867368859119098e-08,
      "loss": 0.001,
      "step": 140230
    },
    {
      "epoch": 25498.18181818182,
      "grad_norm": 0.2657042145729065,
      "learning_rate": 1.2841155042400076e-08,
      "loss": 0.0012,
      "step": 140240
    },
    {
      "epoch": 25500.0,
      "grad_norm": 0.0023231685627251863,
      "learning_rate": 1.2814967607382432e-08,
      "loss": 0.0009,
      "step": 140250
    },
    {
      "epoch": 25501.81818181818,
      "grad_norm": 0.0005626908387057483,
      "learning_rate": 1.2788806555484145e-08,
      "loss": 0.0012,
      "step": 140260
    },
    {
      "epoch": 25503.636363636364,
      "grad_norm": 0.0008006105199456215,
      "learning_rate": 1.2762671888122023e-08,
      "loss": 0.0009,
      "step": 140270
    },
    {
      "epoch": 25505.454545454544,
      "grad_norm": 0.21753472089767456,
      "learning_rate": 1.2736563606711381e-08,
      "loss": 0.001,
      "step": 140280
    },
    {
      "epoch": 25507.272727272728,
      "grad_norm": 0.22024743258953094,
      "learning_rate": 1.2710481712666144e-08,
      "loss": 0.0012,
      "step": 140290
    },
    {
      "epoch": 25509.090909090908,
      "grad_norm": 0.000612941337749362,
      "learning_rate": 1.268442620739868e-08,
      "loss": 0.001,
      "step": 140300
    },
    {
      "epoch": 25510.909090909092,
      "grad_norm": 0.18484428524971008,
      "learning_rate": 1.2658397092320027e-08,
      "loss": 0.0012,
      "step": 140310
    },
    {
      "epoch": 25512.727272727272,
      "grad_norm": 0.0005538160912692547,
      "learning_rate": 1.2632394368839728e-08,
      "loss": 0.001,
      "step": 140320
    },
    {
      "epoch": 25514.545454545456,
      "grad_norm": 0.1724114865064621,
      "learning_rate": 1.260641803836604e-08,
      "loss": 0.0011,
      "step": 140330
    },
    {
      "epoch": 25516.363636363636,
      "grad_norm": 0.0005935752997174859,
      "learning_rate": 1.258046810230562e-08,
      "loss": 0.0009,
      "step": 140340
    },
    {
      "epoch": 25518.18181818182,
      "grad_norm": 0.0012677335180342197,
      "learning_rate": 1.255454456206373e-08,
      "loss": 0.001,
      "step": 140350
    },
    {
      "epoch": 25520.0,
      "grad_norm": 0.16930539906024933,
      "learning_rate": 1.2528647419044246e-08,
      "loss": 0.0013,
      "step": 140360
    },
    {
      "epoch": 25521.81818181818,
      "grad_norm": 0.27125829458236694,
      "learning_rate": 1.2502776674649772e-08,
      "loss": 0.001,
      "step": 140370
    },
    {
      "epoch": 25523.636363636364,
      "grad_norm": 0.20842911303043365,
      "learning_rate": 1.2476932330281131e-08,
      "loss": 0.0012,
      "step": 140380
    },
    {
      "epoch": 25525.454545454544,
      "grad_norm": 0.16680996119976044,
      "learning_rate": 1.245111438733798e-08,
      "loss": 0.0008,
      "step": 140390
    },
    {
      "epoch": 25527.272727272728,
      "grad_norm": 0.2644786834716797,
      "learning_rate": 1.2425322847218367e-08,
      "loss": 0.0015,
      "step": 140400
    },
    {
      "epoch": 25529.090909090908,
      "grad_norm": 0.16786260902881622,
      "learning_rate": 1.2399557711319176e-08,
      "loss": 0.0009,
      "step": 140410
    },
    {
      "epoch": 25530.909090909092,
      "grad_norm": 0.21947836875915527,
      "learning_rate": 1.2373818981035621e-08,
      "loss": 0.0008,
      "step": 140420
    },
    {
      "epoch": 25532.727272727272,
      "grad_norm": 0.23559878766536713,
      "learning_rate": 1.2348106657761537e-08,
      "loss": 0.0015,
      "step": 140430
    },
    {
      "epoch": 25534.545454545456,
      "grad_norm": 0.0004940730286762118,
      "learning_rate": 1.2322420742889417e-08,
      "loss": 0.001,
      "step": 140440
    },
    {
      "epoch": 25536.363636363636,
      "grad_norm": 0.0007201479747891426,
      "learning_rate": 1.2296761237810205e-08,
      "loss": 0.001,
      "step": 140450
    },
    {
      "epoch": 25538.18181818182,
      "grad_norm": 0.0011634837137535214,
      "learning_rate": 1.2271128143913456e-08,
      "loss": 0.0009,
      "step": 140460
    },
    {
      "epoch": 25540.0,
      "grad_norm": 0.0003766900335904211,
      "learning_rate": 1.2245521462587338e-08,
      "loss": 0.0012,
      "step": 140470
    },
    {
      "epoch": 25541.81818181818,
      "grad_norm": 0.1666194200515747,
      "learning_rate": 1.221994119521863e-08,
      "loss": 0.001,
      "step": 140480
    },
    {
      "epoch": 25543.636363636364,
      "grad_norm": 0.20788484811782837,
      "learning_rate": 1.2194387343192502e-08,
      "loss": 0.001,
      "step": 140490
    },
    {
      "epoch": 25545.454545454544,
      "grad_norm": 0.16817474365234375,
      "learning_rate": 1.2168859907892902e-08,
      "loss": 0.0011,
      "step": 140500
    },
    {
      "epoch": 25545.454545454544,
      "eval_loss": 5.157744407653809,
      "eval_runtime": 0.9482,
      "eval_samples_per_second": 10.547,
      "eval_steps_per_second": 5.273,
      "step": 140500
    },
    {
      "epoch": 25547.272727272728,
      "grad_norm": 0.00047650266787968576,
      "learning_rate": 1.2143358890702116e-08,
      "loss": 0.0009,
      "step": 140510
    },
    {
      "epoch": 25549.090909090908,
      "grad_norm": 0.00047703084419481456,
      "learning_rate": 1.2117884293001257e-08,
      "loss": 0.0012,
      "step": 140520
    },
    {
      "epoch": 25550.909090909092,
      "grad_norm": 0.1739957481622696,
      "learning_rate": 1.2092436116169836e-08,
      "loss": 0.0011,
      "step": 140530
    },
    {
      "epoch": 25552.727272727272,
      "grad_norm": 0.00045990649960003793,
      "learning_rate": 1.2067014361585914e-08,
      "loss": 0.001,
      "step": 140540
    },
    {
      "epoch": 25554.545454545456,
      "grad_norm": 0.0005749681149609387,
      "learning_rate": 1.2041619030626282e-08,
      "loss": 0.0009,
      "step": 140550
    },
    {
      "epoch": 25556.363636363636,
      "grad_norm": 0.2860974967479706,
      "learning_rate": 1.2016250124666227e-08,
      "loss": 0.0015,
      "step": 140560
    },
    {
      "epoch": 25558.18181818182,
      "grad_norm": 0.045081544667482376,
      "learning_rate": 1.1990907645079484e-08,
      "loss": 0.001,
      "step": 140570
    },
    {
      "epoch": 25560.0,
      "grad_norm": 0.0003544565988704562,
      "learning_rate": 1.1965591593238511e-08,
      "loss": 0.0009,
      "step": 140580
    },
    {
      "epoch": 25561.81818181818,
      "grad_norm": 0.0007142435642890632,
      "learning_rate": 1.194030197051421e-08,
      "loss": 0.0012,
      "step": 140590
    },
    {
      "epoch": 25563.636363636364,
      "grad_norm": 0.0005246863001957536,
      "learning_rate": 1.1915038778276209e-08,
      "loss": 0.001,
      "step": 140600
    },
    {
      "epoch": 25565.454545454544,
      "grad_norm": 0.00042597297579050064,
      "learning_rate": 1.1889802017892637e-08,
      "loss": 0.001,
      "step": 140610
    },
    {
      "epoch": 25567.272727272728,
      "grad_norm": 0.000617205398157239,
      "learning_rate": 1.1864591690730008e-08,
      "loss": 0.0009,
      "step": 140620
    },
    {
      "epoch": 25569.090909090908,
      "grad_norm": 0.0006717416108585894,
      "learning_rate": 1.1839407798153733e-08,
      "loss": 0.0012,
      "step": 140630
    },
    {
      "epoch": 25570.909090909092,
      "grad_norm": 0.000823870359454304,
      "learning_rate": 1.181425034152761e-08,
      "loss": 0.0012,
      "step": 140640
    },
    {
      "epoch": 25572.727272727272,
      "grad_norm": 0.00064429035410285,
      "learning_rate": 1.1789119322213881e-08,
      "loss": 0.001,
      "step": 140650
    },
    {
      "epoch": 25574.545454545456,
      "grad_norm": 0.006378742400556803,
      "learning_rate": 1.176401474157368e-08,
      "loss": 0.0011,
      "step": 140660
    },
    {
      "epoch": 25576.363636363636,
      "grad_norm": 0.1706928163766861,
      "learning_rate": 1.1738936600966365e-08,
      "loss": 0.001,
      "step": 140670
    },
    {
      "epoch": 25578.18181818182,
      "grad_norm": 0.17714248597621918,
      "learning_rate": 1.1713884901750181e-08,
      "loss": 0.0012,
      "step": 140680
    },
    {
      "epoch": 25580.0,
      "grad_norm": 0.0005673851119354367,
      "learning_rate": 1.1688859645281656e-08,
      "loss": 0.001,
      "step": 140690
    },
    {
      "epoch": 25581.81818181818,
      "grad_norm": 0.011754178442060947,
      "learning_rate": 1.166386083291604e-08,
      "loss": 0.001,
      "step": 140700
    },
    {
      "epoch": 25583.636363636364,
      "grad_norm": 0.0013948180712759495,
      "learning_rate": 1.1638888466007191e-08,
      "loss": 0.0011,
      "step": 140710
    },
    {
      "epoch": 25585.454545454544,
      "grad_norm": 0.2780974209308624,
      "learning_rate": 1.1613942545907418e-08,
      "loss": 0.0012,
      "step": 140720
    },
    {
      "epoch": 25587.272727272728,
      "grad_norm": 0.16670379042625427,
      "learning_rate": 1.1589023073967585e-08,
      "loss": 0.0008,
      "step": 140730
    },
    {
      "epoch": 25589.090909090908,
      "grad_norm": 0.21865512430667877,
      "learning_rate": 1.1564130051537224e-08,
      "loss": 0.0012,
      "step": 140740
    },
    {
      "epoch": 25590.909090909092,
      "grad_norm": 0.20667695999145508,
      "learning_rate": 1.1539263479964534e-08,
      "loss": 0.001,
      "step": 140750
    },
    {
      "epoch": 25592.727272727272,
      "grad_norm": 0.000611695519182831,
      "learning_rate": 1.1514423360595937e-08,
      "loss": 0.0009,
      "step": 140760
    },
    {
      "epoch": 25594.545454545456,
      "grad_norm": 0.0012184371007606387,
      "learning_rate": 1.1489609694776803e-08,
      "loss": 0.001,
      "step": 140770
    },
    {
      "epoch": 25596.363636363636,
      "grad_norm": 0.0005352808511815965,
      "learning_rate": 1.1464822483850723e-08,
      "loss": 0.0012,
      "step": 140780
    },
    {
      "epoch": 25598.18181818182,
      "grad_norm": 0.0007267581531777978,
      "learning_rate": 1.1440061729160233e-08,
      "loss": 0.001,
      "step": 140790
    },
    {
      "epoch": 25600.0,
      "grad_norm": 0.25571468472480774,
      "learning_rate": 1.141532743204604e-08,
      "loss": 0.0012,
      "step": 140800
    },
    {
      "epoch": 25601.81818181818,
      "grad_norm": 0.000988716259598732,
      "learning_rate": 1.1390619593847684e-08,
      "loss": 0.0011,
      "step": 140810
    },
    {
      "epoch": 25603.636363636364,
      "grad_norm": 0.005214376375079155,
      "learning_rate": 1.136593821590326e-08,
      "loss": 0.0012,
      "step": 140820
    },
    {
      "epoch": 25605.454545454544,
      "grad_norm": 0.0009850823553279042,
      "learning_rate": 1.1341283299549365e-08,
      "loss": 0.0012,
      "step": 140830
    },
    {
      "epoch": 25607.272727272728,
      "grad_norm": 0.2237502783536911,
      "learning_rate": 1.1316654846120987e-08,
      "loss": 0.0009,
      "step": 140840
    },
    {
      "epoch": 25609.090909090908,
      "grad_norm": 0.17150452733039856,
      "learning_rate": 1.1292052856952062e-08,
      "loss": 0.0012,
      "step": 140850
    },
    {
      "epoch": 25610.909090909092,
      "grad_norm": 0.0007860315963625908,
      "learning_rate": 1.1267477333374853e-08,
      "loss": 0.0012,
      "step": 140860
    },
    {
      "epoch": 25612.727272727272,
      "grad_norm": 0.1667720377445221,
      "learning_rate": 1.1242928276720132e-08,
      "loss": 0.0009,
      "step": 140870
    },
    {
      "epoch": 25614.545454545456,
      "grad_norm": 0.0010081252548843622,
      "learning_rate": 1.1218405688317446e-08,
      "loss": 0.001,
      "step": 140880
    },
    {
      "epoch": 25616.363636363636,
      "grad_norm": 0.0009381148847751319,
      "learning_rate": 1.1193909569494674e-08,
      "loss": 0.0011,
      "step": 140890
    },
    {
      "epoch": 25618.18181818182,
      "grad_norm": 0.0006486471393145621,
      "learning_rate": 1.1169439921578482e-08,
      "loss": 0.0012,
      "step": 140900
    },
    {
      "epoch": 25620.0,
      "grad_norm": 0.0002780351205728948,
      "learning_rate": 1.114499674589403e-08,
      "loss": 0.0012,
      "step": 140910
    },
    {
      "epoch": 25621.81818181818,
      "grad_norm": 0.21941885352134705,
      "learning_rate": 1.1120580043764926e-08,
      "loss": 0.001,
      "step": 140920
    },
    {
      "epoch": 25623.636363636364,
      "grad_norm": 0.16876941919326782,
      "learning_rate": 1.1096189816513501e-08,
      "loss": 0.0012,
      "step": 140930
    },
    {
      "epoch": 25625.454545454544,
      "grad_norm": 0.0009816422825679183,
      "learning_rate": 1.1071826065460588e-08,
      "loss": 0.001,
      "step": 140940
    },
    {
      "epoch": 25627.272727272728,
      "grad_norm": 0.20480461418628693,
      "learning_rate": 1.104748879192552e-08,
      "loss": 0.0014,
      "step": 140950
    },
    {
      "epoch": 25629.090909090908,
      "grad_norm": 0.1628372073173523,
      "learning_rate": 1.1023177997226296e-08,
      "loss": 0.0009,
      "step": 140960
    },
    {
      "epoch": 25630.909090909092,
      "grad_norm": 0.012125478126108646,
      "learning_rate": 1.0998893682679477e-08,
      "loss": 0.001,
      "step": 140970
    },
    {
      "epoch": 25632.727272727272,
      "grad_norm": 0.0004343391046859324,
      "learning_rate": 1.0974635849600179e-08,
      "loss": 0.0007,
      "step": 140980
    },
    {
      "epoch": 25634.545454545456,
      "grad_norm": 0.28458401560783386,
      "learning_rate": 1.0950404499302012e-08,
      "loss": 0.0015,
      "step": 140990
    },
    {
      "epoch": 25636.363636363636,
      "grad_norm": 0.000514446699526161,
      "learning_rate": 1.0926199633097154e-08,
      "loss": 0.0007,
      "step": 141000
    },
    {
      "epoch": 25636.363636363636,
      "eval_loss": 5.225440502166748,
      "eval_runtime": 0.9514,
      "eval_samples_per_second": 10.511,
      "eval_steps_per_second": 5.255,
      "step": 141000
    },
    {
      "epoch": 25638.18181818182,
      "grad_norm": 0.0003693415492307395,
      "learning_rate": 1.0902021252296556e-08,
      "loss": 0.0012,
      "step": 141010
    },
    {
      "epoch": 25640.0,
      "grad_norm": 0.17812231183052063,
      "learning_rate": 1.0877869358209502e-08,
      "loss": 0.0012,
      "step": 141020
    },
    {
      "epoch": 25641.81818181818,
      "grad_norm": 0.17643725872039795,
      "learning_rate": 1.0853743952143835e-08,
      "loss": 0.0012,
      "step": 141030
    },
    {
      "epoch": 25643.636363636364,
      "grad_norm": 0.018177952617406845,
      "learning_rate": 1.0829645035406177e-08,
      "loss": 0.0011,
      "step": 141040
    },
    {
      "epoch": 25645.454545454544,
      "grad_norm": 0.0003124831710010767,
      "learning_rate": 1.0805572609301539e-08,
      "loss": 0.0009,
      "step": 141050
    },
    {
      "epoch": 25647.272727272728,
      "grad_norm": 0.20760251581668854,
      "learning_rate": 1.078152667513349e-08,
      "loss": 0.0012,
      "step": 141060
    },
    {
      "epoch": 25649.090909090908,
      "grad_norm": 0.0007094934699125588,
      "learning_rate": 1.0757507234204322e-08,
      "loss": 0.001,
      "step": 141070
    },
    {
      "epoch": 25650.909090909092,
      "grad_norm": 0.21084608137607574,
      "learning_rate": 1.073351428781466e-08,
      "loss": 0.0012,
      "step": 141080
    },
    {
      "epoch": 25652.727272727272,
      "grad_norm": 0.0009586967644281685,
      "learning_rate": 1.0709547837263967e-08,
      "loss": 0.0012,
      "step": 141090
    },
    {
      "epoch": 25654.545454545456,
      "grad_norm": 0.21276843547821045,
      "learning_rate": 1.0685607883850034e-08,
      "loss": 0.0009,
      "step": 141100
    },
    {
      "epoch": 25656.363636363636,
      "grad_norm": 0.2222660481929779,
      "learning_rate": 1.0661694428869272e-08,
      "loss": 0.0012,
      "step": 141110
    },
    {
      "epoch": 25658.18181818182,
      "grad_norm": 0.15553604066371918,
      "learning_rate": 1.0637807473616812e-08,
      "loss": 0.0012,
      "step": 141120
    },
    {
      "epoch": 25660.0,
      "grad_norm": 0.0006311325705610216,
      "learning_rate": 1.0613947019386226e-08,
      "loss": 0.001,
      "step": 141130
    },
    {
      "epoch": 25661.81818181818,
      "grad_norm": 0.20857466757297516,
      "learning_rate": 1.0590113067469487e-08,
      "loss": 0.001,
      "step": 141140
    },
    {
      "epoch": 25663.636363636364,
      "grad_norm": 0.0008375360630452633,
      "learning_rate": 1.0566305619157501e-08,
      "loss": 0.001,
      "step": 141150
    },
    {
      "epoch": 25665.454545454544,
      "grad_norm": 0.001662657829001546,
      "learning_rate": 1.0542524675739406e-08,
      "loss": 0.0009,
      "step": 141160
    },
    {
      "epoch": 25667.272727272728,
      "grad_norm": 0.037208013236522675,
      "learning_rate": 1.0518770238503172e-08,
      "loss": 0.0014,
      "step": 141170
    },
    {
      "epoch": 25669.090909090908,
      "grad_norm": 0.20806443691253662,
      "learning_rate": 1.0495042308735103e-08,
      "loss": 0.001,
      "step": 141180
    },
    {
      "epoch": 25670.909090909092,
      "grad_norm": 0.0009152449783869088,
      "learning_rate": 1.047134088772017e-08,
      "loss": 0.0008,
      "step": 141190
    },
    {
      "epoch": 25672.727272727272,
      "grad_norm": 0.17037636041641235,
      "learning_rate": 1.0447665976741959e-08,
      "loss": 0.0013,
      "step": 141200
    },
    {
      "epoch": 25674.545454545456,
      "grad_norm": 0.21846142411231995,
      "learning_rate": 1.04240175770825e-08,
      "loss": 0.0011,
      "step": 141210
    },
    {
      "epoch": 25676.363636363636,
      "grad_norm": 0.0004138649965170771,
      "learning_rate": 1.040039569002249e-08,
      "loss": 0.0007,
      "step": 141220
    },
    {
      "epoch": 25678.18181818182,
      "grad_norm": 0.16439054906368256,
      "learning_rate": 1.0376800316841184e-08,
      "loss": 0.0013,
      "step": 141230
    },
    {
      "epoch": 25680.0,
      "grad_norm": 0.2099151313304901,
      "learning_rate": 1.0353231458816336e-08,
      "loss": 0.001,
      "step": 141240
    },
    {
      "epoch": 25681.81818181818,
      "grad_norm": 0.0005890410393476486,
      "learning_rate": 1.032968911722426e-08,
      "loss": 0.0012,
      "step": 141250
    },
    {
      "epoch": 25683.636363636364,
      "grad_norm": 0.0017613592790439725,
      "learning_rate": 1.0306173293339938e-08,
      "loss": 0.0008,
      "step": 141260
    },
    {
      "epoch": 25685.454545454544,
      "grad_norm": 0.21915219724178314,
      "learning_rate": 1.0282683988436791e-08,
      "loss": 0.0013,
      "step": 141270
    },
    {
      "epoch": 25687.272727272728,
      "grad_norm": 0.0008583323797211051,
      "learning_rate": 1.0259221203786972e-08,
      "loss": 0.0009,
      "step": 141280
    },
    {
      "epoch": 25689.090909090908,
      "grad_norm": 0.0005602709716185927,
      "learning_rate": 1.0235784940660964e-08,
      "loss": 0.0012,
      "step": 141290
    },
    {
      "epoch": 25690.909090909092,
      "grad_norm": 0.0004909730050712824,
      "learning_rate": 1.0212375200327972e-08,
      "loss": 0.0012,
      "step": 141300
    },
    {
      "epoch": 25692.727272727272,
      "grad_norm": 0.0007969888392835855,
      "learning_rate": 1.0188991984055761e-08,
      "loss": 0.001,
      "step": 141310
    },
    {
      "epoch": 25694.545454545456,
      "grad_norm": 0.2229641079902649,
      "learning_rate": 1.0165635293110652e-08,
      "loss": 0.0013,
      "step": 141320
    },
    {
      "epoch": 25696.363636363636,
      "grad_norm": 0.2849758267402649,
      "learning_rate": 1.0142305128757467e-08,
      "loss": 0.0009,
      "step": 141330
    },
    {
      "epoch": 25698.18181818182,
      "grad_norm": 0.16526566445827484,
      "learning_rate": 1.0119001492259638e-08,
      "loss": 0.001,
      "step": 141340
    },
    {
      "epoch": 25700.0,
      "grad_norm": 0.21163134276866913,
      "learning_rate": 1.0095724384879156e-08,
      "loss": 0.001,
      "step": 141350
    },
    {
      "epoch": 25701.81818181818,
      "grad_norm": 0.0006527980440296233,
      "learning_rate": 1.0072473807876569e-08,
      "loss": 0.001,
      "step": 141360
    },
    {
      "epoch": 25703.636363636364,
      "grad_norm": 0.17708879709243774,
      "learning_rate": 1.0049249762511036e-08,
      "loss": 0.001,
      "step": 141370
    },
    {
      "epoch": 25705.454545454544,
      "grad_norm": 0.27551743388175964,
      "learning_rate": 1.0026052250040162e-08,
      "loss": 0.0013,
      "step": 141380
    },
    {
      "epoch": 25707.272727272728,
      "grad_norm": 0.2263202667236328,
      "learning_rate": 1.000288127172022e-08,
      "loss": 0.0009,
      "step": 141390
    },
    {
      "epoch": 25709.090909090908,
      "grad_norm": 0.21084290742874146,
      "learning_rate": 9.979736828806096e-09,
      "loss": 0.0011,
      "step": 141400
    },
    {
      "epoch": 25710.909090909092,
      "grad_norm": 0.0003994608123321086,
      "learning_rate": 9.956618922551009e-09,
      "loss": 0.0009,
      "step": 141410
    },
    {
      "epoch": 25712.727272727272,
      "grad_norm": 0.1672114133834839,
      "learning_rate": 9.933527554207011e-09,
      "loss": 0.0013,
      "step": 141420
    },
    {
      "epoch": 25714.545454545456,
      "grad_norm": 0.00046484233462251723,
      "learning_rate": 9.910462725024603e-09,
      "loss": 0.0008,
      "step": 141430
    },
    {
      "epoch": 25716.363636363636,
      "grad_norm": 0.2166328728199005,
      "learning_rate": 9.887424436252733e-09,
      "loss": 0.0014,
      "step": 141440
    },
    {
      "epoch": 25718.18181818182,
      "grad_norm": 0.19808179140090942,
      "learning_rate": 9.864412689139123e-09,
      "loss": 0.0009,
      "step": 141450
    },
    {
      "epoch": 25720.0,
      "grad_norm": 0.0013512991135939956,
      "learning_rate": 9.841427484929832e-09,
      "loss": 0.001,
      "step": 141460
    },
    {
      "epoch": 25721.81818181818,
      "grad_norm": 0.0004686984757427126,
      "learning_rate": 9.818468824869808e-09,
      "loss": 0.001,
      "step": 141470
    },
    {
      "epoch": 25723.636363636364,
      "grad_norm": 0.17206832766532898,
      "learning_rate": 9.795536710202168e-09,
      "loss": 0.0012,
      "step": 141480
    },
    {
      "epoch": 25725.454545454544,
      "grad_norm": 0.21919797360897064,
      "learning_rate": 9.772631142168863e-09,
      "loss": 0.0012,
      "step": 141490
    },
    {
      "epoch": 25727.272727272728,
      "grad_norm": 0.0007565543055534363,
      "learning_rate": 9.749752122010346e-09,
      "loss": 0.0009,
      "step": 141500
    },
    {
      "epoch": 25727.272727272728,
      "eval_loss": 5.278851509094238,
      "eval_runtime": 0.9501,
      "eval_samples_per_second": 10.526,
      "eval_steps_per_second": 5.263,
      "step": 141500
    },
    {
      "epoch": 25729.090909090908,
      "grad_norm": 0.272723913192749,
      "learning_rate": 9.726899650965626e-09,
      "loss": 0.0013,
      "step": 141510
    },
    {
      "epoch": 25730.909090909092,
      "grad_norm": 0.1972256749868393,
      "learning_rate": 9.7040737302721e-09,
      "loss": 0.0009,
      "step": 141520
    },
    {
      "epoch": 25732.727272727272,
      "grad_norm": 0.0008924877620302141,
      "learning_rate": 9.681274361166114e-09,
      "loss": 0.001,
      "step": 141530
    },
    {
      "epoch": 25734.545454545456,
      "grad_norm": 0.17470163106918335,
      "learning_rate": 9.65850154488218e-09,
      "loss": 0.0014,
      "step": 141540
    },
    {
      "epoch": 25736.363636363636,
      "grad_norm": 0.0005530962371267378,
      "learning_rate": 9.6357552826537e-09,
      "loss": 0.001,
      "step": 141550
    },
    {
      "epoch": 25738.18181818182,
      "grad_norm": 0.001253119669854641,
      "learning_rate": 9.613035575712303e-09,
      "loss": 0.001,
      "step": 141560
    },
    {
      "epoch": 25740.0,
      "grad_norm": 0.0010212830966338515,
      "learning_rate": 9.590342425288445e-09,
      "loss": 0.0012,
      "step": 141570
    },
    {
      "epoch": 25741.81818181818,
      "grad_norm": 0.1724303513765335,
      "learning_rate": 9.567675832611089e-09,
      "loss": 0.0009,
      "step": 141580
    },
    {
      "epoch": 25743.636363636364,
      "grad_norm": 0.002773463726043701,
      "learning_rate": 9.545035798907642e-09,
      "loss": 0.001,
      "step": 141590
    },
    {
      "epoch": 25745.454545454544,
      "grad_norm": 0.0003999621549155563,
      "learning_rate": 9.522422325404233e-09,
      "loss": 0.0011,
      "step": 141600
    },
    {
      "epoch": 25747.272727272728,
      "grad_norm": 0.0013841289328411222,
      "learning_rate": 9.49983541332544e-09,
      "loss": 0.0014,
      "step": 141610
    },
    {
      "epoch": 25749.090909090908,
      "grad_norm": 0.00047820512554608285,
      "learning_rate": 9.477275063894452e-09,
      "loss": 0.0009,
      "step": 141620
    },
    {
      "epoch": 25750.909090909092,
      "grad_norm": 0.16949336230754852,
      "learning_rate": 9.454741278333011e-09,
      "loss": 0.001,
      "step": 141630
    },
    {
      "epoch": 25752.727272727272,
      "grad_norm": 0.1733732372522354,
      "learning_rate": 9.432234057861366e-09,
      "loss": 0.0013,
      "step": 141640
    },
    {
      "epoch": 25754.545454545456,
      "grad_norm": 0.2084827721118927,
      "learning_rate": 9.409753403698373e-09,
      "loss": 0.0009,
      "step": 141650
    },
    {
      "epoch": 25756.363636363636,
      "grad_norm": 0.27161771059036255,
      "learning_rate": 9.387299317061614e-09,
      "loss": 0.0013,
      "step": 141660
    },
    {
      "epoch": 25758.18181818182,
      "grad_norm": 0.16511625051498413,
      "learning_rate": 9.364871799166895e-09,
      "loss": 0.001,
      "step": 141670
    },
    {
      "epoch": 25760.0,
      "grad_norm": 0.17824819684028625,
      "learning_rate": 9.3424708512288e-09,
      "loss": 0.001,
      "step": 141680
    },
    {
      "epoch": 25761.81818181818,
      "grad_norm": 0.20822566747665405,
      "learning_rate": 9.320096474460527e-09,
      "loss": 0.0011,
      "step": 141690
    },
    {
      "epoch": 25763.636363636364,
      "grad_norm": 0.2796283960342407,
      "learning_rate": 9.297748670073657e-09,
      "loss": 0.001,
      "step": 141700
    },
    {
      "epoch": 25765.454545454544,
      "grad_norm": 0.1779787242412567,
      "learning_rate": 9.275427439278338e-09,
      "loss": 0.0009,
      "step": 141710
    },
    {
      "epoch": 25767.272727272728,
      "grad_norm": 0.044134702533483505,
      "learning_rate": 9.253132783283546e-09,
      "loss": 0.0013,
      "step": 141720
    },
    {
      "epoch": 25769.090909090908,
      "grad_norm": 0.17698699235916138,
      "learning_rate": 9.230864703296536e-09,
      "loss": 0.001,
      "step": 141730
    },
    {
      "epoch": 25770.909090909092,
      "grad_norm": 0.0005610849475488067,
      "learning_rate": 9.208623200523235e-09,
      "loss": 0.001,
      "step": 141740
    },
    {
      "epoch": 25772.727272727272,
      "grad_norm": 0.0004187609301880002,
      "learning_rate": 9.186408276168011e-09,
      "loss": 0.0012,
      "step": 141750
    },
    {
      "epoch": 25774.545454545456,
      "grad_norm": 0.0007071950240060687,
      "learning_rate": 9.164219931434014e-09,
      "loss": 0.0008,
      "step": 141760
    },
    {
      "epoch": 25776.363636363636,
      "grad_norm": 0.2597016990184784,
      "learning_rate": 9.142058167522893e-09,
      "loss": 0.0012,
      "step": 141770
    },
    {
      "epoch": 25778.18181818182,
      "grad_norm": 0.0005340680363588035,
      "learning_rate": 9.11992298563463e-09,
      "loss": 0.0009,
      "step": 141780
    },
    {
      "epoch": 25780.0,
      "grad_norm": 0.2654477655887604,
      "learning_rate": 9.097814386968051e-09,
      "loss": 0.0012,
      "step": 141790
    },
    {
      "epoch": 25781.81818181818,
      "grad_norm": 0.0007552969618700445,
      "learning_rate": 9.075732372720413e-09,
      "loss": 0.0012,
      "step": 141800
    },
    {
      "epoch": 25783.636363636364,
      "grad_norm": 0.2215658575296402,
      "learning_rate": 9.053676944087541e-09,
      "loss": 0.001,
      "step": 141810
    },
    {
      "epoch": 25785.454545454544,
      "grad_norm": 0.0006067898357287049,
      "learning_rate": 9.031648102263811e-09,
      "loss": 0.0012,
      "step": 141820
    },
    {
      "epoch": 25787.272727272728,
      "grad_norm": 0.1691068857908249,
      "learning_rate": 9.00964584844216e-09,
      "loss": 0.0009,
      "step": 141830
    },
    {
      "epoch": 25789.090909090908,
      "grad_norm": 0.1740974634885788,
      "learning_rate": 8.987670183814133e-09,
      "loss": 0.0011,
      "step": 141840
    },
    {
      "epoch": 25790.909090909092,
      "grad_norm": 0.2042398601770401,
      "learning_rate": 8.965721109569836e-09,
      "loss": 0.0012,
      "step": 141850
    },
    {
      "epoch": 25792.727272727272,
      "grad_norm": 0.27054107189178467,
      "learning_rate": 8.94379862689787e-09,
      "loss": 0.0012,
      "step": 141860
    },
    {
      "epoch": 25794.545454545456,
      "grad_norm": 0.0005939672118984163,
      "learning_rate": 8.921902736985398e-09,
      "loss": 0.0009,
      "step": 141870
    },
    {
      "epoch": 25796.363636363636,
      "grad_norm": 0.22543765604496002,
      "learning_rate": 8.900033441018306e-09,
      "loss": 0.0012,
      "step": 141880
    },
    {
      "epoch": 25798.18181818182,
      "grad_norm": 0.0008175395778380334,
      "learning_rate": 8.878190740180758e-09,
      "loss": 0.0009,
      "step": 141890
    },
    {
      "epoch": 25800.0,
      "grad_norm": 0.00042220501927658916,
      "learning_rate": 8.856374635655695e-09,
      "loss": 0.0012,
      "step": 141900
    },
    {
      "epoch": 25801.81818181818,
      "grad_norm": 0.21270835399627686,
      "learning_rate": 8.834585128624505e-09,
      "loss": 0.0012,
      "step": 141910
    },
    {
      "epoch": 25803.636363636364,
      "grad_norm": 0.00036736976471729577,
      "learning_rate": 8.812822220267191e-09,
      "loss": 0.0009,
      "step": 141920
    },
    {
      "epoch": 25805.454545454544,
      "grad_norm": 0.2712475061416626,
      "learning_rate": 8.791085911762475e-09,
      "loss": 0.0013,
      "step": 141930
    },
    {
      "epoch": 25807.272727272728,
      "grad_norm": 0.000449535611551255,
      "learning_rate": 8.769376204287193e-09,
      "loss": 0.001,
      "step": 141940
    },
    {
      "epoch": 25809.090909090908,
      "grad_norm": 0.0005661236355081201,
      "learning_rate": 8.747693099017129e-09,
      "loss": 0.0011,
      "step": 141950
    },
    {
      "epoch": 25810.909090909092,
      "grad_norm": 0.0005554581293836236,
      "learning_rate": 8.726036597126618e-09,
      "loss": 0.0012,
      "step": 141960
    },
    {
      "epoch": 25812.727272727272,
      "grad_norm": 0.0004438877513166517,
      "learning_rate": 8.704406699788336e-09,
      "loss": 0.0012,
      "step": 141970
    },
    {
      "epoch": 25814.545454545456,
      "grad_norm": 0.22333191335201263,
      "learning_rate": 8.682803408173679e-09,
      "loss": 0.0009,
      "step": 141980
    },
    {
      "epoch": 25816.363636363636,
      "grad_norm": 0.0004930823342874646,
      "learning_rate": 8.66122672345254e-09,
      "loss": 0.001,
      "step": 141990
    },
    {
      "epoch": 25818.18181818182,
      "grad_norm": 0.000845341884996742,
      "learning_rate": 8.639676646793382e-09,
      "loss": 0.0012,
      "step": 142000
    },
    {
      "epoch": 25818.18181818182,
      "eval_loss": 5.240994930267334,
      "eval_runtime": 0.9464,
      "eval_samples_per_second": 10.566,
      "eval_steps_per_second": 5.283,
      "step": 142000
    },
    {
      "epoch": 25820.0,
      "grad_norm": 0.2931343615055084,
      "learning_rate": 8.618153179363264e-09,
      "loss": 0.0012,
      "step": 142010
    },
    {
      "epoch": 25821.81818181818,
      "grad_norm": 0.17399930953979492,
      "learning_rate": 8.596656322327645e-09,
      "loss": 0.0012,
      "step": 142020
    },
    {
      "epoch": 25823.636363636364,
      "grad_norm": 0.22136624157428741,
      "learning_rate": 8.575186076850871e-09,
      "loss": 0.0012,
      "step": 142030
    },
    {
      "epoch": 25825.454545454544,
      "grad_norm": 0.0004465831443667412,
      "learning_rate": 8.553742444095568e-09,
      "loss": 0.0009,
      "step": 142040
    },
    {
      "epoch": 25827.272727272728,
      "grad_norm": 0.0006200075731612742,
      "learning_rate": 8.53232542522292e-09,
      "loss": 0.001,
      "step": 142050
    },
    {
      "epoch": 25829.090909090908,
      "grad_norm": 0.0005547920009121299,
      "learning_rate": 8.510935021392773e-09,
      "loss": 0.001,
      "step": 142060
    },
    {
      "epoch": 25830.909090909092,
      "grad_norm": 0.21992306411266327,
      "learning_rate": 8.489571233763593e-09,
      "loss": 0.0012,
      "step": 142070
    },
    {
      "epoch": 25832.727272727272,
      "grad_norm": 0.0008024717681109905,
      "learning_rate": 8.468234063492285e-09,
      "loss": 0.0012,
      "step": 142080
    },
    {
      "epoch": 25834.545454545456,
      "grad_norm": 0.20505349338054657,
      "learning_rate": 8.446923511734316e-09,
      "loss": 0.001,
      "step": 142090
    },
    {
      "epoch": 25836.363636363636,
      "grad_norm": 0.0009649557759985328,
      "learning_rate": 8.425639579643761e-09,
      "loss": 0.0009,
      "step": 142100
    },
    {
      "epoch": 25838.18181818182,
      "grad_norm": 0.20228590071201324,
      "learning_rate": 8.404382268373144e-09,
      "loss": 0.0013,
      "step": 142110
    },
    {
      "epoch": 25840.0,
      "grad_norm": 0.0006226556724868715,
      "learning_rate": 8.383151579073877e-09,
      "loss": 0.001,
      "step": 142120
    },
    {
      "epoch": 25841.81818181818,
      "grad_norm": 0.000562087690923363,
      "learning_rate": 8.361947512895429e-09,
      "loss": 0.0012,
      "step": 142130
    },
    {
      "epoch": 25843.636363636364,
      "grad_norm": 0.2041865587234497,
      "learning_rate": 8.340770070986214e-09,
      "loss": 0.001,
      "step": 142140
    },
    {
      "epoch": 25845.454545454544,
      "grad_norm": 0.2067781239748001,
      "learning_rate": 8.319619254493093e-09,
      "loss": 0.0008,
      "step": 142150
    },
    {
      "epoch": 25847.272727272728,
      "grad_norm": 0.00044040303328074515,
      "learning_rate": 8.298495064561429e-09,
      "loss": 0.0011,
      "step": 142160
    },
    {
      "epoch": 25849.090909090908,
      "grad_norm": 0.00035450534778647125,
      "learning_rate": 8.277397502335193e-09,
      "loss": 0.0012,
      "step": 142170
    },
    {
      "epoch": 25850.909090909092,
      "grad_norm": 0.17405790090560913,
      "learning_rate": 8.256326568956862e-09,
      "loss": 0.001,
      "step": 142180
    },
    {
      "epoch": 25852.727272727272,
      "grad_norm": 0.17779980599880219,
      "learning_rate": 8.235282265567634e-09,
      "loss": 0.0012,
      "step": 142190
    },
    {
      "epoch": 25854.545454545456,
      "grad_norm": 0.0006357414531521499,
      "learning_rate": 8.214264593307096e-09,
      "loss": 0.0007,
      "step": 142200
    },
    {
      "epoch": 25856.363636363636,
      "grad_norm": 0.0009413488442078233,
      "learning_rate": 8.19327355331334e-09,
      "loss": 0.0015,
      "step": 142210
    },
    {
      "epoch": 25858.18181818182,
      "grad_norm": 0.00035746130743063986,
      "learning_rate": 8.172309146723234e-09,
      "loss": 0.0009,
      "step": 142220
    },
    {
      "epoch": 25860.0,
      "grad_norm": 0.16785156726837158,
      "learning_rate": 8.151371374672144e-09,
      "loss": 0.0012,
      "step": 142230
    },
    {
      "epoch": 25861.81818181818,
      "grad_norm": 0.17750972509384155,
      "learning_rate": 8.130460238293779e-09,
      "loss": 0.0012,
      "step": 142240
    },
    {
      "epoch": 25863.636363636364,
      "grad_norm": 0.21593229472637177,
      "learning_rate": 8.10957573872062e-09,
      "loss": 0.0007,
      "step": 142250
    },
    {
      "epoch": 25865.454545454544,
      "grad_norm": 0.2192050963640213,
      "learning_rate": 8.088717877083706e-09,
      "loss": 0.0013,
      "step": 142260
    },
    {
      "epoch": 25867.272727272728,
      "grad_norm": 0.0005974815576337278,
      "learning_rate": 8.067886654512524e-09,
      "loss": 0.0009,
      "step": 142270
    },
    {
      "epoch": 25869.090909090908,
      "grad_norm": 0.44433048367500305,
      "learning_rate": 8.047082072135226e-09,
      "loss": 0.0015,
      "step": 142280
    },
    {
      "epoch": 25870.909090909092,
      "grad_norm": 0.0005788991111330688,
      "learning_rate": 8.02630413107841e-09,
      "loss": 0.001,
      "step": 142290
    },
    {
      "epoch": 25872.727272727272,
      "grad_norm": 0.0005024310084991157,
      "learning_rate": 8.005552832467288e-09,
      "loss": 0.0012,
      "step": 142300
    },
    {
      "epoch": 25874.545454545456,
      "grad_norm": 0.00044858368346467614,
      "learning_rate": 7.98482817742574e-09,
      "loss": 0.0011,
      "step": 142310
    },
    {
      "epoch": 25876.363636363636,
      "grad_norm": 0.00040569715201854706,
      "learning_rate": 7.964130167075922e-09,
      "loss": 0.0007,
      "step": 142320
    },
    {
      "epoch": 25878.18181818182,
      "grad_norm": 0.0008272165432572365,
      "learning_rate": 7.943458802538772e-09,
      "loss": 0.0012,
      "step": 142330
    },
    {
      "epoch": 25880.0,
      "grad_norm": 0.0006576837040483952,
      "learning_rate": 7.922814084933838e-09,
      "loss": 0.0012,
      "step": 142340
    },
    {
      "epoch": 25881.81818181818,
      "grad_norm": 0.26647859811782837,
      "learning_rate": 7.90219601537906e-09,
      "loss": 0.0012,
      "step": 142350
    },
    {
      "epoch": 25883.636363636364,
      "grad_norm": 0.0009589698747731745,
      "learning_rate": 7.881604594990932e-09,
      "loss": 0.0009,
      "step": 142360
    },
    {
      "epoch": 25885.454545454544,
      "grad_norm": 0.1644333302974701,
      "learning_rate": 7.861039824884618e-09,
      "loss": 0.0011,
      "step": 142370
    },
    {
      "epoch": 25887.272727272728,
      "grad_norm": 0.20504669845104218,
      "learning_rate": 7.840501706173785e-09,
      "loss": 0.001,
      "step": 142380
    },
    {
      "epoch": 25889.090909090908,
      "grad_norm": 0.280146986246109,
      "learning_rate": 7.819990239970653e-09,
      "loss": 0.001,
      "step": 142390
    },
    {
      "epoch": 25890.909090909092,
      "grad_norm": 0.2390928566455841,
      "learning_rate": 7.799505427386e-09,
      "loss": 0.0012,
      "step": 142400
    },
    {
      "epoch": 25892.727272727272,
      "grad_norm": 0.20704545080661774,
      "learning_rate": 7.779047269529105e-09,
      "loss": 0.0009,
      "step": 142410
    },
    {
      "epoch": 25894.545454545456,
      "grad_norm": 0.1688620001077652,
      "learning_rate": 7.758615767508081e-09,
      "loss": 0.0013,
      "step": 142420
    },
    {
      "epoch": 25896.363636363636,
      "grad_norm": 0.0007709790370427072,
      "learning_rate": 7.7382109224291e-09,
      "loss": 0.0007,
      "step": 142430
    },
    {
      "epoch": 25898.18181818182,
      "grad_norm": 0.0005045887664891779,
      "learning_rate": 7.717832735397334e-09,
      "loss": 0.0013,
      "step": 142440
    },
    {
      "epoch": 25900.0,
      "grad_norm": 0.26486343145370483,
      "learning_rate": 7.697481207516288e-09,
      "loss": 0.0012,
      "step": 142450
    },
    {
      "epoch": 25901.81818181818,
      "grad_norm": 0.21105805039405823,
      "learning_rate": 7.677156339888135e-09,
      "loss": 0.0011,
      "step": 142460
    },
    {
      "epoch": 25903.636363636364,
      "grad_norm": 0.22053277492523193,
      "learning_rate": 7.656858133613498e-09,
      "loss": 0.0012,
      "step": 142470
    },
    {
      "epoch": 25905.454545454544,
      "grad_norm": 0.17388594150543213,
      "learning_rate": 7.63658658979166e-09,
      "loss": 0.0008,
      "step": 142480
    },
    {
      "epoch": 25907.272727272728,
      "grad_norm": 0.0008945026784203947,
      "learning_rate": 7.616341709520357e-09,
      "loss": 0.0011,
      "step": 142490
    },
    {
      "epoch": 25909.090909090908,
      "grad_norm": 0.0005656172870658338,
      "learning_rate": 7.59612349389599e-09,
      "loss": 0.0012,
      "step": 142500
    },
    {
      "epoch": 25909.090909090908,
      "eval_loss": 5.18825626373291,
      "eval_runtime": 0.9488,
      "eval_samples_per_second": 10.539,
      "eval_steps_per_second": 5.27,
      "step": 142500
    },
    {
      "epoch": 25910.909090909092,
      "grad_norm": 0.2199910283088684,
      "learning_rate": 7.575931944013403e-09,
      "loss": 0.001,
      "step": 142510
    },
    {
      "epoch": 25912.727272727272,
      "grad_norm": 0.011132875457406044,
      "learning_rate": 7.555767060966111e-09,
      "loss": 0.0013,
      "step": 142520
    },
    {
      "epoch": 25914.545454545456,
      "grad_norm": 0.0005448677693493664,
      "learning_rate": 7.535628845846076e-09,
      "loss": 0.0008,
      "step": 142530
    },
    {
      "epoch": 25916.363636363636,
      "grad_norm": 0.0005066338344477117,
      "learning_rate": 7.515517299743922e-09,
      "loss": 0.0012,
      "step": 142540
    },
    {
      "epoch": 25918.18181818182,
      "grad_norm": 0.18119487166404724,
      "learning_rate": 7.495432423748727e-09,
      "loss": 0.0013,
      "step": 142550
    },
    {
      "epoch": 25920.0,
      "grad_norm": 0.17706389725208282,
      "learning_rate": 7.475374218948116e-09,
      "loss": 0.001,
      "step": 142560
    },
    {
      "epoch": 25921.81818181818,
      "grad_norm": 0.1752847284078598,
      "learning_rate": 7.4553426864285e-09,
      "loss": 0.0012,
      "step": 142570
    },
    {
      "epoch": 25923.636363636364,
      "grad_norm": 0.1680186539888382,
      "learning_rate": 7.435337827274512e-09,
      "loss": 0.001,
      "step": 142580
    },
    {
      "epoch": 25925.454545454544,
      "grad_norm": 0.0005963282310403883,
      "learning_rate": 7.415359642569563e-09,
      "loss": 0.0008,
      "step": 142590
    },
    {
      "epoch": 25927.272727272728,
      "grad_norm": 0.00037853929097764194,
      "learning_rate": 7.3954081333955085e-09,
      "loss": 0.0015,
      "step": 142600
    },
    {
      "epoch": 25929.090909090908,
      "grad_norm": 0.0008238769369199872,
      "learning_rate": 7.37548330083293e-09,
      "loss": 0.001,
      "step": 142610
    },
    {
      "epoch": 25930.909090909092,
      "grad_norm": 0.20990242063999176,
      "learning_rate": 7.355585145960741e-09,
      "loss": 0.0012,
      "step": 142620
    },
    {
      "epoch": 25932.727272727272,
      "grad_norm": 0.0011973404325544834,
      "learning_rate": 7.335713669856469e-09,
      "loss": 0.001,
      "step": 142630
    },
    {
      "epoch": 25934.545454545456,
      "grad_norm": 0.28217941522598267,
      "learning_rate": 7.315868873596309e-09,
      "loss": 0.0012,
      "step": 142640
    },
    {
      "epoch": 25936.363636363636,
      "grad_norm": 0.17457140982151031,
      "learning_rate": 7.2960507582549565e-09,
      "loss": 0.0008,
      "step": 142650
    },
    {
      "epoch": 25938.18181818182,
      "grad_norm": 0.0006398122641257942,
      "learning_rate": 7.276259324905609e-09,
      "loss": 0.001,
      "step": 142660
    },
    {
      "epoch": 25940.0,
      "grad_norm": 0.0006286823772825301,
      "learning_rate": 7.25649457462002e-09,
      "loss": 0.0012,
      "step": 142670
    },
    {
      "epoch": 25941.81818181818,
      "grad_norm": 0.15457744896411896,
      "learning_rate": 7.236756508468611e-09,
      "loss": 0.0012,
      "step": 142680
    },
    {
      "epoch": 25943.636363636364,
      "grad_norm": 0.2109956443309784,
      "learning_rate": 7.217045127520249e-09,
      "loss": 0.0009,
      "step": 142690
    },
    {
      "epoch": 25945.454545454544,
      "grad_norm": 0.1721186488866806,
      "learning_rate": 7.197360432842359e-09,
      "loss": 0.0012,
      "step": 142700
    },
    {
      "epoch": 25947.272727272728,
      "grad_norm": 0.0013411326799541712,
      "learning_rate": 7.177702425500976e-09,
      "loss": 0.0007,
      "step": 142710
    },
    {
      "epoch": 25949.090909090908,
      "grad_norm": 0.0048258365131914616,
      "learning_rate": 7.158071106560693e-09,
      "loss": 0.0013,
      "step": 142720
    },
    {
      "epoch": 25950.909090909092,
      "grad_norm": 0.16999493539333344,
      "learning_rate": 7.138466477084604e-09,
      "loss": 0.0012,
      "step": 142730
    },
    {
      "epoch": 25952.727272727272,
      "grad_norm": 0.00040858134161680937,
      "learning_rate": 7.11888853813436e-09,
      "loss": 0.0009,
      "step": 142740
    },
    {
      "epoch": 25954.545454545456,
      "grad_norm": 0.2218329757452011,
      "learning_rate": 7.0993372907701686e-09,
      "loss": 0.0012,
      "step": 142750
    },
    {
      "epoch": 25956.363636363636,
      "grad_norm": 0.0004780498566105962,
      "learning_rate": 7.079812736050905e-09,
      "loss": 0.0012,
      "step": 142760
    },
    {
      "epoch": 25958.18181818182,
      "grad_norm": 0.0005372946616262197,
      "learning_rate": 7.060314875033835e-09,
      "loss": 0.0009,
      "step": 142770
    },
    {
      "epoch": 25960.0,
      "grad_norm": 0.0005326942773535848,
      "learning_rate": 7.040843708774835e-09,
      "loss": 0.0012,
      "step": 142780
    },
    {
      "epoch": 25961.81818181818,
      "grad_norm": 0.29013511538505554,
      "learning_rate": 7.021399238328451e-09,
      "loss": 0.0012,
      "step": 142790
    },
    {
      "epoch": 25963.636363636364,
      "grad_norm": 0.0006119855097495019,
      "learning_rate": 7.0019814647475636e-09,
      "loss": 0.0011,
      "step": 142800
    },
    {
      "epoch": 25965.454545454544,
      "grad_norm": 0.20832641422748566,
      "learning_rate": 6.982590389083776e-09,
      "loss": 0.001,
      "step": 142810
    },
    {
      "epoch": 25967.272727272728,
      "grad_norm": 0.0008043359266594052,
      "learning_rate": 6.963226012387191e-09,
      "loss": 0.0011,
      "step": 142820
    },
    {
      "epoch": 25969.090909090908,
      "grad_norm": 0.03639230504631996,
      "learning_rate": 6.943888335706471e-09,
      "loss": 0.0011,
      "step": 142830
    },
    {
      "epoch": 25970.909090909092,
      "grad_norm": 0.04586942866444588,
      "learning_rate": 6.92457736008889e-09,
      "loss": 0.0012,
      "step": 142840
    },
    {
      "epoch": 25972.727272727272,
      "grad_norm": 0.0005505697918124497,
      "learning_rate": 6.905293086580111e-09,
      "loss": 0.0009,
      "step": 142850
    },
    {
      "epoch": 25974.545454545456,
      "grad_norm": 0.00049760309047997,
      "learning_rate": 6.88603551622452e-09,
      "loss": 0.0012,
      "step": 142860
    },
    {
      "epoch": 25976.363636363636,
      "grad_norm": 0.000676482857670635,
      "learning_rate": 6.866804650065061e-09,
      "loss": 0.0011,
      "step": 142870
    },
    {
      "epoch": 25978.18181818182,
      "grad_norm": 0.00048805554979480803,
      "learning_rate": 6.8476004891430125e-09,
      "loss": 0.001,
      "step": 142880
    },
    {
      "epoch": 25980.0,
      "grad_norm": 0.000406972918426618,
      "learning_rate": 6.828423034498487e-09,
      "loss": 0.0012,
      "step": 142890
    },
    {
      "epoch": 25981.81818181818,
      "grad_norm": 0.16940055787563324,
      "learning_rate": 6.809272287169987e-09,
      "loss": 0.0012,
      "step": 142900
    },
    {
      "epoch": 25983.636363636364,
      "grad_norm": 0.17219313979148865,
      "learning_rate": 6.790148248194516e-09,
      "loss": 0.001,
      "step": 142910
    },
    {
      "epoch": 25985.454545454544,
      "grad_norm": 0.14577071368694305,
      "learning_rate": 6.771050918607912e-09,
      "loss": 0.0011,
      "step": 142920
    },
    {
      "epoch": 25987.272727272728,
      "grad_norm": 0.2113770693540573,
      "learning_rate": 6.751980299444237e-09,
      "loss": 0.0009,
      "step": 142930
    },
    {
      "epoch": 25989.090909090908,
      "grad_norm": 0.00043276860378682613,
      "learning_rate": 6.732936391736221e-09,
      "loss": 0.001,
      "step": 142940
    },
    {
      "epoch": 25990.909090909092,
      "grad_norm": 0.1703997254371643,
      "learning_rate": 6.713919196515316e-09,
      "loss": 0.0012,
      "step": 142950
    },
    {
      "epoch": 25992.727272727272,
      "grad_norm": 0.2099391371011734,
      "learning_rate": 6.694928714811255e-09,
      "loss": 0.0012,
      "step": 142960
    },
    {
      "epoch": 25994.545454545456,
      "grad_norm": 0.2852124869823456,
      "learning_rate": 6.6759649476524905e-09,
      "loss": 0.0012,
      "step": 142970
    },
    {
      "epoch": 25996.363636363636,
      "grad_norm": 0.0005092642968520522,
      "learning_rate": 6.6570278960659806e-09,
      "loss": 0.0008,
      "step": 142980
    },
    {
      "epoch": 25998.18181818182,
      "grad_norm": 0.0005280018667690456,
      "learning_rate": 6.638117561077294e-09,
      "loss": 0.001,
      "step": 142990
    },
    {
      "epoch": 26000.0,
      "grad_norm": 0.21798090636730194,
      "learning_rate": 6.61923394371039e-09,
      "loss": 0.0012,
      "step": 143000
    },
    {
      "epoch": 26000.0,
      "eval_loss": 5.219347953796387,
      "eval_runtime": 0.9469,
      "eval_samples_per_second": 10.561,
      "eval_steps_per_second": 5.281,
      "step": 143000
    },
    {
      "epoch": 26001.81818181818,
      "grad_norm": 0.006319588515907526,
      "learning_rate": 6.6003770449880595e-09,
      "loss": 0.0012,
      "step": 143010
    },
    {
      "epoch": 26003.636363636364,
      "grad_norm": 0.009338281117379665,
      "learning_rate": 6.58154686593132e-09,
      "loss": 0.001,
      "step": 143020
    },
    {
      "epoch": 26005.454545454544,
      "grad_norm": 0.0005495860823430121,
      "learning_rate": 6.562743407560078e-09,
      "loss": 0.0007,
      "step": 143030
    },
    {
      "epoch": 26007.272727272728,
      "grad_norm": 0.0005623680772259831,
      "learning_rate": 6.5439666708924645e-09,
      "loss": 0.0014,
      "step": 143040
    },
    {
      "epoch": 26009.090909090908,
      "grad_norm": 0.0009950180537998676,
      "learning_rate": 6.525216656945387e-09,
      "loss": 0.0011,
      "step": 143050
    },
    {
      "epoch": 26010.909090909092,
      "grad_norm": 0.20413318276405334,
      "learning_rate": 6.506493366734256e-09,
      "loss": 0.0011,
      "step": 143060
    },
    {
      "epoch": 26012.727272727272,
      "grad_norm": 0.28643035888671875,
      "learning_rate": 6.487796801272982e-09,
      "loss": 0.001,
      "step": 143070
    },
    {
      "epoch": 26014.545454545456,
      "grad_norm": 0.00046817775000818074,
      "learning_rate": 6.469126961574034e-09,
      "loss": 0.001,
      "step": 143080
    },
    {
      "epoch": 26016.363636363636,
      "grad_norm": 0.0005321709322743118,
      "learning_rate": 6.450483848648547e-09,
      "loss": 0.0009,
      "step": 143090
    },
    {
      "epoch": 26018.18181818182,
      "grad_norm": 0.1764041930437088,
      "learning_rate": 6.431867463506046e-09,
      "loss": 0.0013,
      "step": 143100
    },
    {
      "epoch": 26020.0,
      "grad_norm": 0.00042219233000651,
      "learning_rate": 6.413277807154727e-09,
      "loss": 0.001,
      "step": 143110
    },
    {
      "epoch": 26021.81818181818,
      "grad_norm": 0.0007522849482484162,
      "learning_rate": 6.394714880601282e-09,
      "loss": 0.0011,
      "step": 143120
    },
    {
      "epoch": 26023.636363636364,
      "grad_norm": 0.17127223312854767,
      "learning_rate": 6.376178684850963e-09,
      "loss": 0.001,
      "step": 143130
    },
    {
      "epoch": 26025.454545454544,
      "grad_norm": 0.17099910974502563,
      "learning_rate": 6.357669220907635e-09,
      "loss": 0.0013,
      "step": 143140
    },
    {
      "epoch": 26027.272727272728,
      "grad_norm": 0.17349225282669067,
      "learning_rate": 6.3391864897736625e-09,
      "loss": 0.0009,
      "step": 143150
    },
    {
      "epoch": 26029.090909090908,
      "grad_norm": 0.1757662147283554,
      "learning_rate": 6.3207304924498e-09,
      "loss": 0.001,
      "step": 143160
    },
    {
      "epoch": 26030.909090909092,
      "grad_norm": 0.20737428963184357,
      "learning_rate": 6.302301229935747e-09,
      "loss": 0.0012,
      "step": 143170
    },
    {
      "epoch": 26032.727272727272,
      "grad_norm": 0.0005199037841521204,
      "learning_rate": 6.283898703229429e-09,
      "loss": 0.0009,
      "step": 143180
    },
    {
      "epoch": 26034.545454545456,
      "grad_norm": 0.20054569840431213,
      "learning_rate": 6.265522913327325e-09,
      "loss": 0.001,
      "step": 143190
    },
    {
      "epoch": 26036.363636363636,
      "grad_norm": 0.017542818561196327,
      "learning_rate": 6.247173861224753e-09,
      "loss": 0.0014,
      "step": 143200
    },
    {
      "epoch": 26038.18181818182,
      "grad_norm": 0.1629863828420639,
      "learning_rate": 6.228851547915193e-09,
      "loss": 0.001,
      "step": 143210
    },
    {
      "epoch": 26040.0,
      "grad_norm": 0.2391359955072403,
      "learning_rate": 6.210555974391074e-09,
      "loss": 0.0011,
      "step": 143220
    },
    {
      "epoch": 26041.81818181818,
      "grad_norm": 0.27722179889678955,
      "learning_rate": 6.192287141642993e-09,
      "loss": 0.0012,
      "step": 143230
    },
    {
      "epoch": 26043.636363636364,
      "grad_norm": 0.00039589114021509886,
      "learning_rate": 6.174045050660381e-09,
      "loss": 0.0009,
      "step": 143240
    },
    {
      "epoch": 26045.454545454544,
      "grad_norm": 0.22214974462985992,
      "learning_rate": 6.15582970243117e-09,
      "loss": 0.0012,
      "step": 143250
    },
    {
      "epoch": 26047.272727272728,
      "grad_norm": 0.18101467192173004,
      "learning_rate": 6.137641097941682e-09,
      "loss": 0.0012,
      "step": 143260
    },
    {
      "epoch": 26049.090909090908,
      "grad_norm": 0.18561974167823792,
      "learning_rate": 6.119479238176961e-09,
      "loss": 0.0011,
      "step": 143270
    },
    {
      "epoch": 26050.909090909092,
      "grad_norm": 0.16264978051185608,
      "learning_rate": 6.101344124120555e-09,
      "loss": 0.0009,
      "step": 143280
    },
    {
      "epoch": 26052.727272727272,
      "grad_norm": 0.005543320439755917,
      "learning_rate": 6.0832357567545125e-09,
      "loss": 0.0012,
      "step": 143290
    },
    {
      "epoch": 26054.545454545456,
      "grad_norm": 0.0008258920279331505,
      "learning_rate": 6.065154137059603e-09,
      "loss": 0.001,
      "step": 143300
    },
    {
      "epoch": 26056.363636363636,
      "grad_norm": 0.000966553227044642,
      "learning_rate": 6.0470992660148764e-09,
      "loss": 0.001,
      "step": 143310
    },
    {
      "epoch": 26058.18181818182,
      "grad_norm": 0.00040263525443151593,
      "learning_rate": 6.029071144598163e-09,
      "loss": 0.001,
      "step": 143320
    },
    {
      "epoch": 26060.0,
      "grad_norm": 0.17774011194705963,
      "learning_rate": 6.011069773785737e-09,
      "loss": 0.0013,
      "step": 143330
    },
    {
      "epoch": 26061.81818181818,
      "grad_norm": 0.0005191638483665884,
      "learning_rate": 5.99309515455243e-09,
      "loss": 0.001,
      "step": 143340
    },
    {
      "epoch": 26063.636363636364,
      "grad_norm": 0.0005879339296370745,
      "learning_rate": 5.975147287871685e-09,
      "loss": 0.0011,
      "step": 143350
    },
    {
      "epoch": 26065.454545454544,
      "grad_norm": 0.0005456121289171278,
      "learning_rate": 5.957226174715391e-09,
      "loss": 0.0012,
      "step": 143360
    },
    {
      "epoch": 26067.272727272728,
      "grad_norm": 0.0006053729448467493,
      "learning_rate": 5.9393318160541605e-09,
      "loss": 0.001,
      "step": 143370
    },
    {
      "epoch": 26069.090909090908,
      "grad_norm": 0.00745978532359004,
      "learning_rate": 5.921464212856886e-09,
      "loss": 0.001,
      "step": 143380
    },
    {
      "epoch": 26070.909090909092,
      "grad_norm": 0.16662222146987915,
      "learning_rate": 5.903623366091348e-09,
      "loss": 0.0009,
      "step": 143390
    },
    {
      "epoch": 26072.727272727272,
      "grad_norm": 0.20406697690486908,
      "learning_rate": 5.8858092767236076e-09,
      "loss": 0.0012,
      "step": 143400
    },
    {
      "epoch": 26074.545454545456,
      "grad_norm": 0.0004915014724247158,
      "learning_rate": 5.868021945718394e-09,
      "loss": 0.001,
      "step": 143410
    },
    {
      "epoch": 26076.363636363636,
      "grad_norm": 0.2724412977695465,
      "learning_rate": 5.850261374038934e-09,
      "loss": 0.0015,
      "step": 143420
    },
    {
      "epoch": 26078.18181818182,
      "grad_norm": 0.21022894978523254,
      "learning_rate": 5.832527562647016e-09,
      "loss": 0.0011,
      "step": 143430
    },
    {
      "epoch": 26080.0,
      "grad_norm": 0.17064864933490753,
      "learning_rate": 5.8148205125031494e-09,
      "loss": 0.0007,
      "step": 143440
    },
    {
      "epoch": 26081.81818181818,
      "grad_norm": 0.21003180742263794,
      "learning_rate": 5.797140224566122e-09,
      "loss": 0.001,
      "step": 143450
    },
    {
      "epoch": 26083.636363636364,
      "grad_norm": 0.0005932764033786952,
      "learning_rate": 5.779486699793334e-09,
      "loss": 0.0011,
      "step": 143460
    },
    {
      "epoch": 26085.454545454544,
      "grad_norm": 0.28045299649238586,
      "learning_rate": 5.761859939140967e-09,
      "loss": 0.0014,
      "step": 143470
    },
    {
      "epoch": 26087.272727272728,
      "grad_norm": 0.21195685863494873,
      "learning_rate": 5.744259943563423e-09,
      "loss": 0.001,
      "step": 143480
    },
    {
      "epoch": 26089.090909090908,
      "grad_norm": 0.1705787479877472,
      "learning_rate": 5.7266867140139955e-09,
      "loss": 0.0011,
      "step": 143490
    },
    {
      "epoch": 26090.909090909092,
      "grad_norm": 0.27878376841545105,
      "learning_rate": 5.7091402514442e-09,
      "loss": 0.001,
      "step": 143500
    },
    {
      "epoch": 26090.909090909092,
      "eval_loss": 5.279892444610596,
      "eval_runtime": 0.9483,
      "eval_samples_per_second": 10.545,
      "eval_steps_per_second": 5.273,
      "step": 143500
    },
    {
      "epoch": 26092.727272727272,
      "grad_norm": 0.189569890499115,
      "learning_rate": 5.691620556804222e-09,
      "loss": 0.0012,
      "step": 143510
    },
    {
      "epoch": 26094.545454545456,
      "grad_norm": 0.1696234494447708,
      "learning_rate": 5.674127631043024e-09,
      "loss": 0.0009,
      "step": 143520
    },
    {
      "epoch": 26096.363636363636,
      "grad_norm": 0.21350109577178955,
      "learning_rate": 5.656661475107738e-09,
      "loss": 0.0011,
      "step": 143530
    },
    {
      "epoch": 26098.18181818182,
      "grad_norm": 0.0006116288132034242,
      "learning_rate": 5.6392220899442736e-09,
      "loss": 0.001,
      "step": 143540
    },
    {
      "epoch": 26100.0,
      "grad_norm": 0.0008745138184167445,
      "learning_rate": 5.621809476497097e-09,
      "loss": 0.0012,
      "step": 143550
    },
    {
      "epoch": 26101.81818181818,
      "grad_norm": 0.16521725058555603,
      "learning_rate": 5.604423635709121e-09,
      "loss": 0.0012,
      "step": 143560
    },
    {
      "epoch": 26103.636363636364,
      "grad_norm": 0.17605525255203247,
      "learning_rate": 5.587064568521871e-09,
      "loss": 0.001,
      "step": 143570
    },
    {
      "epoch": 26105.454545454544,
      "grad_norm": 0.0010872863931581378,
      "learning_rate": 5.5697322758754274e-09,
      "loss": 0.0009,
      "step": 143580
    },
    {
      "epoch": 26107.272727272728,
      "grad_norm": 0.16916072368621826,
      "learning_rate": 5.552426758708429e-09,
      "loss": 0.0013,
      "step": 143590
    },
    {
      "epoch": 26109.090909090908,
      "grad_norm": 0.0006119210738688707,
      "learning_rate": 5.535148017958014e-09,
      "loss": 0.0009,
      "step": 143600
    },
    {
      "epoch": 26110.909090909092,
      "grad_norm": 0.0009109739330597222,
      "learning_rate": 5.517896054559879e-09,
      "loss": 0.0012,
      "step": 143610
    },
    {
      "epoch": 26112.727272727272,
      "grad_norm": 0.00037649308796972036,
      "learning_rate": 5.5006708694483315e-09,
      "loss": 0.001,
      "step": 143620
    },
    {
      "epoch": 26114.545454545456,
      "grad_norm": 0.0005368883139453828,
      "learning_rate": 5.483472463556183e-09,
      "loss": 0.0012,
      "step": 143630
    },
    {
      "epoch": 26116.363636363636,
      "grad_norm": 0.21110454201698303,
      "learning_rate": 5.4663008378147965e-09,
      "loss": 0.0013,
      "step": 143640
    },
    {
      "epoch": 26118.18181818182,
      "grad_norm": 0.0015278615755960345,
      "learning_rate": 5.449155993153987e-09,
      "loss": 0.0007,
      "step": 143650
    },
    {
      "epoch": 26120.0,
      "grad_norm": 0.2709122896194458,
      "learning_rate": 5.4320379305023975e-09,
      "loss": 0.0012,
      "step": 143660
    },
    {
      "epoch": 26121.81818181818,
      "grad_norm": 0.0008475802023895085,
      "learning_rate": 5.414946650786956e-09,
      "loss": 0.001,
      "step": 143670
    },
    {
      "epoch": 26123.636363636364,
      "grad_norm": 0.1466750204563141,
      "learning_rate": 5.397882154933198e-09,
      "loss": 0.001,
      "step": 143680
    },
    {
      "epoch": 26125.454545454544,
      "grad_norm": 0.20628198981285095,
      "learning_rate": 5.380844443865273e-09,
      "loss": 0.001,
      "step": 143690
    },
    {
      "epoch": 26127.272727272728,
      "grad_norm": 0.17745447158813477,
      "learning_rate": 5.3638335185058335e-09,
      "loss": 0.0014,
      "step": 143700
    },
    {
      "epoch": 26129.090909090908,
      "grad_norm": 0.00033595110289752483,
      "learning_rate": 5.346849379776142e-09,
      "loss": 0.0009,
      "step": 143710
    },
    {
      "epoch": 26130.909090909092,
      "grad_norm": 0.0005383033421821892,
      "learning_rate": 5.32989202859585e-09,
      "loss": 0.0012,
      "step": 143720
    },
    {
      "epoch": 26132.727272727272,
      "grad_norm": 0.212842658162117,
      "learning_rate": 5.312961465883392e-09,
      "loss": 0.001,
      "step": 143730
    },
    {
      "epoch": 26134.545454545456,
      "grad_norm": 0.1749555766582489,
      "learning_rate": 5.296057692555533e-09,
      "loss": 0.001,
      "step": 143740
    },
    {
      "epoch": 26136.363636363636,
      "grad_norm": 0.0005715764127671719,
      "learning_rate": 5.279180709527764e-09,
      "loss": 0.0009,
      "step": 143750
    },
    {
      "epoch": 26138.18181818182,
      "grad_norm": 0.0006193786393851042,
      "learning_rate": 5.262330517713964e-09,
      "loss": 0.0012,
      "step": 143760
    },
    {
      "epoch": 26140.0,
      "grad_norm": 0.0005386632983572781,
      "learning_rate": 5.245507118026682e-09,
      "loss": 0.0012,
      "step": 143770
    },
    {
      "epoch": 26141.81818181818,
      "grad_norm": 0.010811226442456245,
      "learning_rate": 5.228710511377021e-09,
      "loss": 0.0012,
      "step": 143780
    },
    {
      "epoch": 26143.636363636364,
      "grad_norm": 0.21061691641807556,
      "learning_rate": 5.211940698674533e-09,
      "loss": 0.0009,
      "step": 143790
    },
    {
      "epoch": 26145.454545454544,
      "grad_norm": 0.21014830470085144,
      "learning_rate": 5.195197680827379e-09,
      "loss": 0.001,
      "step": 143800
    },
    {
      "epoch": 26147.272727272728,
      "grad_norm": 0.223069429397583,
      "learning_rate": 5.1784814587422235e-09,
      "loss": 0.0012,
      "step": 143810
    },
    {
      "epoch": 26149.090909090908,
      "grad_norm": 0.17209362983703613,
      "learning_rate": 5.161792033324397e-09,
      "loss": 0.0011,
      "step": 143820
    },
    {
      "epoch": 26150.909090909092,
      "grad_norm": 0.15261270105838776,
      "learning_rate": 5.145129405477733e-09,
      "loss": 0.0012,
      "step": 143830
    },
    {
      "epoch": 26152.727272727272,
      "grad_norm": 0.0006063852342776954,
      "learning_rate": 5.1284935761044535e-09,
      "loss": 0.0008,
      "step": 143840
    },
    {
      "epoch": 26154.545454545456,
      "grad_norm": 0.006576680578291416,
      "learning_rate": 5.111884546105505e-09,
      "loss": 0.0015,
      "step": 143850
    },
    {
      "epoch": 26156.363636363636,
      "grad_norm": 0.23497644066810608,
      "learning_rate": 5.0953023163803895e-09,
      "loss": 0.001,
      "step": 143860
    },
    {
      "epoch": 26158.18181818182,
      "grad_norm": 0.0008946041343733668,
      "learning_rate": 5.078746887827112e-09,
      "loss": 0.0009,
      "step": 143870
    },
    {
      "epoch": 26160.0,
      "grad_norm": 0.0007101800874806941,
      "learning_rate": 5.062218261342121e-09,
      "loss": 0.0011,
      "step": 143880
    },
    {
      "epoch": 26161.81818181818,
      "grad_norm": 0.2108631134033203,
      "learning_rate": 5.045716437820591e-09,
      "loss": 0.001,
      "step": 143890
    },
    {
      "epoch": 26163.636363636364,
      "grad_norm": 0.000914169882889837,
      "learning_rate": 5.029241418156138e-09,
      "loss": 0.0012,
      "step": 143900
    },
    {
      "epoch": 26165.454545454544,
      "grad_norm": 0.22180120646953583,
      "learning_rate": 5.012793203240995e-09,
      "loss": 0.0011,
      "step": 143910
    },
    {
      "epoch": 26167.272727272728,
      "grad_norm": 0.22242587804794312,
      "learning_rate": 4.9963717939658365e-09,
      "loss": 0.0012,
      "step": 143920
    },
    {
      "epoch": 26169.090909090908,
      "grad_norm": 0.0022147351410239935,
      "learning_rate": 4.979977191220008e-09,
      "loss": 0.001,
      "step": 143930
    },
    {
      "epoch": 26170.909090909092,
      "grad_norm": 0.00044241463183425367,
      "learning_rate": 4.9636093958913e-09,
      "loss": 0.0012,
      "step": 143940
    },
    {
      "epoch": 26172.727272727272,
      "grad_norm": 0.22607533633708954,
      "learning_rate": 4.947268408866112e-09,
      "loss": 0.0011,
      "step": 143950
    },
    {
      "epoch": 26174.545454545456,
      "grad_norm": 0.21817895770072937,
      "learning_rate": 4.93095423102935e-09,
      "loss": 0.0012,
      "step": 143960
    },
    {
      "epoch": 26176.363636363636,
      "grad_norm": 0.18034254014492035,
      "learning_rate": 4.914666863264527e-09,
      "loss": 0.0013,
      "step": 143970
    },
    {
      "epoch": 26178.18181818182,
      "grad_norm": 0.28599458932876587,
      "learning_rate": 4.898406306453773e-09,
      "loss": 0.0012,
      "step": 143980
    },
    {
      "epoch": 26180.0,
      "grad_norm": 0.17867405712604523,
      "learning_rate": 4.882172561477438e-09,
      "loss": 0.0009,
      "step": 143990
    },
    {
      "epoch": 26181.81818181818,
      "grad_norm": 0.0005755594465881586,
      "learning_rate": 4.865965629214819e-09,
      "loss": 0.0011,
      "step": 144000
    },
    {
      "epoch": 26181.81818181818,
      "eval_loss": 5.157451629638672,
      "eval_runtime": 0.947,
      "eval_samples_per_second": 10.559,
      "eval_steps_per_second": 5.28,
      "step": 144000
    },
    {
      "epoch": 26183.636363636364,
      "grad_norm": 0.0005190999363549054,
      "learning_rate": 4.849785510543602e-09,
      "loss": 0.0006,
      "step": 144010
    },
    {
      "epoch": 26185.454545454544,
      "grad_norm": 0.17082750797271729,
      "learning_rate": 4.8336322063399215e-09,
      "loss": 0.0014,
      "step": 144020
    },
    {
      "epoch": 26187.272727272728,
      "grad_norm": 0.000436715898104012,
      "learning_rate": 4.817505717478576e-09,
      "loss": 0.001,
      "step": 144030
    },
    {
      "epoch": 26189.090909090908,
      "grad_norm": 0.2030048817396164,
      "learning_rate": 4.801406044832812e-09,
      "loss": 0.0011,
      "step": 144040
    },
    {
      "epoch": 26190.909090909092,
      "grad_norm": 0.00045263543142937124,
      "learning_rate": 4.7853331892746544e-09,
      "loss": 0.0011,
      "step": 144050
    },
    {
      "epoch": 26192.727272727272,
      "grad_norm": 0.0010932249715551734,
      "learning_rate": 4.769287151674406e-09,
      "loss": 0.0008,
      "step": 144060
    },
    {
      "epoch": 26194.545454545456,
      "grad_norm": 0.23228341341018677,
      "learning_rate": 4.753267932901039e-09,
      "loss": 0.0012,
      "step": 144070
    },
    {
      "epoch": 26196.363636363636,
      "grad_norm": 0.2210700362920761,
      "learning_rate": 4.7372755338220825e-09,
      "loss": 0.0014,
      "step": 144080
    },
    {
      "epoch": 26198.18181818182,
      "grad_norm": 0.21410562098026276,
      "learning_rate": 4.721309955303565e-09,
      "loss": 0.001,
      "step": 144090
    },
    {
      "epoch": 26200.0,
      "grad_norm": 0.17506639659404755,
      "learning_rate": 4.705371198210128e-09,
      "loss": 0.001,
      "step": 144100
    },
    {
      "epoch": 26201.81818181818,
      "grad_norm": 0.22016753256320953,
      "learning_rate": 4.689459263404916e-09,
      "loss": 0.0012,
      "step": 144110
    },
    {
      "epoch": 26203.636363636364,
      "grad_norm": 0.0005080259870737791,
      "learning_rate": 4.673574151749571e-09,
      "loss": 0.0008,
      "step": 144120
    },
    {
      "epoch": 26205.454545454544,
      "grad_norm": 0.0003885654441546649,
      "learning_rate": 4.657715864104405e-09,
      "loss": 0.0011,
      "step": 144130
    },
    {
      "epoch": 26207.272727272728,
      "grad_norm": 0.2831822633743286,
      "learning_rate": 4.641884401328178e-09,
      "loss": 0.0015,
      "step": 144140
    },
    {
      "epoch": 26209.090909090908,
      "grad_norm": 0.22317878901958466,
      "learning_rate": 4.626079764278201e-09,
      "loss": 0.0011,
      "step": 144150
    },
    {
      "epoch": 26210.909090909092,
      "grad_norm": 0.21971996128559113,
      "learning_rate": 4.6103019538104024e-09,
      "loss": 0.0012,
      "step": 144160
    },
    {
      "epoch": 26212.727272727272,
      "grad_norm": 0.0007954570464789867,
      "learning_rate": 4.59455097077932e-09,
      "loss": 0.0009,
      "step": 144170
    },
    {
      "epoch": 26214.545454545456,
      "grad_norm": 0.0009467886411584914,
      "learning_rate": 4.578826816037717e-09,
      "loss": 0.0013,
      "step": 144180
    },
    {
      "epoch": 26216.363636363636,
      "grad_norm": 0.16463705897331238,
      "learning_rate": 4.563129490437245e-09,
      "loss": 0.0009,
      "step": 144190
    },
    {
      "epoch": 26218.18181818182,
      "grad_norm": 0.0008421026868745685,
      "learning_rate": 4.547458994828002e-09,
      "loss": 0.001,
      "step": 144200
    },
    {
      "epoch": 26220.0,
      "grad_norm": 0.01089431345462799,
      "learning_rate": 4.531815330058586e-09,
      "loss": 0.0012,
      "step": 144210
    },
    {
      "epoch": 26221.81818181818,
      "grad_norm": 0.18856951594352722,
      "learning_rate": 4.516198496976154e-09,
      "loss": 0.0011,
      "step": 144220
    },
    {
      "epoch": 26223.636363636364,
      "grad_norm": 0.00083601736696437,
      "learning_rate": 4.500608496426417e-09,
      "loss": 0.0011,
      "step": 144230
    },
    {
      "epoch": 26225.454545454544,
      "grad_norm": 0.20520175993442535,
      "learning_rate": 4.485045329253645e-09,
      "loss": 0.0012,
      "step": 144240
    },
    {
      "epoch": 26227.272727272728,
      "grad_norm": 0.16868388652801514,
      "learning_rate": 4.469508996300664e-09,
      "loss": 0.0009,
      "step": 144250
    },
    {
      "epoch": 26229.090909090908,
      "grad_norm": 0.2041386514902115,
      "learning_rate": 4.4539994984088e-09,
      "loss": 0.0011,
      "step": 144260
    },
    {
      "epoch": 26230.909090909092,
      "grad_norm": 0.0005917003727518022,
      "learning_rate": 4.438516836417994e-09,
      "loss": 0.001,
      "step": 144270
    },
    {
      "epoch": 26232.727272727272,
      "grad_norm": 0.0003877450944855809,
      "learning_rate": 4.423061011166684e-09,
      "loss": 0.0009,
      "step": 144280
    },
    {
      "epoch": 26234.545454545456,
      "grad_norm": 0.0007333348039537668,
      "learning_rate": 4.407632023491925e-09,
      "loss": 0.0013,
      "step": 144290
    },
    {
      "epoch": 26236.363636363636,
      "grad_norm": 0.0012312914477661252,
      "learning_rate": 4.3922298742291585e-09,
      "loss": 0.0009,
      "step": 144300
    },
    {
      "epoch": 26238.18181818182,
      "grad_norm": 0.16999810934066772,
      "learning_rate": 4.376854564212496e-09,
      "loss": 0.0015,
      "step": 144310
    },
    {
      "epoch": 26240.0,
      "grad_norm": 0.23568499088287354,
      "learning_rate": 4.361506094274659e-09,
      "loss": 0.001,
      "step": 144320
    },
    {
      "epoch": 26241.81818181818,
      "grad_norm": 0.0005183789180591702,
      "learning_rate": 4.346184465246761e-09,
      "loss": 0.0012,
      "step": 144330
    },
    {
      "epoch": 26243.636363636364,
      "grad_norm": 0.21237145364284515,
      "learning_rate": 4.330889677958471e-09,
      "loss": 0.001,
      "step": 144340
    },
    {
      "epoch": 26245.454545454544,
      "grad_norm": 0.0009321804391220212,
      "learning_rate": 4.315621733238184e-09,
      "loss": 0.0013,
      "step": 144350
    },
    {
      "epoch": 26247.272727272728,
      "grad_norm": 0.17268459498882294,
      "learning_rate": 4.3003806319127366e-09,
      "loss": 0.0008,
      "step": 144360
    },
    {
      "epoch": 26249.090909090908,
      "grad_norm": 0.011842196807265282,
      "learning_rate": 4.285166374807358e-09,
      "loss": 0.0013,
      "step": 144370
    },
    {
      "epoch": 26250.909090909092,
      "grad_norm": 0.0005749525735154748,
      "learning_rate": 4.269978962746112e-09,
      "loss": 0.0011,
      "step": 144380
    },
    {
      "epoch": 26252.727272727272,
      "grad_norm": 0.005931003950536251,
      "learning_rate": 4.254818396551341e-09,
      "loss": 0.0012,
      "step": 144390
    },
    {
      "epoch": 26254.545454545456,
      "grad_norm": 0.17565903067588806,
      "learning_rate": 4.239684677044164e-09,
      "loss": 0.001,
      "step": 144400
    },
    {
      "epoch": 26256.363636363636,
      "grad_norm": 0.23024794459342957,
      "learning_rate": 4.224577805044094e-09,
      "loss": 0.0011,
      "step": 144410
    },
    {
      "epoch": 26258.18181818182,
      "grad_norm": 0.00047229917254298925,
      "learning_rate": 4.209497781369143e-09,
      "loss": 0.0009,
      "step": 144420
    },
    {
      "epoch": 26260.0,
      "grad_norm": 0.0005878262454643846,
      "learning_rate": 4.194444606836101e-09,
      "loss": 0.0012,
      "step": 144430
    },
    {
      "epoch": 26261.81818181818,
      "grad_norm": 0.006172967609018087,
      "learning_rate": 4.179418282260094e-09,
      "loss": 0.0012,
      "step": 144440
    },
    {
      "epoch": 26263.636363636364,
      "grad_norm": 0.0005567340995185077,
      "learning_rate": 4.164418808454806e-09,
      "loss": 0.0007,
      "step": 144450
    },
    {
      "epoch": 26265.454545454544,
      "grad_norm": 0.1707691103219986,
      "learning_rate": 4.149446186232586e-09,
      "loss": 0.0013,
      "step": 144460
    },
    {
      "epoch": 26267.272727272728,
      "grad_norm": 0.20745493471622467,
      "learning_rate": 4.134500416404285e-09,
      "loss": 0.001,
      "step": 144470
    },
    {
      "epoch": 26269.090909090908,
      "grad_norm": 0.0004915361641906202,
      "learning_rate": 4.119581499779201e-09,
      "loss": 0.001,
      "step": 144480
    },
    {
      "epoch": 26270.909090909092,
      "grad_norm": 0.000908420595806092,
      "learning_rate": 4.1046894371653536e-09,
      "loss": 0.001,
      "step": 144490
    },
    {
      "epoch": 26272.727272727272,
      "grad_norm": 0.2237381488084793,
      "learning_rate": 4.089824229369154e-09,
      "loss": 0.0012,
      "step": 144500
    },
    {
      "epoch": 26272.727272727272,
      "eval_loss": 5.208399772644043,
      "eval_runtime": 0.9506,
      "eval_samples_per_second": 10.52,
      "eval_steps_per_second": 5.26,
      "step": 144500
    },
    {
      "epoch": 26274.545454545456,
      "grad_norm": 0.17815545201301575,
      "learning_rate": 4.0749858771956245e-09,
      "loss": 0.001,
      "step": 144510
    },
    {
      "epoch": 26276.363636363636,
      "grad_norm": 0.17638269066810608,
      "learning_rate": 4.060174381448289e-09,
      "loss": 0.0012,
      "step": 144520
    },
    {
      "epoch": 26278.18181818182,
      "grad_norm": 0.2074621021747589,
      "learning_rate": 4.0453897429292834e-09,
      "loss": 0.001,
      "step": 144530
    },
    {
      "epoch": 26280.0,
      "grad_norm": 0.2784509062767029,
      "learning_rate": 4.030631962439302e-09,
      "loss": 0.001,
      "step": 144540
    },
    {
      "epoch": 26281.81818181818,
      "grad_norm": 0.2008492797613144,
      "learning_rate": 4.0159010407774805e-09,
      "loss": 0.0012,
      "step": 144550
    },
    {
      "epoch": 26283.636363636364,
      "grad_norm": 0.17376157641410828,
      "learning_rate": 4.001196978741572e-09,
      "loss": 0.0012,
      "step": 144560
    },
    {
      "epoch": 26285.454545454544,
      "grad_norm": 0.0003862104786094278,
      "learning_rate": 3.986519777127883e-09,
      "loss": 0.0009,
      "step": 144570
    },
    {
      "epoch": 26287.272727272728,
      "grad_norm": 0.0010778933065012097,
      "learning_rate": 3.971869436731224e-09,
      "loss": 0.0011,
      "step": 144580
    },
    {
      "epoch": 26289.090909090908,
      "grad_norm": 0.21931622922420502,
      "learning_rate": 3.9572459583450126e-09,
      "loss": 0.0012,
      "step": 144590
    },
    {
      "epoch": 26290.909090909092,
      "grad_norm": 0.21319864690303802,
      "learning_rate": 3.9426493427611175e-09,
      "loss": 0.001,
      "step": 144600
    },
    {
      "epoch": 26292.727272727272,
      "grad_norm": 0.16890107095241547,
      "learning_rate": 3.928079590770017e-09,
      "loss": 0.0012,
      "step": 144610
    },
    {
      "epoch": 26294.545454545456,
      "grad_norm": 0.27117374539375305,
      "learning_rate": 3.913536703160747e-09,
      "loss": 0.001,
      "step": 144620
    },
    {
      "epoch": 26296.363636363636,
      "grad_norm": 0.21823468804359436,
      "learning_rate": 3.8990206807208434e-09,
      "loss": 0.001,
      "step": 144630
    },
    {
      "epoch": 26298.18181818182,
      "grad_norm": 0.17593203485012054,
      "learning_rate": 3.884531524236401e-09,
      "loss": 0.0013,
      "step": 144640
    },
    {
      "epoch": 26300.0,
      "grad_norm": 0.20959331095218658,
      "learning_rate": 3.8700692344921236e-09,
      "loss": 0.0009,
      "step": 144650
    },
    {
      "epoch": 26301.81818181818,
      "grad_norm": 0.000886717694811523,
      "learning_rate": 3.855633812271164e-09,
      "loss": 0.001,
      "step": 144660
    },
    {
      "epoch": 26303.636363636364,
      "grad_norm": 0.0005434443010017276,
      "learning_rate": 3.841225258355285e-09,
      "loss": 0.0013,
      "step": 144670
    },
    {
      "epoch": 26305.454545454544,
      "grad_norm": 0.00040317821549251676,
      "learning_rate": 3.826843573524752e-09,
      "loss": 0.0008,
      "step": 144680
    },
    {
      "epoch": 26307.272727272728,
      "grad_norm": 0.0008682123152539134,
      "learning_rate": 3.812488758558385e-09,
      "loss": 0.0013,
      "step": 144690
    },
    {
      "epoch": 26309.090909090908,
      "grad_norm": 0.2752567231655121,
      "learning_rate": 3.798160814233564e-09,
      "loss": 0.0013,
      "step": 144700
    },
    {
      "epoch": 26310.909090909092,
      "grad_norm": 0.0007759095169603825,
      "learning_rate": 3.783859741326223e-09,
      "loss": 0.0009,
      "step": 144710
    },
    {
      "epoch": 26312.727272727272,
      "grad_norm": 0.2773153781890869,
      "learning_rate": 3.769585540610798e-09,
      "loss": 0.0011,
      "step": 144720
    },
    {
      "epoch": 26314.545454545456,
      "grad_norm": 0.22344958782196045,
      "learning_rate": 3.755338212860337e-09,
      "loss": 0.0012,
      "step": 144730
    },
    {
      "epoch": 26316.363636363636,
      "grad_norm": 0.0012502615572884679,
      "learning_rate": 3.74111775884639e-09,
      "loss": 0.0009,
      "step": 144740
    },
    {
      "epoch": 26318.18181818182,
      "grad_norm": 0.22216613590717316,
      "learning_rate": 3.7269241793390084e-09,
      "loss": 0.0012,
      "step": 144750
    },
    {
      "epoch": 26320.0,
      "grad_norm": 0.0011230960953980684,
      "learning_rate": 3.7127574751068535e-09,
      "loss": 0.001,
      "step": 144760
    },
    {
      "epoch": 26321.81818181818,
      "grad_norm": 0.0014812290901318192,
      "learning_rate": 3.6986176469170903e-09,
      "loss": 0.0007,
      "step": 144770
    },
    {
      "epoch": 26323.636363636364,
      "grad_norm": 0.00051072210771963,
      "learning_rate": 3.6845046955354954e-09,
      "loss": 0.0018,
      "step": 144780
    },
    {
      "epoch": 26325.454545454544,
      "grad_norm": 0.00034522757050581276,
      "learning_rate": 3.6704186217263457e-09,
      "loss": 0.0007,
      "step": 144790
    },
    {
      "epoch": 26327.272727272728,
      "grad_norm": 0.012032187543809414,
      "learning_rate": 3.65635942625242e-09,
      "loss": 0.0014,
      "step": 144800
    },
    {
      "epoch": 26329.090909090908,
      "grad_norm": 0.005065464414656162,
      "learning_rate": 3.642327109875165e-09,
      "loss": 0.001,
      "step": 144810
    },
    {
      "epoch": 26330.909090909092,
      "grad_norm": 0.0004768193466588855,
      "learning_rate": 3.628321673354362e-09,
      "loss": 0.0011,
      "step": 144820
    },
    {
      "epoch": 26332.727272727272,
      "grad_norm": 0.2065311074256897,
      "learning_rate": 3.6143431174485705e-09,
      "loss": 0.0011,
      "step": 144830
    },
    {
      "epoch": 26334.545454545456,
      "grad_norm": 0.0012169177643954754,
      "learning_rate": 3.600391442914741e-09,
      "loss": 0.0009,
      "step": 144840
    },
    {
      "epoch": 26336.363636363636,
      "grad_norm": 0.0005057184025645256,
      "learning_rate": 3.5864666505083795e-09,
      "loss": 0.001,
      "step": 144850
    },
    {
      "epoch": 26338.18181818182,
      "grad_norm": 0.17298628389835358,
      "learning_rate": 3.5725687409836612e-09,
      "loss": 0.0014,
      "step": 144860
    },
    {
      "epoch": 26340.0,
      "grad_norm": 0.0011019163066521287,
      "learning_rate": 3.5586977150932063e-09,
      "loss": 0.001,
      "step": 144870
    },
    {
      "epoch": 26341.81818181818,
      "grad_norm": 0.0012995050055906177,
      "learning_rate": 3.54485357358808e-09,
      "loss": 0.0009,
      "step": 144880
    },
    {
      "epoch": 26343.636363636364,
      "grad_norm": 0.23815064132213593,
      "learning_rate": 3.531036317218128e-09,
      "loss": 0.0012,
      "step": 144890
    },
    {
      "epoch": 26345.454545454544,
      "grad_norm": 0.0011003137333318591,
      "learning_rate": 3.5172459467315286e-09,
      "loss": 0.001,
      "step": 144900
    },
    {
      "epoch": 26347.272727272728,
      "grad_norm": 0.17120447754859924,
      "learning_rate": 3.503482462875129e-09,
      "loss": 0.0012,
      "step": 144910
    },
    {
      "epoch": 26349.090909090908,
      "grad_norm": 0.22012074291706085,
      "learning_rate": 3.4897458663942782e-09,
      "loss": 0.0012,
      "step": 144920
    },
    {
      "epoch": 26350.909090909092,
      "grad_norm": 0.20375652611255646,
      "learning_rate": 3.47603615803288e-09,
      "loss": 0.001,
      "step": 144930
    },
    {
      "epoch": 26352.727272727272,
      "grad_norm": 0.17054496705532074,
      "learning_rate": 3.4623533385333415e-09,
      "loss": 0.001,
      "step": 144940
    },
    {
      "epoch": 26354.545454545456,
      "grad_norm": 0.22310476005077362,
      "learning_rate": 3.448697408636625e-09,
      "loss": 0.0012,
      "step": 144950
    },
    {
      "epoch": 26356.363636363636,
      "grad_norm": 0.0005942644202150404,
      "learning_rate": 3.4350683690823058e-09,
      "loss": 0.0009,
      "step": 144960
    },
    {
      "epoch": 26358.18181818182,
      "grad_norm": 0.0008387731504626572,
      "learning_rate": 3.42146622060846e-09,
      "loss": 0.0012,
      "step": 144970
    },
    {
      "epoch": 26360.0,
      "grad_norm": 0.0012398078106343746,
      "learning_rate": 3.4078909639517206e-09,
      "loss": 0.0012,
      "step": 144980
    },
    {
      "epoch": 26361.81818181818,
      "grad_norm": 0.20956963300704956,
      "learning_rate": 3.394342599847111e-09,
      "loss": 0.001,
      "step": 144990
    },
    {
      "epoch": 26363.636363636364,
      "grad_norm": 0.17362312972545624,
      "learning_rate": 3.380821129028488e-09,
      "loss": 0.0012,
      "step": 145000
    },
    {
      "epoch": 26363.636363636364,
      "eval_loss": 5.197956085205078,
      "eval_runtime": 0.9475,
      "eval_samples_per_second": 10.554,
      "eval_steps_per_second": 5.277,
      "step": 145000
    },
    {
      "epoch": 26365.454545454544,
      "grad_norm": 0.0016908947145566344,
      "learning_rate": 3.367326552228045e-09,
      "loss": 0.0009,
      "step": 145010
    },
    {
      "epoch": 26367.272727272728,
      "grad_norm": 0.22402070462703705,
      "learning_rate": 3.353858870176529e-09,
      "loss": 0.0012,
      "step": 145020
    },
    {
      "epoch": 26369.090909090908,
      "grad_norm": 0.22497273981571198,
      "learning_rate": 3.3404180836033026e-09,
      "loss": 0.001,
      "step": 145030
    },
    {
      "epoch": 26370.909090909092,
      "grad_norm": 0.17507018148899078,
      "learning_rate": 3.327004193236227e-09,
      "loss": 0.0012,
      "step": 145040
    },
    {
      "epoch": 26372.727272727272,
      "grad_norm": 0.220269113779068,
      "learning_rate": 3.313617199801777e-09,
      "loss": 0.0012,
      "step": 145050
    },
    {
      "epoch": 26374.545454545456,
      "grad_norm": 0.0007461701170541346,
      "learning_rate": 3.300257104024873e-09,
      "loss": 0.0012,
      "step": 145060
    },
    {
      "epoch": 26376.363636363636,
      "grad_norm": 0.21813541650772095,
      "learning_rate": 3.286923906628991e-09,
      "loss": 0.0009,
      "step": 145070
    },
    {
      "epoch": 26378.18181818182,
      "grad_norm": 0.04585959017276764,
      "learning_rate": 3.273617608336221e-09,
      "loss": 0.0012,
      "step": 145080
    },
    {
      "epoch": 26380.0,
      "grad_norm": 0.1705428957939148,
      "learning_rate": 3.2603382098671527e-09,
      "loss": 0.0009,
      "step": 145090
    },
    {
      "epoch": 26381.81818181818,
      "grad_norm": 0.22250717878341675,
      "learning_rate": 3.247085711940878e-09,
      "loss": 0.0012,
      "step": 145100
    },
    {
      "epoch": 26383.636363636364,
      "grad_norm": 0.0005720713525079191,
      "learning_rate": 3.2338601152751e-09,
      "loss": 0.001,
      "step": 145110
    },
    {
      "epoch": 26385.454545454544,
      "grad_norm": 0.21727026998996735,
      "learning_rate": 3.2206614205860794e-09,
      "loss": 0.0012,
      "step": 145120
    },
    {
      "epoch": 26387.272727272728,
      "grad_norm": 0.20985914766788483,
      "learning_rate": 3.2074896285885224e-09,
      "loss": 0.0012,
      "step": 145130
    },
    {
      "epoch": 26389.090909090908,
      "grad_norm": 0.18828456103801727,
      "learning_rate": 3.1943447399958024e-09,
      "loss": 0.0014,
      "step": 145140
    },
    {
      "epoch": 26390.909090909092,
      "grad_norm": 0.0004987218999303877,
      "learning_rate": 3.1812267555196836e-09,
      "loss": 0.0009,
      "step": 145150
    },
    {
      "epoch": 26392.727272727272,
      "grad_norm": 0.0006086437497287989,
      "learning_rate": 3.1681356758706536e-09,
      "loss": 0.0008,
      "step": 145160
    },
    {
      "epoch": 26394.545454545456,
      "grad_norm": 0.17349597811698914,
      "learning_rate": 3.1550715017575892e-09,
      "loss": 0.0013,
      "step": 145170
    },
    {
      "epoch": 26396.363636363636,
      "grad_norm": 0.2192908525466919,
      "learning_rate": 3.1420342338879803e-09,
      "loss": 0.0011,
      "step": 145180
    },
    {
      "epoch": 26398.18181818182,
      "grad_norm": 0.001096682040952146,
      "learning_rate": 3.1290238729678733e-09,
      "loss": 0.001,
      "step": 145190
    },
    {
      "epoch": 26400.0,
      "grad_norm": 0.27286550402641296,
      "learning_rate": 3.116040419701815e-09,
      "loss": 0.0012,
      "step": 145200
    },
    {
      "epoch": 26401.81818181818,
      "grad_norm": 0.00041991431498900056,
      "learning_rate": 3.103083874792911e-09,
      "loss": 0.001,
      "step": 145210
    },
    {
      "epoch": 26403.636363636364,
      "grad_norm": 0.28975996375083923,
      "learning_rate": 3.0901542389428214e-09,
      "loss": 0.0011,
      "step": 145220
    },
    {
      "epoch": 26405.454545454544,
      "grad_norm": 0.011613969691097736,
      "learning_rate": 3.0772515128517087e-09,
      "loss": 0.0012,
      "step": 145230
    },
    {
      "epoch": 26407.272727272728,
      "grad_norm": 0.20993001759052277,
      "learning_rate": 3.0643756972183467e-09,
      "loss": 0.001,
      "step": 145240
    },
    {
      "epoch": 26409.090909090908,
      "grad_norm": 0.27054569125175476,
      "learning_rate": 3.051526792740011e-09,
      "loss": 0.0013,
      "step": 145250
    },
    {
      "epoch": 26410.909090909092,
      "grad_norm": 0.0021479434799402952,
      "learning_rate": 3.0387048001125348e-09,
      "loss": 0.0008,
      "step": 145260
    },
    {
      "epoch": 26412.727272727272,
      "grad_norm": 0.2753202021121979,
      "learning_rate": 3.0259097200302508e-09,
      "loss": 0.001,
      "step": 145270
    },
    {
      "epoch": 26414.545454545456,
      "grad_norm": 0.0005156279657967389,
      "learning_rate": 3.0131415531861048e-09,
      "loss": 0.0009,
      "step": 145280
    },
    {
      "epoch": 26416.363636363636,
      "grad_norm": 0.0005102770519442856,
      "learning_rate": 3.0004003002714883e-09,
      "loss": 0.0012,
      "step": 145290
    },
    {
      "epoch": 26418.18181818182,
      "grad_norm": 0.0006387952598743141,
      "learning_rate": 2.9876859619764606e-09,
      "loss": 0.0012,
      "step": 145300
    },
    {
      "epoch": 26420.0,
      "grad_norm": 0.17133505642414093,
      "learning_rate": 2.974998538989526e-09,
      "loss": 0.0012,
      "step": 145310
    },
    {
      "epoch": 26421.81818181818,
      "grad_norm": 0.0005335149471648037,
      "learning_rate": 2.962338031997691e-09,
      "loss": 0.0009,
      "step": 145320
    },
    {
      "epoch": 26423.636363636364,
      "grad_norm": 0.000604812114033848,
      "learning_rate": 2.94970444168674e-09,
      "loss": 0.001,
      "step": 145330
    },
    {
      "epoch": 26425.454545454544,
      "grad_norm": 0.16635262966156006,
      "learning_rate": 2.9370977687406815e-09,
      "loss": 0.0012,
      "step": 145340
    },
    {
      "epoch": 26427.272727272728,
      "grad_norm": 0.2064175307750702,
      "learning_rate": 2.924518013842303e-09,
      "loss": 0.0013,
      "step": 145350
    },
    {
      "epoch": 26429.090909090908,
      "grad_norm": 0.1734384447336197,
      "learning_rate": 2.9119651776728372e-09,
      "loss": 0.0011,
      "step": 145360
    },
    {
      "epoch": 26430.909090909092,
      "grad_norm": 0.0017281359760090709,
      "learning_rate": 2.8994392609120176e-09,
      "loss": 0.001,
      "step": 145370
    },
    {
      "epoch": 26432.727272727272,
      "grad_norm": 0.0006218547350727022,
      "learning_rate": 2.8869402642382467e-09,
      "loss": 0.001,
      "step": 145380
    },
    {
      "epoch": 26434.545454545456,
      "grad_norm": 0.27832895517349243,
      "learning_rate": 2.874468188328427e-09,
      "loss": 0.0011,
      "step": 145390
    },
    {
      "epoch": 26436.363636363636,
      "grad_norm": 0.16355128586292267,
      "learning_rate": 2.862023033857852e-09,
      "loss": 0.0011,
      "step": 145400
    },
    {
      "epoch": 26438.18181818182,
      "grad_norm": 0.0018451595678925514,
      "learning_rate": 2.849604801500538e-09,
      "loss": 0.001,
      "step": 145410
    },
    {
      "epoch": 26440.0,
      "grad_norm": 0.17112329602241516,
      "learning_rate": 2.837213491929058e-09,
      "loss": 0.0012,
      "step": 145420
    },
    {
      "epoch": 26441.81818181818,
      "grad_norm": 0.0003831380163319409,
      "learning_rate": 2.82484910581432e-09,
      "loss": 0.0008,
      "step": 145430
    },
    {
      "epoch": 26443.636363636364,
      "grad_norm": 0.2792802155017853,
      "learning_rate": 2.81251164382601e-09,
      "loss": 0.0013,
      "step": 145440
    },
    {
      "epoch": 26445.454545454544,
      "grad_norm": 0.00040768750477582216,
      "learning_rate": 2.800201106632205e-09,
      "loss": 0.0012,
      "step": 145450
    },
    {
      "epoch": 26447.272727272728,
      "grad_norm": 0.2638751268386841,
      "learning_rate": 2.7879174948995945e-09,
      "loss": 0.0014,
      "step": 145460
    },
    {
      "epoch": 26449.090909090908,
      "grad_norm": 0.20691218972206116,
      "learning_rate": 2.7756608092933677e-09,
      "loss": 0.0009,
      "step": 145470
    },
    {
      "epoch": 26450.909090909092,
      "grad_norm": 0.22069020569324493,
      "learning_rate": 2.7634310504773272e-09,
      "loss": 0.001,
      "step": 145480
    },
    {
      "epoch": 26452.727272727272,
      "grad_norm": 0.18001320958137512,
      "learning_rate": 2.7512282191136658e-09,
      "loss": 0.0012,
      "step": 145490
    },
    {
      "epoch": 26454.545454545456,
      "grad_norm": 0.0005496389348991215,
      "learning_rate": 2.739052315863355e-09,
      "loss": 0.001,
      "step": 145500
    },
    {
      "epoch": 26454.545454545456,
      "eval_loss": 5.155248165130615,
      "eval_runtime": 0.95,
      "eval_samples_per_second": 10.527,
      "eval_steps_per_second": 5.263,
      "step": 145500
    },
    {
      "epoch": 26456.363636363636,
      "grad_norm": 0.001187357702292502,
      "learning_rate": 2.7269033413856447e-09,
      "loss": 0.001,
      "step": 145510
    },
    {
      "epoch": 26458.18181818182,
      "grad_norm": 0.16715490818023682,
      "learning_rate": 2.7147812963385086e-09,
      "loss": 0.0013,
      "step": 145520
    },
    {
      "epoch": 26460.0,
      "grad_norm": 0.20693981647491455,
      "learning_rate": 2.7026861813783665e-09,
      "loss": 0.001,
      "step": 145530
    },
    {
      "epoch": 26461.81818181818,
      "grad_norm": 0.17741742730140686,
      "learning_rate": 2.6906179971603602e-09,
      "loss": 0.001,
      "step": 145540
    },
    {
      "epoch": 26463.636363636364,
      "grad_norm": 0.15030045807361603,
      "learning_rate": 2.6785767443378017e-09,
      "loss": 0.0012,
      "step": 145550
    },
    {
      "epoch": 26465.454545454544,
      "grad_norm": 0.0008621322340331972,
      "learning_rate": 2.6665624235629456e-09,
      "loss": 0.0012,
      "step": 145560
    },
    {
      "epoch": 26467.272727272728,
      "grad_norm": 0.17238439619541168,
      "learning_rate": 2.654575035486384e-09,
      "loss": 0.0009,
      "step": 145570
    },
    {
      "epoch": 26469.090909090908,
      "grad_norm": 0.203616201877594,
      "learning_rate": 2.6426145807573186e-09,
      "loss": 0.001,
      "step": 145580
    },
    {
      "epoch": 26470.909090909092,
      "grad_norm": 0.00043491178075782955,
      "learning_rate": 2.630681060023343e-09,
      "loss": 0.0012,
      "step": 145590
    },
    {
      "epoch": 26472.727272727272,
      "grad_norm": 0.20550596714019775,
      "learning_rate": 2.618774473930829e-09,
      "loss": 0.0012,
      "step": 145600
    },
    {
      "epoch": 26474.545454545456,
      "grad_norm": 0.0004808774683624506,
      "learning_rate": 2.6068948231244835e-09,
      "loss": 0.0007,
      "step": 145610
    },
    {
      "epoch": 26476.363636363636,
      "grad_norm": 0.0004191115440335125,
      "learning_rate": 2.59504210824768e-09,
      "loss": 0.0013,
      "step": 145620
    },
    {
      "epoch": 26478.18181818182,
      "grad_norm": 0.007101644296199083,
      "learning_rate": 2.5832163299422947e-09,
      "loss": 0.0012,
      "step": 145630
    },
    {
      "epoch": 26480.0,
      "grad_norm": 0.0006380415288731456,
      "learning_rate": 2.571417488848704e-09,
      "loss": 0.0009,
      "step": 145640
    },
    {
      "epoch": 26481.81818181818,
      "grad_norm": 0.011003362946212292,
      "learning_rate": 2.559645585605896e-09,
      "loss": 0.001,
      "step": 145650
    },
    {
      "epoch": 26483.636363636364,
      "grad_norm": 0.0008309854893013835,
      "learning_rate": 2.5479006208514176e-09,
      "loss": 0.0009,
      "step": 145660
    },
    {
      "epoch": 26485.454545454544,
      "grad_norm": 0.2803306579589844,
      "learning_rate": 2.5361825952212033e-09,
      "loss": 0.0015,
      "step": 145670
    },
    {
      "epoch": 26487.272727272728,
      "grad_norm": 0.0008077833917923272,
      "learning_rate": 2.524491509349913e-09,
      "loss": 0.001,
      "step": 145680
    },
    {
      "epoch": 26489.090909090908,
      "grad_norm": 0.22229644656181335,
      "learning_rate": 2.5128273638706508e-09,
      "loss": 0.0009,
      "step": 145690
    },
    {
      "epoch": 26490.909090909092,
      "grad_norm": 0.1827010065317154,
      "learning_rate": 2.5011901594150787e-09,
      "loss": 0.0012,
      "step": 145700
    },
    {
      "epoch": 26492.727272727272,
      "grad_norm": 0.2676897644996643,
      "learning_rate": 2.4895798966133583e-09,
      "loss": 0.0012,
      "step": 145710
    },
    {
      "epoch": 26494.545454545456,
      "grad_norm": 0.22111298143863678,
      "learning_rate": 2.477996576094321e-09,
      "loss": 0.001,
      "step": 145720
    },
    {
      "epoch": 26496.363636363636,
      "grad_norm": 0.17137907445430756,
      "learning_rate": 2.4664401984851867e-09,
      "loss": 0.0009,
      "step": 145730
    },
    {
      "epoch": 26498.18181818182,
      "grad_norm": 0.22276191413402557,
      "learning_rate": 2.4549107644117883e-09,
      "loss": 0.0012,
      "step": 145740
    },
    {
      "epoch": 26500.0,
      "grad_norm": 0.0012858499540016055,
      "learning_rate": 2.4434082744984595e-09,
      "loss": 0.0011,
      "step": 145750
    },
    {
      "epoch": 26501.81818181818,
      "grad_norm": 0.0005000587552785873,
      "learning_rate": 2.431932729368202e-09,
      "loss": 0.0009,
      "step": 145760
    },
    {
      "epoch": 26503.636363636364,
      "grad_norm": 0.17551058530807495,
      "learning_rate": 2.4204841296424085e-09,
      "loss": 0.0013,
      "step": 145770
    },
    {
      "epoch": 26505.454545454544,
      "grad_norm": 0.2009403258562088,
      "learning_rate": 2.4090624759410816e-09,
      "loss": 0.0009,
      "step": 145780
    },
    {
      "epoch": 26507.272727272728,
      "grad_norm": 0.00040385901229456067,
      "learning_rate": 2.3976677688827274e-09,
      "loss": 0.0012,
      "step": 145790
    },
    {
      "epoch": 26509.090909090908,
      "grad_norm": 0.21963460743427277,
      "learning_rate": 2.3863000090844076e-09,
      "loss": 0.0012,
      "step": 145800
    },
    {
      "epoch": 26510.909090909092,
      "grad_norm": 0.000800561101641506,
      "learning_rate": 2.3749591971617965e-09,
      "loss": 0.001,
      "step": 145810
    },
    {
      "epoch": 26512.727272727272,
      "grad_norm": 0.2182128131389618,
      "learning_rate": 2.363645333729014e-09,
      "loss": 0.0014,
      "step": 145820
    },
    {
      "epoch": 26514.545454545456,
      "grad_norm": 0.170731782913208,
      "learning_rate": 2.352358419398681e-09,
      "loss": 0.0009,
      "step": 145830
    },
    {
      "epoch": 26516.363636363636,
      "grad_norm": 0.20980365574359894,
      "learning_rate": 2.3410984547821975e-09,
      "loss": 0.0015,
      "step": 145840
    },
    {
      "epoch": 26518.18181818182,
      "grad_norm": 0.0013474717270582914,
      "learning_rate": 2.329865440489187e-09,
      "loss": 0.0008,
      "step": 145850
    },
    {
      "epoch": 26520.0,
      "grad_norm": 0.0005029939347878098,
      "learning_rate": 2.3186593771280517e-09,
      "loss": 0.0012,
      "step": 145860
    },
    {
      "epoch": 26521.81818181818,
      "grad_norm": 0.0007569562876597047,
      "learning_rate": 2.3074802653055835e-09,
      "loss": 0.001,
      "step": 145870
    },
    {
      "epoch": 26523.636363636364,
      "grad_norm": 0.0006724810227751732,
      "learning_rate": 2.2963281056272434e-09,
      "loss": 0.001,
      "step": 145880
    },
    {
      "epoch": 26525.454545454544,
      "grad_norm": 0.0015014039818197489,
      "learning_rate": 2.2852028986968808e-09,
      "loss": 0.001,
      "step": 145890
    },
    {
      "epoch": 26527.272727272728,
      "grad_norm": 0.0008060906548053026,
      "learning_rate": 2.2741046451170698e-09,
      "loss": 0.001,
      "step": 145900
    },
    {
      "epoch": 26529.090909090908,
      "grad_norm": 0.14783091843128204,
      "learning_rate": 2.2630333454887187e-09,
      "loss": 0.0011,
      "step": 145910
    },
    {
      "epoch": 26530.909090909092,
      "grad_norm": 0.1550268679857254,
      "learning_rate": 2.251989000411514e-09,
      "loss": 0.0012,
      "step": 145920
    },
    {
      "epoch": 26532.727272727272,
      "grad_norm": 0.16883914172649384,
      "learning_rate": 2.2409716104834775e-09,
      "loss": 0.0009,
      "step": 145930
    },
    {
      "epoch": 26534.545454545456,
      "grad_norm": 0.0004034097073599696,
      "learning_rate": 2.2299811763012434e-09,
      "loss": 0.0013,
      "step": 145940
    },
    {
      "epoch": 26536.363636363636,
      "grad_norm": 0.0007747287745587528,
      "learning_rate": 2.2190176984600016e-09,
      "loss": 0.0009,
      "step": 145950
    },
    {
      "epoch": 26538.18181818182,
      "grad_norm": 0.00039156299317255616,
      "learning_rate": 2.2080811775535004e-09,
      "loss": 0.001,
      "step": 145960
    },
    {
      "epoch": 26540.0,
      "grad_norm": 0.17394176125526428,
      "learning_rate": 2.1971716141739316e-09,
      "loss": 0.0012,
      "step": 145970
    },
    {
      "epoch": 26541.81818181818,
      "grad_norm": 0.0008403902756981552,
      "learning_rate": 2.1862890089121564e-09,
      "loss": 0.001,
      "step": 145980
    },
    {
      "epoch": 26543.636363636364,
      "grad_norm": 0.0008328749681822956,
      "learning_rate": 2.1754333623574816e-09,
      "loss": 0.0012,
      "step": 145990
    },
    {
      "epoch": 26545.454545454544,
      "grad_norm": 0.0006817042594775558,
      "learning_rate": 2.1646046750978253e-09,
      "loss": 0.001,
      "step": 146000
    },
    {
      "epoch": 26545.454545454544,
      "eval_loss": 5.197636127471924,
      "eval_runtime": 0.9505,
      "eval_samples_per_second": 10.52,
      "eval_steps_per_second": 5.26,
      "step": 146000
    },
    {
      "epoch": 26547.272727272728,
      "grad_norm": 0.22169972956180573,
      "learning_rate": 2.153802947719552e-09,
      "loss": 0.001,
      "step": 146010
    },
    {
      "epoch": 26549.090909090908,
      "grad_norm": 0.0012350290780887008,
      "learning_rate": 2.143028180807582e-09,
      "loss": 0.001,
      "step": 146020
    },
    {
      "epoch": 26550.909090909092,
      "grad_norm": 0.22334201633930206,
      "learning_rate": 2.132280374945561e-09,
      "loss": 0.0012,
      "step": 146030
    },
    {
      "epoch": 26552.727272727272,
      "grad_norm": 0.0006676122429780662,
      "learning_rate": 2.1215595307154666e-09,
      "loss": 0.0011,
      "step": 146040
    },
    {
      "epoch": 26554.545454545456,
      "grad_norm": 0.19933076202869415,
      "learning_rate": 2.1108656486977795e-09,
      "loss": 0.0011,
      "step": 146050
    },
    {
      "epoch": 26556.363636363636,
      "grad_norm": 0.0007592677720822394,
      "learning_rate": 2.100198729471758e-09,
      "loss": 0.0009,
      "step": 146060
    },
    {
      "epoch": 26558.18181818182,
      "grad_norm": 0.00043157380423508584,
      "learning_rate": 2.089558773614941e-09,
      "loss": 0.0012,
      "step": 146070
    },
    {
      "epoch": 26560.0,
      "grad_norm": 0.019109414890408516,
      "learning_rate": 2.078945781703645e-09,
      "loss": 0.0012,
      "step": 146080
    },
    {
      "epoch": 26561.81818181818,
      "grad_norm": 0.0003787270688917488,
      "learning_rate": 2.068359754312465e-09,
      "loss": 0.0009,
      "step": 146090
    },
    {
      "epoch": 26563.636363636364,
      "grad_norm": 0.17582054436206818,
      "learning_rate": 2.057800692014833e-09,
      "loss": 0.0012,
      "step": 146100
    },
    {
      "epoch": 26565.454545454544,
      "grad_norm": 0.22428075969219208,
      "learning_rate": 2.0472685953824566e-09,
      "loss": 0.0014,
      "step": 146110
    },
    {
      "epoch": 26567.272727272728,
      "grad_norm": 0.0010057357139885426,
      "learning_rate": 2.036763464985769e-09,
      "loss": 0.0008,
      "step": 146120
    },
    {
      "epoch": 26569.090909090908,
      "grad_norm": 0.22742705047130585,
      "learning_rate": 2.0262853013935377e-09,
      "loss": 0.0013,
      "step": 146130
    },
    {
      "epoch": 26570.909090909092,
      "grad_norm": 0.0004686153552029282,
      "learning_rate": 2.015834105173364e-09,
      "loss": 0.0009,
      "step": 146140
    },
    {
      "epoch": 26572.727272727272,
      "grad_norm": 0.17312534153461456,
      "learning_rate": 2.0054098768911842e-09,
      "loss": 0.0013,
      "step": 146150
    },
    {
      "epoch": 26574.545454545456,
      "grad_norm": 0.0012933543184772134,
      "learning_rate": 1.9950126171114355e-09,
      "loss": 0.001,
      "step": 146160
    },
    {
      "epoch": 26576.363636363636,
      "grad_norm": 0.0010394926648586988,
      "learning_rate": 1.984642326397168e-09,
      "loss": 0.0007,
      "step": 146170
    },
    {
      "epoch": 26578.18181818182,
      "grad_norm": 0.1691300868988037,
      "learning_rate": 1.974299005310098e-09,
      "loss": 0.0013,
      "step": 146180
    },
    {
      "epoch": 26580.0,
      "grad_norm": 0.22270305454730988,
      "learning_rate": 1.963982654410279e-09,
      "loss": 0.001,
      "step": 146190
    },
    {
      "epoch": 26581.81818181818,
      "grad_norm": 0.27260273694992065,
      "learning_rate": 1.953693274256374e-09,
      "loss": 0.0012,
      "step": 146200
    },
    {
      "epoch": 26583.636363636364,
      "grad_norm": 0.2242528796195984,
      "learning_rate": 1.943430865405604e-09,
      "loss": 0.0012,
      "step": 146210
    },
    {
      "epoch": 26585.454545454544,
      "grad_norm": 0.16843141615390778,
      "learning_rate": 1.9331954284137476e-09,
      "loss": 0.001,
      "step": 146220
    },
    {
      "epoch": 26587.272727272728,
      "grad_norm": 0.1682610958814621,
      "learning_rate": 1.922986963835138e-09,
      "loss": 0.0009,
      "step": 146230
    },
    {
      "epoch": 26589.090909090908,
      "grad_norm": 0.18902236223220825,
      "learning_rate": 1.9128054722225007e-09,
      "loss": 0.001,
      "step": 146240
    },
    {
      "epoch": 26590.909090909092,
      "grad_norm": 0.22010427713394165,
      "learning_rate": 1.9026509541272273e-09,
      "loss": 0.0012,
      "step": 146250
    },
    {
      "epoch": 26592.727272727272,
      "grad_norm": 0.0005624897312372923,
      "learning_rate": 1.8925234100993226e-09,
      "loss": 0.0009,
      "step": 146260
    },
    {
      "epoch": 26594.545454545456,
      "grad_norm": 0.20083996653556824,
      "learning_rate": 1.8824228406871257e-09,
      "loss": 0.0016,
      "step": 146270
    },
    {
      "epoch": 26596.363636363636,
      "grad_norm": 0.22397848963737488,
      "learning_rate": 1.872349246437699e-09,
      "loss": 0.0009,
      "step": 146280
    },
    {
      "epoch": 26598.18181818182,
      "grad_norm": 0.012267788872122765,
      "learning_rate": 1.8623026278964948e-09,
      "loss": 0.0012,
      "step": 146290
    },
    {
      "epoch": 26600.0,
      "grad_norm": 0.2646920680999756,
      "learning_rate": 1.8522829856076894e-09,
      "loss": 0.0009,
      "step": 146300
    },
    {
      "epoch": 26601.81818181818,
      "grad_norm": 0.0005819395300932229,
      "learning_rate": 1.842290320113793e-09,
      "loss": 0.0009,
      "step": 146310
    },
    {
      "epoch": 26603.636363636364,
      "grad_norm": 0.179500550031662,
      "learning_rate": 1.8323246319559282e-09,
      "loss": 0.0013,
      "step": 146320
    },
    {
      "epoch": 26605.454545454544,
      "grad_norm": 0.0005414299084804952,
      "learning_rate": 1.8223859216738858e-09,
      "loss": 0.0009,
      "step": 146330
    },
    {
      "epoch": 26607.272727272728,
      "grad_norm": 0.0008902906556613743,
      "learning_rate": 1.812474189805846e-09,
      "loss": 0.0012,
      "step": 146340
    },
    {
      "epoch": 26609.090909090908,
      "grad_norm": 0.20928987860679626,
      "learning_rate": 1.802589436888602e-09,
      "loss": 0.0012,
      "step": 146350
    },
    {
      "epoch": 26610.909090909092,
      "grad_norm": 0.006934259086847305,
      "learning_rate": 1.7927316634573364e-09,
      "loss": 0.001,
      "step": 146360
    },
    {
      "epoch": 26612.727272727272,
      "grad_norm": 0.029125355184078217,
      "learning_rate": 1.7829008700460114e-09,
      "loss": 0.0012,
      "step": 146370
    },
    {
      "epoch": 26614.545454545456,
      "grad_norm": 0.20594950020313263,
      "learning_rate": 1.773097057186923e-09,
      "loss": 0.0009,
      "step": 146380
    },
    {
      "epoch": 26616.363636363636,
      "grad_norm": 0.21110978722572327,
      "learning_rate": 1.7633202254110357e-09,
      "loss": 0.0014,
      "step": 146390
    },
    {
      "epoch": 26618.18181818182,
      "grad_norm": 0.0009873703820630908,
      "learning_rate": 1.7535703752478147e-09,
      "loss": 0.0008,
      "step": 146400
    },
    {
      "epoch": 26620.0,
      "grad_norm": 0.17486314475536346,
      "learning_rate": 1.7438475072252822e-09,
      "loss": 0.0012,
      "step": 146410
    },
    {
      "epoch": 26621.81818181818,
      "grad_norm": 0.22384753823280334,
      "learning_rate": 1.7341516218698505e-09,
      "loss": 0.0012,
      "step": 146420
    },
    {
      "epoch": 26623.636363636364,
      "grad_norm": 0.0006198177579790354,
      "learning_rate": 1.7244827197067102e-09,
      "loss": 0.0009,
      "step": 146430
    },
    {
      "epoch": 26625.454545454544,
      "grad_norm": 0.17007660865783691,
      "learning_rate": 1.714840801259443e-09,
      "loss": 0.001,
      "step": 146440
    },
    {
      "epoch": 26627.272727272728,
      "grad_norm": 0.16839896142482758,
      "learning_rate": 1.7052258670501308e-09,
      "loss": 0.0011,
      "step": 146450
    },
    {
      "epoch": 26629.090909090908,
      "grad_norm": 0.41586554050445557,
      "learning_rate": 1.6956379175995793e-09,
      "loss": 0.0012,
      "step": 146460
    },
    {
      "epoch": 26630.909090909092,
      "grad_norm": 0.00043394198291935027,
      "learning_rate": 1.6860769534269291e-09,
      "loss": 0.0011,
      "step": 146470
    },
    {
      "epoch": 26632.727272727272,
      "grad_norm": 0.21238066256046295,
      "learning_rate": 1.6765429750499882e-09,
      "loss": 0.001,
      "step": 146480
    },
    {
      "epoch": 26634.545454545456,
      "grad_norm": 0.17180506885051727,
      "learning_rate": 1.6670359829850654e-09,
      "loss": 0.0013,
      "step": 146490
    },
    {
      "epoch": 26636.363636363636,
      "grad_norm": 0.0011540526757016778,
      "learning_rate": 1.6575559777469717e-09,
      "loss": 0.0009,
      "step": 146500
    },
    {
      "epoch": 26636.363636363636,
      "eval_loss": 5.186341285705566,
      "eval_runtime": 0.9484,
      "eval_samples_per_second": 10.544,
      "eval_steps_per_second": 5.272,
      "step": 146500
    },
    {
      "epoch": 26638.18181818182,
      "grad_norm": 0.0011057502124458551,
      "learning_rate": 1.6481029598491292e-09,
      "loss": 0.0008,
      "step": 146510
    },
    {
      "epoch": 26640.0,
      "grad_norm": 0.2115647792816162,
      "learning_rate": 1.6386769298034064e-09,
      "loss": 0.0012,
      "step": 146520
    },
    {
      "epoch": 26641.81818181818,
      "grad_norm": 0.22067499160766602,
      "learning_rate": 1.629277888120284e-09,
      "loss": 0.0012,
      "step": 146530
    },
    {
      "epoch": 26643.636363636364,
      "grad_norm": 0.0012074366677552462,
      "learning_rate": 1.6199058353087991e-09,
      "loss": 0.0009,
      "step": 146540
    },
    {
      "epoch": 26645.454545454544,
      "grad_norm": 0.0010743667371571064,
      "learning_rate": 1.6105607718764346e-09,
      "loss": 0.0011,
      "step": 146550
    },
    {
      "epoch": 26647.272727272728,
      "grad_norm": 0.0009041780140250921,
      "learning_rate": 1.601242698329286e-09,
      "loss": 0.0009,
      "step": 146560
    },
    {
      "epoch": 26649.090909090908,
      "grad_norm": 0.0003244923718739301,
      "learning_rate": 1.5919516151719493e-09,
      "loss": 0.0012,
      "step": 146570
    },
    {
      "epoch": 26650.909090909092,
      "grad_norm": 0.16721729934215546,
      "learning_rate": 1.582687522907633e-09,
      "loss": 0.0012,
      "step": 146580
    },
    {
      "epoch": 26652.727272727272,
      "grad_norm": 0.1771848052740097,
      "learning_rate": 1.5734504220379919e-09,
      "loss": 0.0009,
      "step": 146590
    },
    {
      "epoch": 26654.545454545456,
      "grad_norm": 0.0006204289384186268,
      "learning_rate": 1.5642403130632364e-09,
      "loss": 0.0012,
      "step": 146600
    },
    {
      "epoch": 26656.363636363636,
      "grad_norm": 0.22243687510490417,
      "learning_rate": 1.5550571964820792e-09,
      "loss": 0.0013,
      "step": 146610
    },
    {
      "epoch": 26658.18181818182,
      "grad_norm": 0.2850973606109619,
      "learning_rate": 1.5459010727919552e-09,
      "loss": 0.0012,
      "step": 146620
    },
    {
      "epoch": 26660.0,
      "grad_norm": 0.21007423102855682,
      "learning_rate": 1.536771942488635e-09,
      "loss": 0.0009,
      "step": 146630
    },
    {
      "epoch": 26661.81818181818,
      "grad_norm": 0.0005578320706263185,
      "learning_rate": 1.5276698060665005e-09,
      "loss": 0.0012,
      "step": 146640
    },
    {
      "epoch": 26663.636363636364,
      "grad_norm": 0.2749127149581909,
      "learning_rate": 1.518594664018491e-09,
      "loss": 0.0012,
      "step": 146650
    },
    {
      "epoch": 26665.454545454544,
      "grad_norm": 0.0007836328004486859,
      "learning_rate": 1.5095465168360466e-09,
      "loss": 0.0008,
      "step": 146660
    },
    {
      "epoch": 26667.272727272728,
      "grad_norm": 0.0007081034127622843,
      "learning_rate": 1.5005253650091088e-09,
      "loss": 0.0013,
      "step": 146670
    },
    {
      "epoch": 26669.090909090908,
      "grad_norm": 0.20890198647975922,
      "learning_rate": 1.4915312090263422e-09,
      "loss": 0.001,
      "step": 146680
    },
    {
      "epoch": 26670.909090909092,
      "grad_norm": 0.0005957446992397308,
      "learning_rate": 1.482564049374746e-09,
      "loss": 0.001,
      "step": 146690
    },
    {
      "epoch": 26672.727272727272,
      "grad_norm": 0.18199077248573303,
      "learning_rate": 1.4736238865398765e-09,
      "loss": 0.0012,
      "step": 146700
    },
    {
      "epoch": 26674.545454545456,
      "grad_norm": 0.0003212172887288034,
      "learning_rate": 1.4647107210059572e-09,
      "loss": 0.0009,
      "step": 146710
    },
    {
      "epoch": 26676.363636363636,
      "grad_norm": 0.0009723833063617349,
      "learning_rate": 1.4558245532556578e-09,
      "loss": 0.0013,
      "step": 146720
    },
    {
      "epoch": 26678.18181818182,
      "grad_norm": 0.0006598341278731823,
      "learning_rate": 1.446965383770149e-09,
      "loss": 0.0009,
      "step": 146730
    },
    {
      "epoch": 26680.0,
      "grad_norm": 0.0005828692810609937,
      "learning_rate": 1.4381332130292689e-09,
      "loss": 0.0011,
      "step": 146740
    },
    {
      "epoch": 26681.81818181818,
      "grad_norm": 0.14047205448150635,
      "learning_rate": 1.4293280415113018e-09,
      "loss": 0.0011,
      "step": 146750
    },
    {
      "epoch": 26683.636363636364,
      "grad_norm": 0.18872003257274628,
      "learning_rate": 1.420549869693033e-09,
      "loss": 0.0011,
      "step": 146760
    },
    {
      "epoch": 26685.454545454544,
      "grad_norm": 0.23718903958797455,
      "learning_rate": 1.4117986980498597e-09,
      "loss": 0.0012,
      "step": 146770
    },
    {
      "epoch": 26687.272727272728,
      "grad_norm": 0.0004798928857780993,
      "learning_rate": 1.4030745270557365e-09,
      "loss": 0.0009,
      "step": 146780
    },
    {
      "epoch": 26689.090909090908,
      "grad_norm": 0.1969825178384781,
      "learning_rate": 1.3943773571831186e-09,
      "loss": 0.0013,
      "step": 146790
    },
    {
      "epoch": 26690.909090909092,
      "grad_norm": 0.17595452070236206,
      "learning_rate": 1.3857071889029071e-09,
      "loss": 0.0012,
      "step": 146800
    },
    {
      "epoch": 26692.727272727272,
      "grad_norm": 0.0006195548339746892,
      "learning_rate": 1.3770640226846708e-09,
      "loss": 0.0012,
      "step": 146810
    },
    {
      "epoch": 26694.545454545456,
      "grad_norm": 0.20365867018699646,
      "learning_rate": 1.36844785899648e-09,
      "loss": 0.0008,
      "step": 146820
    },
    {
      "epoch": 26696.363636363636,
      "grad_norm": 0.00040273385820910335,
      "learning_rate": 1.3598586983049608e-09,
      "loss": 0.0009,
      "step": 146830
    },
    {
      "epoch": 26698.18181818182,
      "grad_norm": 0.0008598177810199559,
      "learning_rate": 1.351296541075242e-09,
      "loss": 0.0012,
      "step": 146840
    },
    {
      "epoch": 26700.0,
      "grad_norm": 0.27902737259864807,
      "learning_rate": 1.342761387770952e-09,
      "loss": 0.0013,
      "step": 146850
    },
    {
      "epoch": 26701.81818181818,
      "grad_norm": 0.17218659818172455,
      "learning_rate": 1.3342532388543881e-09,
      "loss": 0.0012,
      "step": 146860
    },
    {
      "epoch": 26703.636363636364,
      "grad_norm": 0.04361095279455185,
      "learning_rate": 1.3257720947861817e-09,
      "loss": 0.001,
      "step": 146870
    },
    {
      "epoch": 26705.454545454544,
      "grad_norm": 0.20884805917739868,
      "learning_rate": 1.317317956025743e-09,
      "loss": 0.0012,
      "step": 146880
    },
    {
      "epoch": 26707.272727272728,
      "grad_norm": 0.2771446406841278,
      "learning_rate": 1.3088908230308726e-09,
      "loss": 0.0011,
      "step": 146890
    },
    {
      "epoch": 26709.090909090908,
      "grad_norm": 0.20835597813129425,
      "learning_rate": 1.300490696257872e-09,
      "loss": 0.0009,
      "step": 146900
    },
    {
      "epoch": 26710.909090909092,
      "grad_norm": 0.000355284457327798,
      "learning_rate": 1.292117576161711e-09,
      "loss": 0.001,
      "step": 146910
    },
    {
      "epoch": 26712.727272727272,
      "grad_norm": 0.1746058613061905,
      "learning_rate": 1.2837714631958595e-09,
      "loss": 0.0012,
      "step": 146920
    },
    {
      "epoch": 26714.545454545456,
      "grad_norm": 0.2792030870914459,
      "learning_rate": 1.2754523578121236e-09,
      "loss": 0.0011,
      "step": 146930
    },
    {
      "epoch": 26716.363636363636,
      "grad_norm": 0.2075044810771942,
      "learning_rate": 1.267160260461253e-09,
      "loss": 0.001,
      "step": 146940
    },
    {
      "epoch": 26718.18181818182,
      "grad_norm": 0.17734377086162567,
      "learning_rate": 1.2588951715921114e-09,
      "loss": 0.0011,
      "step": 146950
    },
    {
      "epoch": 26720.0,
      "grad_norm": 0.20712675154209137,
      "learning_rate": 1.2506570916523406e-09,
      "loss": 0.001,
      "step": 146960
    },
    {
      "epoch": 26721.81818181818,
      "grad_norm": 0.0006642405642196536,
      "learning_rate": 1.2424460210881394e-09,
      "loss": 0.001,
      "step": 146970
    },
    {
      "epoch": 26723.636363636364,
      "grad_norm": 0.2670658230781555,
      "learning_rate": 1.2342619603440963e-09,
      "loss": 0.001,
      "step": 146980
    },
    {
      "epoch": 26725.454545454544,
      "grad_norm": 0.21044161915779114,
      "learning_rate": 1.2261049098634125e-09,
      "loss": 0.0011,
      "step": 146990
    },
    {
      "epoch": 26727.272727272728,
      "grad_norm": 0.1747434437274933,
      "learning_rate": 1.217974870087901e-09,
      "loss": 0.001,
      "step": 147000
    },
    {
      "epoch": 26727.272727272728,
      "eval_loss": 5.205904006958008,
      "eval_runtime": 0.9514,
      "eval_samples_per_second": 10.511,
      "eval_steps_per_second": 5.255,
      "step": 147000
    },
    {
      "epoch": 26729.090909090908,
      "grad_norm": 0.00050010671839118,
      "learning_rate": 1.2098718414577102e-09,
      "loss": 0.001,
      "step": 147010
    },
    {
      "epoch": 26730.909090909092,
      "grad_norm": 0.17279238998889923,
      "learning_rate": 1.2017958244117665e-09,
      "loss": 0.0009,
      "step": 147020
    },
    {
      "epoch": 26732.727272727272,
      "grad_norm": 0.21813015639781952,
      "learning_rate": 1.1937468193873868e-09,
      "loss": 0.0014,
      "step": 147030
    },
    {
      "epoch": 26734.545454545456,
      "grad_norm": 0.20360147953033447,
      "learning_rate": 1.1857248268204444e-09,
      "loss": 0.001,
      "step": 147040
    },
    {
      "epoch": 26736.363636363636,
      "grad_norm": 0.2824992537498474,
      "learning_rate": 1.1777298471453701e-09,
      "loss": 0.0012,
      "step": 147050
    },
    {
      "epoch": 26738.18181818182,
      "grad_norm": 0.21821188926696777,
      "learning_rate": 1.1697618807951503e-09,
      "loss": 0.0012,
      "step": 147060
    },
    {
      "epoch": 26740.0,
      "grad_norm": 0.18126614391803741,
      "learning_rate": 1.161820928201218e-09,
      "loss": 0.001,
      "step": 147070
    },
    {
      "epoch": 26741.81818181818,
      "grad_norm": 0.1695513278245926,
      "learning_rate": 1.1539069897937293e-09,
      "loss": 0.0012,
      "step": 147080
    },
    {
      "epoch": 26743.636363636364,
      "grad_norm": 0.1682066172361374,
      "learning_rate": 1.1460200660011188e-09,
      "loss": 0.001,
      "step": 147090
    },
    {
      "epoch": 26745.454545454544,
      "grad_norm": 0.0006584323127754033,
      "learning_rate": 1.1381601572505451e-09,
      "loss": 0.0007,
      "step": 147100
    },
    {
      "epoch": 26747.272727272728,
      "grad_norm": 0.0006550810649059713,
      "learning_rate": 1.1303272639677231e-09,
      "loss": 0.0015,
      "step": 147110
    },
    {
      "epoch": 26749.090909090908,
      "grad_norm": 0.2104092836380005,
      "learning_rate": 1.1225213865767024e-09,
      "loss": 0.0011,
      "step": 147120
    },
    {
      "epoch": 26750.909090909092,
      "grad_norm": 0.2600442171096802,
      "learning_rate": 1.1147425255003673e-09,
      "loss": 0.0011,
      "step": 147130
    },
    {
      "epoch": 26752.727272727272,
      "grad_norm": 0.16934578120708466,
      "learning_rate": 1.1069906811598806e-09,
      "loss": 0.0012,
      "step": 147140
    },
    {
      "epoch": 26754.545454545456,
      "grad_norm": 0.0003870452055707574,
      "learning_rate": 1.0992658539750177e-09,
      "loss": 0.0008,
      "step": 147150
    },
    {
      "epoch": 26756.363636363636,
      "grad_norm": 0.271720826625824,
      "learning_rate": 1.0915680443641662e-09,
      "loss": 0.0015,
      "step": 147160
    },
    {
      "epoch": 26758.18181818182,
      "grad_norm": 0.17210526764392853,
      "learning_rate": 1.0838972527441592e-09,
      "loss": 0.0009,
      "step": 147170
    },
    {
      "epoch": 26760.0,
      "grad_norm": 0.2090275138616562,
      "learning_rate": 1.0762534795303869e-09,
      "loss": 0.001,
      "step": 147180
    },
    {
      "epoch": 26761.81818181818,
      "grad_norm": 0.20454679429531097,
      "learning_rate": 1.0686367251368512e-09,
      "loss": 0.0012,
      "step": 147190
    },
    {
      "epoch": 26763.636363636364,
      "grad_norm": 0.2233370691537857,
      "learning_rate": 1.061046989976e-09,
      "loss": 0.001,
      "step": 147200
    },
    {
      "epoch": 26765.454545454544,
      "grad_norm": 0.26408490538597107,
      "learning_rate": 1.0534842744588379e-09,
      "loss": 0.001,
      "step": 147210
    },
    {
      "epoch": 26767.272727272728,
      "grad_norm": 0.22071605920791626,
      "learning_rate": 1.0459485789949263e-09,
      "loss": 0.001,
      "step": 147220
    },
    {
      "epoch": 26769.090909090908,
      "grad_norm": 0.00042056114762090147,
      "learning_rate": 1.0384399039923274e-09,
      "loss": 0.001,
      "step": 147230
    },
    {
      "epoch": 26770.909090909092,
      "grad_norm": 0.012244337238371372,
      "learning_rate": 1.0309582498577719e-09,
      "loss": 0.0012,
      "step": 147240
    },
    {
      "epoch": 26772.727272727272,
      "grad_norm": 0.1717064082622528,
      "learning_rate": 1.023503616996324e-09,
      "loss": 0.001,
      "step": 147250
    },
    {
      "epoch": 26774.545454545456,
      "grad_norm": 0.0004809608217328787,
      "learning_rate": 1.0160760058116612e-09,
      "loss": 0.001,
      "step": 147260
    },
    {
      "epoch": 26776.363636363636,
      "grad_norm": 0.22322048246860504,
      "learning_rate": 1.0086754167060729e-09,
      "loss": 0.0013,
      "step": 147270
    },
    {
      "epoch": 26778.18181818182,
      "grad_norm": 0.20249997079372406,
      "learning_rate": 1.0013018500803494e-09,
      "loss": 0.0012,
      "step": 147280
    },
    {
      "epoch": 26780.0,
      "grad_norm": 0.1467953622341156,
      "learning_rate": 9.939553063337824e-10,
      "loss": 0.0008,
      "step": 147290
    },
    {
      "epoch": 26781.81818181818,
      "grad_norm": 0.0007494595483876765,
      "learning_rate": 9.866357858642205e-10,
      "loss": 0.0012,
      "step": 147300
    },
    {
      "epoch": 26783.636363636364,
      "grad_norm": 0.27353549003601074,
      "learning_rate": 9.793432890680131e-10,
      "loss": 0.0012,
      "step": 147310
    },
    {
      "epoch": 26785.454545454544,
      "grad_norm": 0.20054392516613007,
      "learning_rate": 9.720778163401222e-10,
      "loss": 0.0007,
      "step": 147320
    },
    {
      "epoch": 26787.272727272728,
      "grad_norm": 0.00047461999929510057,
      "learning_rate": 9.648393680740107e-10,
      "loss": 0.0011,
      "step": 147330
    },
    {
      "epoch": 26789.090909090908,
      "grad_norm": 0.22561199963092804,
      "learning_rate": 9.576279446615876e-10,
      "loss": 0.0012,
      "step": 147340
    },
    {
      "epoch": 26790.909090909092,
      "grad_norm": 0.0004158012452535331,
      "learning_rate": 9.504435464934846e-10,
      "loss": 0.0012,
      "step": 147350
    },
    {
      "epoch": 26792.727272727272,
      "grad_norm": 0.29065263271331787,
      "learning_rate": 9.432861739586683e-10,
      "loss": 0.0013,
      "step": 147360
    },
    {
      "epoch": 26794.545454545456,
      "grad_norm": 0.0019043446518480778,
      "learning_rate": 9.36155827444829e-10,
      "loss": 0.0006,
      "step": 147370
    },
    {
      "epoch": 26796.363636363636,
      "grad_norm": 0.0005986441974528134,
      "learning_rate": 9.290525073381017e-10,
      "loss": 0.0015,
      "step": 147380
    },
    {
      "epoch": 26798.18181818182,
      "grad_norm": 0.0003965555806644261,
      "learning_rate": 9.219762140231235e-10,
      "loss": 0.0009,
      "step": 147390
    },
    {
      "epoch": 26800.0,
      "grad_norm": 0.2097007930278778,
      "learning_rate": 9.149269478830879e-10,
      "loss": 0.0012,
      "step": 147400
    },
    {
      "epoch": 26801.81818181818,
      "grad_norm": 0.17395594716072083,
      "learning_rate": 9.079047092997449e-10,
      "loss": 0.0012,
      "step": 147410
    },
    {
      "epoch": 26803.636363636364,
      "grad_norm": 0.17088480293750763,
      "learning_rate": 9.009094986534571e-10,
      "loss": 0.0009,
      "step": 147420
    },
    {
      "epoch": 26805.454545454544,
      "grad_norm": 0.20633463561534882,
      "learning_rate": 8.939413163229215e-10,
      "loss": 0.0013,
      "step": 147430
    },
    {
      "epoch": 26807.272727272728,
      "grad_norm": 0.0007166275754570961,
      "learning_rate": 8.870001626856138e-10,
      "loss": 0.0008,
      "step": 147440
    },
    {
      "epoch": 26809.090909090908,
      "grad_norm": 0.17066290974617004,
      "learning_rate": 8.800860381173447e-10,
      "loss": 0.0013,
      "step": 147450
    },
    {
      "epoch": 26810.909090909092,
      "grad_norm": 0.2193906456232071,
      "learning_rate": 8.731989429925923e-10,
      "loss": 0.0009,
      "step": 147460
    },
    {
      "epoch": 26812.727272727272,
      "grad_norm": 0.005485899746417999,
      "learning_rate": 8.663388776843361e-10,
      "loss": 0.0013,
      "step": 147470
    },
    {
      "epoch": 26814.545454545456,
      "grad_norm": 0.20512878894805908,
      "learning_rate": 8.595058425640011e-10,
      "loss": 0.0012,
      "step": 147480
    },
    {
      "epoch": 26816.363636363636,
      "grad_norm": 0.0005016746581532061,
      "learning_rate": 8.526998380016803e-10,
      "loss": 0.0008,
      "step": 147490
    },
    {
      "epoch": 26818.18181818182,
      "grad_norm": 0.0005641247262246907,
      "learning_rate": 8.459208643659121e-10,
      "loss": 0.001,
      "step": 147500
    },
    {
      "epoch": 26818.18181818182,
      "eval_loss": 5.215897560119629,
      "eval_runtime": 0.9513,
      "eval_samples_per_second": 10.512,
      "eval_steps_per_second": 5.256,
      "step": 147500
    },
    {
      "epoch": 26820.0,
      "grad_norm": 0.1685090810060501,
      "learning_rate": 8.391689220238474e-10,
      "loss": 0.0012,
      "step": 147510
    },
    {
      "epoch": 26821.81818181818,
      "grad_norm": 0.2237921506166458,
      "learning_rate": 8.324440113410824e-10,
      "loss": 0.0011,
      "step": 147520
    },
    {
      "epoch": 26823.636363636364,
      "grad_norm": 0.0008552430081181228,
      "learning_rate": 8.25746132681826e-10,
      "loss": 0.0012,
      "step": 147530
    },
    {
      "epoch": 26825.454545454544,
      "grad_norm": 0.0007080080104060471,
      "learning_rate": 8.190752864088435e-10,
      "loss": 0.0008,
      "step": 147540
    },
    {
      "epoch": 26827.272727272728,
      "grad_norm": 0.1754092574119568,
      "learning_rate": 8.124314728833459e-10,
      "loss": 0.0015,
      "step": 147550
    },
    {
      "epoch": 26829.090909090908,
      "grad_norm": 0.16678665578365326,
      "learning_rate": 8.05814692465101e-10,
      "loss": 0.0007,
      "step": 147560
    },
    {
      "epoch": 26830.909090909092,
      "grad_norm": 0.1692834198474884,
      "learning_rate": 7.992249455124889e-10,
      "loss": 0.0012,
      "step": 147570
    },
    {
      "epoch": 26832.727272727272,
      "grad_norm": 0.2215397208929062,
      "learning_rate": 7.926622323822796e-10,
      "loss": 0.0014,
      "step": 147580
    },
    {
      "epoch": 26834.545454545456,
      "grad_norm": 0.2242186963558197,
      "learning_rate": 7.861265534300221e-10,
      "loss": 0.0009,
      "step": 147590
    },
    {
      "epoch": 26836.363636363636,
      "grad_norm": 0.0010373309487476945,
      "learning_rate": 7.79617909009489e-10,
      "loss": 0.0011,
      "step": 147600
    },
    {
      "epoch": 26838.18181818182,
      "grad_norm": 0.2685621678829193,
      "learning_rate": 7.731362994732871e-10,
      "loss": 0.0015,
      "step": 147610
    },
    {
      "epoch": 26840.0,
      "grad_norm": 0.0008464170387014747,
      "learning_rate": 7.666817251723024e-10,
      "loss": 0.0009,
      "step": 147620
    },
    {
      "epoch": 26841.81818181818,
      "grad_norm": 0.0006048691575415432,
      "learning_rate": 7.60254186456144e-10,
      "loss": 0.001,
      "step": 147630
    },
    {
      "epoch": 26843.636363636364,
      "grad_norm": 0.2188113033771515,
      "learning_rate": 7.538536836729225e-10,
      "loss": 0.0012,
      "step": 147640
    },
    {
      "epoch": 26845.454545454544,
      "grad_norm": 0.2753935158252716,
      "learning_rate": 7.474802171691941e-10,
      "loss": 0.001,
      "step": 147650
    },
    {
      "epoch": 26847.272727272728,
      "grad_norm": 0.000476932676974684,
      "learning_rate": 7.411337872900714e-10,
      "loss": 0.0009,
      "step": 147660
    },
    {
      "epoch": 26849.090909090908,
      "grad_norm": 0.0010780879529193044,
      "learning_rate": 7.348143943793905e-10,
      "loss": 0.0012,
      "step": 147670
    },
    {
      "epoch": 26850.909090909092,
      "grad_norm": 0.16778779029846191,
      "learning_rate": 7.285220387792113e-10,
      "loss": 0.0012,
      "step": 147680
    },
    {
      "epoch": 26852.727272727272,
      "grad_norm": 0.16797509789466858,
      "learning_rate": 7.22256720830372e-10,
      "loss": 0.0009,
      "step": 147690
    },
    {
      "epoch": 26854.545454545456,
      "grad_norm": 0.0004507362318690866,
      "learning_rate": 7.160184408721571e-10,
      "loss": 0.0013,
      "step": 147700
    },
    {
      "epoch": 26856.363636363636,
      "grad_norm": 0.22274620831012726,
      "learning_rate": 7.098071992423516e-10,
      "loss": 0.001,
      "step": 147710
    },
    {
      "epoch": 26858.18181818182,
      "grad_norm": 0.00040983379585668445,
      "learning_rate": 7.036229962774087e-10,
      "loss": 0.0009,
      "step": 147720
    },
    {
      "epoch": 26860.0,
      "grad_norm": 0.0008975081727840006,
      "learning_rate": 6.974658323121719e-10,
      "loss": 0.0012,
      "step": 147730
    },
    {
      "epoch": 26861.81818181818,
      "grad_norm": 0.0005954460939392447,
      "learning_rate": 6.913357076800408e-10,
      "loss": 0.0012,
      "step": 147740
    },
    {
      "epoch": 26863.636363636364,
      "grad_norm": 0.0005417441134341061,
      "learning_rate": 6.852326227130833e-10,
      "loss": 0.0009,
      "step": 147750
    },
    {
      "epoch": 26865.454545454544,
      "grad_norm": 0.17581528425216675,
      "learning_rate": 6.791565777417018e-10,
      "loss": 0.0012,
      "step": 147760
    },
    {
      "epoch": 26867.272727272728,
      "grad_norm": 0.0007929629064165056,
      "learning_rate": 6.73107573095022e-10,
      "loss": 0.0012,
      "step": 147770
    },
    {
      "epoch": 26869.090909090908,
      "grad_norm": 0.0003644223033916205,
      "learning_rate": 6.67085609100615e-10,
      "loss": 0.0009,
      "step": 147780
    },
    {
      "epoch": 26870.909090909092,
      "grad_norm": 0.17700354754924774,
      "learning_rate": 6.610906860845533e-10,
      "loss": 0.001,
      "step": 147790
    },
    {
      "epoch": 26872.727272727272,
      "grad_norm": 0.0005454567144624889,
      "learning_rate": 6.551228043715218e-10,
      "loss": 0.0013,
      "step": 147800
    },
    {
      "epoch": 26874.545454545456,
      "grad_norm": 0.0009856459219008684,
      "learning_rate": 6.491819642846508e-10,
      "loss": 0.0007,
      "step": 147810
    },
    {
      "epoch": 26876.363636363636,
      "grad_norm": 0.0008618133724667132,
      "learning_rate": 6.432681661457384e-10,
      "loss": 0.0012,
      "step": 147820
    },
    {
      "epoch": 26878.18181818182,
      "grad_norm": 0.003947298973798752,
      "learning_rate": 6.373814102750286e-10,
      "loss": 0.001,
      "step": 147830
    },
    {
      "epoch": 26880.0,
      "grad_norm": 0.0012845461023971438,
      "learning_rate": 6.315216969912662e-10,
      "loss": 0.0012,
      "step": 147840
    },
    {
      "epoch": 26881.81818181818,
      "grad_norm": 0.0003204481618013233,
      "learning_rate": 6.256890266118087e-10,
      "loss": 0.0008,
      "step": 147850
    },
    {
      "epoch": 26883.636363636364,
      "grad_norm": 0.16545425355434418,
      "learning_rate": 6.198833994525144e-10,
      "loss": 0.0015,
      "step": 147860
    },
    {
      "epoch": 26885.454545454544,
      "grad_norm": 0.0003691453312058002,
      "learning_rate": 6.141048158277429e-10,
      "loss": 0.0011,
      "step": 147870
    },
    {
      "epoch": 26887.272727272728,
      "grad_norm": 0.16569006443023682,
      "learning_rate": 6.083532760505216e-10,
      "loss": 0.001,
      "step": 147880
    },
    {
      "epoch": 26889.090909090908,
      "grad_norm": 0.17792421579360962,
      "learning_rate": 6.026287804322683e-10,
      "loss": 0.001,
      "step": 147890
    },
    {
      "epoch": 26890.909090909092,
      "grad_norm": 0.2048482447862625,
      "learning_rate": 5.969313292830125e-10,
      "loss": 0.001,
      "step": 147900
    },
    {
      "epoch": 26892.727272727272,
      "grad_norm": 0.00034078367752954364,
      "learning_rate": 5.912609229112297e-10,
      "loss": 0.001,
      "step": 147910
    },
    {
      "epoch": 26894.545454545456,
      "grad_norm": 0.0007054498419165611,
      "learning_rate": 5.856175616240078e-10,
      "loss": 0.0012,
      "step": 147920
    },
    {
      "epoch": 26896.363636363636,
      "grad_norm": 0.0005805586115457118,
      "learning_rate": 5.800012457270465e-10,
      "loss": 0.0013,
      "step": 147930
    },
    {
      "epoch": 26898.18181818182,
      "grad_norm": 0.1680031567811966,
      "learning_rate": 5.744119755244359e-10,
      "loss": 0.0008,
      "step": 147940
    },
    {
      "epoch": 26900.0,
      "grad_norm": 0.0007042334182187915,
      "learning_rate": 5.688497513188228e-10,
      "loss": 0.001,
      "step": 147950
    },
    {
      "epoch": 26901.81818181818,
      "grad_norm": 0.0016372897662222385,
      "learning_rate": 5.633145734114663e-10,
      "loss": 0.001,
      "step": 147960
    },
    {
      "epoch": 26903.636363636364,
      "grad_norm": 0.20138414204120636,
      "learning_rate": 5.578064421021821e-10,
      "loss": 0.001,
      "step": 147970
    },
    {
      "epoch": 26905.454545454544,
      "grad_norm": 0.0004460265627130866,
      "learning_rate": 5.523253576891207e-10,
      "loss": 0.001,
      "step": 147980
    },
    {
      "epoch": 26907.272727272728,
      "grad_norm": 0.0008313278085552156,
      "learning_rate": 5.46871320469211e-10,
      "loss": 0.001,
      "step": 147990
    },
    {
      "epoch": 26909.090909090908,
      "grad_norm": 0.0004569673037622124,
      "learning_rate": 5.41444330737717e-10,
      "loss": 0.0012,
      "step": 148000
    },
    {
      "epoch": 26909.090909090908,
      "eval_loss": 5.1352949142456055,
      "eval_runtime": 0.948,
      "eval_samples_per_second": 10.548,
      "eval_steps_per_second": 5.274,
      "step": 148000
    },
    {
      "epoch": 26910.909090909092,
      "grad_norm": 0.2820793092250824,
      "learning_rate": 5.360443887886812e-10,
      "loss": 0.001,
      "step": 148010
    },
    {
      "epoch": 26912.727272727272,
      "grad_norm": 0.0005445924471132457,
      "learning_rate": 5.306714949143698e-10,
      "loss": 0.001,
      "step": 148020
    },
    {
      "epoch": 26914.545454545456,
      "grad_norm": 0.00047030681162141263,
      "learning_rate": 5.253256494058834e-10,
      "loss": 0.0012,
      "step": 148030
    },
    {
      "epoch": 26916.363636363636,
      "grad_norm": 0.0007145419367589056,
      "learning_rate": 5.200068525526014e-10,
      "loss": 0.0012,
      "step": 148040
    },
    {
      "epoch": 26918.18181818182,
      "grad_norm": 0.0004963848041370511,
      "learning_rate": 5.147151046426823e-10,
      "loss": 0.0009,
      "step": 148050
    },
    {
      "epoch": 26920.0,
      "grad_norm": 0.0007288545020855963,
      "learning_rate": 5.094504059625637e-10,
      "loss": 0.0012,
      "step": 148060
    },
    {
      "epoch": 26921.81818181818,
      "grad_norm": 0.0006402761209756136,
      "learning_rate": 5.042127567974618e-10,
      "loss": 0.0012,
      "step": 148070
    },
    {
      "epoch": 26923.636363636364,
      "grad_norm": 0.16996726393699646,
      "learning_rate": 4.990021574309832e-10,
      "loss": 0.0012,
      "step": 148080
    },
    {
      "epoch": 26925.454545454544,
      "grad_norm": 0.0004970520967617631,
      "learning_rate": 4.93818608145291e-10,
      "loss": 0.0009,
      "step": 148090
    },
    {
      "epoch": 26927.272727272728,
      "grad_norm": 0.0005704594659619033,
      "learning_rate": 4.886621092211052e-10,
      "loss": 0.0009,
      "step": 148100
    },
    {
      "epoch": 26929.090909090908,
      "grad_norm": 0.40821751952171326,
      "learning_rate": 4.835326609376467e-10,
      "loss": 0.0013,
      "step": 148110
    },
    {
      "epoch": 26930.909090909092,
      "grad_norm": 0.0004562119720503688,
      "learning_rate": 4.784302635727489e-10,
      "loss": 0.001,
      "step": 148120
    },
    {
      "epoch": 26932.727272727272,
      "grad_norm": 0.005743055604398251,
      "learning_rate": 4.733549174026352e-10,
      "loss": 0.0009,
      "step": 148130
    },
    {
      "epoch": 26934.545454545456,
      "grad_norm": 0.26760923862457275,
      "learning_rate": 4.68306622702308e-10,
      "loss": 0.0012,
      "step": 148140
    },
    {
      "epoch": 26936.363636363636,
      "grad_norm": 0.17487090826034546,
      "learning_rate": 4.6328537974499315e-10,
      "loss": 0.0011,
      "step": 148150
    },
    {
      "epoch": 26938.18181818182,
      "grad_norm": 0.0008627640781924129,
      "learning_rate": 4.5829118880275073e-10,
      "loss": 0.001,
      "step": 148160
    },
    {
      "epoch": 26940.0,
      "grad_norm": 0.20880548655986786,
      "learning_rate": 4.5332405014592013e-10,
      "loss": 0.0012,
      "step": 148170
    },
    {
      "epoch": 26941.81818181818,
      "grad_norm": 0.0017643390456214547,
      "learning_rate": 4.483839640435638e-10,
      "loss": 0.001,
      "step": 148180
    },
    {
      "epoch": 26943.636363636364,
      "grad_norm": 0.23138275742530823,
      "learning_rate": 4.434709307631901e-10,
      "loss": 0.0012,
      "step": 148190
    },
    {
      "epoch": 26945.454545454544,
      "grad_norm": 0.18950515985488892,
      "learning_rate": 4.3858495057080836e-10,
      "loss": 0.0012,
      "step": 148200
    },
    {
      "epoch": 26947.272727272728,
      "grad_norm": 0.0002861264511011541,
      "learning_rate": 4.337260237311513e-10,
      "loss": 0.0007,
      "step": 148210
    },
    {
      "epoch": 26949.090909090908,
      "grad_norm": 0.0009786166483536363,
      "learning_rate": 4.288941505071753e-10,
      "loss": 0.0012,
      "step": 148220
    },
    {
      "epoch": 26950.909090909092,
      "grad_norm": 0.2160528004169464,
      "learning_rate": 4.240893311607263e-10,
      "loss": 0.0012,
      "step": 148230
    },
    {
      "epoch": 26952.727272727272,
      "grad_norm": 0.20952636003494263,
      "learning_rate": 4.1931156595187423e-10,
      "loss": 0.0011,
      "step": 148240
    },
    {
      "epoch": 26954.545454545456,
      "grad_norm": 0.0004245402233209461,
      "learning_rate": 4.1456085513935646e-10,
      "loss": 0.001,
      "step": 148250
    },
    {
      "epoch": 26956.363636363636,
      "grad_norm": 0.1700218766927719,
      "learning_rate": 4.0983719898052274e-10,
      "loss": 0.001,
      "step": 148260
    },
    {
      "epoch": 26958.18181818182,
      "grad_norm": 0.16599301993846893,
      "learning_rate": 4.0514059773111285e-10,
      "loss": 0.0012,
      "step": 148270
    },
    {
      "epoch": 26960.0,
      "grad_norm": 0.006159674376249313,
      "learning_rate": 4.004710516455345e-10,
      "loss": 0.001,
      "step": 148280
    },
    {
      "epoch": 26961.81818181818,
      "grad_norm": 0.0008799180504865944,
      "learning_rate": 3.9582856097658547e-10,
      "loss": 0.0012,
      "step": 148290
    },
    {
      "epoch": 26963.636363636364,
      "grad_norm": 0.00046069841482676566,
      "learning_rate": 3.912131259757312e-10,
      "loss": 0.0012,
      "step": 148300
    },
    {
      "epoch": 26965.454545454544,
      "grad_norm": 0.000596693716943264,
      "learning_rate": 3.8662474689288294e-10,
      "loss": 0.001,
      "step": 148310
    },
    {
      "epoch": 26967.272727272728,
      "grad_norm": 0.0005852311733178794,
      "learning_rate": 3.820634239765641e-10,
      "loss": 0.0009,
      "step": 148320
    },
    {
      "epoch": 26969.090909090908,
      "grad_norm": 0.019097303971648216,
      "learning_rate": 3.775291574737438e-10,
      "loss": 0.0013,
      "step": 148330
    },
    {
      "epoch": 26970.909090909092,
      "grad_norm": 0.18490387499332428,
      "learning_rate": 3.730219476300034e-10,
      "loss": 0.001,
      "step": 148340
    },
    {
      "epoch": 26972.727272727272,
      "grad_norm": 0.22447043657302856,
      "learning_rate": 3.6854179468942536e-10,
      "loss": 0.0011,
      "step": 148350
    },
    {
      "epoch": 26974.545454545456,
      "grad_norm": 0.1701686680316925,
      "learning_rate": 3.640886988945935e-10,
      "loss": 0.0011,
      "step": 148360
    },
    {
      "epoch": 26976.363636363636,
      "grad_norm": 0.00034380180295556784,
      "learning_rate": 3.596626604866482e-10,
      "loss": 0.001,
      "step": 148370
    },
    {
      "epoch": 26978.18181818182,
      "grad_norm": 0.0010216484079137444,
      "learning_rate": 3.5526367970539764e-10,
      "loss": 0.0011,
      "step": 148380
    },
    {
      "epoch": 26980.0,
      "grad_norm": 0.2192091941833496,
      "learning_rate": 3.508917567889291e-10,
      "loss": 0.0012,
      "step": 148390
    },
    {
      "epoch": 26981.81818181818,
      "grad_norm": 0.0005529285408556461,
      "learning_rate": 3.465468919740533e-10,
      "loss": 0.0012,
      "step": 148400
    },
    {
      "epoch": 26983.636363636364,
      "grad_norm": 0.18532615900039673,
      "learning_rate": 3.4222908549608185e-10,
      "loss": 0.0011,
      "step": 148410
    },
    {
      "epoch": 26985.454545454544,
      "grad_norm": 0.0005036133225075901,
      "learning_rate": 3.3793833758882784e-10,
      "loss": 0.0008,
      "step": 148420
    },
    {
      "epoch": 26987.272727272728,
      "grad_norm": 0.21202290058135986,
      "learning_rate": 3.336746484846609e-10,
      "loss": 0.0015,
      "step": 148430
    },
    {
      "epoch": 26989.090909090908,
      "grad_norm": 0.011013680137693882,
      "learning_rate": 3.294380184143963e-10,
      "loss": 0.0012,
      "step": 148440
    },
    {
      "epoch": 26990.909090909092,
      "grad_norm": 0.1718970388174057,
      "learning_rate": 3.2522844760762833e-10,
      "loss": 0.001,
      "step": 148450
    },
    {
      "epoch": 26992.727272727272,
      "grad_norm": 0.17279380559921265,
      "learning_rate": 3.210459362921747e-10,
      "loss": 0.001,
      "step": 148460
    },
    {
      "epoch": 26994.545454545456,
      "grad_norm": 0.17285838723182678,
      "learning_rate": 3.1689048469457635e-10,
      "loss": 0.0012,
      "step": 148470
    },
    {
      "epoch": 26996.363636363636,
      "grad_norm": 0.0005006905412301421,
      "learning_rate": 3.1276209303993106e-10,
      "loss": 0.0013,
      "step": 148480
    },
    {
      "epoch": 26998.18181818182,
      "grad_norm": 0.271588534116745,
      "learning_rate": 3.086607615517267e-10,
      "loss": 0.001,
      "step": 148490
    },
    {
      "epoch": 27000.0,
      "grad_norm": 0.17410355806350708,
      "learning_rate": 3.0458649045211894e-10,
      "loss": 0.0009,
      "step": 148500
    },
    {
      "epoch": 27000.0,
      "eval_loss": 5.146825313568115,
      "eval_runtime": 0.9465,
      "eval_samples_per_second": 10.565,
      "eval_steps_per_second": 5.283,
      "step": 148500
    },
    {
      "epoch": 27001.81818181818,
      "grad_norm": 0.0047311666421592236,
      "learning_rate": 3.0053927996176453e-10,
      "loss": 0.0012,
      "step": 148510
    },
    {
      "epoch": 27003.636363636364,
      "grad_norm": 0.16518832743167877,
      "learning_rate": 2.9651913029976604e-10,
      "loss": 0.0009,
      "step": 148520
    },
    {
      "epoch": 27005.454545454544,
      "grad_norm": 0.0007121649687178433,
      "learning_rate": 2.9252604168383823e-10,
      "loss": 0.001,
      "step": 148530
    },
    {
      "epoch": 27007.272727272728,
      "grad_norm": 0.2218393087387085,
      "learning_rate": 2.8856001433030796e-10,
      "loss": 0.0014,
      "step": 148540
    },
    {
      "epoch": 27009.090909090908,
      "grad_norm": 0.000358235789462924,
      "learning_rate": 2.8462104845383697e-10,
      "loss": 0.001,
      "step": 148550
    },
    {
      "epoch": 27010.909090909092,
      "grad_norm": 0.14046163856983185,
      "learning_rate": 2.807091442678655e-10,
      "loss": 0.0012,
      "step": 148560
    },
    {
      "epoch": 27012.727272727272,
      "grad_norm": 0.0012594573199748993,
      "learning_rate": 2.768243019841132e-10,
      "loss": 0.0011,
      "step": 148570
    },
    {
      "epoch": 27014.545454545456,
      "grad_norm": 0.17536379396915436,
      "learning_rate": 2.729665218130783e-10,
      "loss": 0.0009,
      "step": 148580
    },
    {
      "epoch": 27016.363636363636,
      "grad_norm": 0.0010420571779832244,
      "learning_rate": 2.691358039635938e-10,
      "loss": 0.001,
      "step": 148590
    },
    {
      "epoch": 27018.18181818182,
      "grad_norm": 0.012402743101119995,
      "learning_rate": 2.6533214864310483e-10,
      "loss": 0.0015,
      "step": 148600
    },
    {
      "epoch": 27020.0,
      "grad_norm": 0.22433659434318542,
      "learning_rate": 2.6155555605761325e-10,
      "loss": 0.0012,
      "step": 148610
    },
    {
      "epoch": 27021.81818181818,
      "grad_norm": 0.2636561691761017,
      "learning_rate": 2.578060264116777e-10,
      "loss": 0.001,
      "step": 148620
    },
    {
      "epoch": 27023.636363636364,
      "grad_norm": 0.0005672852275893092,
      "learning_rate": 2.5408355990835796e-10,
      "loss": 0.0011,
      "step": 148630
    },
    {
      "epoch": 27025.454545454544,
      "grad_norm": 0.0006501594325527549,
      "learning_rate": 2.503881567491595e-10,
      "loss": 0.0013,
      "step": 148640
    },
    {
      "epoch": 27027.272727272728,
      "grad_norm": 0.2778090238571167,
      "learning_rate": 2.4671981713419997e-10,
      "loss": 0.0013,
      "step": 148650
    },
    {
      "epoch": 27029.090909090908,
      "grad_norm": 0.2762430012226105,
      "learning_rate": 2.430785412622649e-10,
      "loss": 0.001,
      "step": 148660
    },
    {
      "epoch": 27030.909090909092,
      "grad_norm": 0.21602965891361237,
      "learning_rate": 2.394643293304188e-10,
      "loss": 0.001,
      "step": 148670
    },
    {
      "epoch": 27032.727272727272,
      "grad_norm": 0.17420393228530884,
      "learning_rate": 2.3587718153444957e-10,
      "loss": 0.001,
      "step": 148680
    },
    {
      "epoch": 27034.545454545456,
      "grad_norm": 0.21118587255477905,
      "learning_rate": 2.3231709806859067e-10,
      "loss": 0.001,
      "step": 148690
    },
    {
      "epoch": 27036.363636363636,
      "grad_norm": 0.18503054976463318,
      "learning_rate": 2.2878407912563237e-10,
      "loss": 0.001,
      "step": 148700
    },
    {
      "epoch": 27038.18181818182,
      "grad_norm": 0.17169462144374847,
      "learning_rate": 2.2527812489692155e-10,
      "loss": 0.0012,
      "step": 148710
    },
    {
      "epoch": 27040.0,
      "grad_norm": 0.0016000241739675403,
      "learning_rate": 2.217992355723064e-10,
      "loss": 0.001,
      "step": 148720
    },
    {
      "epoch": 27041.81818181818,
      "grad_norm": 0.0008681024191901088,
      "learning_rate": 2.1834741134019174e-10,
      "loss": 0.0012,
      "step": 148730
    },
    {
      "epoch": 27043.636363636364,
      "grad_norm": 0.0012850398197770119,
      "learning_rate": 2.1492265238748363e-10,
      "loss": 0.0009,
      "step": 148740
    },
    {
      "epoch": 27045.454545454544,
      "grad_norm": 0.1847744882106781,
      "learning_rate": 2.1152495889970035e-10,
      "loss": 0.0012,
      "step": 148750
    },
    {
      "epoch": 27047.272727272728,
      "grad_norm": 0.2218794971704483,
      "learning_rate": 2.0815433106080581e-10,
      "loss": 0.001,
      "step": 148760
    },
    {
      "epoch": 27049.090909090908,
      "grad_norm": 0.17280210554599762,
      "learning_rate": 2.0481076905332072e-10,
      "loss": 0.0012,
      "step": 148770
    },
    {
      "epoch": 27050.909090909092,
      "grad_norm": 0.006291007157415152,
      "learning_rate": 2.0149427305837796e-10,
      "loss": 0.001,
      "step": 148780
    },
    {
      "epoch": 27052.727272727272,
      "grad_norm": 0.0004977047210559249,
      "learning_rate": 1.9820484325544507e-10,
      "loss": 0.0009,
      "step": 148790
    },
    {
      "epoch": 27054.545454545456,
      "grad_norm": 0.0003911096719093621,
      "learning_rate": 1.9494247982282387e-10,
      "loss": 0.0011,
      "step": 148800
    },
    {
      "epoch": 27056.363636363636,
      "grad_norm": 0.17163391411304474,
      "learning_rate": 1.917071829370953e-10,
      "loss": 0.0013,
      "step": 148810
    },
    {
      "epoch": 27058.18181818182,
      "grad_norm": 0.17241379618644714,
      "learning_rate": 1.884989527733971e-10,
      "loss": 0.0011,
      "step": 148820
    },
    {
      "epoch": 27060.0,
      "grad_norm": 0.0005078815156593919,
      "learning_rate": 1.8531778950564568e-10,
      "loss": 0.001,
      "step": 148830
    },
    {
      "epoch": 27061.81818181818,
      "grad_norm": 0.22050724923610687,
      "learning_rate": 1.8216369330598114e-10,
      "loss": 0.001,
      "step": 148840
    },
    {
      "epoch": 27063.636363636364,
      "grad_norm": 0.284046471118927,
      "learning_rate": 1.7903666434521126e-10,
      "loss": 0.0012,
      "step": 148850
    },
    {
      "epoch": 27065.454545454544,
      "grad_norm": 0.2207706868648529,
      "learning_rate": 1.7593670279275608e-10,
      "loss": 0.0009,
      "step": 148860
    },
    {
      "epoch": 27067.272727272728,
      "grad_norm": 0.001126679009757936,
      "learning_rate": 1.728638088164258e-10,
      "loss": 0.0014,
      "step": 148870
    },
    {
      "epoch": 27069.090909090908,
      "grad_norm": 0.1722366213798523,
      "learning_rate": 1.6981798258264291e-10,
      "loss": 0.0009,
      "step": 148880
    },
    {
      "epoch": 27070.909090909092,
      "grad_norm": 0.0006603339570574462,
      "learning_rate": 1.667992242563865e-10,
      "loss": 0.0011,
      "step": 148890
    },
    {
      "epoch": 27072.727272727272,
      "grad_norm": 0.003956238739192486,
      "learning_rate": 1.6380753400108137e-10,
      "loss": 0.001,
      "step": 148900
    },
    {
      "epoch": 27074.545454545456,
      "grad_norm": 0.22058145701885223,
      "learning_rate": 1.6084291197876466e-10,
      "loss": 0.0013,
      "step": 148910
    },
    {
      "epoch": 27076.363636363636,
      "grad_norm": 0.22487911581993103,
      "learning_rate": 1.5790535835003005e-10,
      "loss": 0.0009,
      "step": 148920
    },
    {
      "epoch": 27078.18181818182,
      "grad_norm": 0.0007935254252515733,
      "learning_rate": 1.5499487327386152e-10,
      "loss": 0.001,
      "step": 148930
    },
    {
      "epoch": 27080.0,
      "grad_norm": 0.0007139960071071982,
      "learning_rate": 1.5211145690796622e-10,
      "loss": 0.0011,
      "step": 148940
    },
    {
      "epoch": 27081.81818181818,
      "grad_norm": 0.17440757155418396,
      "learning_rate": 1.4925510940844154e-10,
      "loss": 0.0012,
      "step": 148950
    },
    {
      "epoch": 27083.636363636364,
      "grad_norm": 0.005918129812926054,
      "learning_rate": 1.4642583092999706e-10,
      "loss": 0.001,
      "step": 148960
    },
    {
      "epoch": 27085.454545454544,
      "grad_norm": 0.20294594764709473,
      "learning_rate": 1.4362362162578802e-10,
      "loss": 0.0009,
      "step": 148970
    },
    {
      "epoch": 27087.272727272728,
      "grad_norm": 0.0004913088632747531,
      "learning_rate": 1.408484816476374e-10,
      "loss": 0.0011,
      "step": 148980
    },
    {
      "epoch": 27089.090909090908,
      "grad_norm": 0.22299252450466156,
      "learning_rate": 1.3810041114581396e-10,
      "loss": 0.0012,
      "step": 148990
    },
    {
      "epoch": 27090.909090909092,
      "grad_norm": 0.0012115566059947014,
      "learning_rate": 1.3537941026914302e-10,
      "loss": 0.0011,
      "step": 149000
    },
    {
      "epoch": 27090.909090909092,
      "eval_loss": 5.187465190887451,
      "eval_runtime": 0.9516,
      "eval_samples_per_second": 10.509,
      "eval_steps_per_second": 5.255,
      "step": 149000
    },
    {
      "epoch": 27092.727272727272,
      "grad_norm": 0.0003402278816793114,
      "learning_rate": 1.326854791649512e-10,
      "loss": 0.001,
      "step": 149010
    },
    {
      "epoch": 27094.545454545456,
      "grad_norm": 0.0007614191854372621,
      "learning_rate": 1.3001861797917735e-10,
      "loss": 0.001,
      "step": 149020
    },
    {
      "epoch": 27096.363636363636,
      "grad_norm": 0.0009615141898393631,
      "learning_rate": 1.2737882685615043e-10,
      "loss": 0.001,
      "step": 149030
    },
    {
      "epoch": 27098.18181818182,
      "grad_norm": 0.22284191846847534,
      "learning_rate": 1.2476610593892268e-10,
      "loss": 0.0012,
      "step": 149040
    },
    {
      "epoch": 27100.0,
      "grad_norm": 0.2135816067457199,
      "learning_rate": 1.221804553689365e-10,
      "loss": 0.001,
      "step": 149050
    },
    {
      "epoch": 27101.81818181818,
      "grad_norm": 0.21312589943408966,
      "learning_rate": 1.1962187528624657e-10,
      "loss": 0.0013,
      "step": 149060
    },
    {
      "epoch": 27103.636363636364,
      "grad_norm": 0.1699695587158203,
      "learning_rate": 1.1709036582935315e-10,
      "loss": 0.0012,
      "step": 149070
    },
    {
      "epoch": 27105.454545454544,
      "grad_norm": 0.2773001492023468,
      "learning_rate": 1.1458592713536885e-10,
      "loss": 0.001,
      "step": 149080
    },
    {
      "epoch": 27107.272727272728,
      "grad_norm": 0.0009195927996188402,
      "learning_rate": 1.1210855933996289e-10,
      "loss": 0.0009,
      "step": 149090
    },
    {
      "epoch": 27109.090909090908,
      "grad_norm": 0.16779200732707977,
      "learning_rate": 1.0965826257725019e-10,
      "loss": 0.0011,
      "step": 149100
    },
    {
      "epoch": 27110.909090909092,
      "grad_norm": 0.0005635924753732979,
      "learning_rate": 1.0723503697995795e-10,
      "loss": 0.001,
      "step": 149110
    },
    {
      "epoch": 27112.727272727272,
      "grad_norm": 0.0009335241629742086,
      "learning_rate": 1.04838882679259e-10,
      "loss": 0.0013,
      "step": 149120
    },
    {
      "epoch": 27114.545454545456,
      "grad_norm": 0.2361879199743271,
      "learning_rate": 1.0246979980499393e-10,
      "loss": 0.0013,
      "step": 149130
    },
    {
      "epoch": 27116.363636363636,
      "grad_norm": 0.0011598608689382672,
      "learning_rate": 1.001277884853935e-10,
      "loss": 0.0012,
      "step": 149140
    },
    {
      "epoch": 27118.18181818182,
      "grad_norm": 0.21475015580654144,
      "learning_rate": 9.781284884735619e-11,
      "loss": 0.001,
      "step": 149150
    },
    {
      "epoch": 27120.0,
      "grad_norm": 0.22470857203006744,
      "learning_rate": 9.552498101611517e-11,
      "loss": 0.001,
      "step": 149160
    },
    {
      "epoch": 27121.81818181818,
      "grad_norm": 0.1575516313314438,
      "learning_rate": 9.326418511573786e-11,
      "loss": 0.0012,
      "step": 149170
    },
    {
      "epoch": 27123.636363636364,
      "grad_norm": 0.210348442196846,
      "learning_rate": 9.103046126851532e-11,
      "loss": 0.0009,
      "step": 149180
    },
    {
      "epoch": 27125.454545454544,
      "grad_norm": 0.20577135682106018,
      "learning_rate": 8.882380959551739e-11,
      "loss": 0.0013,
      "step": 149190
    },
    {
      "epoch": 27127.272727272728,
      "grad_norm": 0.20514774322509766,
      "learning_rate": 8.664423021614853e-11,
      "loss": 0.001,
      "step": 149200
    },
    {
      "epoch": 27129.090909090908,
      "grad_norm": 0.0004135346389375627,
      "learning_rate": 8.449172324848097e-11,
      "loss": 0.0009,
      "step": 149210
    },
    {
      "epoch": 27130.909090909092,
      "grad_norm": 0.0005128789343871176,
      "learning_rate": 8.236628880914365e-11,
      "loss": 0.0012,
      "step": 149220
    },
    {
      "epoch": 27132.727272727272,
      "grad_norm": 0.0005220097955316305,
      "learning_rate": 8.026792701315566e-11,
      "loss": 0.0012,
      "step": 149230
    },
    {
      "epoch": 27134.545454545456,
      "grad_norm": 0.0004131514870095998,
      "learning_rate": 7.819663797414833e-11,
      "loss": 0.001,
      "step": 149240
    },
    {
      "epoch": 27136.363636363636,
      "grad_norm": 0.0006176350871101022,
      "learning_rate": 7.615242180436521e-11,
      "loss": 0.0009,
      "step": 149250
    },
    {
      "epoch": 27138.18181818182,
      "grad_norm": 0.17453642189502716,
      "learning_rate": 7.413527861449554e-11,
      "loss": 0.0013,
      "step": 149260
    },
    {
      "epoch": 27140.0,
      "grad_norm": 0.0005390606820583344,
      "learning_rate": 7.214520851367423e-11,
      "loss": 0.001,
      "step": 149270
    },
    {
      "epoch": 27141.81818181818,
      "grad_norm": 0.1766429841518402,
      "learning_rate": 7.018221160981497e-11,
      "loss": 0.0012,
      "step": 149280
    },
    {
      "epoch": 27143.636363636364,
      "grad_norm": 0.17595715820789337,
      "learning_rate": 6.824628800911059e-11,
      "loss": 0.001,
      "step": 149290
    },
    {
      "epoch": 27145.454545454544,
      "grad_norm": 0.00045376684283837676,
      "learning_rate": 6.633743781642164e-11,
      "loss": 0.0007,
      "step": 149300
    },
    {
      "epoch": 27147.272727272728,
      "grad_norm": 0.17715699970722198,
      "learning_rate": 6.445566113516543e-11,
      "loss": 0.0013,
      "step": 149310
    },
    {
      "epoch": 27149.090909090908,
      "grad_norm": 0.000756974273826927,
      "learning_rate": 6.26009580672604e-11,
      "loss": 0.001,
      "step": 149320
    },
    {
      "epoch": 27150.909090909092,
      "grad_norm": 0.00041075682383961976,
      "learning_rate": 6.077332871307072e-11,
      "loss": 0.0012,
      "step": 149330
    },
    {
      "epoch": 27152.727272727272,
      "grad_norm": 0.018938938155770302,
      "learning_rate": 5.897277317157279e-11,
      "loss": 0.001,
      "step": 149340
    },
    {
      "epoch": 27154.545454545456,
      "grad_norm": 0.20638816058635712,
      "learning_rate": 5.719929154035519e-11,
      "loss": 0.001,
      "step": 149350
    },
    {
      "epoch": 27156.363636363636,
      "grad_norm": 0.16865745186805725,
      "learning_rate": 5.545288391539671e-11,
      "loss": 0.001,
      "step": 149360
    },
    {
      "epoch": 27158.18181818182,
      "grad_norm": 0.17165380716323853,
      "learning_rate": 5.373355039128835e-11,
      "loss": 0.0011,
      "step": 149370
    },
    {
      "epoch": 27160.0,
      "grad_norm": 0.2811964750289917,
      "learning_rate": 5.204129106117783e-11,
      "loss": 0.001,
      "step": 149380
    },
    {
      "epoch": 27161.81818181818,
      "grad_norm": 0.28094878792762756,
      "learning_rate": 5.037610601665854e-11,
      "loss": 0.0009,
      "step": 149390
    },
    {
      "epoch": 27163.636363636364,
      "grad_norm": 0.0004424990329425782,
      "learning_rate": 4.873799534788059e-11,
      "loss": 0.0013,
      "step": 149400
    },
    {
      "epoch": 27165.454545454544,
      "grad_norm": 0.15412242710590363,
      "learning_rate": 4.7126959143661826e-11,
      "loss": 0.0009,
      "step": 149410
    },
    {
      "epoch": 27167.272727272728,
      "grad_norm": 0.0005644845077767968,
      "learning_rate": 4.5542997491099246e-11,
      "loss": 0.001,
      "step": 149420
    },
    {
      "epoch": 27169.090909090908,
      "grad_norm": 0.0006385044544003904,
      "learning_rate": 4.3986110476124105e-11,
      "loss": 0.0012,
      "step": 149430
    },
    {
      "epoch": 27170.909090909092,
      "grad_norm": 0.0004630573675967753,
      "learning_rate": 4.245629818289132e-11,
      "loss": 0.001,
      "step": 149440
    },
    {
      "epoch": 27172.727272727272,
      "grad_norm": 0.0004350443778093904,
      "learning_rate": 4.0953560694390044e-11,
      "loss": 0.0009,
      "step": 149450
    },
    {
      "epoch": 27174.545454545456,
      "grad_norm": 0.1691908836364746,
      "learning_rate": 3.947789809194413e-11,
      "loss": 0.0013,
      "step": 149460
    },
    {
      "epoch": 27176.363636363636,
      "grad_norm": 0.1692841649055481,
      "learning_rate": 3.8029310455434116e-11,
      "loss": 0.001,
      "step": 149470
    },
    {
      "epoch": 27178.18181818182,
      "grad_norm": 0.2729579210281372,
      "learning_rate": 3.660779786335277e-11,
      "loss": 0.0015,
      "step": 149480
    },
    {
      "epoch": 27180.0,
      "grad_norm": 0.0005055788205936551,
      "learning_rate": 3.521336039263856e-11,
      "loss": 0.0009,
      "step": 149490
    },
    {
      "epoch": 27181.81818181818,
      "grad_norm": 0.2783510982990265,
      "learning_rate": 3.3845998118897657e-11,
      "loss": 0.0012,
      "step": 149500
    },
    {
      "epoch": 27181.81818181818,
      "eval_loss": 5.174019813537598,
      "eval_runtime": 0.952,
      "eval_samples_per_second": 10.505,
      "eval_steps_per_second": 5.252,
      "step": 149500
    },
    {
      "epoch": 27183.636363636364,
      "grad_norm": 0.17062349617481232,
      "learning_rate": 3.250571111607092e-11,
      "loss": 0.001,
      "step": 149510
    },
    {
      "epoch": 27185.454545454544,
      "grad_norm": 0.22170788049697876,
      "learning_rate": 3.119249945676694e-11,
      "loss": 0.001,
      "step": 149520
    },
    {
      "epoch": 27187.272727272728,
      "grad_norm": 0.22173051536083221,
      "learning_rate": 2.99063632120955e-11,
      "loss": 0.0012,
      "step": 149530
    },
    {
      "epoch": 27189.090909090908,
      "grad_norm": 0.28001415729522705,
      "learning_rate": 2.8647302451778598e-11,
      "loss": 0.0011,
      "step": 149540
    },
    {
      "epoch": 27190.909090909092,
      "grad_norm": 0.21270589530467987,
      "learning_rate": 2.7415317243928427e-11,
      "loss": 0.0009,
      "step": 149550
    },
    {
      "epoch": 27192.727272727272,
      "grad_norm": 0.00032700892188586295,
      "learning_rate": 2.6210407655269383e-11,
      "loss": 0.001,
      "step": 149560
    },
    {
      "epoch": 27194.545454545456,
      "grad_norm": 0.0007591127068735659,
      "learning_rate": 2.5032573751082586e-11,
      "loss": 0.001,
      "step": 149570
    },
    {
      "epoch": 27196.363636363636,
      "grad_norm": 0.2804561257362366,
      "learning_rate": 2.3881815595150344e-11,
      "loss": 0.0012,
      "step": 149580
    },
    {
      "epoch": 27198.18181818182,
      "grad_norm": 0.00047519663348793983,
      "learning_rate": 2.2758133249756173e-11,
      "loss": 0.0009,
      "step": 149590
    },
    {
      "epoch": 27200.0,
      "grad_norm": 0.0005239691818132997,
      "learning_rate": 2.1661526775795802e-11,
      "loss": 0.0012,
      "step": 149600
    },
    {
      "epoch": 27201.81818181818,
      "grad_norm": 0.16675637662410736,
      "learning_rate": 2.0591996232610654e-11,
      "loss": 0.0012,
      "step": 149610
    },
    {
      "epoch": 27203.636363636364,
      "grad_norm": 0.21271316707134247,
      "learning_rate": 1.954954167809886e-11,
      "loss": 0.0009,
      "step": 149620
    },
    {
      "epoch": 27205.454545454544,
      "grad_norm": 0.0004731411172542721,
      "learning_rate": 1.853416316882628e-11,
      "loss": 0.0011,
      "step": 149630
    },
    {
      "epoch": 27207.272727272728,
      "grad_norm": 0.0003440376603975892,
      "learning_rate": 1.7545860759693442e-11,
      "loss": 0.0015,
      "step": 149640
    },
    {
      "epoch": 27209.090909090908,
      "grad_norm": 0.0007741939043626189,
      "learning_rate": 1.65846345042131e-11,
      "loss": 0.0007,
      "step": 149650
    },
    {
      "epoch": 27210.909090909092,
      "grad_norm": 0.16673041880130768,
      "learning_rate": 1.5650484454454716e-11,
      "loss": 0.0012,
      "step": 149660
    },
    {
      "epoch": 27212.727272727272,
      "grad_norm": 0.21272189915180206,
      "learning_rate": 1.4743410661044453e-11,
      "loss": 0.0015,
      "step": 149670
    },
    {
      "epoch": 27214.545454545456,
      "grad_norm": 0.16666725277900696,
      "learning_rate": 1.3863413173054172e-11,
      "loss": 0.001,
      "step": 149680
    },
    {
      "epoch": 27216.363636363636,
      "grad_norm": 0.0005569207132793963,
      "learning_rate": 1.3010492038223464e-11,
      "loss": 0.001,
      "step": 149690
    },
    {
      "epoch": 27218.18181818182,
      "grad_norm": 0.0006130416295491159,
      "learning_rate": 1.2184647302626582e-11,
      "loss": 0.0009,
      "step": 149700
    },
    {
      "epoch": 27220.0,
      "grad_norm": 0.2127050906419754,
      "learning_rate": 1.1385879011005517e-11,
      "loss": 0.0012,
      "step": 149710
    },
    {
      "epoch": 27221.81818181818,
      "grad_norm": 0.0005749710835516453,
      "learning_rate": 1.0614187206714475e-11,
      "loss": 0.001,
      "step": 149720
    },
    {
      "epoch": 27223.636363636364,
      "grad_norm": 0.00037506193621084094,
      "learning_rate": 9.869571931442333e-12,
      "loss": 0.0013,
      "step": 149730
    },
    {
      "epoch": 27225.454545454544,
      "grad_norm": 0.22173801064491272,
      "learning_rate": 9.152033225545697e-12,
      "loss": 0.0013,
      "step": 149740
    },
    {
      "epoch": 27227.272727272728,
      "grad_norm": 0.2712164521217346,
      "learning_rate": 8.461571127882373e-12,
      "loss": 0.0012,
      "step": 149750
    },
    {
      "epoch": 27229.090909090908,
      "grad_norm": 0.16666945815086365,
      "learning_rate": 7.798185675866876e-12,
      "loss": 0.001,
      "step": 149760
    },
    {
      "epoch": 27230.909090909092,
      "grad_norm": 0.22172172367572784,
      "learning_rate": 7.161876905414921e-12,
      "loss": 0.001,
      "step": 149770
    },
    {
      "epoch": 27232.727272727272,
      "grad_norm": 0.0006333448109216988,
      "learning_rate": 6.552644850998934e-12,
      "loss": 0.0012,
      "step": 149780
    },
    {
      "epoch": 27234.545454545456,
      "grad_norm": 0.00047253561206161976,
      "learning_rate": 5.9704895455370275e-12,
      "loss": 0.0012,
      "step": 149790
    },
    {
      "epoch": 27236.363636363636,
      "grad_norm": 0.16667363047599792,
      "learning_rate": 5.415411020615046e-12,
      "loss": 0.001,
      "step": 149800
    },
    {
      "epoch": 27238.18181818182,
      "grad_norm": 0.16732805967330933,
      "learning_rate": 4.887409306320034e-12,
      "loss": 0.0012,
      "step": 149810
    },
    {
      "epoch": 27240.0,
      "grad_norm": 0.0009317385847680271,
      "learning_rate": 4.386484431184723e-12,
      "loss": 0.0009,
      "step": 149820
    },
    {
      "epoch": 27241.81818181818,
      "grad_norm": 0.00037984884693287313,
      "learning_rate": 3.912636422354065e-12,
      "loss": 0.001,
      "step": 149830
    },
    {
      "epoch": 27243.636363636364,
      "grad_norm": 0.00041159719694405794,
      "learning_rate": 3.4658653054742136e-12,
      "loss": 0.001,
      "step": 149840
    },
    {
      "epoch": 27245.454545454544,
      "grad_norm": 0.21271473169326782,
      "learning_rate": 3.046171104803541e-12,
      "loss": 0.0013,
      "step": 149850
    },
    {
      "epoch": 27247.272727272728,
      "grad_norm": 0.2127157747745514,
      "learning_rate": 2.653553843046108e-12,
      "loss": 0.0009,
      "step": 149860
    },
    {
      "epoch": 27249.090909090908,
      "grad_norm": 0.0006663483800366521,
      "learning_rate": 2.2880135414071744e-12,
      "loss": 0.001,
      "step": 149870
    },
    {
      "epoch": 27250.909090909092,
      "grad_norm": 0.0008245722856372595,
      "learning_rate": 1.9495502197042213e-12,
      "loss": 0.0012,
      "step": 149880
    },
    {
      "epoch": 27252.727272727272,
      "grad_norm": 0.27819710969924927,
      "learning_rate": 1.6381638963114397e-12,
      "loss": 0.0012,
      "step": 149890
    },
    {
      "epoch": 27254.545454545456,
      "grad_norm": 0.00039933351217769086,
      "learning_rate": 1.3538545881042196e-12,
      "loss": 0.0006,
      "step": 149900
    },
    {
      "epoch": 27256.363636363636,
      "grad_norm": 0.21272926032543182,
      "learning_rate": 1.0966223103481276e-12,
      "loss": 0.0016,
      "step": 149910
    },
    {
      "epoch": 27258.18181818182,
      "grad_norm": 0.21271343529224396,
      "learning_rate": 8.66467077142996e-13,
      "loss": 0.0009,
      "step": 149920
    },
    {
      "epoch": 27260.0,
      "grad_norm": 0.0003815178934019059,
      "learning_rate": 6.633889008678118e-13,
      "loss": 0.001,
      "step": 149930
    },
    {
      "epoch": 27261.81818181818,
      "grad_norm": 0.2766137421131134,
      "learning_rate": 4.873877924582715e-13,
      "loss": 0.001,
      "step": 149940
    },
    {
      "epoch": 27263.636363636364,
      "grad_norm": 0.0004524163668975234,
      "learning_rate": 3.384637615733155e-13,
      "loss": 0.001,
      "step": 149950
    },
    {
      "epoch": 27265.454545454544,
      "grad_norm": 0.16666419804096222,
      "learning_rate": 2.166168162065496e-13,
      "loss": 0.001,
      "step": 149960
    },
    {
      "epoch": 27267.272727272728,
      "grad_norm": 0.0004757485876325518,
      "learning_rate": 1.218469629638008e-13,
      "loss": 0.001,
      "step": 149970
    },
    {
      "epoch": 27269.090909090908,
      "grad_norm": 0.22173550724983215,
      "learning_rate": 5.41542070076062e-14,
      "loss": 0.0012,
      "step": 149980
    },
    {
      "epoch": 27270.909090909092,
      "grad_norm": 0.0005436548381112516,
      "learning_rate": 1.353855194619058e-14,
      "loss": 0.001,
      "step": 149990
    },
    {
      "epoch": 27272.727272727272,
      "grad_norm": 0.0006883703172206879,
      "learning_rate": 0.0,
      "loss": 0.0012,
      "step": 150000
    },
    {
      "epoch": 27272.727272727272,
      "eval_loss": 5.1643829345703125,
      "eval_runtime": 0.9508,
      "eval_samples_per_second": 10.518,
      "eval_steps_per_second": 5.259,
      "step": 150000
    },
    {
      "epoch": 27272.727272727272,
      "step": 150000,
      "total_flos": 1.803036172348293e+18,
      "train_loss": 0.09175598427843923,
      "train_runtime": 759080.9065,
      "train_samples_per_second": 3.438,
      "train_steps_per_second": 0.198
    }
  ],
  "logging_steps": 10,
  "max_steps": 150000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 30000,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.803036172348293e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
